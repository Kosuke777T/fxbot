############################################################
# project_snapshot
############################################################
generated_at: 2025-11-22 17:27:44
project_root: D:\fxbot

NOTE:
  - ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ ChatGPT ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã¨ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¼ãˆã‚‹ãŸã‚ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§ã™ã€‚
  - ãƒ­ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ».gitãƒ».venv ãªã©ã¯é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
  - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ„ãƒªãƒ¼ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ (updated: ...) ã‚’å«ã¿ã¾ã™ã€‚

========================================
=== DIRECTORY TREE =====================
========================================

fxbot/
.env.example (updated: 2025-11-20 17:32:02)
.gitignore (updated: 2025-11-20 18:15:42)
.importlinter.ini (updated: 2025-11-20 17:32:02)
7C9B198C-ADA6-4308-A321-C13EA50E0707.jpg (updated: 2025-11-21 12:09:25)
active_model.json (updated: 2025-11-20 17:32:02)
Agent HQ ç”¨ã€Œç´°åˆ†åŒ–ç‰ˆã€.txt (updated: 2025-11-20 17:32:02)
app/
  __init__.py (updated: 2025-11-20 17:32:02)
  core/
    config_loader.py (updated: 2025-11-20 17:32:02)
    data_finder.py (updated: 2025-11-20 17:32:02)
    logger.py (updated: 2025-11-20 17:32:02)
    market.py (updated: 2025-11-20 17:32:02)
    mt5_client.py (updated: 2025-11-22 17:10:36)
    strategy_profile.py (updated: 2025-11-22 16:53:04)
  gui/
    __init__.py (updated: 2025-11-20 17:32:02)
    ai_tab.py (updated: 2025-11-20 17:32:02)
    backtest_tab.py (updated: 2025-11-20 17:32:02)
    control_tab.py (updated: 2025-11-20 17:32:02)
    dashboard_tab.py (updated: 2025-11-20 17:32:02)
    dashboard_tab_qt.py (updated: 2025-11-20 17:32:02)
    history_tab.py (updated: 2025-11-20 17:32:02)
    main.py (updated: 2025-11-20 17:32:02)
    settings_tab.py (updated: 2025-11-20 17:32:02)
    widgets/
      feature_importance.py (updated: 2025-11-20 17:32:02)
      shap_bar.py (updated: 2025-11-20 17:32:02)
  main_tk.py (updated: 2025-11-20 17:32:02)
  services/
    __init__.py (updated: 2025-11-20 17:32:02)
    ai_service.py (updated: 2025-11-20 17:32:02)
    aisvc_loader.py (updated: 2025-11-20 17:32:02)
    circuit_breaker.py (updated: 2025-11-20 17:32:02)
    data_guard.py (updated: 2025-11-20 17:32:02)
    decision_log.py (updated: 2025-11-20 17:32:02)
    event_store.py (updated: 2025-11-20 17:32:02)
    execution_stub.py (updated: 2025-11-20 17:32:02)
    feature_importance.py (updated: 2025-11-20 17:32:02)
    metrics.py (updated: 2025-11-20 17:32:02)
    mt5_account_store.py (updated: 2025-11-20 17:32:02)
    mt5_selftest.py (updated: 2025-11-20 17:32:02)
    mt5_service.py (updated: 2025-11-20 17:32:02)
    orderbook_stub.py (updated: 2025-11-20 17:32:02)
    recent_kpi.py (updated: 2025-11-20 17:32:02)
    shap_service.py (updated: 2025-11-20 17:32:02)
    trade_service.py (updated: 2025-11-22 17:18:00)
    trade_state.py (updated: 2025-11-20 17:32:02)
    trailing.py (updated: 2025-11-20 17:32:02)
    trailing_hook.py (updated: 2025-11-20 17:32:02)
  strategies/
    __init__.py (updated: 2025-11-20 17:32:02)
    ai_strategy.py (updated: 2025-11-20 17:32:02)
apply_order.py (updated: 2025-11-20 19:07:27)
backtests/
  michibiki_std/
config/
  mt5_accounts.json (updated: 2025-11-20 17:32:02)
configs/
  .env.example (updated: 2025-11-20 17:32:02)
  config copy.yaml (updated: 2025-11-20 17:32:02)
  config.local.yaml (updated: 2025-11-20 17:32:02)
  config.yaml (updated: 2025-11-20 17:32:02)
core/
  __init__.py (updated: 2025-11-20 17:32:02)
  ai/
    __init__.py (updated: 2025-11-20 17:32:02)
    calibration.py (updated: 2025-11-20 17:32:02)
    features.py (updated: 2025-11-20 17:32:02)
    loader.py (updated: 2025-11-20 17:32:02)
    service.py (updated: 2025-11-20 17:32:02)
  config.py (updated: 2025-11-20 17:32:02)
  indicators.py (updated: 2025-11-20 17:32:02)
  metrics/
  metrics.py (updated: 2025-11-20 17:32:02)
    __init__.py (updated: 2025-11-20 17:32:02)
    fi_extractor.py (updated: 2025-11-20 17:32:02)
    registry.py (updated: 2025-11-20 17:32:02)
  position_guard.py (updated: 2025-11-20 17:32:02)
  risk.py (updated: 2025-11-22 16:50:23)
  utils/
    __init__.py (updated: 2025-11-20 17:32:02)
    clock.py (updated: 2025-11-20 17:32:02)
    hashing.py (updated: 2025-11-20 17:32:02)
    runtime.py (updated: 2025-11-20 17:32:02)
    timeutil.py (updated: 2025-11-20 17:32:02)
fxbot_path.py (updated: 2025-11-20 17:32:02)
mypy.ini (updated: 2025-11-21 17:38:08)
PROJECT_MAP.md (updated: 2025-11-20 17:32:02)
project_snapshot-DESKTOP-KK77.txt (updated: 2025-11-20 17:32:02)
project_snapshot.txt (updated: 2025-11-20 18:22:24)
project_tree.txt (updated: 2025-11-22 17:20:34)
pyproject.toml (updated: 2025-11-20 17:32:02)
pytest.ini (updated: 2025-11-20 17:32:02)
README.md (updated: 2025-11-20 17:32:02)
requirements.txt (updated: 2025-11-20 17:32:02)
ruff.toml (updated: 2025-11-21 17:38:08)
runtime/
  metrics.json (updated: 2025-11-20 17:32:02)
  metrics_044ho80o.json (updated: 2025-11-20 17:32:02)
  metrics_04i6g_bm.json (updated: 2025-11-20 17:32:02)
  metrics_08lb8guj.json (updated: 2025-11-20 17:32:02)
  metrics_0fdsym9v.json (updated: 2025-11-20 17:32:02)
  metrics_0ks5iv74.json (updated: 2025-11-20 17:32:02)
  metrics_0m3pzxsa.json (updated: 2025-11-20 17:32:02)
  metrics_0s0y1mq_.json (updated: 2025-11-20 17:32:02)
  metrics_0so3t4e4.json (updated: 2025-11-20 17:32:02)
  metrics_0t6las4q.json (updated: 2025-11-20 17:32:02)
  metrics_13d1atob.json (updated: 2025-11-20 17:32:02)
  metrics_1_r7f8xe.json (updated: 2025-11-20 17:32:02)
  metrics_1ce8in0z.json (updated: 2025-11-20 17:32:02)
  metrics_1f8wcifb.json (updated: 2025-11-20 17:32:02)
  metrics_1nqq6c9g.json (updated: 2025-11-20 17:32:02)
  metrics_1y_j88c6.json (updated: 2025-11-20 17:32:02)
  metrics_2b5irl55.json (updated: 2025-11-20 17:32:02)
  metrics_2llsmgld.json (updated: 2025-11-20 17:32:02)
  metrics_2wh_r543.json (updated: 2025-11-20 17:32:02)
  metrics_32ej_3qs.json (updated: 2025-11-20 17:32:02)
  metrics_384xt0qj.json (updated: 2025-11-20 17:32:02)
  metrics_3_z7j5dq.json (updated: 2025-11-20 17:32:02)
  metrics_3op0smhm.json (updated: 2025-11-20 17:32:02)
  metrics_3pck01qi.json (updated: 2025-11-20 17:32:02)
  metrics_3pw7kppo.json (updated: 2025-11-20 17:32:02)
  metrics_3w4e28bl.json (updated: 2025-11-20 17:32:02)
  metrics_3wwdl7sf.json (updated: 2025-11-20 17:32:02)
  metrics_3xjcntun.json (updated: 2025-11-20 17:32:02)
  metrics_4fx6u97k.json (updated: 2025-11-20 17:32:02)
  metrics_4jkcw838.json (updated: 2025-11-20 17:32:02)
  metrics_4jwlaw79.json (updated: 2025-11-20 17:32:02)
  metrics_4sqd0ccq.json (updated: 2025-11-20 17:32:02)
  metrics_4u83ovxn.json (updated: 2025-11-20 17:32:02)
  metrics_51fzibz0.json (updated: 2025-11-20 17:32:02)
  metrics_56tbpp98.json (updated: 2025-11-20 17:32:02)
  metrics_5_uus6ny.json (updated: 2025-11-20 17:32:02)
  metrics_5c1keku_.json (updated: 2025-11-20 17:32:02)
  metrics_681i8dz0.json (updated: 2025-11-20 17:32:02)
  metrics_6g2ygykl.json (updated: 2025-11-20 17:32:02)
  metrics_6jx0f61y.json (updated: 2025-11-20 17:32:02)
  metrics_6mv45wka.json (updated: 2025-11-20 17:32:02)
  metrics_6n99dteb.json (updated: 2025-11-20 17:32:02)
  metrics_6oizo1ba.json (updated: 2025-11-20 17:32:02)
  metrics_6u8fjy4e.json (updated: 2025-11-20 17:32:02)
  metrics_751tqi7x.json (updated: 2025-11-20 17:32:02)
  metrics_78ji6p4u.json (updated: 2025-11-20 17:32:02)
  metrics_7bbmsyvw.json (updated: 2025-11-20 17:32:02)
  metrics_7caerxlw.json (updated: 2025-11-20 17:32:02)
  metrics_7pcppci4.json (updated: 2025-11-20 17:32:02)
  metrics_7upf8_is.json (updated: 2025-11-20 17:32:02)
  metrics_7wtig7zq.json (updated: 2025-11-20 17:32:02)
  metrics_7yx4f9p_.json (updated: 2025-11-20 17:32:02)
  metrics_8i0ku3__.json (updated: 2025-11-20 17:32:02)
  metrics_8iecfllz.json (updated: 2025-11-20 17:32:02)
  metrics_8s427o_h.json (updated: 2025-11-20 17:32:02)
  metrics_8t5wcv0v.json (updated: 2025-11-20 17:32:02)
  metrics_8wbixgeh.json (updated: 2025-11-20 17:32:02)
  metrics_9clf0tik.json (updated: 2025-11-20 17:32:02)
  metrics_9wqc4wjf.json (updated: 2025-11-20 17:32:02)
  metrics_9y2xjt1a.json (updated: 2025-11-20 17:32:02)
  metrics__7n81rbm.json (updated: 2025-11-20 17:32:02)
  metrics__9tk6o8d.json (updated: 2025-11-20 17:32:02)
  metrics___1o6ki2.json (updated: 2025-11-20 17:32:02)
  metrics__cx1yvq5.json (updated: 2025-11-20 17:32:02)
  metrics__g0tzwco.json (updated: 2025-11-20 17:32:02)
  metrics__g6j76yc.json (updated: 2025-11-20 17:32:02)
  metrics__l9umv1h.json (updated: 2025-11-20 17:32:02)
  metrics__nrd_y1k.json (updated: 2025-11-20 17:32:02)
  metrics__rjbfcpd.json (updated: 2025-11-20 17:32:02)
  metrics__s4hfi2b.json (updated: 2025-11-20 17:32:02)
  metrics__tv_40i0.json (updated: 2025-11-20 17:32:02)
  metrics__vtagxy_.json (updated: 2025-11-20 17:32:02)
  metrics_a4qsce4c.json (updated: 2025-11-20 17:32:02)
  metrics_a_jw5ekj.json (updated: 2025-11-20 17:32:02)
  metrics_abcuwh_8.json (updated: 2025-11-20 17:32:02)
  metrics_aen0sw0t.json (updated: 2025-11-20 17:32:02)
  metrics_avlwot5z.json (updated: 2025-11-20 17:32:02)
  metrics_b_e39daa.json (updated: 2025-11-20 17:32:02)
  metrics_beh8fq83.json (updated: 2025-11-20 17:32:02)
  metrics_bkm4j_2_.json (updated: 2025-11-20 17:32:02)
  metrics_bmocgusp.json (updated: 2025-11-20 17:32:02)
  metrics_bsejl6ul.json (updated: 2025-11-20 17:32:02)
  metrics_c19f7jhr.json (updated: 2025-11-20 17:32:02)
  metrics_c6dlu7j3.json (updated: 2025-11-20 17:32:02)
  metrics_c7srgc6l.json (updated: 2025-11-20 17:32:02)
  metrics_cfvm2vuv.json (updated: 2025-11-20 17:32:02)
  metrics_cs6cyrr4.json (updated: 2025-11-20 17:32:02)
  metrics_ctdk6hr_.json (updated: 2025-11-20 17:32:02)
  metrics_d0zio68n.json (updated: 2025-11-20 17:32:02)
  metrics_d2fneb6c.json (updated: 2025-11-20 17:32:02)
  metrics_d83v0qrg.json (updated: 2025-11-20 17:32:02)
  metrics_d_7cu4co.json (updated: 2025-11-20 17:32:02)
  metrics_e55ccqf4.json (updated: 2025-11-20 17:32:02)
  metrics_efewdoso.json (updated: 2025-11-20 17:32:02)
  metrics_es10kbkj.json (updated: 2025-11-20 17:32:02)
  metrics_exyc2aoi.json (updated: 2025-11-20 17:32:02)
  metrics_f4ba8yj0.json (updated: 2025-11-20 17:32:02)
  metrics_fusrtchw.json (updated: 2025-11-20 17:32:02)
  metrics_fvkjx3i3.json (updated: 2025-11-20 17:32:02)
  metrics_fwgrd7ek.json (updated: 2025-11-20 17:32:02)
  metrics_fx59gkw6.json (updated: 2025-11-20 17:32:02)
  metrics_gctnecas.json (updated: 2025-11-20 17:32:02)
  metrics_gdrsfom2.json (updated: 2025-11-20 17:32:02)
  metrics_gh0exqfy.json (updated: 2025-11-20 17:32:02)
  metrics_gn3r368k.json (updated: 2025-11-20 17:32:02)
  metrics_gre0l2vv.json (updated: 2025-11-20 17:32:02)
  metrics_gu2jj4x6.json (updated: 2025-11-20 17:32:02)
  metrics_gy7yzni_.json (updated: 2025-11-20 17:32:02)
  metrics_h2ke1que.json (updated: 2025-11-20 17:32:02)
  metrics_h2zbzsi6.json (updated: 2025-11-20 17:32:02)
  metrics_i5pwbdy8.json (updated: 2025-11-20 17:32:02)
  metrics_i9ixee1p.json (updated: 2025-11-20 17:32:02)
  metrics_iegm05ma.json (updated: 2025-11-20 17:32:02)
  metrics_jeul60z4.json (updated: 2025-11-20 17:32:02)
  metrics_jldkp6oj.json (updated: 2025-11-20 17:32:02)
  metrics_joyqy1yz.json (updated: 2025-11-20 17:32:02)
  metrics_k452r_bd.json (updated: 2025-11-20 17:32:02)
  metrics_keyuq093.json (updated: 2025-11-20 17:32:02)
  metrics_kijozz54.json (updated: 2025-11-20 17:32:02)
  metrics_l0nrt8ax.json (updated: 2025-11-20 17:32:02)
  metrics_l4qkqhvl.json (updated: 2025-11-20 17:32:02)
  metrics_l9a3g7f_.json (updated: 2025-11-20 17:32:02)
  metrics_lg3z_ojj.json (updated: 2025-11-20 17:32:02)
  metrics_lij48idf.json (updated: 2025-11-20 17:32:02)
  metrics_mfx0u92s.json (updated: 2025-11-20 17:32:02)
  metrics_ml129ig1.json (updated: 2025-11-20 17:32:02)
  metrics_mziwmuji.json (updated: 2025-11-20 17:32:02)
  metrics_mzkihnhl.json (updated: 2025-11-20 17:32:02)
  metrics_n92kltdx.json (updated: 2025-11-20 17:32:02)
  metrics_naa4pb8_.json (updated: 2025-11-20 17:32:02)
  metrics_o0bwp99t.json (updated: 2025-11-20 17:32:02)
  metrics_oe0gx0ow.json (updated: 2025-11-20 17:32:02)
  metrics_og6_x57t.json (updated: 2025-11-20 17:32:02)
  metrics_okatezba.json (updated: 2025-11-20 17:32:02)
  metrics_olwp427j.json (updated: 2025-11-20 17:32:02)
  metrics_oy5jty8q.json (updated: 2025-11-20 17:32:02)
  metrics_pbfm5_s8.json (updated: 2025-11-20 17:32:02)
  metrics_pguw1jt8.json (updated: 2025-11-20 17:32:02)
  metrics_q1mfycvq.json (updated: 2025-11-20 17:32:02)
  metrics_q32vexb9.json (updated: 2025-11-20 17:32:02)
  metrics_qd9sdimy.json (updated: 2025-11-20 17:32:02)
  metrics_qmjxkoxo.json (updated: 2025-11-20 17:32:02)
  metrics_qygbv_em.json (updated: 2025-11-20 17:32:02)
  metrics_r1diayz3.json (updated: 2025-11-20 17:32:02)
  metrics_r1oyukcw.json (updated: 2025-11-20 17:32:02)
  metrics_r2iljktq.json (updated: 2025-11-20 17:32:02)
  metrics_r4cxpbus.json (updated: 2025-11-20 17:32:02)
  metrics_ri5kzye8.json (updated: 2025-11-20 17:32:02)
  metrics_s_yeu75a.json (updated: 2025-11-20 17:32:02)
  metrics_smrtxq4q.json (updated: 2025-11-20 17:32:02)
  metrics_sunrm50l.json (updated: 2025-11-20 17:32:02)
  metrics_szni51cl.json (updated: 2025-11-20 17:32:02)
  metrics_szy5o819.json (updated: 2025-11-20 17:32:02)
  metrics_t09rkk69.json (updated: 2025-11-20 17:32:02)
  metrics_tnyuui1p.json (updated: 2025-11-20 17:32:02)
  metrics_tvk5w34y.json (updated: 2025-11-20 17:32:02)
  metrics_u85k0bty.json (updated: 2025-11-20 17:32:02)
  metrics_ue987r_m.json (updated: 2025-11-20 17:32:02)
  metrics_ugsv8j1h.json (updated: 2025-11-20 17:32:02)
  metrics_uhoep47z.json (updated: 2025-11-20 17:32:02)
  metrics_uiqkfigp.json (updated: 2025-11-20 17:32:02)
  metrics_ujl1qfgc.json (updated: 2025-11-20 17:32:02)
  metrics_v30su220.json (updated: 2025-11-20 17:32:02)
  metrics_v372j6bp.json (updated: 2025-11-20 17:32:02)
  metrics_vd8t6g2k.json (updated: 2025-11-20 17:32:02)
  metrics_vi859rsu.json (updated: 2025-11-20 17:32:02)
  metrics_vlx06rwe.json (updated: 2025-11-20 17:32:02)
  metrics_vm46jhw3.json (updated: 2025-11-20 17:32:02)
  metrics_vzimyuwy.json (updated: 2025-11-20 17:32:02)
  metrics_w88v4g45.json (updated: 2025-11-20 17:32:02)
  metrics_wcsasy33.json (updated: 2025-11-20 17:32:02)
  metrics_x7rwrpb_.json (updated: 2025-11-20 17:32:02)
  metrics_xbjh179p.json (updated: 2025-11-20 17:32:02)
  metrics_xcr4m91b.json (updated: 2025-11-20 17:32:02)
  metrics_xcyd97h5.json (updated: 2025-11-20 17:32:02)
  metrics_xe7x_sje.json (updated: 2025-11-20 17:32:02)
  metrics_xjeelk50.json (updated: 2025-11-20 17:32:02)
  metrics_xqnkdh58.json (updated: 2025-11-20 17:32:02)
  metrics_xuw93ulk.json (updated: 2025-11-20 17:32:02)
  metrics_xxx7__b9.json (updated: 2025-11-20 17:32:02)
  metrics_y1mt7hlm.json (updated: 2025-11-20 17:32:02)
  metrics_y5_m62os.json (updated: 2025-11-20 17:32:02)
  metrics_yehnyu4c.json (updated: 2025-11-20 17:32:02)
  metrics_yuncl873.json (updated: 2025-11-20 17:32:02)
  metrics_yv04iu3_.json (updated: 2025-11-20 17:32:02)
  metrics_z4d75mrh.json (updated: 2025-11-20 17:32:02)
  metrics_zn9wzer_.json (updated: 2025-11-20 17:32:02)
  metrics_zrz0v00o.json (updated: 2025-11-20 17:32:02)
  metrics_zskuik5i.json (updated: 2025-11-20 17:32:02)
  trade_state.json (updated: 2025-11-20 17:32:02)
scripts/
  cb_smoke.py (updated: 2025-11-20 17:32:02)
  diagnose_symbol.py (updated: 2025-11-20 17:32:02)
  dryrun_smoke.py (updated: 2025-11-20 17:32:02)
  export_mt5_history.py (updated: 2025-11-20 17:32:02)
  export_val_probs.py (updated: 2025-11-20 17:32:02)
  make_csv_from_mt5.py (updated: 2025-11-20 17:32:02)
  make_project_snapshot.py (updated: 2025-11-20 17:32:02)
  make_toy_model.py (updated: 2025-11-20 17:32:02)
  mt5_export_csv.py (updated: 2025-11-20 17:32:02)
  mt5_smoke.py (updated: 2025-11-20 17:32:02)
  print_runtime.py (updated: 2025-11-20 17:32:02)
  promote_model.py (updated: 2025-11-20 17:32:02)
  register_weekly_task.ps1 (updated: 2025-11-20 17:32:02)
  rollback_model.py (updated: 2025-11-20 17:32:02)
  selftest_mt5.py (updated: 2025-11-20 17:32:02)
  selftest_order_flow.py (updated: 2025-11-20 17:32:02)
  sim_trailing.py (updated: 2025-11-20 17:32:02)
  swap_model.py (updated: 2025-11-20 17:32:02)
  train_calibrator.py (updated: 2025-11-20 17:32:02)
  verify_smoke.ps1 (updated: 2025-11-20 17:32:02)
  walkforward_retrain.py (updated: 2025-11-20 17:32:02)
  walkforward_train.py (updated: 2025-11-20 17:32:02)
  weekly_retrain.py (updated: 2025-11-20 17:32:03)
  weekly_wf.ps1 (updated: 2025-11-20 17:32:03)
setup_weekly_job.ps1 (updated: 2025-11-20 17:32:03)
temp_equity_check.py (updated: 2025-11-20 17:32:03)
tests/
  test_sanity.py (updated: 2025-11-20 17:32:03)
tmp_view.py (updated: 2025-11-20 17:32:03)
To Doãƒªã‚¹ãƒˆ.txt (updated: 2025-11-20 17:32:02)
To Doãƒªã‚¹ãƒˆ2.txt (updated: 2025-11-20 17:32:02)
To Doãƒªã‚¹ãƒˆv4.txt (updated: 2025-11-20 12:59:03)
To Doãƒªã‚¹ãƒˆv5.txt (updated: 2025-11-21 17:49:20)
ToDoãƒªã‚¹ãƒˆ Masterç‰ˆ.txt (updated: 2025-11-22 16:57:13)
ToDoãƒªã‚¹ãƒˆ ä¸‹ä½ç‰ˆ.txt (updated: 2025-11-21 18:10:53)
tools/
  __init__.py (updated: 2025-11-20 17:32:03)
  backtest_equity_curve.py (updated: 2025-11-20 17:32:03)
  backtest_run.py (updated: 2025-11-22 16:30:29)
  dump_feature_importance.py (updated: 2025-11-20 17:32:03)
  export_tree_clean.ps1 (updated: 2025-11-20 17:32:03)
  inspect_report.py (updated: 2025-11-20 17:32:03)
  list_wfo_reports.py (updated: 2025-11-20 17:32:03)
  train_lightgbm.py (updated: 2025-11-20 17:32:03)
  train_scaler.py (updated: 2025-11-20 17:32:03)
ã‚„ã‚‹ã¹ãã“ã¨.txt (updated: 2025-11-20 17:32:03)
ãƒ•ã‚§ãƒ¼ã‚ºA~Hã¾ã§ã®çµæœ.txt (updated: 2025-11-20 17:32:03)
ãƒŸãƒãƒ“ã‚­ FX è‡ªå‹•å£²è²·ã‚·ã‚¹ãƒ†ãƒ ï¼šä»•æ§˜æ›¸ v4ï¼ˆæ­£å¼ç‰ˆï¼‰.txt (updated: 2025-11-21 17:44:55)
ä»•æ§˜ã®çµ±åˆã¾ã¨ã‚.docx (updated: 2025-11-20 17:32:03)
ä»•æ§˜æ›¸.txt (updated: 2025-11-20 17:32:03)
ä»•æ§˜æ›¸v3.txt (updated: 2025-11-20 12:58:23)
ä½œæ¥­6ï½8.txt (updated: 2025-11-20 17:32:03)
ä½œæ¥­9.txt (updated: 2025-11-20 17:32:03)
å‚™å¿˜éŒ².txt (updated: 2025-11-20 17:32:03)
è¨­å®š/
  ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ.txt (updated: 2025-11-20 18:35:54)
  ä»®.txt (updated: 2025-11-21 17:37:52)
  ä»®æƒ³ç’°å¢ƒ.txt (updated: 2025-11-20 17:52:57)
è³ªå•.txt (updated: 2025-11-20 17:32:03)


========================================
=== FILE CONTENTS ======================
========================================


=== file: .importlinter.ini ===

[importlinter]
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆï¼ import è§£æã®èµ·ç‚¹ï¼‰
root_package = app
include_external_packages = False

# ãƒ¬ã‚¤ãƒ¤ãƒ¼é †åºï¼šgui -> services -> core
[contract:layers_gui_services_core]
name = GUI/Service/Core ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼å³å®ˆ
type = layers
layers =
    app.gui
    app.services
    app.core

# GUI â†’ core ã¸ã®â€œç›´ãƒªãƒ³ã‚¯â€ã‚’ç¦æ­¢ï¼ˆå¿…ãš services çµŒç”±ï¼‰
[contract:forbid_gui_to_core_direct]
name = GUI ã‹ã‚‰ core ç›´è¼¸å…¥ã®ç¦æ­¢
type = forbidden
source_modules =
    app.gui
forbidden_modules =
    app.core

# batchï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰â†’ gui ä¾å­˜ã‚’ç¦æ­¢ï¼ˆãƒãƒƒãƒã¯UIã«ä¾ã‚‰ãªã„ï¼‰
[contract:batch_isolation]
name = batch ã¯ GUI ã«ä¾å­˜ã—ãªã„
type = forbidden
source_modules =
    app.batch
forbidden_modules =
    app.gui

# é‡è¦ï¼šãƒ«ãƒ¼ãƒˆç›´ä¸‹ã® core/ ã¸ã¯ä¾å­˜ã•ã›ãªã„ï¼ˆæ··ç·šé˜²æ­¢ï¼‰
[contract:forbid_top_core]
name = app.* ã‹ã‚‰ ãƒ«ãƒ¼ãƒˆç›´ä¸‹ core ã¸ã®ä¾å­˜ã‚’ç¦æ­¢
type = forbidden
source_modules =
    app
forbidden_modules =
    core



=== file: active_model.json ===

{
  "model_name": "LightGBM_20251113_0914.pkl",
  "version": "20251113_0914",
  "features_hash": "placeholder_hash",
  "trained_at_jst": "2025-11-13 09:14:58"
}


=== file: Agent HQ ç”¨ã€Œç´°åˆ†åŒ–ç‰ˆã€.txt ===

äº†è§£ã§ã™ã€‚
ã‚ãªãŸãŒ **Agent HQï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ç®¡ç†ï¼‰** ã‚’æœ€å¤§é™æ´»ã‹ã›ã‚‹ã‚ˆã†ã€
ToDoãƒªã‚¹ãƒˆï¼’ã‚’ â€œã‚»ãƒƒã‚·ãƒ§ãƒ³ã§1ã¤ãšã¤é€²ã‚ã‚‰ã‚Œã‚‹ç²’åº¦â€ ã«ã¾ã§ **å®Œå…¨ã«ç´°åˆ†åŒ–** ã—ã¾ã—ãŸã€‚

ã“ã‚Œã‚’ãã®ã¾ã¾
**Agent Sessions â†’ æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³åã«ã‚³ãƒ”ãƒšã—ã¦ä½¿ãˆã‚‹**
â€œã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä½ToDoç‰ˆâ€ ã¨ã—ã¦ä½œã£ã¦ã„ã¾ã™ã€‚

---

# ğŸ”¥ ToDoãƒªã‚¹ãƒˆï¼’ â€• Agent HQ ç”¨ã€Œç´°åˆ†åŒ–ç‰ˆã€

ï¼ˆï¼ãã®ã¾ã¾1ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¨ã—ã¦åˆ‡ã‚Œã‚‹ã‚µã‚¤ã‚ºã«æœ€é©åŒ–ï¼‰

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³1ï¼šãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ & MT5å±¥æ­´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ€çµ‚çµ±ä¸€**

### ğŸ¯ ã‚´ãƒ¼ãƒ«

PC3å°ï¼ˆé–‹ç™ºA / B / é‹ç”¨VPSï¼‰ã§ **åŒã˜ã‚³ãƒãƒ³ãƒ‰ã§MT5å±¥æ­´ã‚’CSVåŒ–** ã§ãã‚‹ã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. `fxbot_path.py` ã®ç¾ä»•æ§˜ã‚’ç¢ºèªã—ã€ã€Œrootã€ã€Œdataã€ãƒ‘ã‚¹æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ã‚’æ–‡ç« åŒ–
2. `.env` or `setx` ã®ã©ã‚Œã‚’æ¡ç”¨ã™ã‚‹ã‹æœ€çµ‚æ±ºå®š
3. `scripts/export_mt5_history.py` / `scripts/make_csv_from_mt5.py` / `scripts/mt5_export_csv.py` ã‚’æ¯”è¼ƒ
4. ã©ã‚Œã‚’â€œæ­£å¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆâ€ã«ã™ã‚‹ã‹ã‚’é¸å®š
5. ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…éƒ¨ã®

   * å‡ºåŠ›ãƒ‘ã‚¹
   * timeframeå‡¦ç†
   * root/data çµ±ä¸€
     ã®é½Ÿé½¬ã‚’å¾¹åº•ä¿®æ­£
6. 3å°ã®PCã§åŒã˜ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ â†’ å‡ºåŠ›å…ˆã«ã‚ºãƒ¬ãŒãªã„ã‹ç¢ºèª
7. `README.md` ã«ã€Œãƒ‡ãƒ¼ã‚¿æ›´æ–°æ‰‹é †ã€ã‚’æ˜æ–‡åŒ–

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³2ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ CLI ã«æœŸé–“æŒ‡å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ **

### ğŸ¯ ã‚´ãƒ¼ãƒ«

`tools/backtest_run.py` ã‚’ **--start-date / --end-date** ä»˜ãã§å‹•ã‹ã›ã‚‹ã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. `backtest_run.py` ã®ç¾çŠ¶ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿ã€ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã€œå®Ÿè¡Œã®æµã‚Œã‚’æŠŠæ¡
2. argparse ã‚’è¿½åŠ ï¼ˆ--start-date, --end-dateï¼‰
3. æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè£…
4. å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€å‘½åã‚’
   `backtest_YYYY-MM-DD_to_YYYY-MM-DD`
   ã®å½¢å¼ã§æ‹¡å¼µ
5. æœŸé–“æŒ‡å®šãªã—ã®å ´åˆã€Œå…¨æœŸé–“ã€ã‚’ä¿æŒã™ã‚‹ã‚ˆã†åˆ†å²å®Ÿè£…
6. ãƒ†ã‚¹ãƒˆï¼š

   * å…¨æœŸé–“
   * åŠå¹´ã ã‘
   * 1ãƒ¶æœˆã ã‘
     ã§ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ãŒå¤‰ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³3ï¼šGUI ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¿ãƒ–ã®æœŸé–“æŒ‡å®šã¨ã€ŒALLã€ãƒœã‚¿ãƒ³é€£å‹•**

### ğŸ¯ ã‚´ãƒ¼ãƒ«

Backtestã‚¿ãƒ–ã§æ—¥ä»˜æŒ‡å®šã—ã¦ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ â†’ ã‚°ãƒ©ãƒ•æ›´æ–°ã§ãã‚‹ã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. GUIå†…ã®Backtestã‚¿ãƒ–ï¼ˆ`history_tab.py` ã¾ãŸã¯ `backtest_tab.py`ï¼‰ã®æ§‹é€ ç¢ºèª
2. From/To æ—¥ä»˜ãƒ”ãƒƒã‚«ãƒ¼ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’è¨­ç½®
3. ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã« CLI ã® backtest_run.py ã‚’å‘¼ã¶ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ 
4. æœŸé–“æŒ‡å®š â†’ CLIå®Ÿè¡Œ â†’ CSVèª­è¾¼ â†’ ã‚°ãƒ©ãƒ•æ›´æ–°
5. ã€ŒALLãƒœã‚¿ãƒ³ã€ã‚¯ãƒªãƒƒã‚¯ã§

   * æ—¥ä»˜ãƒ”ãƒƒã‚«ãƒ¼ã‚’æœ€åˆã®ç¯„å›²ã«ãƒªã‚»ãƒƒãƒˆ
   * ã‚°ãƒ©ãƒ•ã‚ºãƒ¼ãƒ ã‚’å…¨æœŸé–“ã«æˆ»ã™
6. ã‚¨ãƒ©ãƒ¼æ™‚ã® GUI è¡¨ç¤ºï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒœãƒƒã‚¯ã‚¹ or ãƒ©ãƒ™ãƒ«ï¼‰

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³4ï¼šBacktest vs Walkforward çµæœã®åˆ‡æ›¿ï¼†é‡ã­æç”»**

### ğŸ¯ ã‚´ãƒ¼ãƒ«

Backtestçµæœãƒ»Walkforwardçµæœã‚’
**åˆ‡æ›¿ or overlayï¼ˆé‡ã­æç”»ï¼‰** ã§ãã‚‹ã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. Walkforward ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’æ•´ç†

   * `logs/retrain/report_*.json`
   * ã¾ãŸã¯ WFOé›†è¨ˆãƒ•ã‚¡ã‚¤ãƒ«
2. Backtestã‚¿ãƒ–ã« UIè¿½åŠ 

   * ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³

     * Backtest
     * Walkforward
     * Overlay
3. WFOçµæœèª­è¾¼ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½œæˆ
4. ã‚°ãƒ©ãƒ•ã«è¤‡æ•°ã‚·ãƒªãƒ¼ã‚ºæç”»ã§ãã‚‹ã‚ˆã†èª¿æ•´
5. æœŸé–“æŒ‡å®šã¨ WFO ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæœŸé–“å¤–ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„ï¼‰
6. UIåˆ‡æ›¿æ™‚ã®ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥å‡¦ç†

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³5ï¼šAIã‚¿ãƒ–ï¼ˆKPI / Feature Importance / SHAP ï¼‰ã®é‹ç”¨ç‰ˆå®Œæˆ**

### ğŸ¯ ã‚´ãƒ¼ãƒ«

AIã‚¿ãƒ–ãŒ **é‹ç”¨ã®æ„æ€æ±ºå®šã«ååˆ†ãªæƒ…å ±** ã‚’å‡ºã›ã‚‹çŠ¶æ…‹ã«ã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. æ±ºå®šãƒ­ã‚°ï¼ˆ`logs/decisions_*.jsonl`ï¼‰ã®èª­è¾¼é–¢æ•°ã‚’ä½œã‚‹
2. recent N trades ã®

   * å‹ç‡
   * PF
   * æœ€å¤§DD
   * é€£å‹/é€£æ•—
     ã‚’è¨ˆç®—ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½œæˆ
3. ãƒ¢ãƒ‡ãƒ«ã®

   * åå‰
   * ãƒãƒ¼ã‚¸ãƒ§ãƒ³
   * æ¤œè¨¼æŒ‡æ¨™ï¼ˆLogloss, AUCï¼‰
     ã‚’ `model_meta.json` ã‹ã‚‰å–å¾—
4. Feature Importance ã®æŠ½å‡º

   * LightGBM â†’ `.feature_importance()`
   * XGBoost â†’ Booster.get_score()
ä½œæ¥­5 â†’ AIã‚¿ãƒ– Ã— Feature Importance æ­£å¼çµ±åˆ
6. SHAPãƒˆãƒƒãƒ—3ã®ç°¡æ˜“å¯è¦–åŒ–ï¼ˆé«˜é€Ÿç‰ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰
7. GUIã‚¿ãƒ–ã«æç”»
8. ãƒ¢ãƒ‡ãƒ«åˆ‡æ›¿ãƒ»å†å­¦ç¿’å¾Œã«ã‚‚æ­£ã—ãåæ˜ ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³6ï¼šMT5ç™ºæ³¨ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆãƒ‡ãƒ¢å£åº§ï¼‰æ§‹ç¯‰**

### ğŸ¯ ã‚´ãƒ¼ãƒ«

MT5 ãƒ‡ãƒ¢ã§ **å®Ÿæ³¨æ–‡ â†’ ç´„å®š â†’ æ±ºæ¸ˆ** ãŒæœ€å°æ§‹æˆã§æˆåŠŸã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. `core/mt5_client.py`ï¼ˆæ–°è¦ or æ”¹ä¿®ï¼‰ã‚’ä½œæˆ
2. initialize / login / shutdown ã®å®Ÿè£…
3. æˆè¡Œ BUY/SELL ã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤å«ã‚€ï¼‰
4. Closeï¼ˆæ±ºæ¸ˆï¼‰ã®ãƒ©ãƒƒãƒ‘ãƒ¼
5. ãƒã‚¸å–å¾—ï¼ˆpositions_getï¼‰å®Ÿè£…
6. ãƒ­ã‚°å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰ã‚’çµ±ä¸€
7. `scripts/selftest_order_flow.py` ã‚’ä½œæˆ

   * 0.01lot â†’ ç™ºæ³¨
   * ç´„å®šç¢ºèª
   * ã™ãæ±ºæ¸ˆ
8. GUIã‹ã‚‰è©¦é¨“ç™ºæ³¨ã™ã‚‹æœ€ä½é™ã®ãƒœã‚¿ãƒ³è¿½åŠ 

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³7ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼ˆv0ï¼‰ã®å®Ÿè£…**

### ğŸ¯ ã‚´ãƒ¼ãƒ«

1ã‚³ãƒãƒ³ãƒ‰ã§ **ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ** ã‚’ç”Ÿæˆã§ãã‚‹ã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. `logs/backtest/.../equity_curve.csv` ã‚’èª­è¾¼ã‚€è§£æã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
2. æ±ºå®šãƒ­ã‚°ã¨çªåˆã—ã¦

   * ãƒˆãƒ¬ãƒ³ãƒ‰æ™‚ã®æˆç¸¾
   * ãƒ¬ãƒ³ã‚¸æ™‚ã®æˆç¸¾
   * æ™‚é–“å¸¯åˆ¥å‹ç‡
3. é€£æ•—å‰å¾Œã®ç‰¹å¾´åˆ†æï¼ˆPFæ€¥è½ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®šï¼‰
4. ATR/ADX ã¨å‹ç‡ã®é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
5. çµæœã‚’

   * ãƒ†ã‚­ã‚¹ãƒˆ
   * HTML
     ã©ã¡ã‚‰ã‹ã§å‡ºåŠ›
6. â€œäººé–“ãŒèª­ã‚“ã§ç´å¾—ã§ãã‚‹ã‚³ãƒ¡ãƒ³ãƒˆâ€ ã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ï¼‹ãƒ«ãƒ¼ãƒ«ã§ç”Ÿæˆ

---

# **ã‚»ãƒƒã‚·ãƒ§ãƒ³8ï¼ˆä»»æ„ï¼‰ï¼šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹/ä¾å­˜é–¢ä¿‚ã®å¯è¦–åŒ–**

### ğŸ¯ ã‚´ãƒ¼ãƒ«

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’ Agent HQ ã§ **ã€Œä¾å­˜é–¢ä¿‚åœ°å›³ã€åŒ–** ã™ã‚‹ã€‚

### ğŸ”§ ã‚µãƒ–ã‚¿ã‚¹ã‚¯

1. `tree` + Agentãƒãƒ£ãƒƒãƒˆã§æ§‹é€ ã‚’æ•´ç†
2. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ / ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æµã‚Œã‚’å›³è§£ã•ã›ã‚‹
3. ToDoãƒªã‚¹ãƒˆï¼’ä»¥é™ã®ä½œæ¥­ã®å„ªå…ˆåº¦è¦‹ç›´ã—

---

# â­ ã“ã®ã€Œç´°åˆ†åŒ–ç‰ˆToDoã€ã‚’ã©ã†ä½¿ã†ï¼Ÿ

### 1. Agent HQ â†’ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ

ä¾‹
**[ToDo2-1] ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹çµ±ä¸€**
**[ToDo2-2] ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“æŒ‡å®š**
â€¦ã¨ã„ã†åå‰ã§ä½œã‚‹ã€‚

### 2. ã“ã®ç´°åˆ†åŒ–ç‰ˆã‚’ã‚³ãƒ”ãƒš

æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è²¼ã‚‹ã€‚

### 3. ã€Œã¾ãšã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®é †åºã‚’æ•´ç†ã—ã¦ã€ã¨æŒ‡ç¤º

Plan Agent ãŒãã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å°‚ç”¨ã®è¨­è¨ˆå›³ã‚’è¿”ã—ã¦ãã‚‹ã€‚

### 4. ä½œæ¥­ãŒçµ‚ã‚ã‚‹ãŸã³ã«

ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‰ã˜ã‚‹ or Completed ã¨ã—ã¦ä¿å­˜ã€‚

---

# ğŸ“Œ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ã„ã¾ã“ã®ãƒªã‚¹ãƒˆãŒã§ããŸã®ã§ã€

ğŸ‘‰ **æ¬¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã€Œã©ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å–ã‚Šã‹ã‹ã‚‹ã‹ã€æ•™ãˆã¦ãã ã•ã„ã€‚**
ï¼ˆä¾‹ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³1ã‹ã‚‰ã‚„ã‚ŠãŸã„ï¼‰

ãã®ä½œæ¥­å°‚ç”¨ã® **Agent HQ ã§ä½¿ãˆã‚‹åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ** ã‚‚ã‚»ãƒƒãƒˆã§ä½œã‚Šã¾ã™ã€‚



=== file: app/__init__.py ===




=== file: app/core/config_loader.py ===

from pathlib import Path
from typing import Any, Dict

import yaml


def _deep_merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    for k, v in (b or {}).items():
        if isinstance(v, dict) and isinstance(a.get(k), dict):
            a[k] = _deep_merge(a[k], v)
        else:
            a[k] = v
    return a


def load_config() -> Dict[str, Any]:
    base: Dict[str, Any] = {}
    base_path = Path("configs/config.yaml")
    if base_path.exists():
        base = yaml.safe_load(base_path.read_text(encoding="utf-8")) or {}
    local_path = Path("configs/config.local.yaml")
    if local_path.exists():
        local: Dict[str, Any] = yaml.safe_load(local_path.read_text(encoding="utf-8")) or {}
        base = _deep_merge(base, local)
    return base



=== file: app/core/data_finder.py ===

# app/core/data_finder.py
from __future__ import annotations
import os, glob
from pathlib import Path
from typing import Iterable, Optional, Tuple
import pandas as pd

def _expand_path(p: str) -> Iterable[Path]:
    # ç’°å¢ƒå¤‰æ•°ãƒ»%APPDATA% ã‚’è§£æ±ºã—ã€glob ã‚’å±•é–‹
    p = os.path.expandvars(p)
    p = os.path.expanduser(p)
    for hit in glob.glob(p, recursive=True):
        yield Path(hit)

def _load_csv(path: Path) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        low = {c.lower(): c for c in df.columns}
        need = ["time","open","high","low","close"]
        if not all(k in low for k in need):
            return None
        return df
    except Exception:
        return None

def resolve_csv(symbol: str, timeframe: str, search_paths: Iterable[str]) -> Tuple[Optional[Path], Optional[pd.DataFrame]]:
    """search_paths ã‹ã‚‰ {SYMBOL}_{TF}.csv ã‚’æ¢ã—ã¦è¿”ã™ã€‚æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’æ¡ç”¨ã€‚"""
    target_name = f"{symbol.upper()}_{timeframe.upper()}.csv"
    for base in search_paths:
        for base_path in _expand_path(base):
            if not base_path.exists():
                continue
            # 1) ç›´ä¸‹ã«ã‚ã‚‹å ´åˆ
            p1 = base_path / target_name
            if p1.exists():
                df = _load_csv(p1)
                if df is not None:
                    return p1, df
            # 2) ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚‚æ¢ç´¢
            for hit in base_path.rglob(target_name):
                df = _load_csv(hit)
                if df is not None:
                    return hit, df
    return None, None



=== file: app/core/logger.py ===

# app/core/logger.py
from loguru import logger
import sys
from pathlib import Path

LOG_DIR = Path(__file__).resolve().parents[2] / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)

def setup():
    """Loguru ãƒ­ã‚¬ãƒ¼ã®å…±é€šè¨­å®š"""
    logger.remove()
    logger.add(sys.stdout, level="INFO", format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}")
    logger.add(
        LOG_DIR / "app.log",
        rotation="10 MB",
        retention="10 days",
        compression="zip",
        level="INFO",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    )
    return logger

# ã™ãåˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«åˆæœŸåŒ–
setup()



=== file: app/core/market.py ===

import MetaTrader5 as mt5
from typing import Any, Optional, Tuple


def _pip_from_symbol_info(si: Any) -> float:
    # ä¾‹: USDJPY ãªã‚‰ point=0.001 â†’ pip=0.01ï¼ˆ= point*10ï¼‰
    return float(si.point) * 10.0 if si and si.point else 0.0

def select_symbol(symbol: str) -> bool:
    si = mt5.symbol_info(symbol)
    if si is None:
        return False
    if not si.visible:
        if not mt5.symbol_select(symbol, True):
            return False
    return True

def spread_pips(symbol: str) -> Optional[float]:
    if not select_symbol(symbol):
        return None
    si = mt5.symbol_info(symbol)
    tick = mt5.symbol_info_tick(symbol)
    pip = _pip_from_symbol_info(si)
    if tick and pip > 0:
        return (float(tick.ask) - float(tick.bid)) / pip
    if si and pip > 0 and si.ask and si.bid:
        return (float(si.ask) - float(si.bid)) / pip
    if si and pip > 0 and si.spread:
        try:
            return (float(si.spread) * float(si.point)) / pip
        except Exception:
            pass
    return None

def tick(symbol: str) -> Optional[Tuple[float, float]]:
    """(bid, ask) ã‚’è¿”ã™ã€‚å–å¾—å¤±æ•—ã§ Noneã€‚"""
    if not select_symbol(symbol):
        return None
    t = mt5.symbol_info_tick(symbol)
    if t is None:
        return None
    return float(t.bid), float(t.ask)

def pips_to_price(symbol: str, pips: float) -> Optional[float]:
    if not select_symbol(symbol):
        return None
    si = mt5.symbol_info(symbol)
    pip = _pip_from_symbol_info(si)
    return pips * pip if pip > 0 else None



=== file: app/core/mt5_client.py ===

import time
import MetaTrader5 as MT5
import pandas as pd
from loguru import logger
from typing import Optional, Dict, Any
from typing import NamedTuple



POSITION_COLUMNS = [
    "ticket",
    "time",
    "time_msc",
    "time_update",
    "time_update_msc",
    "symbol",
    "magic",
    "volume",
    "price_open",
    "sl",
    "tp",
    "price_current",
    "swap",
    "profit",
    "comment",
]

class TickSpec(NamedTuple):
    tick_size: float   # 1ãƒ†ã‚£ãƒƒã‚¯ã®ä¾¡æ ¼å¹…ï¼ˆä¾‹: 0.01 JPYï¼‰
    tick_value: float  # 1ãƒ†ã‚£ãƒƒã‚¯å‹•ã„ãŸã¨ãã®æç›Šï¼ˆ1ãƒ­ãƒƒãƒˆã‚ãŸã‚Šã®é€šè²¨ï¼‰


class MT5Client:
    """MT5 ç™ºæ³¨ãƒ»æ¥ç¶šãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆæœ€å°æ§‹æˆï¼‰"""

    def __init__(self, login: int, password: str, server: str, timeout: float = 5.0):
        self.login = login
        self.password = password
        self.server = server
        self.timeout = timeout
        self.connected = False
        self.logger = logger

    # ------------------------
    # æ¥ç¶šç³»
    # ------------------------
    def initialize(self) -> bool:
        """MT5ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®åˆæœŸåŒ–ï¼ˆãƒ­ã‚°ã‚¤ãƒ³ã¯ login_account()ï¼‰"""
        logger.info("MT5 initialize() called...")

        if not MT5.initialize():
            err = MT5.last_error()
            logger.error(f"MT5 initialize() failed: {err}")
            self.connected = False
            return False

        logger.info("MT5 initialize() succeeded")
        self.connected = True
        return True

    def login_account(self) -> bool:
        """è¨­å®šã•ã‚ŒãŸãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã§ MT5.login() ã‚’å®Ÿè¡Œ"""
        logger.info(
            f"MT5 login() called with login={self.login}, server={self.server}"
        )

        ok = MT5.login(
            self.login,
            password=self.password,
            server=self.server,
        )
        if not ok:
            err = MT5.last_error()
            logger.error(f"MT5 login() failed: {err}")
            return False

        logger.info("MT5 login() succeeded")
        return True

    def shutdown(self):
        """MT5 ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        logger.info("MT5 shutdown()")
        MT5.shutdown()
        self.connected = False

    # ------------------------
    # ç™ºæ³¨
    # ------------------------
    def order_send(
        self,
        symbol: str,
        order_type: str,
        lot: float,
        sl: Optional[float] = None,
        tp: Optional[float] = None,
        retries: int = 3,
    ) -> Optional[int]:
        """
        æˆè¡Œç™ºæ³¨ï¼ˆBUY / SELLï¼‰

        Parameters
        ----------
        symbol : str
        order_type : "BUY" or "SELL"
        lot : float
        sl, tp : Optional[float]
        retries : int

        Returns
        -------
        Optional[int]
            æˆåŠŸ: ãƒã‚±ãƒƒãƒˆç•ªå·ï¼ˆintï¼‰
            å¤±æ•—: None
        """

        if order_type not in ("BUY", "SELL"):
            raise ValueError(f"order_type must be BUY/SELL: got {order_type}")

        # --- 1) ã‚·ãƒ³ãƒœãƒ«æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯ ---
        info = MT5.symbol_info(symbol)
        if info is None:
            logger.error(f"[order_send] symbol_info({symbol}) ãŒ Noneã€‚ã‚·ãƒ³ãƒœãƒ«ãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§")
            return None

        if not info.visible:
            logger.info(f"[order_send] {symbol} ãŒéè¡¨ç¤ºãªã®ã§ symbol_select() ã—ã¾ã™")
            if not MT5.symbol_select(symbol, True):
                logger.error(f"[order_send] symbol_select({symbol}, True) ã«å¤±æ•—")
                return None

        # --- 2) æœ€æ–°ãƒ†ã‚£ãƒƒã‚¯ ---
        tick = MT5.symbol_info_tick(symbol)
        if tick is None:
            logger.error(f"[order_send] symbol_info_tick({symbol}) ãŒ Noneã€‚ãƒ†ã‚£ãƒƒã‚¯ãŒå–å¾—ã§ããªã„")
            return None

        # --- 3) æ³¨æ–‡ç¨®åˆ¥ã¨ä¾¡æ ¼ ---
        if order_type == "BUY":
            mt_type = MT5.ORDER_TYPE_BUY
            price = tick.ask
        else:
            mt_type = MT5.ORDER_TYPE_SELL
            price = tick.bid

        # --- 4) æ³¨æ–‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ ---
        request: Dict[str, Any] = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": float(lot),
            "type": mt_type,
            "price": float(price),
            "sl": float(sl) if sl is not None else 0.0,
            "tp": float(tp) if tp is not None else 0.0,
            "magic": 123456,
            "comment": "fxbot_test_order",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        last_error: Optional[tuple[int, str]] = None

        # --- 5) ãƒªãƒˆãƒ©ã‚¤ä»˜ã order_send ---
        for attempt in range(1, retries + 1):
            logger.info(
                f"[order_send] Try {attempt}/{retries}: {order_type} {lot} lot @ {price} {symbol}"
            )

            result = MT5.order_send(request)

            if result is None:
                last_error = MT5.last_error()
                logger.error(f"[order_send] result is None, last_error={last_error}")

            else:
                logger.info(
                    "[order_send] retcode=%s, order=%s, deal=%s, comment=%s",
                    getattr(result, "retcode", None),
                    getattr(result, "order", None),
                    getattr(result, "deal", None),
                    getattr(result, "comment", None),
                )

                # æˆè¡Œãªã®ã§ DONE = æˆåŠŸ
                if result.retcode == MT5.TRADE_RETCODE_DONE:
                    ticket = int(result.order or result.deal or 0)
                    if ticket > 0:
                        logger.info(f"[order_send] æˆåŠŸ: ticket={ticket}")
                        return ticket
                    else:
                        logger.warning(f"[order_send] DONE ã ãŒ ticket ãŒå–å¾—ã§ããªã„: {result}")

                else:
                    logger.warning(
                        f"[order_send] å¤±æ•— retcode={result.retcode}ã€‚å†è©¦è¡Œã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™"
                    )

            if attempt < retries:
                time.sleep(1.0)

        logger.error(f"[order_send] å…¨ {retries} å›ãƒªãƒˆãƒ©ã‚¤ã—ã¦ã‚‚å¤±æ•—ã€‚last_error={last_error}")
        return None

    # ------------------------
    # æ±ºæ¸ˆï¼ˆã‚¯ãƒ­ãƒ¼ã‚ºï¼‰
    # ------------------------
    def close_position(self, ticket: int, symbol: str, retries: int = 3) -> bool:
        """æŒ‡å®šãƒã‚±ãƒƒãƒˆã®æˆè¡Œã‚¯ãƒ­ãƒ¼ã‚º"""

        pos = MT5.positions_get(ticket=ticket)
        if not pos:
            logger.error(f"ticket={ticket} ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return False

        position = pos[0]
        lot = position.volume

        # position.type: 0=BUY, 1=SELL
        order_type = MT5.ORDER_TYPE_SELL if position.type == 0 else MT5.ORDER_TYPE_BUY

        # ã‚¯ãƒ­ãƒ¼ã‚ºä¾¡æ ¼
        t = MT5.symbol_info_tick(symbol)
        if t is None:
            logger.error(f"[close_position] symbol_info_tick({symbol}) ãŒ None")
            return False

        price = t.bid if order_type == MT5.ORDER_TYPE_SELL else t.ask

        request = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": lot,
            "type": order_type,
            "position": ticket,
            "price": price,
            "magic": 123456,
            "comment": "fxbot_test_close",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        for attempt in range(1, retries + 1):
            logger.info(f"[close_position] Try {attempt}: ticket={ticket}")
            result = MT5.order_send(request)

            if result and result.retcode == MT5.TRADE_RETCODE_DONE:
                logger.info(f"ã‚¯ãƒ­ãƒ¼ã‚ºæˆåŠŸ: ticket={ticket}")
                return True

            logger.error(
                f"retcode={result.retcode if result else None}, err={MT5.last_error()}"
            )
            time.sleep(1.0)

        logger.error("[close_position] å…¨ãƒªãƒˆãƒ©ã‚¤å¤±æ•—")
        return False

    # ------------------------
    # ãƒã‚¸ã‚·ãƒ§ãƒ³ä¸€è¦§
    # ------------------------
    def get_positions(self):
        try:
            pos = MT5.positions_get()
            if pos is None:
                self.logger.warning("positions_get() returned None")
                return []
            return list(pos)
        except Exception as exc:
            self.logger.exception(f"positions_get() failed: {exc}")
            return []

    def get_positions_by_symbol(self, symbol: str):
        rows = self.get_positions()
        out = [p for p in rows if getattr(p, "symbol", None) == symbol]
        self.logger.info(f"get_positions_by_symbol: {symbol} count={len(out)}")
        return out

    def get_positions_df(self, symbol: Optional[str] = None):
        rows = self.get_positions()
        if symbol:
            rows = [p for p in rows if getattr(p, "symbol", None) == symbol]

        if not rows:
            return pd.DataFrame(columns=POSITION_COLUMNS)

        data = []
        for p in rows:
            data.append(
                {
                    "ticket": p.ticket,
                    "time": p.time,
                    "time_msc": p.time_msc,
                    "time_update": p.time_update,
                    "time_update_msc": p.time_update_msc,
                    "symbol": p.symbol,
                    "magic": p.magic,
                    "volume": p.volume,
                    "price_open": p.price_open,
                    "sl": p.sl,
                    "tp": p.tp,
                    "price_current": p.price_current,
                    "swap": p.swap,
                    "profit": p.profit,
                    "comment": p.comment,
                }
            )

        return pd.DataFrame(data, columns=POSITION_COLUMNS)

    def get_equity(self) -> float:
        """ç¾åœ¨å£åº§ã®æœ‰åŠ¹è¨¼æ‹ é‡‘ï¼ˆequityï¼‰ã‚’è¿”ã™ã€‚"""
        info = MT5.account_info()
        if info is None:
            raise RuntimeError("account_info() ãŒ None ã‚’è¿”ã—ã¾ã—ãŸï¼ˆMT5 æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")

        return float(info.equity)

    def get_tick_spec(self, symbol: str) -> TickSpec:
        """
        æŒ‡å®šã‚·ãƒ³ãƒœãƒ«ã® tick_size / tick_value ã‚’è¿”ã™ã€‚
        - tick_size: ä¾¡æ ¼ãŒ 1 tick å‹•ãå¹…
        - tick_value: ãã® 1 tick ã§ 1 ãƒ­ãƒƒãƒˆã‚ãŸã‚Šã®æç›Š
        """
        info = MT5.symbol_info(symbol)
        if info is None:
            raise RuntimeError(f"symbol_info({symbol!r}) ãŒ None ã‚’è¿”ã—ã¾ã—ãŸï¼ˆã‚·ãƒ³ãƒœãƒ«åã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")

        # broker ã«ã‚ˆã£ã¦ã¯ trade_tick_size / trade_tick_value ã‚’ä½¿ã†å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚
        # ã“ã“ã§ã¯ point / trade_tick_value ã‚’ä½¿ã†æƒ³å®šã§ã™ã€‚
        tick_size = float(getattr(info, "point", 0.0))
        tick_value = float(getattr(info, "trade_tick_value", 0.0))

        if tick_size <= 0:
            raise RuntimeError(f"{symbol!r} ã® tick_size ãŒ 0 ä»¥ä¸‹ã§ã™: {tick_size}")
        if tick_value <= 0:
            raise RuntimeError(f"{symbol!r} ã® tick_value ãŒ 0 ä»¥ä¸‹ã§ã™: {tick_value}")

        return TickSpec(tick_size=tick_size, tick_value=tick_value)



=== file: app/core/strategy_profile.py ===

"""
app.core.strategy_profile

ãƒŸãƒãƒ“ã‚­ã®ã€Œæˆ¦ç•¥ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

- ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« ID (profile.name) ã¯
  - config
  - backtests/{profile}/...
  - JobScheduler ã®ã‚³ãƒãƒ³ãƒ‰
  ãªã©ã§å…±é€šã—ã¦ä½¿ã†ã‚­ãƒ¼ã¨ã™ã‚‹ã€‚

ã¾ãšã¯ M-A2 å¯¾å¿œã¨ã—ã¦ã€Œmichibiki_stdã€ï¼ˆãƒŸãƒãƒ“ã‚­æ¨™æº–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã ã‘ã‚’å®šç¾©ã™ã‚‹ã€‚
"""

from __future__ import annotations

from dataclasses import dataclass
from core.risk import LotSizingResult, compute_lot_size_from_atr

from pathlib import Path
from typing import Dict


@dataclass(frozen=True)
class StrategyProfile:
    """æˆ¦ç•¥ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« 1 ä»¶åˆ†ã®å®šç¾©ã€‚

    å°†æ¥çš„ã«ã¯ã€ã“ã“ã«
    - ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
    - ç‰¹å¾´é‡ã‚»ãƒƒãƒˆå
    - è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
    ãªã©ã‚’è¶³ã—ã¦ã„ãã€‚
    """

    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« IDï¼ˆãƒ•ã‚©ãƒ«ãƒ€å / è¨­å®šã‚­ãƒ¼ã«ä½¿ã†ï¼‰
    name: str

    # äººé–“ãŒèª­ã‚€ç”¨ã®èª¬æ˜
    description: str

    # å–å¼•å¯¾è±¡
    symbol: str  # ä¾‹: "USDJPY" / "USDJPY-"
    timeframe: str  # ä¾‹: "M5", "M15"

    # KPI ç›®æ¨™
    target_monthly_return: float  # ä¾‹: 0.03 (= +3%)
    max_monthly_dd: float  # ä¾‹: -0.20 (= -20%)

    # ATR ãƒ™ãƒ¼ã‚¹æˆ¦ç•¥ã®ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    atr_period: int
    atr_mult_entry: float
    atr_mult_sl: float

    # Walk-Forward çª“
    wf_train_months: int
    wf_test_months: int

    @property
    def backtest_root(self) -> Path:
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚’ç½®ããƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’è¿”ã™ã€‚

        ä¾‹:
            backtests/michibiki_std
        """
        return Path("backtests") / self.name

    @property
    def monthly_returns_path(self) -> Path:
        """monthly_returns.csv ã®æ¨™æº–ãƒ‘ã‚¹ã€‚

        M-A1 ã§æ±ºã‚ãŸã€Œbacktests/{profile}/monthly_returns.csvã€ã¨æ•´åˆã•ã›ã‚‹ã€‚
        """
        return self.backtest_root / "monthly_returns.csv"

    def compute_lot_size_from_atr(
        self,
        *,
        equity: float,
        atr: float,
        tick_value: float,
        tick_size: float,
        expected_trades_per_month: int = 40,
        worst_case_trades_for_dd: int = 10,
        avg_r_multiple: float = 0.6,
        min_lot: float = 0.01,
        max_lot: float = 1.0,
    ) -> LotSizingResult:
        """
        ã“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚ŒãŸ target_monthly_return / max_monthly_dd / atr_mult_sl ã‚’ä½¿ç”¨ã—ã¦
        æ¨å¥¨ãƒ­ãƒƒãƒˆã‚’è¨ˆç®—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã€‚

        Parameters
        ----------
        equity:
            ç¾åœ¨ã®å£åº§æ®‹é«˜ or æœ‰åŠ¹è¨¼æ‹ é‡‘ã€‚
        atr:
            ç¾åœ¨ã® ATR å€¤ï¼ˆä¾¡æ ¼å˜ä½ï¼‰ã€‚
        tick_value, tick_size:
            MT5 ã® symbol_info(...).trade_tick_value / trade_tick_size ç­‰ã‹ã‚‰å–å¾—ã—ãŸå€¤ã€‚
        expected_trades_per_month, worst_case_trades_for_dd, avg_r_multiple, min_lot, max_lot:
            ãƒ­ãƒƒãƒˆè¨ˆç®—ã®å‰æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚å¿…è¦ãªã‚‰å¤–ã‹ã‚‰ä¸Šæ›¸ãå¯èƒ½ã€‚

        Returns
        -------
        LotSizingResult
        """
        return compute_lot_size_from_atr(
            equity=equity,
            atr=atr,
            atr_mult_sl=self.atr_mult_sl,
            target_monthly_return=self.target_monthly_return,
            max_monthly_dd=self.max_monthly_dd,
            tick_value=tick_value,
            tick_size=tick_size,
            expected_trades_per_month=expected_trades_per_month,
            worst_case_trades_for_dd=worst_case_trades_for_dd,
            avg_r_multiple=avg_r_multiple,
            min_lot=min_lot,
            max_lot=max_lot,
        )


# ==== ãƒŸãƒãƒ“ã‚­æ¨™æº–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« (M-A2) ======================================


MICHIBIKI_STD = StrategyProfile(
    name="michibiki_std",
    description="ãƒŸãƒãƒ“ã‚­æ¨™æº–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« v1ï¼ˆUSDJPY M5 / ATRãƒ™ãƒ¼ã‚¹ / æœˆæ¬¡3ï¼…ç›®æ¨™ï¼‰",
    symbol="USDJPY",
    timeframe="M5",
    # KPI ç›®æ¨™
    target_monthly_return=0.03,  # +3%/æœˆã‚’ç‹™ã†
    max_monthly_dd=-0.20,  # -20% ä»¥å†…ã«åã‚ãŸã„
    # ATRæˆ¦ç•¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæš«å®šå€¤ï¼šã‚ã¨ã§WFOã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
    atr_period=14,
    atr_mult_entry=1.5,
    atr_mult_sl=3.0,
    # Walk-Forward çª“ï¼ˆ12ãƒ¶æœˆè¨“ç·´ -> 1ãƒ¶æœˆãƒ†ã‚¹ãƒˆ ã‚’åŸºæœ¬ã¨ã™ã‚‹ï¼‰
    wf_train_months=12,
    wf_test_months=1,
)


# ä»Šå¾Œãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¢—ã‚„ã™å ´åˆã¯ã“ã“ã«è¿½åŠ ã—ã¦ã„ã
_PROFILES: Dict[str, StrategyProfile] = {
    MICHIBIKI_STD.name: MICHIBIKI_STD,
}


def get_profile(name: str = "michibiki_std") -> StrategyProfile:
    """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« ID ã‹ã‚‰ StrategyProfile ã‚’å–å¾—ã™ã‚‹ã€‚

    æœªå®šç¾©ã®åå‰ãŒæ¥ãŸå ´åˆã¯ KeyError ã§ã¯ãªã ValueError ã«ã—ã¦ã€
    GUI ã‚„ CLI ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºã—ã‚„ã™ãã—ã¦ãŠãã€‚
    """
    try:
        return _PROFILES[name]
    except KeyError as exc:  # pragma: no cover - å˜ç´”ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¹
        known = ", ".join(sorted(_PROFILES.keys()))
        raise ValueError(f"æœªçŸ¥ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã§ã™: {name!r} (known: {known})") from exc


def list_profiles() -> Dict[str, StrategyProfile]:
    """å®šç¾©æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’ dict ã§è¿”ã™ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨æƒ³å®šï¼‰ã€‚"""
    return dict(_PROFILES)



=== file: app/gui/__init__.py ===




=== file: app/gui/ai_tab.py ===

# app/gui/ai_tab.py
from __future__ import annotations
from pathlib import Path
from PyQt6.QtWidgets import (
    QWidget,
    QVBoxLayout,
    QGroupBox,
    QFormLayout,
    QLabel,
    QSpinBox,
    QPushButton,
    QHBoxLayout,
    QTabWidget,
)
import joblib

from loguru import logger

from app.services.ai_service import AISvc
from app.services.recent_kpi import compute_recent_kpi_from_decisions
from app.gui.widgets.feature_importance import FeatureImportanceWidget
from app.gui.widgets.shap_bar import ShapBarWidget


class AITab(QWidget):
    def __init__(self, ai_service: AISvc | None = None, parent: QWidget | None = None) -> None:
        super().__init__(parent)
        self.ai_service = ai_service or AISvc()

        try:
            p = Path("models/LightGBM_clf.pkl")
            if p.exists():
                obj = joblib.load(p)
                model = obj.get("model", obj) if isinstance(obj, dict) else obj
                self.ai_service.models.setdefault("lgbm_cls", model)
        except Exception as e:
            print(f"[AITab] model autoload skipped: {e}")

        main_layout = QVBoxLayout(self)
        self.tab_widget = QTabWidget(self)
        main_layout.addWidget(self.tab_widget, 1)

        self.tab_kpi = QWidget(self.tab_widget)
        kpi_layout = QVBoxLayout(self.tab_kpi)

        self.recent_kpi_group = QGroupBox("Recent Trades KPI", self.tab_kpi)
        kpi_form = QFormLayout(self.recent_kpi_group)

        self.spin_recent_n = QSpinBox(self.recent_kpi_group)
        self.spin_recent_n.setRange(1, 100000)
        self.spin_recent_n.setValue(100)

        self.lbl_n_trades = QLabel("-", self.recent_kpi_group)
        self.lbl_win_rate = QLabel("-", self.recent_kpi_group)
        self.lbl_profit_factor = QLabel("-", self.recent_kpi_group)
        self.lbl_max_drawdown = QLabel("0.0", self.recent_kpi_group)
        self.lbl_max_dd_ratio = QLabel("N/A", self.recent_kpi_group)
        self.lbl_best_streaks = QLabel("0 / 0", self.recent_kpi_group)

        self.btn_refresh_kpi = QPushButton("Refresh", self.recent_kpi_group)

        kpi_form.addRow("Window (N trades)", self.spin_recent_n)
        kpi_form.addRow("Total trades", self.lbl_n_trades)
        kpi_form.addRow("Win rate", self.lbl_win_rate)
        kpi_form.addRow("Profit factor", self.lbl_profit_factor)
        kpi_form.addRow("Max drawdown", self.lbl_max_drawdown)
        kpi_form.addRow("Max DD ratio", self.lbl_max_dd_ratio)
        kpi_form.addRow("Best win / loss streak", self.lbl_best_streaks)

        btn_row = QHBoxLayout()
        btn_row.addStretch(1)
        btn_row.addWidget(self.btn_refresh_kpi)
        kpi_form.addRow(btn_row)

        kpi_layout.addWidget(self.recent_kpi_group)
        kpi_layout.addStretch(1)

        self.tab_widget.addTab(self.tab_kpi, "KPI")

        self.tab_fi = QWidget(self.tab_widget)
        fi_layout = QVBoxLayout(self.tab_fi)
        self.feature_importance = FeatureImportanceWidget(self.ai_service, self.tab_fi)
        fi_layout.addWidget(self.feature_importance)
        self.tab_widget.addTab(self.tab_fi, "Feature Importance")

        self.tab_shap = QWidget(self.tab_widget)
        shap_layout = QVBoxLayout(self.tab_shap)

        self.shap_group = QGroupBox("SHAP Global Importance", self.tab_shap)
        shap_group_layout = QVBoxLayout(self.shap_group)

        self.shap_widget = ShapBarWidget(self.ai_service, self.shap_group)
        shap_group_layout.addWidget(self.shap_widget)

        shap_layout.addWidget(self.shap_group)
        shap_layout.addStretch(1)

        self.tab_widget.addTab(self.tab_shap, "SHAP")

        self.btn_refresh_kpi.clicked.connect(self.refresh_kpi)
        self.refresh_kpi()

    def refresh_kpi(self) -> None:
        """
        recent_kpi.compute_recent_kpi_from_decisions ã‚’å‘¼ã³å‡ºã—ã€
        ãƒ©ãƒ™ãƒ«ã« KPI ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
        """
        limit = self.spin_recent_n.value()

        try:
            result = compute_recent_kpi_from_decisions(
                limit=limit,
                starting_equity=100000.0,
            )
        except Exception as e:
            self.lbl_n_trades.setText("Error")
            self.lbl_win_rate.setText("Error")
            self.lbl_profit_factor.setText("Error")
            self.lbl_max_drawdown.setText("Error")
            self.lbl_max_dd_ratio.setText("Error")
            self.lbl_best_streaks.setText("Error")

            print(f"[AITab] refresh_kpi error: {e!r}")
            return

        self.lbl_n_trades.setText(str(result.n_trades))

        if result.win_rate is not None:
            self.lbl_win_rate.setText(f"{result.win_rate * 100:.1f} %")
        else:
            self.lbl_win_rate.setText("N/A")

        if result.profit_factor is not None:
            self.lbl_profit_factor.setText(f"{result.profit_factor:.2f}")
        else:
            self.lbl_profit_factor.setText("N/A")

        self.lbl_max_drawdown.setText(f"{result.max_drawdown:.1f}")

        if result.max_drawdown_ratio is not None:
            self.lbl_max_dd_ratio.setText(f"{result.max_drawdown_ratio * 100:.2f} %")
        else:
            self.lbl_max_dd_ratio.setText("N/A")

        self.lbl_best_streaks.setText(
            f"{result.best_win_streak} / {result.best_loss_streak}"
        )

        if getattr(self, "shap_widget", None) is not None:
            try:
                self.shap_widget.refresh(force=True)
            except Exception as e:
                logger.exception("failed to refresh SHAP bar: %s", e)



=== file: app/gui/backtest_tab.py ===

# app/gui/backtest_tab.py
from __future__ import annotations

import json
import pathlib
import sys
import re
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import pandas as pd
import numpy as np
from PyQt6 import QtCore, QtWidgets
from PyQt6.QtCore import Qt, QProcess
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as Canvas
from matplotlib.backends.backend_qtagg import NavigationToolbar2QT as Toolbar
from matplotlib.dates import AutoDateLocator, ConciseDateFormatter
from matplotlib.figure import Figure
from matplotlib.ticker import FuncFormatter, AutoLocator
from matplotlib.widgets import SpanSelector
import matplotlib.gridspec as gridspec
from pandas.api.types import is_datetime64_any_dtype
from app.services.data_guard import ensure_data
from functools import partial
from datetime import datetime, timedelta

def _thousands(x, pos):
    try:
        return f"{int(x):,}"
    except Exception:
        return str(x)

def _find_trades_csv(equity_csv: Path):
    """equity_curve.csv ã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹ trades*.csv ã‚’æ¢ã™ï¼ˆå„ªå…ˆ: test -> train -> tradesï¼‰"""
    cand = [
        equity_csv.with_name("trades_test.csv"),
        equity_csv.with_name("trades_train.csv"),
        equity_csv.with_name("trades.csv"),
    ]
    for c in cand:
        if c.exists():
            return c
    return None

def plot_equity_with_markers_to_figure(fig: Figure, csv_path: str, note: str = ""):
    """equity_curve.csv ã‚’æç”»ã€‚signalå¤‰åŒ–ç‚¹ã§ãƒãƒ¼ã‚¯ã€‚å¤‰åŒ–ãŒç„¡ã‘ã‚Œã° trades*.csv ã® entry_time ã§ãƒãƒ¼ã‚¯ã€‚"""
    p = Path(csv_path)
    try:
        df = pd.read_csv(p)
    except Exception as e:
        print(f"[gui] plot error: failed to read {csv_path}: {e}")
        return

    if "equity" not in df.columns:
        print(f"[gui] plot error: CSVã« 'equity' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚columns={df.columns.tolist()}")
        return

    # æ—¥æ™‚åŒ–
    if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
        df["time"] = pd.to_datetime(df["time"], errors="coerce")

    # ãƒ™ãƒ¼ã‚¹ç·š
    fig.clear()
    ax = fig.add_subplot(111)
    ax.set_title("Equity Curve & Trade Markers" + (f" â€” {p.name}" if p.name else ""))
    ax.set_xlabel("time")
    ax.set_ylabel("equity (JPY)")
    ax.plot(df["time"], df["equity"], lw=1.4, antialiased=True, label="Equity", zorder=2)
    ax.margins(x=0.01, y=0.06)
    ax.grid(True, which="major", alpha=0.25)
    ax.yaxis.set_major_formatter(FuncFormatter(_thousands))
    ax.yaxis.set_major_locator(AutoLocator())

    # --- ãƒãƒ¼ã‚«ãƒ¼â‘ : signal ã®å¤‰åŒ–ç‚¹ ---
    buys = sells = pd.DataFrame()
    if "signal" in df.columns:
        sig = pd.to_numeric(df["signal"], errors="coerce").fillna(0).astype(int)
        chg = sig.ne(sig.shift(1)).fillna(sig.iloc[0] != 0)
        if len(sig) > 0 and sig.iloc[0] != 0:
            chg.iloc[0] = True
        buys  = df[(sig ==  1) & chg]
        sells = df[(sig == -1) & chg]

    # --- ãƒãƒ¼ã‚«ãƒ¼â‘¡: trades*.csv ã® entry_time ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
    if (buys.empty and sells.empty):
        tcsv = _find_trades_csv(p)
        if tcsv is not None:
            try:
                tdf = pd.read_csv(tcsv)
                # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
                if "entry_time" in tdf.columns:
                    tdf["entry_time"] = pd.to_datetime(tdf["entry_time"], errors="coerce")
                    # æ–¹å‘ã®æ¨å®šï¼šdirectionãŒç„¡ã‘ã‚Œã° pnl ã®ç¬¦å·ã§ä»£ç”¨
                    if "direction" in tdf.columns:
                        dirv = pd.to_numeric(tdf["direction"], errors="coerce").fillna(0).astype(int)
                    else:
                        dirv = np.sign(pd.to_numeric(tdf.get("pnl", 0.0), errors="coerce").fillna(0.0)).astype(int)
                    # ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ä¸Šã® Y åº§æ¨™ã¯è¿‘ã„æ™‚åˆ»ã® equity ã‚’ä½¿ã†
                    if "time" in df.columns:
                        eq = df[["time", "equity"]].dropna().copy()
                        eq["time"] = pd.to_datetime(eq["time"], errors="coerce")
                        eq = eq.set_index("time").sort_index()

                        # entry ã®æœ€è¿‘å‚ equity ã‚’æ‹¾ã†
                        def _y_at(t):
                            try:
                                idx = eq.index.searchsorted(t, side="left")
                                if idx == len(eq):
                                    idx -= 1
                                return float(eq.iloc[idx, 0])
                            except Exception:
                                return np.nan

                        tdf["y"] = tdf["entry_time"].map(_y_at)
                        tb = tdf[(dirv ==  1)].copy()
                        ts = tdf[(dirv == -1)].copy()
                        tb = tb.dropna(subset=["y"])
                        ts = ts.dropna(subset=["y"])
                        buys  = pd.DataFrame({"time": tb["entry_time"], "equity": tb["y"]})
                        sells = pd.DataFrame({"time": ts["entry_time"], "equity": ts["y"]})
                        print(f"[gui] fallback markers from trades: buys={len(buys)} sells={len(sells)} ({tcsv.name})")
            except Exception as e:
                print(f"[gui] trades fallback error: {e}")

    # ãƒãƒ¼ã‚«ãƒ¼æç”»
    if not buys.empty:
        ax.scatter(buys["time"], buys["equity"],
                   marker="o", s=48, facecolors="tab:blue", edgecolors="black",
                   linewidths=0.7, label="Buy", zorder=5)
    if not sells.empty:
        ax.scatter(sells["time"], sells["equity"],
                   marker="x", s=64, c="tab:orange", linewidths=1.4,
                   label="Sell", zorder=6)

    # æ—¥ä»˜ç›®ç››ã‚Š
    if "time" in df.columns:
        loc = AutoDateLocator()
        ax.xaxis.set_major_locator(loc)
        ax.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        fig.autofmt_xdate()

    ax.legend(loc="upper left")

PROJECT_ROOT = Path(__file__).resolve().parents[2]

class PlotWindow(QtWidgets.QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("FXBot â€” Chart")
        self.resize(1000, 650)

        self.figure = Figure(figsize=(10, 6), tight_layout=True)
        self.canvas = Canvas(self.figure)
        self.toolbar = Toolbar(self.canvas, self)
        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.toolbar)
        layout.addWidget(self.canvas)

        # 2æ®µã®axesï¼ˆä¸Šï¼šæ‹¡å¤§ã€ä¸‹ï¼šå…¨ä½“ãƒŠãƒ“ï¼‰
        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.08, figure=self.figure)
        self.ax_main = self.figure.add_subplot(gs[0])
        self.ax_nav  = self.figure.add_subplot(gs[1], sharex=self.ax_main)
        self.span = None
        self._last_kind: str | None = None
        self._last_csv: str | None = None
        self._overlay_lines: list = []

    def plot_equity_csv(self, csv_path: str):
        # ä¸Šæ®µã‚’ã‚¯ãƒªã‚¢ã—ã¦æœ¬æç”»ï¼ˆæ—¢å­˜ã®æç”»é–¢æ•°ã‚’å†åˆ©ç”¨ï¼‰
        self.figure.clear()
        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.08, figure=self.figure)
        ax_main = self.figure.add_subplot(gs[0])
        ax_nav  = self.figure.add_subplot(gs[1], sharex=ax_main)

        # èª­ã¿è¾¼ã¿
        df = pd.read_csv(csv_path)
        if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
            df["time"] = pd.to_datetime(df["time"], errors="coerce")

        # ä¸Šæ®µï¼šæœ¬æç”»ï¼ˆãƒãƒ¼ã‚«ãƒ¼ä»˜ãï¼‰
        ax_main.plot(df["time"], df["equity"], lw=1.4, label="Equity")
        ax_main.grid(True, alpha=0.25)
        sig = df.get("signal", pd.Series(0, index=df.index)).astype(int)
        chg = sig.ne(sig.shift(1)).fillna(False)
        if len(sig) > 0 and sig.iloc[0] != 0:
            chg.iloc[0] = True
        buys  = df[(sig == 1)  & chg]
        sells = df[(sig == -1) & chg]
        if not buys.empty:
            ax_main.scatter(buys["time"], buys["equity"], marker="o", s=48,
                            facecolors="tab:blue", edgecolors="black", linewidths=0.7, label="Buy", zorder=5)
        if not sells.empty:
            ax_main.scatter(sells["time"], sells["equity"], marker="x", s=64,
                            c="tab:orange", linewidths=1.4, label="Sell", zorder=6)
        ax_main.legend(loc="upper left")
        ax_main.margins(x=0.01, y=0.05)

        # ç›®ç››ã‚Šãƒ»ã‚¿ã‚¤ãƒˆãƒ«
        from matplotlib.ticker import AutoLocator, FuncFormatter
        ax_main.set_title("Equity Curve & Trade Markers â€” " + Path(csv_path).name)
        ax_main.set_ylabel("equity (JPY)")
        ax_main.yaxis.set_major_locator(AutoLocator())
        ax_main.yaxis.set_major_formatter(FuncFormatter(_thousands))

        # ä¸‹æ®µï¼šå…¨ä½“ãƒŠãƒ“ï¼ˆè–„ã„ç·šï¼‰
        ax_nav.plot(df["time"], df["equity"], lw=1.0, alpha=0.5)
        ax_nav.grid(True, alpha=0.2)

        # SpanSelector ã§ç¯„å›²é¸æŠ â†’ ä¸Šæ®µã® xlim ã‚’åŒæœŸ
        def onselect(xmin, xmax):
            ax_main.set_xlim(xmin, xmax)
            self.canvas.draw_idle()

        self.span = SpanSelector(ax_nav, onselect, "horizontal", useblit=True,
                                 interactive=True, props=dict(alpha=0.15))

        # ç›®ç››ã‚Šä½“è£
        loc = AutoDateLocator()
        ax_nav.xaxis.set_major_locator(loc)
        ax_nav.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        ax_main.xaxis.set_major_locator(loc)
        ax_main.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        self.figure.autofmt_xdate()
        self.canvas.draw_idle()

        # ä¿å­˜ï¼ˆæœŸé–“ã‚¸ãƒ£ãƒ³ãƒ—ç”¨ã«å‚ç…§ï¼‰
        self.ax_main = ax_main
        self.ax_nav  = ax_nav
        self._last_kind = "equity"
        self._last_csv = str(csv_path)

    def overlay_wfo_equity(
        self,
        df_train: Optional[pd.DataFrame],
        df_test: Optional[pd.DataFrame],
    ) -> None:
        """Overlay WFO train/test equity lines on this window's axes."""
        axes = self.figure.get_axes()
        if not axes:
            return
        ax_main = axes[0]

        for ln in getattr(self, "_overlay_lines", []):
            try:
                ln.remove()
            except Exception:
                pass
        self._overlay_lines = []

        def _prepare(df: Optional[pd.DataFrame]) -> Optional[Tuple[pd.Series, pd.Series]]:
            if df is None or "equity" not in df.columns:
                return None
            y = pd.to_numeric(df["equity"], errors="coerce")
            if "time" in df.columns:
                x = pd.to_datetime(df["time"], errors="coerce")
            else:
                x = pd.Series(df.index)
            mask = x.notna() & y.notna()
            if not mask.any():
                return None
            return x[mask], y[mask]

        train_data = _prepare(df_train)
        if train_data is not None:
            ln1 = ax_main.plot(
                train_data[0],
                train_data[1],
                linestyle="--",
                linewidth=1,
                color="tab:blue",
                alpha=0.7,
                label="WFO Train",
            )[0]
            self._overlay_lines.append(ln1)

        test_data = _prepare(df_test)
        if test_data is not None:
            ln2 = ax_main.plot(
                test_data[0],
                test_data[1],
                linestyle="-",
                linewidth=2,
                color="tab:red",
                alpha=0.9,
                label="WFO Test",
            )[0]
            self._overlay_lines.append(ln2)

        if self._overlay_lines:
            ax_main.legend(loc="upper left")
        self.canvas.draw_idle()

    def plot_price_preview(self, csv_path: str, note: str = ""):
        import pandas as pd
        from matplotlib.dates import AutoDateLocator, ConciseDateFormatter
        df = pd.read_csv(csv_path)
        if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
            df["time"] = pd.to_datetime(df["time"], errors="coerce")
        if "close" not in df.columns:
            raise ValueError("CSVã« 'close' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        self.figure.clear()
        ax = self.figure.add_subplot(111)
        price = pd.to_numeric(df["close"], errors="coerce").ffill()
        norm = price / price.iloc[0] * 100.0
        ax.plot(df["time"] if "time" in df.columns else range(len(norm)),
                norm.values, lw=1.4, label="Price (close, =100@start)")
        ax.set_title("Price Preview (from OHLCV)" + (f" â€” {note}" if note else ""))
        ax.set_ylabel("index (=100@start)")
        ax.set_xlabel("time")
        ax.grid(True, alpha=0.25)
        if "time" in df.columns:
            loc = AutoDateLocator()
            ax.xaxis.set_major_locator(loc)
            ax.xaxis.set_major_formatter(ConciseDateFormatter(loc))
            self.figure.autofmt_xdate()
        ax.legend(loc="best")
        self.canvas.draw_idle()
        self.ax_main = ax
        self.ax_nav = None
        self._last_kind = "price"
        self._last_csv = str(csv_path)

    def plot_heatmap(self, df, note: str = ""):
        """
        tools/backtest_run ãŒç”Ÿæˆã™ã‚‹ monthly_returns_*.csv ã®å½¢å¼ã«å¯¾å¿œã—ãŸãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ã€‚

        æœŸå¾…ã‚«ãƒ©ãƒ :
            - 'year'
            - 'month'
            - å€¤åˆ—: 'return' or 'ret' or 'pnl' or 'pnl_pct' ã®ã„ãšã‚Œã‹
        """
        import pandas as pd
        from matplotlib.ticker import PercentFormatter

        self.figure.clear()
        ax = self.figure.add_subplot(111)

        if "year" not in df.columns or "month" not in df.columns:
            QtWidgets.QMessageBox.information(
                self, "æƒ…å ±",
                "æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³CSVã« 'year' ã¾ãŸã¯ 'month' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            )
            return

        value_col = None
        for cand in ("return", "ret", "pnl_pct", "pnl"):
            if cand in df.columns:
                value_col = cand
                break

        df = df.copy()

        if value_col is not None:
            df["value"] = pd.to_numeric(df[value_col], errors="coerce")
        else:
            candidate_cols = [c for c in df.columns if c not in ("year", "month")]
            auto_col = None
            for c in candidate_cols:
                s = pd.to_numeric(df[c], errors="coerce")
                if s.notna().any():
                    auto_col = c
                    df["value"] = s
                    break

            if auto_col is None:
                QtWidgets.QMessageBox.information(
                    self,
                    "æƒ…å ±",
                    "æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³CSVã«æ•°å€¤ã®ãƒªã‚¿ãƒ¼ãƒ³åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                    f"åˆ—ä¸€è¦§: {list(df.columns)}"
                )
                return

            print(f"[heatmap] auto-selected value column: {auto_col}")
        df = df.dropna(subset=["value"])

        if df.empty:
            QtWidgets.QMessageBox.information(self, "æƒ…å ±", "æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        pivot = df.pivot_table(
            index="year",
            columns="month",
            values="value",
            aggfunc="sum"
        ).reindex(columns=range(1, 13)).fillna(0.0)

        if pivot.empty:
            QtWidgets.QMessageBox.information(self, "æƒ…å ±", "æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ãŒç©ºã§ã™ã€‚")
            return

        im = ax.imshow(pivot.values, aspect="auto", interpolation="nearest")

        ax.set_xticks(range(12))
        ax.set_xticklabels([f"{m:02d}" for m in range(1, 13)])
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index.astype(str))

        ax.set_title("Monthly Return Heatmap" + (f" â€” {note}" if note else ""))
        ax.set_xlabel("month")
        ax.set_ylabel("year")

        cbar = self.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.ax.yaxis.set_major_formatter(PercentFormatter(1.0))

        self.canvas.draw_idle()

    # === ï¿½ï¿½ï¿½ÔƒWï¿½ï¿½ï¿½ï¿½ï¿½vAPIï¿½i1W / 1M / ALLï¿½j ===
    def jump_range(self, mode: str) -> None:
        """
        ãƒãƒƒãƒ—ã‚¢ã‚¦ãƒˆè¡¨ç¤ºã§ 1W / 1M / ALL ã® X ç¯„å›²ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚
        1) CSV ã® time åˆ— â†’ 2) æç”»ä¸­ xdata â†’ 3) ãƒ•ã‚¡ã‚¤ãƒ«åã®TFæ¨å®šã§ãƒãƒ¼æ•°ã‚ºãƒ¼ãƒ ã€‚
        """
        valid_modes = {"1W", "1M", "ALL"}
        mode = (mode or "").upper()
        if mode not in valid_modes:
            raise ValueError(f"Unknown mode: {mode}")

        if self._last_kind not in {"equity", "price"} or not self._last_csv:
            raise RuntimeError("ç›´è¿‘ã«ä¾¡æ ¼/ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã‚’è¡¨ç¤ºã—ã¦ã‹ã‚‰æ“ä½œã—ã¦ãã ã•ã„ã€‚")

        csv_path = Path(self._last_csv)
        if not csv_path.exists():
            raise RuntimeError(f"ç›´è¿‘ã®CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")

        def _target_axes():
            axes = []
            seen: set[int] = set()
            for candidate in (getattr(self, "ax_main", None), getattr(self, "ax_nav", None)):
                if candidate is None or candidate not in self.figure.axes:
                    continue
                ident = id(candidate)
                if ident in seen:
                    continue
                seen.add(ident)
                axes.append(candidate)
            if not axes:
                for ax in self.figure.axes:
                    ident = id(ax)
                    if ident in seen:
                        continue
                    seen.add(ident)
                    axes.append(ax)
            return axes

        def _apply_xlim(xmin, xmax):
            axes = _target_axes()
            if not axes:
                raise RuntimeError("æç”»æ¸ˆã¿ã®AxesãŒç„¡ã„ãŸã‚ã‚¸ãƒ£ãƒ³ãƒ—ã§ãã¾ã›ã‚“ã€‚")
            for ax in axes:
                ax.set_xlim(xmin, xmax)
            self.canvas.draw_idle()

        def _extract_xdata():
            for ax in (getattr(self, "ax_nav", None), getattr(self, "ax_main", None)):
                if ax is None:
                    continue
                lines = ax.get_lines()
                if lines:
                    data = lines[0].get_xdata()
                    if data is not None and len(data) >= 2:
                        return data
            for ax in self.figure.axes:
                lines = ax.get_lines()
                if lines:
                    data = lines[0].get_xdata()
                    if data is not None and len(data) >= 2:
                        return data
            return None

        def _apply_time_window(series: pd.Series) -> bool:
            series = pd.Series(series).dropna()
            if len(series) < 2:
                return False
            series = series.sort_values()
            latest = pd.Timestamp(series.iloc[-1])
            if mode == "ALL":
                earliest = pd.Timestamp(series.iloc[0])
            elif mode == "1W":
                earliest = latest - pd.Timedelta(days=7)
            else:
                earliest = latest - pd.Timedelta(days=31)
            first = pd.Timestamp(series.iloc[0])
            earliest = max(first, earliest)
            _apply_xlim(earliest.to_pydatetime(), latest.to_pydatetime())
            return True

        tf_keywords = {
            "M1": 1, "M3": 3, "M5": 5, "M10": 10, "M15": 15, "M30": 30,
            "H1": 60, "H2": 120, "H3": 180, "H4": 240, "H6": 360, "H8": 480, "H12": 720,
            "D1": 1440, "D2": 2880, "W1": 10080
        }

        def _infer_tf_minutes(path: Path) -> int | None:
            chunks = [path.stem.upper(), *(part.upper() for part in path.parts)]
            for name in chunks:
                for token, minutes in tf_keywords.items():
                    if re.search(rf"(?<![A-Z0-9]){token}(?![A-Z0-9])", name):
                        return minutes
                for part in re.split(r"[^A-Z0-9]+", name):
                    if not part:
                        continue
                    if part in tf_keywords:
                        return tf_keywords[part]
                    m = re.fullmatch(r"(M|H|D|W)(\d{1,3})", part)
                    if m:
                        unit = m.group(1)
                        value = int(m.group(2))
                        if value == 0:
                            continue
                        if unit == "M":
                            return value
                        if unit == "H":
                            return value * 60
                        if unit == "D":
                            return value * 1440
                        if unit == "W":
                            return value * 10080
            return None

        def _bars_for_mode(tf_minutes: int, total: int) -> int:
            if mode == "ALL":
                return total
            days = 7 if mode == "1W" else 31
            bars = int(days * 24 * 60 / max(tf_minutes, 1))
            return max(1, bars)

        try:
            df_time = pd.read_csv(csv_path, usecols=["time"])
        except ValueError:
            df_time = None
        except Exception as e:
            print(f"[pop] jump_range warn: failed to read time column from {csv_path}: {e}")
            df_time = None

        if df_time is not None and "time" in df_time.columns:
            times = pd.to_datetime(df_time["time"], errors="coerce")
            if _apply_time_window(times):
                return

        xdata = _extract_xdata()
        if xdata is None:
            raise RuntimeError("æç”»æ¸ˆã¿ã®Xãƒ‡ãƒ¼ã‚¿ãŒç„¡ã„ãŸã‚ã‚¸ãƒ£ãƒ³ãƒ—ã§ãã¾ã›ã‚“ã€‚")

        arr = np.asarray(xdata)
        if arr.size < 2:
            raise RuntimeError("æç”»æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚")

        is_datetime = np.issubdtype(arr.dtype, np.datetime64)
        if not is_datetime and arr.dtype == object:
            sample = next((v for v in arr if v is not None), None)
            if isinstance(sample, (pd.Timestamp, datetime, np.datetime64)):
                is_datetime = True

        if is_datetime:
            if _apply_time_window(pd.Series(arr)):
                return

        tf_minutes = _infer_tf_minutes(csv_path)
        if tf_minutes is None:
            raise RuntimeError("ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¨å®šã§ãã¾ã›ã‚“ã€‚ä¾‹: *_M5_*.csv")

        total = arr.size
        bars = _bars_for_mode(tf_minutes, total)
        start_idx = 0 if mode == "ALL" else max(0, total - bars)
        xmin = arr[start_idx]
        xmax = arr[-1]
        if isinstance(xmin, np.generic):
            xmin = xmin.item()
        if isinstance(xmax, np.generic):
            xmax = xmax.item()
        _apply_xlim(xmin, xmax)

class BacktestTab(QtWidgets.QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)

        # === å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ  ===
        self.symbol_edit = QtWidgets.QLineEdit("USDJPY")
        self.tf_combo = QtWidgets.QComboBox(); self.tf_combo.addItems(["M5", "M15", "H1"])

        # å®Ÿè¡Œï¼è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆBacktest / Walk-Forward / Overlayï¼‰
        self.mode_bt = QtWidgets.QRadioButton("Backtest")
        self.mode_wfo = QtWidgets.QRadioButton("Walk-Forward")
        self.mode_overlay = QtWidgets.QRadioButton("Overlay")

        self.mode_bt.setChecked(True)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ Backtest

        self.mode_group = QtWidgets.QButtonGroup(self)
        self.mode_group.addButton(self.mode_bt)
        self.mode_group.addButton(self.mode_wfo)
        self.mode_group.addButton(self.mode_overlay)

        # From/To æ—¥ä»˜ãƒ”ãƒƒã‚«ãƒ¼
        self.start_edit = QtWidgets.QDateEdit(QtCore.QDate(2024, 1, 1))
        self.start_edit.setDisplayFormat("yyyy-MM-dd")
        self.start_edit.setCalendarPopup(True)

        self.end_edit = QtWidgets.QDateEdit(QtCore.QDate.currentDate())
        self.end_edit.setDisplayFormat("yyyy-MM-dd")
        self.end_edit.setCalendarPopup(True)

        self.capital_edit = QtWidgets.QLineEdit("100000")
        self.layout_combo = QtWidgets.QComboBox(); self.layout_combo.addItems(["per-symbol", "flat"])
        self.train_ratio_edit = QtWidgets.QLineEdit("0.7")

        form = QtWidgets.QGridLayout()
        r = 0
        form.addWidget(QtWidgets.QLabel("Symbol"), r, 0); form.addWidget(self.symbol_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("Timeframe"), r, 2); form.addWidget(self.tf_combo, r, 3); r += 1

        # Mode è¡Œï¼ˆãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‚’æ¨ªä¸¦ã³ï¼‰
        mode_layout = QtWidgets.QHBoxLayout()
        mode_layout.addWidget(self.mode_bt)
        mode_layout.addWidget(self.mode_wfo)
        mode_layout.addWidget(self.mode_overlay)
        mode_layout.addStretch(1)

        form.addWidget(QtWidgets.QLabel("Mode"), r, 0)
        form.addLayout(mode_layout, r, 1, 1, 3)
        r += 1

        form.addWidget(QtWidgets.QLabel("Layout"), r, 0); form.addWidget(self.layout_combo, r, 1); r += 1
        form.addWidget(QtWidgets.QLabel("Start"), r, 0); form.addWidget(self.start_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("End"),   r, 2); form.addWidget(self.end_edit,   r, 3); r += 1
        form.addWidget(QtWidgets.QLabel("Initial Capital (JPY)"), r, 0); form.addWidget(self.capital_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("Train Ratio (WFO)"), r, 2); form.addWidget(self.train_ratio_edit, r, 3); r += 1

        # === ãƒœã‚¿ãƒ³ ===
        self.btn_update = QtWidgets.QPushButton("ãƒ‡ãƒ¼ã‚¿ç¢ºèªï¼†æ›´æ–°")
        self.btn_run    = QtWidgets.QPushButton("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        self.btn_popout = QtWidgets.QPushButton("åˆ¥ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ã§è¡¨ç¤º")
        self.btn_export = QtWidgets.QPushButton("çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ(JSON)")
        self.btn_savepng= QtWidgets.QPushButton("ã‚°ãƒ©ãƒ•ã‚’PNGä¿å­˜")
        self.btn_heatmap= QtWidgets.QPushButton("æœˆæ¬¡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")

        btns = QtWidgets.QHBoxLayout()
        btns.addWidget(self.btn_update); btns.addWidget(self.btn_run)
        btns.addWidget(self.btn_popout); btns.addWidget(self.btn_heatmap)
        btns.addStretch(1)
        btns.addWidget(self.btn_savepng); btns.addWidget(self.btn_export)

        # === æœŸé–“ã‚¸ãƒ£ãƒ³ãƒ—ï¼ˆ1W / 1M / ALLï¼‰ ===
        range_box = QtWidgets.QHBoxLayout()
        range_box.setContentsMargins(0, 0, 0, 0)
        range_box.setSpacing(6)
        range_box.addWidget(QtWidgets.QLabel("æœŸé–“:"))

        self.btn_1w  = QtWidgets.QPushButton("1W")
        self.btn_1m  = QtWidgets.QPushButton("1M")
        self.btn_all = QtWidgets.QPushButton("ALL")
        for b in (self.btn_1w, self.btn_1m, self.btn_all):
            b.setFixedHeight(26)

        self.btn_1w.clicked.connect(partial(self._on_range_jump, "1W"))
        self.btn_1m.clicked.connect(partial(self._on_range_jump, "1M"))
        self.btn_all.clicked.connect(partial(self._on_range_jump, "ALL"))

        range_box.addWidget(self.btn_1w)
        range_box.addWidget(self.btn_1m)
        range_box.addWidget(self.btn_all)
        range_box.addStretch(1)

        # === ãƒ—ãƒ­ãƒƒãƒˆãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ===
        self.fig = Figure(figsize=(6,3), constrained_layout=True)
        self.canvas = Canvas(self.fig)
        self.label_meta = QtWidgets.QLabel("æœªå®Ÿè¡Œ"); self.label_meta.setWordWrap(True)

        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ï¼ˆactive_model.jsonï¼‰
        self.model_info = QtWidgets.QLabel("model: -")
        self.model_info.setStyleSheet("color: #666;")

        # å‡ºåŠ›ãƒ‘ã‚¹
        self.path_edit = QtWidgets.QLineEdit(""); self.path_edit.setReadOnly(True)
        self.btn_open  = QtWidgets.QPushButton("CSVã‚’é–‹ãâ€¦")
        path_line = QtWidgets.QHBoxLayout()
        path_line.addWidget(QtWidgets.QLabel("Equity CSV:")); path_line.addWidget(self.path_edit, 1); path_line.addWidget(self.btn_open)

        # é€²æ—ãƒ­ã‚°
        self.progress_box = QtWidgets.QPlainTextEdit(); self.progress_box.setReadOnly(True); self.progress_box.setMaximumBlockCount(1000)

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨
        self.table = QtWidgets.QTableWidget(0, 2)
        self.table.setHorizontalHeaderLabels(["Metric", "Value"])
        self.table.horizontalHeader().setStretchLastSection(True)
        self.table.setMinimumHeight(140)

        # Walk-Forward Stats ãƒ‘ãƒãƒ«ï¼ˆå¾Œã§ metrics_wfo.json ã‹ã‚‰æ›´æ–°ï¼‰
        self.wfo_pf_label = QtWidgets.QLabel("WFO PF: -")
        self.wfo_winrate_label = QtWidgets.QLabel("WFO Win%: -")
        self.wfo_trades_label = QtWidgets.QLabel("WFO Trades: -")
        self.wfo_maxdd_label = QtWidgets.QLabel("WFO MaxDD: -")

        wfo_stats_layout = QtWidgets.QVBoxLayout()
        wfo_stats_layout.addWidget(self.wfo_pf_label)
        wfo_stats_layout.addWidget(self.wfo_winrate_label)
        wfo_stats_layout.addWidget(self.wfo_trades_label)
        wfo_stats_layout.addWidget(self.wfo_maxdd_label)

        self.wfo_stats_group = QtWidgets.QGroupBox("Walk-Forward Stats")
        self.wfo_stats_group.setLayout(wfo_stats_layout)

        # å…¨ä½“é…ç½®
        lay = QtWidgets.QVBoxLayout(self)
        lay.addLayout(form)
        lay.addLayout(btns)
        lay.addLayout(range_box)          # â† æœŸé–“ãƒœã‚¿ãƒ³åˆ—ã‚’è¿½åŠ 
        lay.addWidget(self.canvas, 1)
        lay.addWidget(self.label_meta)
        lay.addLayout(path_line)
        lay.addWidget(QtWidgets.QLabel("ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")); lay.addWidget(self.model_info)
        lay.addWidget(QtWidgets.QLabel("ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")); lay.addWidget(self.table)
        lay.addWidget(QtWidgets.QLabel("é€²æ—ãƒ­ã‚°:")); lay.addWidget(self.progress_box, 2)
        lay.addWidget(self.wfo_stats_group)

        # å†…éƒ¨çŠ¶æ…‹
        self.proc: QProcess | None = None
        self._last_plot_kind = None   # "equity" or "price"
        self._last_plot_data = None
        self._last_plot_note = ""
        self._pop = None              # PlotWindow
        self.plot_window: PlotWindow | None = None
        self._wfo_train_df: Optional[pd.DataFrame] = None
        self._wfo_test_df: Optional[pd.DataFrame] = None
        self._wfo_overlay_lines: list = []
        self._last_monthly_returns: Path | None = None

        # ã‚·ã‚°ãƒŠãƒ«
        self.btn_update.clicked.connect(self._update_data)
        self.btn_run.clicked.connect(self._run_test)
        self.btn_open.clicked.connect(self._pick_file)
        self.btn_popout.clicked.connect(self._pop_out)
        self.btn_export.clicked.connect(self._export_result_json)
        self.btn_savepng.clicked.connect(self._save_png)
        self.btn_heatmap.clicked.connect(self._show_heatmap)
        # ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã§ãƒãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        def _on_canvas_dbl(ev):
            if ev.dblclick: self._pop_out()
        self.canvas.mpl_connect("button_press_event", _on_canvas_dbl)

        # åˆå›ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
        self._load_model_info()
        # ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´æ™‚ã« Walk-Forward å›ºæœ‰ã®å…¥åŠ›ã‚’ ON/OFF
        self.mode_bt.toggled.connect(self._on_mode_changed)
        self.mode_wfo.toggled.connect(self._on_mode_changed)
        self.mode_overlay.toggled.connect(self._on_mode_changed)

        # åˆæœŸçŠ¶æ…‹ã®åæ˜ 
        self._on_mode_changed()

    # ------------------ ãƒ˜ãƒ«ãƒ‘ ------------------
    def _append_progress(self, text: str):
        self.progress_box.appendPlainText(text.rstrip())

    def _on_mode_changed(self, checked: bool = False):
        # Walk-Forward ãƒ©ã‚¸ã‚ªãŒ ON ã®ã¨ãã ã‘ train_ratio ã‚’æœ‰åŠ¹ã«ã™ã‚‹
        is_wfo = self.mode_wfo.isChecked()
        self.train_ratio_edit.setEnabled(is_wfo)

    def _current_mode_text(self) -> str:
        """UI ä¸Šã®ãƒ¢ãƒ¼ãƒ‰æ–‡å­—åˆ—ã‚’è¿”ã™ï¼ˆBacktest / Walk-Forward / Overlayï¼‰ã€‚"""
        if self.mode_wfo.isChecked():
            return "Walk-Forward"
        if self.mode_overlay.isChecked():
            return "Overlay"
        return "Backtest"

    def _find_latest_wfo_dir(self) -> Optional[pathlib.Path]:
        base = pathlib.Path("logs") / "backtest"
        if not base.exists():
            return None

        candidates = list(base.rglob("metrics_wfo.json"))
        if not candidates:
            return None

        latest = max(candidates, key=lambda p: p.stat().st_mtime)
        return latest.parent

    def _load_latest_wfo_data(self) -> Optional[Dict[str, object]]:
        wfo_dir = self._find_latest_wfo_dir()
        if wfo_dir is None:
            print("[gui] no WFO dir found under logs/backtest")
            return None

        metrics_path = wfo_dir / "metrics_wfo.json"
        train_path = wfo_dir / "equity_train.csv"
        test_path = wfo_dir / "equity_test.csv"

        if not (metrics_path.exists() and train_path.exists() and test_path.exists()):
            print("[gui] missing WFO files:", metrics_path, train_path, test_path)
            return None

        with metrics_path.open("r", encoding="utf-8") as f:
            metrics = json.load(f)

        df_train = pd.read_csv(train_path)
        df_test = pd.read_csv(test_path)

        return {"metrics": metrics, "train": df_train, "test": df_test}

    def _update_wfo_stats_panel(self, metrics: Dict[str, object]) -> None:
        test = metrics.get("test") or {}
        pf = test.get("profit_factor") or test.get("pf")
        win_rate = test.get("win_rate")
        trades = test.get("trades") or test.get("num_trades")
        max_dd = test.get("max_drawdown") or test.get("max_dd")

        if isinstance(pf, (int, float)):
            self.wfo_pf_label.setText(f"WFO PF: {pf:.2f}")
        else:
            self.wfo_pf_label.setText("WFO PF: -")

        if isinstance(win_rate, (int, float)):
            val = win_rate * 100 if win_rate <= 1 else win_rate
            self.wfo_winrate_label.setText(f"WFO Win%: {val:.1f}%")
        else:
            self.wfo_winrate_label.setText("WFO Win%: -")

        self.wfo_trades_label.setText(f"WFO Trades: {trades}" if trades is not None else "WFO Trades: -")

        if isinstance(max_dd, (int, float)):
            self.wfo_maxdd_label.setText(f"WFO MaxDD: {max_dd:.2f}")
        else:
            self.wfo_maxdd_label.setText("WFO MaxDD: -")

    def _overlay_wfo_equity(
        self,
        df_train: Optional[pd.DataFrame],
        df_test: Optional[pd.DataFrame],
    ) -> None:
        """Overlay Train/Test equity lines onto the current plot axes."""
        lines = getattr(self, "_wfo_overlay_lines", [])
        for line in lines:
            try:
                line.remove()
            except Exception:
                pass
        self._wfo_overlay_lines = []

        if not self.fig.axes:
            return
        ax = self.fig.axes[0]

        def _prepare(df: Optional[pd.DataFrame]) -> Optional[Tuple[pd.Series, pd.Series]]:
            if df is None or "equity" not in df.columns:
                return None
            y = pd.to_numeric(df["equity"], errors="coerce")
            if "time" in df.columns:
                x = pd.to_datetime(df["time"], errors="coerce")
            else:
                x = pd.Series(df.index)
            mask = x.notna() & y.notna()
            if not mask.any():
                return None
            return x[mask], y[mask]

        train_data = _prepare(df_train)
        if train_data is not None:
            line, = ax.plot(
                train_data[0], train_data[1],
                linestyle="--", linewidth=1.0, color="tab:blue",
            )
            self._wfo_overlay_lines.append(line)

        test_data = _prepare(df_test)
        if test_data is not None:
            line, = ax.plot(
                test_data[0], test_data[1],
                linestyle="-", linewidth=2.2, color="tab:red",
            )
            self._wfo_overlay_lines.append(line)

        self.canvas.draw_idle()

    class _WFOResult(QtCore.QObject):
        """
        Walk-Forward æ¤œè¨¼ã®çµæœã‚»ãƒƒãƒˆã‚’ã¾ã¨ã‚ã¦æŒã¤ã ã‘ã®å°ã•ãªå…¥ã‚Œç‰©ã€‚
        report_json: logs/retrain/report_*.json ã®ä¸­èº«
        equity_train: equity_train_*.csv â†’ DataFrame
        equity_test:  equity_test_*.csv â†’ DataFrame
        run_id: "report_XXXX.json" ã® XXXX éƒ¨åˆ†
        """

        def __init__(
            self,
            report_json: Dict[str, Any],
            equity_train: Optional[pd.DataFrame],
            equity_test: Optional[pd.DataFrame],
            run_id: str,
            parent: Optional[QtCore.QObject] = None,
        ) -> None:
            super().__init__(parent)
            self.report_json = report_json
            self.equity_train = equity_train
            self.equity_test = equity_test
            self.run_id = run_id

    # ---------------------------------------------------------
    # ã“ã“ã‹ã‚‰ãŒå®Ÿéš›ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # ---------------------------------------------------------

    def _find_latest_wfo_files(self) -> Optional["_WFOResult"]:
        """
        logs/retrain/ é…ä¸‹ã‹ã‚‰æœ€æ–°ã® report_*.json ã‚’æ¢ã—ã€
        å¯¾å¿œã™ã‚‹ equity_train_*.csv / equity_test_*.csv ã‚’èª­ã¿è¾¼ã‚“ã§è¿”ã™ã€‚
        è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° None ã‚’è¿”ã™ã€‚
        """
        base_dir = Path("logs") / "retrain"
        if not base_dir.exists():
            # ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€è‡ªä½“ãŒãªã„å ´åˆ
            print("[WFO] logs/retrain ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return None

        # report_*.json ã‚’æ›´æ–°æ—¥æ™‚é †ã«ã‚½ãƒ¼ãƒˆ
        report_files = sorted(
            base_dir.glob("report_*.json"),
            key=lambda p: p.stat().st_mtime,
        )
        if not report_files:
            print("[WFO] report_*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None

        latest_report = report_files[-1]
        stem = latest_report.stem  # ä¾‹: "report_1762505757473982"
        parts = stem.split("_", 1)
        run_id = parts[1] if len(parts) == 2 else ""

        try:
            with latest_report.open("r", encoding="utf-8") as f:
                report_json = json.load(f)
        except Exception as e:  # noqa: BLE001
            print(f"[WFO] report JSON ã®èª­è¾¼ã«å¤±æ•—: {latest_report} ({e})")
            return None

        def _load_equity_csv(prefix: str) -> Optional[pd.DataFrame]:
            # run_id ãŒã‚ã‚‹ã¨ãã¯ equity_train_{id}.csv ã‚’å„ªå…ˆ
            # ãªã‘ã‚Œã° equity_train.csv ã‚’è¦‹ã‚‹
            if run_id:
                candidate = base_dir / f"{prefix}_{run_id}.csv"
                if candidate.exists():
                    path = candidate
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    path = base_dir / f"{prefix}.csv"
            else:
                path = base_dir / f"{prefix}.csv"

            if not path.exists():
                print(f"[WFO] {path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                return None

            try:
                df = pd.read_csv(path)
            except Exception as e:  # noqa: BLE001
                print(f"[WFO] {path} ã®èª­è¾¼ã«å¤±æ•— ({e})")
                return None

            # time ã‚«ãƒ©ãƒ ãŒã‚ã‚Œã° datetime ã«ã—ã¦ãŠãï¼ˆã‚°ãƒ©ãƒ•ç”¨ï¼‰
            if "time" in df.columns:
                df["time"] = pd.to_datetime(df["time"], errors="coerce")

            return df

        equity_train = _load_equity_csv("equity_train")
        equity_test = _load_equity_csv("equity_test")

        return self._WFOResult(
            report_json=report_json,
            equity_train=equity_train,
            equity_test=equity_test,
            run_id=run_id,
            parent=self,
        )

    def _debug_print_wfo_summary(self, wfo: "_WFOResult") -> None:
        """
        ã¨ã‚Šã‚ãˆãšã€Œã¡ã‚ƒã‚“ã¨èª­ã‚ãŸã‹ã€ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€
        ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã–ã£ãã‚Šã‚µãƒãƒªã‚’åãã€‚
        ã‚ã¨ã§ã“ã“ã‹ã‚‰ GUI ã®ãƒ©ãƒ™ãƒ«æ›´æ–°ãªã©ã«å·®ã—æ›¿ãˆã‚Œã°OKã€‚
        """
        print("========== [WFO Latest Result] ==========")
        print(f"run_id       : {wfo.run_id}")
        print(f"report keys  : {list(wfo.report_json.keys())}")

        train_len = len(wfo.equity_train) if wfo.equity_train is not None else 0
        test_len = len(wfo.equity_test) if wfo.equity_test is not None else 0
        print(f"equity_train : {train_len} rows")
        print(f"equity_test  : {test_len} rows")

        # ã‚ˆãã‚ã‚Šãã†ãªã‚­ãƒ¼ãŒã‚ã‚Œã°ãƒãƒ©è¦‹ã›ï¼ˆãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã§OKï¼‰
        for k in ("symbol", "timeframe", "train_period", "test_period", "pf_train", "pf_test", "win_rate_train", "win_rate_test"):
            if k in wfo.report_json:
                print(f"{k:>15}: {wfo.report_json[k]}")

        print("=========================================")

    def _load_model_info(self):
        p = PROJECT_ROOT / "active_model.json"
        if p.exists():
            try:
                j = json.loads(p.read_text(encoding="utf-8"))
                name = j.get("model_name") or j.get("name") or "-"
                th   = j.get("best_threshold") or j.get("threshold") or "-"
                fh   = j.get("features_hash") or "-"
                self.model_info.setText(f"name={name}  best_threshold={th}  features_hash={fh}")
            except Exception as e:
                self.model_info.setText(f"(model info error: {e})")
        else:
            self.model_info.setText("(active_model.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)")

    # ------------------ ãƒ‡ãƒ¼ã‚¿æ›´æ–° ------------------
    def _update_data(self):
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()
        mode_text = self._current_mode_text()
        layout = self.layout_combo.currentText()

        # start/end ã‚’æ—¥ä»˜ãƒ”ãƒƒã‚«ãƒ¼ã‹ã‚‰ "YYYY-MM-DD" å½¢å¼ã§å–å¾—
        if isinstance(self.start_edit, QtWidgets.QDateEdit):
            start = self.start_edit.date().toString("yyyy-MM-dd")
        else:
            start = self.start_edit.text().strip()

        if isinstance(self.end_edit, QtWidgets.QDateEdit):
            end = self.end_edit.date().toString("yyyy-MM-dd")
        else:
            end = self.end_edit.text().strip()

        # ã“ã“ã§ ensure_data ã®ãƒ­ã‚°ã‚’å‡ºã™
        self._append_progress(
            f"[gui] ensure_data sym={sym} tf={tf} start={start} end={end} layout={layout}"
        )

        try:
            csv = ensure_data(sym, tf, start, end, env="laptop", layout=layout)
            self._load_plot(csv)  # OHLCVãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            self.label_meta.setText(f"ãƒ‡ãƒ¼ã‚¿OK: {csv}")
        except Exception as e:
            self.label_meta.setText(f"ãƒ‡ãƒ¼ã‚¿æ›´æ–°å¤±æ•—: {e}")
            self._append_progress(f"[update] error: {e}")

    # ------------------ å®Ÿè¡Œï¼ˆQProcessï¼‰ ------------------
    def _run_test(self):
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()

        # ãƒ¢ãƒ¼ãƒ‰æ–‡å­—åˆ—ï¼ˆBacktest / Walk-Forward / Overlayï¼‰
        mode_text = self._current_mode_text()
        # CLI ã«æ¸¡ã™ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°ï¼ˆtools.backtest_run å´ã®æƒ³å®šï¼‰
        mode = "wfo" if mode_text == "Walk-Forward" else "bt"

        # æ—¥ä»˜ã¯ QDateEdit ãªã‚‰ date() ã‹ã‚‰ã€ãã†ã§ãªã‘ã‚Œã° text() ã‹ã‚‰
        if isinstance(self.start_edit, QtWidgets.QDateEdit):
            start = self.start_edit.date().toString("yyyy-MM-dd")
        else:
            start = self.start_edit.text().strip()

        if isinstance(self.end_edit, QtWidgets.QDateEdit):
            end = self.end_edit.date().toString("yyyy-MM-dd")
        else:
            end = self.end_edit.text().strip()

        capital = self.capital_edit.text().strip()
        layout  = self.layout_combo.currentText()
        train_ratio = self.train_ratio_edit.text().strip() or "0.7"

        # å®Ÿè¡Œå‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿è¨¼
        try:
            csv = ensure_data(sym, tf, start, end, env="laptop", layout=layout)
        except Exception as e:
            self.label_meta.setText(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {e}")
            self._append_progress(f"[ensure_data] error before run: {e}")
            return

        # æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚ã‚Œã°æ®ºã™
        if self.proc:
            self.proc.kill()
            self.proc = None

        # â€œ-m tools.backtest_runâ€ ã§ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
        args = [
            "-m", "tools.backtest_run",
            "--csv", str(csv),
            "--start-date", start,
            "--end-date", end,
            "--capital", capital,
            "--mode", mode,
            "--symbol", sym,
            "--timeframe", tf,
            "--layout", layout,
        ]
        if mode == "wfo":
            args += ["--train-ratio", train_ratio]

        self._append_progress("[gui] run: python " + " ".join(args))

        self.proc = QProcess(self)
        self.proc.setProgram(sys.executable)
        self.proc.setArguments(args)
        self.proc.setWorkingDirectory(str(PROJECT_ROOT))  # é‡è¦ï¼šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã‚’CWDã«

        self.proc.setProcessChannelMode(QProcess.ProcessChannelMode.MergedChannels)
        self.proc.readyReadStandardOutput.connect(self._on_proc_output)
        self.proc.finished.connect(
            lambda code, status: self._on_proc_finished(code, status, sym, tf, mode)
        )
        self.label_meta.setText("å®Ÿè¡Œä¸­â€¦")
        self.proc.start()

        # Walk-Forward ã®å ´åˆã¯ã€å­¦ç¿’å´WFOã®æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚µãƒãƒªè¡¨ç¤º
        if mode_text == "Walk-Forward":
            try:
                wfo = self._find_latest_wfo_files()
                if wfo is None:
                    print("[WFO] çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                else:
                    self._debug_print_wfo_summary(wfo)
            except Exception as e:  # å¿µã®ãŸã‚ã“ã“ã§ä¾‹å¤–ã‚’æ½°ã—ã¦ãŠãã¨GUIã”ã¨è½ã¡ãªã„
                print(f"[WFO] summary error: {e}")

    def _on_proc_output(self):
        if not self.proc: return
        out = bytes(self.proc.readAllStandardOutput()).decode("utf-8", errors="replace")
        if out:
            for line in out.splitlines():
                self._append_progress(line)

    def _on_proc_finished(self, code: int, status: QtCore.QProcess.ExitStatus, sym: str, tf: str, mode: str):
        if code != 0:
            self.label_meta.setText(f"å¤±æ•—(code={code})")
            self._append_progress(f"[gui] process failed code={code}")
            return

        out_dir = PROJECT_ROOT / "logs" / "backtest" / sym / tf
        out_csv = out_dir / "equity_curve.csv"
        self.path_edit.setText(str(out_csv))
        if not out_csv.exists():
            self.label_meta.setText(f"å‡ºåŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {out_csv}")
            self._append_progress(f"[gui] missing file: {out_csv}")
            return

        self._load_plot(out_csv)  # Equityæç”»

        # ã¾ãšã¯æ¯å›ã‚¯ãƒªã‚¢ã—ã¦ãŠã
        self._wfo_train_df = None
        self._wfo_test_df = None

        if mode == "wfo":
            wfo = self._load_latest_wfo_data()
            if wfo:
                self._update_wfo_stats_panel(wfo["metrics"])
                self._overlay_wfo_equity(wfo["train"], wfo["test"])
                # æœ€æ–°ã® overlay ãƒ‡ãƒ¼ã‚¿ã‚’å±æ€§ã¨ã—ã¦ä¿æŒã—ã¦ãŠã
                self._wfo_train_df = wfo["train"]
                self._wfo_test_df = wfo["test"]
            else:
                # WFOã ã‘ã©çµæœèª­ã‚ãªã‹ã£ãŸ â†’ ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ overlay ã‚‚æ¶ˆã™
                self._overlay_wfo_equity(None, None)
        else:
            # Backtestãƒ¢ãƒ¼ãƒ‰ãªã© â†’ overlay ã¯å…¨éƒ¨ã‚¯ãƒªã‚¢
            self._overlay_wfo_equity(None, None)

        if hasattr(self, "plot_window") and self.plot_window is not None:
            try:
                self.plot_window.overlay_wfo_equity(self._wfo_train_df, self._wfo_test_df)
            except Exception as e:
                print("[gui] WFO overlay on new window failed:", e)

        metrics_path = out_dir / ("metrics.json" if mode=="bt" else "metrics_wfo.json")
        self._load_metrics(metrics_path)

        # æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ä¿å­˜ãƒ‘ã‚¹ã‚’æ§ãˆã‚‹
        self._last_monthly_returns = out_dir / ("monthly_returns.csv" if mode=="bt" else "monthly_returns_test.csv")

        self.label_meta.setText("å®Œäº†")

    # ------------------ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£UI ------------------
    def _pick_file(self):
        p, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Equity CSV", str(PROJECT_ROOT), "CSV (*.csv)")
        if p:
            self.path_edit.setText(p)
            self._load_plot(p)

    def _save_png(self):
        p, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Save PNG", str(PROJECT_ROOT / "logs"), "PNG (*.png)")
        if not p: return
        try:
            self.fig.savefig(p, dpi=150)
            self._append_progress(f"[gui] saved png: {p}")
        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "ä¿å­˜å¤±æ•—", str(e))

    def _export_result_json(self):
        # ç”»é¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‹ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ã¾ã¨ã‚ã¦ä¿å­˜
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()
        mode = self._current_mode_text()
        payload = {
            "params": {
                "symbol": sym, "timeframe": tf,
                "mode": mode,
                "start": self.start_edit.text().strip(),
                "end": self.end_edit.text().strip(),
                "capital": float(self.capital_edit.text().strip() or 0),
                "layout": self.layout_combo.currentText(),
                "train_ratio": float(self.train_ratio_edit.text().strip() or 0.7)
            },
            "model": self.model_info.text(),
        }
        # metrics è¡¨ã‹ã‚‰ã‚‚åé›†
        for r in range(self.table.rowCount()):
            k = self.table.item(r,0).text()
            v = self.table.item(r,1).text()
            payload.setdefault("metrics", {})[k] = v

        p, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Export JSON", str(PROJECT_ROOT / "logs"), "JSON (*.json)")
        if not p: return
        Path(p).write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        self._append_progress(f"[gui] exported: {p}")

    def _show_heatmap(self):
        if not self._last_monthly_returns or not self._last_monthly_returns.exists():
            QtWidgets.QMessageBox.information(
                self,
                "æƒ…å ±",
                "æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ãŒã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )
            return
        try:
            df = pd.read_csv(self._last_monthly_returns)
        except Exception as e:
            QtWidgets.QMessageBox.critical(
                self,
                "ã‚¨ãƒ©ãƒ¼",
                f"æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{e}"
            )
            self._append_progress(f"[gui] heatmap load error: {e}")
            return

        if self._pop is None:
            self._pop = PlotWindow(self)
        self.plot_window = self._pop
        self._pop.show(); self._pop.raise_(); self._pop.activateWindow()
        note = self._last_monthly_returns.name
        try:
            self._pop.plot_heatmap(df, note)
        except Exception as e:
            QtWidgets.QMessageBox.critical(
                self,
                "ã‚¨ãƒ©ãƒ¼",
                f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n{e}"
            )
            self._append_progress(f"[gui] heatmap plot error: {e}")

    def _pop_out(self):
        if self._last_plot_kind is None or self._last_plot_data is None:
            self._append_progress("[gui] popout: ç›´è¿‘ã®æç”»ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            QtWidgets.QMessageBox.information(self, "æƒ…å ±", "ã¾ã æç”»ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«ã€Œãƒ‡ãƒ¼ã‚¿ç¢ºèªï¼†æ›´æ–°ã€ã¾ãŸã¯ã€Œãƒ†ã‚¹ãƒˆå®Ÿè¡Œã€ã‚’ã—ã¦ãã ã•ã„ã€‚")
            return
        if self._pop is None:
            self._pop = PlotWindow(self)
        self.plot_window = self._pop
        self._pop.show(); self._pop.raise_(); self._pop.activateWindow()
        if self._last_plot_kind == "equity":
            csvp = self._last_plot_data
            self._pop.plot_equity_csv(csvp)

            if self._wfo_train_df is not None or self._wfo_test_df is not None:
                try:
                    self._pop.overlay_wfo_equity(self._wfo_train_df, self._wfo_test_df)
                except Exception as e:
                    self._append_progress(f"[pop] WFO overlay failed: {e}")
        else:
            self._pop.plot_price_preview(self._last_plot_data, self._last_plot_note)

    # ------------------ æç”»ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ------------------
    def _load_plot(self, path_or_csv):
        p = Path(path_or_csv)
        csv_path = str(p)
        ##
        if "equity_curve.csv" in csv_path:
            try:
                plot_equity_with_markers_to_figure(self.fig, csv_path, note=p.name)
                self.canvas.draw_idle()
                self._append_progress(f"[gui] plotted EQUITY (markers) from {p.name}")
                self._last_plot_kind = "equity"
                self._last_plot_data = str(p)  # CSVãƒ‘ã‚¹ã‚’è¨˜æ†¶ï¼ˆæœŸé–“ã‚¸ãƒ£ãƒ³ãƒ—ã§åˆ©ç”¨ï¼‰
                self._last_plot_note = p.name
            except Exception as e:
                self.label_meta.setText(f"æç”»å¤±æ•—: {e}")
                self._append_progress(f"[gui] plot error: {e}")
            return

        ##
        try:
            df = pd.read_csv(p)
            self.fig.clear()
            ax = self.fig.add_subplot(111)

            if "close" in df.columns:
                price = pd.to_numeric(df["close"], errors="coerce").ffill()
                if len(price) == 0 or price.iloc[0] == 0:
                    raise ValueError("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ 'close' åˆ—ãŒç©ºã§ã™ã€‚")
                norm = price / price.iloc[0] * 100.0
                ax.plot(norm.values, label="Price (close, =100@start)")
                ax.set_title("Price Preview (from OHLCV)")
                ax.set_ylabel("index (=100@start)")
                self._append_progress(f"[gui] plotted PRICE {len(df)} rows from {p.name}")
                self._last_plot_kind = "price"
                self._last_plot_data = str(p)
                self._last_plot_note = p.name
            else:
                raise ValueError(f"CSVã« 'close' åˆ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚columns={list(df.columns)}")

            ax.set_xlabel("bars")
            ax.legend()
            #self.fig.tight_layout()
            self.canvas.draw_idle()

        except Exception as e:
            self.label_meta.setText(f"æç”»å¤±æ•—: {e}")
            self._append_progress(f"[gui] plot error: {e}")

    def _load_metrics(self, metrics_path: Path):
        self.table.setRowCount(0)
        try:
            txt = Path(metrics_path).read_text(encoding="utf-8")
            m = json.loads(txt)
            # WFO ã®å ´åˆã¯ {train:{}, test:{}} å½¢å¼
            if "train" in m and "test" in m:
                # test å´ã‚’è¡¨ã«å‡ºã™ã€train ã¯ãƒ­ã‚°ã«
                ts = m["test"]; tr = m["train"]
                msg = (f"[WFO] Train ret={tr['total_return']*100:.2f}% mdd={tr['max_drawdown']*100:.2f}% sharpeâ‰ˆ{tr['sharpe_like']:.2f} "
                       f"| Test ret={ts['total_return']*100:.2f}% mdd={ts['max_drawdown']*100:.2f}% sharpeâ‰ˆ{ts['sharpe_like']:.2f}")
                self.label_meta.setText(msg)
                rows = ts
            else:
                rows = m
                self.label_meta.setText(
                    f"[BT] ret={m['total_return']*100:.2f}% mdd={m['max_drawdown']*100:.2f}% sharpeâ‰ˆ{m['sharpe_like']:.2f} bars={m['bars']}"
                )

            # è¡¨ã¸
            for k in [
                "start_equity","end_equity","total_return","max_drawdown","max_dd_days","sharpe_like","bars",
                "trades","win_rate","avg_pnl","profit_factor","avg_holding_bars","avg_holding_days",
                "max_consec_win","max_consec_loss"
            ]:
                if k in rows:
                    r = self.table.rowCount()
                    self.table.insertRow(r)
                    self.table.setItem(r, 0, QtWidgets.QTableWidgetItem(k))
                    val = rows[k]
                    if isinstance(val, float):
                        if "return" in k or "drawdown" in k or k=="win_rate":
                            val = f"{val*100:.2f}%"
                        else:
                            val = f"{val:.4g}"
                    self.table.setItem(r, 1, QtWidgets.QTableWidgetItem(str(val)))

            self._append_progress("[gui] metrics loaded")
        except Exception as e:
            self.label_meta.setText(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­è¾¼å¤±æ•—: {e}")
            self._append_progress(f"[gui] metrics error: {e}")

    # ------------------ æœŸé–“ã‚¸ãƒ£ãƒ³ãƒ—ï¼ˆæœ¬ä½“ï¼‰ ------------------
    def _on_range_jump(self, mode: str) -> None:
        """
        Backtestã‚¿ãƒ–ã®ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æç”»ã¨ã€ãƒãƒƒãƒ—ã‚¢ã‚¦ãƒˆæ¸ˆã¿ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ä¸¡æ–¹ã«æœŸé–“ã‚¸ãƒ£ãƒ³ãƒ—ã‚’é©ç”¨ã™ã‚‹ã€‚
        - ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³: equity ã ã‘ã§ãªã price ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«ã‚‚å¯¾å¿œã€‚
        - ãƒãƒƒãƒ—ã‚¢ã‚¦ãƒˆ: PlotWindow.jump_range() ã«å§”è­²ã€‚
        """
        try:
            # 1) ãƒãƒƒãƒ—ã‚¢ã‚¦ãƒˆå´ï¼ˆèµ·å‹•æ¸ˆã¿ãªã‚‰ï¼‰
            if self._pop is not None:
                try:
                    self._pop.jump_range(mode)
                except Exception as e:
                    self._append_progress(f"[pop] jump_range warn: {e}")

            # 2) ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å´ï¼šequity
            if self._last_plot_kind == "equity" and isinstance(self._last_plot_data, str):
                csvp = Path(self._last_plot_data)
                if csvp.exists():
                    df = pd.read_csv(csvp)
                    if "time" in df.columns:
                        t = pd.to_datetime(df["time"], errors="coerce").dropna()
                        if len(t) >= 2:
                            tmax = t.iloc[-1]
                            if mode == "ALL":
                                tmin = t.iloc[0]
                            elif mode == "1W":
                                tmin = tmax - pd.Timedelta(days=7)
                            elif mode == "1M":
                                tmin = tmax - pd.Timedelta(days=31)
                            else:
                                raise ValueError(f"Unknown mode: {mode}")
                            tmin = max(t.iloc[0], tmin)
                            if self.fig.axes:
                                ax = self.fig.axes[0]
                                ax.set_xlim(tmin.to_pydatetime(), tmax.to_pydatetime())
                                self.canvas.draw_idle()
                                if mode == "ALL" and isinstance(self.start_edit, QtWidgets.QDateEdit):
                                    first = t.iloc[0]
                                    last = t.iloc[-1]
                                    self.start_edit.setDate(
                                        QtCore.QDate(first.year, first.month, first.day)
                                    )
                                    self.end_edit.setDate(
                                        QtCore.QDate(last.year, last.month, last.day)
                                    )
                                    self._append_progress(
                                        f"[gui] date pickers reset to ALL (equity): {first.date()} .. {last.date()}"
                                    )
                        else:
                            raise RuntimeError("ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã®æ™‚ç³»åˆ—ãŒçŸ­ã™ãã¾ã™ã€‚")
                    else:
                        raise RuntimeError("equity_curve.csv ã« 'time' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    raise RuntimeError("æœ€å¾Œã«æç”»ã—ãŸCSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

            # 3) ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å´ï¼špriceï¼ˆtimeåˆ—ã‚ã‚Šãªã‚‰æ—¥ä»˜ã€ãªã‘ã‚Œã°ãƒãƒ¼æ•°ã§ã‚ºãƒ¼ãƒ ï¼‰
            elif self._last_plot_kind == "price" and isinstance(self._last_plot_data, str):
                csvp = Path(self._last_plot_data)
                if not csvp.exists():
                    raise RuntimeError("æœ€å¾Œã«æç”»ã—ãŸä¾¡æ ¼CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                df = pd.read_csv(csvp)

                if "time" in df.columns:
                    t = pd.to_datetime(df["time"], errors="coerce").dropna()
                    if len(t) < 2:
                        raise RuntimeError("ä¾¡æ ¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ™‚ç³»åˆ—ãŒçŸ­ã™ãã¾ã™ã€‚")
                    tmax = t.iloc[-1]
                    if mode == "ALL":
                        tmin = t.iloc[0]
                    elif mode == "1W":
                        tmin = tmax - pd.Timedelta(days=7)
                    elif mode == "1M":
                        tmin = tmax - pd.Timedelta(days=31)
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    tmin = max(t.iloc[0], tmin)
                    if self.fig.axes:
                        ax = self.fig.axes[0]
                        ax.set_xlim(tmin.to_pydatetime(), tmax.to_pydatetime())
                        self.canvas.draw_idle()
                        if mode == "ALL" and isinstance(self.start_edit, QtWidgets.QDateEdit) and "time" in df.columns:
                            try:
                                first = t.iloc[0]
                                last = t.iloc[-1]
                                self.start_edit.setDate(
                                    QtCore.QDate(first.year, first.month, first.day)
                                )
                                self.end_edit.setDate(
                                    QtCore.QDate(last.year, last.month, last.day)
                                )
                                self._append_progress(
                                    f"[gui] date pickers reset to ALL (price): {first.date()} .. {last.date()}"
                                )
                            except Exception as e:
                                self._append_progress(f"[gui] failed to sync date pickers on price: {e}")
                else:
                    # xè»¸ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0..n-1ï¼‰ã®å ´åˆã¯ã€Œãƒãƒ¼æ•°ã€ã§è¿‘ä¼¼
                    n = len(df)
                    if n < 2:
                        raise RuntimeError("ä¾¡æ ¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è¡Œæ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                    tf = (self.tf_combo.currentText() or "M5").upper()
                    tf_min = {"M1":1, "M5":5, "M15":15, "M30":30, "H1":60, "H4":240, "D1":1440}.get(tf, 5)
                    def bars_for_days(days: int) -> int:
                        return int(days * 24 * 60 / tf_min)
                    if mode == "ALL":
                        start_idx = 0
                    elif mode == "1W":
                        start_idx = max(0, n - bars_for_days(7))
                    elif mode == "1M":
                        start_idx = max(0, n - bars_for_days(31))
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    if self.fig.axes:
                        ax = self.fig.axes[0]
                        ax.set_xlim(start_idx, n - 1)
                        self.canvas.draw_idle()

            else:
                # ãã‚Œä»¥å¤–ï¼ˆæœªæç”»ãªã©ï¼‰
                self._append_progress("[gui] æœŸé–“ã‚¸ãƒ£ãƒ³ãƒ—ã¯ç›´è¿‘ã®æç”»å¯¾è±¡ã«å¯¾ã—ã¦ã®ã¿å‹•ä½œã—ã¾ã™ã€‚")

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
            self.label_meta.setText(f"è¡¨ç¤ºæœŸé–“ã‚’ {mode} ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")

        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "æœŸé–“ã‚¸ãƒ£ãƒ³ãƒ—ã‚¨ãƒ©ãƒ¼", str(e))
            self._append_progress(f"[range] error: {e}")
    



=== file: app/gui/control_tab.py ===

from typing import Optional
from PyQt6.QtCore import Qt
from PyQt6.QtWidgets import (
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QMessageBox,
    QPushButton,
    QSlider,
    QSpinBox,
    QVBoxLayout,
    QWidget,
)

from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_state
from app.services.orderbook_stub import orderbook


class ControlTab(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setLayout(QVBoxLayout())
        root_layout: Optional[QVBoxLayout] = self.layout()  # guard for static analyzers
        if root_layout is None:
            root_layout = QVBoxLayout()
            self.setLayout(root_layout)

        # é‹è»¢
        box_run = QGroupBox("é‹è»¢")
        lay_run = QHBoxLayout()
        self.btn_toggle = QPushButton("å–å¼•ï¼šåœæ­¢ä¸­ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é–‹å§‹ï¼‰")
        self.btn_toggle.setCheckable(True)
        self.btn_toggle.clicked.connect(self._toggle_trading)

        self.btn_close_all = QPushButton("å…¨ã‚¯ãƒ­ãƒ¼ã‚ºï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼‰")
        self.btn_close_all.clicked.connect(self._close_all_mock)

        self.btn_cb_reset = QPushButton("ã‚µãƒ¼ã‚­ãƒƒãƒˆè§£é™¤")
        self.btn_cb_reset.clicked.connect(self._cb_reset)

        lay_run.addWidget(self.btn_toggle)
        lay_run.addWidget(self.btn_close_all)
        lay_run.addWidget(self.btn_cb_reset)
        box_run.setLayout(lay_run)
        root_layout.addWidget(box_run)

        # ã—ãã„å€¤
        box_thr = QGroupBox("ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã—ãã„å€¤ï¼ˆç¢ºä¿¡åº¦ï¼‰")
        lay_thr = QHBoxLayout()
        self.lbl_buy = QLabel("è²·ã„: 0.60")
        self.sld_buy = QSlider(Qt.Orientation.Horizontal)
        self.sld_buy.setRange(50, 80)
        self.sld_buy.setValue(60)
        self.sld_buy.valueChanged.connect(self._on_thr_changed)

        self.lbl_sell = QLabel("å£²ã‚Š: 0.60")
        self.sld_sell = QSlider(Qt.Orientation.Horizontal)
        self.sld_sell.setRange(50, 80)
        self.sld_sell.setValue(60)
        self.sld_sell.valueChanged.connect(self._on_thr_changed)

        lay_thr.addWidget(self.lbl_buy)
        lay_thr.addWidget(self.sld_buy)
        lay_thr.addSpacing(12)
        lay_thr.addWidget(self.lbl_sell)
        lay_thr.addWidget(self.sld_sell)
        box_thr.setLayout(lay_thr)
        root_layout.addWidget(box_thr)

        # æ±ºæ¸ˆ
        box_exit = QGroupBox("æ±ºæ¸ˆï¼ˆå›ºå®špipsï¼‰")
        lay_exit = QHBoxLayout()
        self.sp_sl = QSpinBox()
        self.sp_sl.setRange(1, 200)
        self.sp_sl.setValue(10)
        self.sp_sl.valueChanged.connect(self._on_exit_changed)

        self.sp_tp = QSpinBox()
        self.sp_tp.setRange(1, 300)
        self.sp_tp.setValue(15)
        self.sp_tp.valueChanged.connect(self._on_exit_changed)

        lay_exit.addWidget(QLabel("SL"))
        lay_exit.addWidget(self.sp_sl)
        lay_exit.addSpacing(16)
        lay_exit.addWidget(QLabel("TP"))
        lay_exit.addWidget(self.sp_tp)
        box_exit.setLayout(lay_exit)
        root_layout.addWidget(box_exit)

        # çŠ¶æ…‹è¡¨ç¤º
        self.lbl_status = QLabel("")
        root_layout.addWidget(self.lbl_status)

        self._sync_from_state()

    def _sync_from_state(self):
        s = trade_state.get_settings()
        self.btn_toggle.setChecked(s.trading_enabled)
        self.btn_toggle.setText(
            "å–å¼•ï¼šç¨¼åƒä¸­ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§åœæ­¢ï¼‰" if s.trading_enabled else "å–å¼•ï¼šåœæ­¢ä¸­ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é–‹å§‹ï¼‰"
        )
        self.sld_buy.setValue(int(s.threshold_buy * 100))
        self.sld_sell.setValue(int(s.threshold_sell * 100))
        self.sp_sl.setValue(int(s.sl_pips))
        self.sp_tp.setValue(int(s.tp_pips))
        self._refresh_status()

    def _refresh_status(self):
        s = trade_state.as_dict()
        state_txt = "ç¨¼åƒä¸­" if s["trading_enabled"] else "åœæ­¢ä¸­"
        self.lbl_status.setText(
            f"çŠ¶æ…‹: {state_txt} / è²·ã„é–¾å€¤: {s['threshold_buy']:.2f} / å£²ã‚Šé–¾å€¤: {s['threshold_sell']:.2f} / "
            f"SL: {s['sl_pips']} / TP: {s['tp_pips']}"
        )

    def _toggle_trading(self):
        try:
            enabled = self.btn_toggle.isChecked()
            trade_state.update(trading_enabled=enabled)
            self.btn_toggle.setText(
                "å–å¼•ï¼šç¨¼åƒä¸­ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§åœæ­¢ï¼‰" if enabled else "å–å¼•ï¼šåœæ­¢ä¸­ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é–‹å§‹ï¼‰"
            )
            self._refresh_status()
        except Exception as e:
            self.btn_toggle.setChecked(not self.btn_toggle.isChecked())
            QMessageBox.critical(self, "Trading switch error", str(e))
            print("[control_tab] toggle failed:", e)

    def _on_thr_changed(self, *_):
        buy = self.sld_buy.value() / 100.0
        sell = self.sld_sell.value() / 100.0
        trade_state.update(threshold_buy=buy, threshold_sell=sell)
        self.lbl_buy.setText(f"è²·ã„: {buy:.2f}")
        self.lbl_sell.setText(f"å£²ã‚Š: {sell:.2f}")
        self._refresh_status()

    def _on_exit_changed(self, *_):
        trade_state.update(sl_pips=int(self.sp_sl.value()), tp_pips=int(self.sp_tp.value()))
        self._refresh_status()

    def _close_all_mock(self):
        cfg = load_config()
        symbol = cfg.get("runtime", {}).get("symbol", "USDJPY")
        orderbook().close_all(symbol)

    def _cb_reset(self):
        circuit_breaker.reset()
        circuit_breaker.scan_and_update()
        self._refresh_status()



=== file: app/gui/dashboard_tab.py ===

# app/gui/dashboard_tab.py
import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parents[2]  # D:\macht\OneDrive\fxbot
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))
import json, os
from core.metrics import METRICS_JSON

import time
try:
    import tkinter as tk
    from tkinter import ttk
except ImportError:
    # TkinterãŒç„¡ã„ç’°å¢ƒç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆWindowså…¬å¼Pythonãªã‚‰å…¥ã£ã¦ã¾ã™ï¼‰
    raise

from app.services import trade_service
from core.metrics import METRICS


class DashboardTab(ttk.Frame):
    """
    Realtime Metrics (ATR / Grace / Trail) ã‚’è¡¨ç¤ºã™ã‚‹ã ã‘ã®è»½é‡ãƒ‘ãƒãƒ«ã€‚
    åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§METRICSãŒæ›´æ–°ã•ã‚Œã‚‹å‰æã€‚self.afterã§1ç§’ã”ã¨ã«pullã€‚
    """
    def __init__(self, master, *args, **kwargs):
        super().__init__(master, *args, **kwargs)

        self._vars = {}

        box = ttk.LabelFrame(self, text="Realtime Metrics (ATR / Grace / Trail)")
        box.pack(fill="x", padx=8, pady=6)

        grid = ttk.Frame(box)
        grid.pack(fill="x", padx=8, pady=8)

        rows = [
            ("Last decision", "last_decision"),
            ("Reason", "last_reason"),
            ("ATR ref", "atr_ref"),
            ("ATR gate", "atr_gate_state"),
            ("Post-fill grace", "post_fill_grace"),
            ("Spread", "spread"),
            ("ADX / Min", "adx_min"),
            ("Prob threshold", "prob_threshold"),
            ("Min ATR %", "min_atr_pct"),
            ("Trail: activated", "trail_activated"),
            ("Trail: BE locked", "trail_be_locked"),
            ("Trail: layers", "trail_layers"),
            ("Trail: current SL", "trail_current_sl"),
            ("Guard/Open", "guard_open"),
            ("Guard/Inflight", "guard_inflight"),
            ("Guard/LastFix", "guard_last_fix"),
            ("CB/Tripped", "cb_tripped"),
            ("CB/Reason", "cb_reason"),
            ("CB/ConsecLoss", "cb_consec"),
            ("CB/DailyLossJPY", "cb_daily_loss"),
            ("Counts ENTRY/SKIP/BLOCK", "counts"),
            ("Updated (local)", "ts"),
        ]

        for i, (label, key) in enumerate(rows):
            ttk.Label(grid, text=label, width=22).grid(row=i, column=0, sticky="w", padx=4, pady=2)
            var = tk.StringVar(value="-")
            ttk.Label(grid, textvariable=var, width=28).grid(row=i, column=1, sticky="w", padx=4, pady=2)
            self._vars[key] = var

        # æœ€åˆã®æ›´æ–°ã‚’ã‚»ãƒƒãƒˆ
        self.after(500, self._refresh_metrics)

    def _refresh_metrics(self):
        # ã¾ãšã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã‚€ï¼ˆåˆ¥ãƒ—ãƒ­ã‚»ã‚¹æ›´æ–°ã«å¯¾å¿œï¼‰
        kv = {}
        try:
            with open(METRICS_JSON, "r", encoding="utf-8") as f:
                kv = json.load(f)
        except Exception:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã¾ã ç„¡ã„ï¼å£Šã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«KVSã‚’å‚ç…§
            from core.metrics import METRICS
            kv = METRICS.get()

        # ä»¥é™ã¯åŒã˜ï¼ˆå€¤ã®åæ˜ å‡¦ç†ï¼‰
        self._vars["last_decision"].set(str(kv.get("last_decision", "-")))
        self._vars["last_reason"].set(str(kv.get("last_reason", "-")))
        self._vars["atr_ref"].set(f"{float(kv.get('atr_ref', 0) or 0):.6f}")
        self._vars["atr_gate_state"].set(str(kv.get("atr_gate_state", "-")))
        self._vars["post_fill_grace"].set("ON" if kv.get("post_fill_grace") else "OFF")
        self._vars["spread"].set(str(kv.get("spread", "-")))
        self._vars["prob_threshold"].set(str(kv.get("prob_threshold", "-")))
        self._vars["min_atr_pct"].set(str(kv.get("min_atr_pct", "-")))
        adx = kv.get("adx"); m = kv.get("min_adx")
        self._vars["adx_min"].set(f"{adx} / {m}")
        self._vars["trail_activated"].set("ON" if kv.get("trail_activated") else "OFF")
        self._vars["trail_be_locked"].set("ON" if kv.get("trail_be_locked") else "OFF")
        self._vars["trail_layers"].set(str(kv.get("trail_layers", 0)))
        self._vars["trail_current_sl"].set(str(kv.get("trail_current_sl", "-")))
        cE = int(kv.get("count_entry", 0)); cS = int(kv.get("count_skip", 0)); cB = int(kv.get("count_blocked", 0))
        self._vars["counts"].set(f"{cE} / {cS} / {cB}")
        ts = kv.get("ts")
        local = "-" if not ts else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ts))
        self._vars["ts"].set(local)

        svc = getattr(trade_service, "SERVICE", None)
        guard_state = getattr(svc, "pos_guard", None)
        if guard_state:
            self._vars["guard_open"].set(str(guard_state.state.open_count))
            self._vars["guard_inflight"].set(str(len(guard_state.state.inflight_orders)))
            self._vars["guard_last_fix"].set(guard_state.state.last_fix_reason or "-")
        else:
            self._vars["guard_open"].set("-")
            self._vars["guard_inflight"].set("-")
            self._vars["guard_last_fix"].set("-")

        cb = getattr(svc, "cb", None) if svc else None
        cb_status = cb.status() if cb else {}
        self._vars["cb_tripped"].set(str(cb_status.get("tripped", False)))
        self._vars["cb_reason"].set(str(cb_status.get("reason", "-")))
        self._vars["cb_consec"].set(str(cb_status.get("consecutive_losses", "-")))
        self._vars["cb_daily_loss"].set(f"{float(cb_status.get('daily_loss_accum_jpy', 0.0)):.0f}")

        self.after(1000, self._refresh_metrics)



=== file: app/gui/dashboard_tab_qt.py ===

# app/gui/dashboard_tab_qt.py
from __future__ import annotations
from typing import Dict, Any
import json
import time

from PyQt6 import QtCore, QtWidgets
from core.metrics import METRICS_JSON, METRICS

class DashboardTab(QtWidgets.QWidget):
    """
    PyQt6ç‰ˆ Dashboardã€‚runtime/metrics.json ã‚’1ç§’ã”ã¨ã«å†èª­è¾¼ã—ã€
    å€¤ãŒç„¡ã‘ã‚Œã° core.metrics.METRICS(KVS) ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‚ç…§ã€‚
    """
    def __init__(self, parent: QtWidgets.QWidget | None = None) -> None:
        super().__init__(parent)
        self._labels: Dict[str, QtWidgets.QLabel] = {}

        group = QtWidgets.QGroupBox("Realtime Metrics (ATR / Grace / Trail)")
        grid = QtWidgets.QGridLayout()
        group.setLayout(grid)

        rows = [
            ("Last decision", "last_decision"),
            ("Reason", "last_reason"),
            ("ATR ref", "atr_ref"),
            ("ATR gate", "atr_gate_state"),
            ("Post-fill grace", "post_fill_grace"),
            ("Spread", "spread"),
            ("ADX / Min", "adx_min"),
            ("Prob threshold", "prob_threshold"),
            ("Min ATR %", "min_atr_pct"),
            ("Trail: activated", "trail_activated"),
            ("Trail: BE locked", "trail_be_locked"),
            ("Trail: layers", "trail_layers"),
            ("Trail: current SL", "trail_current_sl"),
            ("Guard/Open", "guard_open"),
            ("Guard/Inflight", "guard_inflight"),
            ("Guard/LastFix", "guard_last_fix"),
            ("CB/Tripped", "cb_tripped"),
            ("CB/Reason", "cb_reason"),
            ("CB/ConsecLoss", "cb_consec"),
            ("CB/DailyLossJPY", "cb_daily_loss"),
            ("Counts ENTRY/SKIP/BLOCK", "counts"),
            ("Updated (local)", "ts"),
        ]

        for r, (label, key) in enumerate(rows):
            grid.addWidget(QtWidgets.QLabel(label), r, 0, alignment=QtCore.Qt.AlignmentFlag.AlignLeft)
            val = QtWidgets.QLabel("-")
            val.setMinimumWidth(220)
            grid.addWidget(val, r, 1, alignment=QtCore.Qt.AlignmentFlag.AlignLeft)
            self._labels[key] = val

        lay = QtWidgets.QVBoxLayout(self)
        lay.addWidget(group)

        # ã‚¿ã‚¤ãƒãƒ¼ã§å®šæœŸæ›´æ–°
        self._timer = QtCore.QTimer(self)
        self._timer.setInterval(1000)
        self._timer.timeout.connect(self._refresh_metrics)
        self._timer.start()

        self._refresh_metrics()

    def _refresh_metrics(self) -> None:
        kv: Dict[str, Any] = {}
        try:
            with open(METRICS_JSON, "r", encoding="utf-8") as f:
                kv = json.load(f)
        except Exception:
            kv = METRICS.get()  # åŒä¸€ãƒ—ãƒ­ã‚»ã‚¹KVSã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        # å€¤ã®æ•´å½¢ã¨æç”»
        self._set("last_decision", str(kv.get("last_decision", "-")))
        self._set("last_reason", str(kv.get("last_reason", "-")))
        self._set("atr_ref", f"{float(kv.get('atr_ref', 0) or 0):.6f}")
        self._set("atr_gate_state", str(kv.get("atr_gate_state", "-")))
        self._set("post_fill_grace", "ON" if kv.get("post_fill_grace") else "OFF")
        self._set("spread", str(kv.get("spread", "-")))
        self._set("prob_threshold", str(kv.get("prob_threshold", "-")))
        self._set("min_atr_pct", str(kv.get("min_atr_pct", "-")))
        adx = kv.get("adx"); m = kv.get("min_adx")
        self._set("adx_min", f"{adx} / {m}")
        self._set("trail_activated", "ON" if kv.get("trail_activated") else "OFF")
        self._set("trail_be_locked", "ON" if kv.get("trail_be_locked") else "OFF")
        self._set("trail_layers", str(kv.get("trail_layers", 0)))
        self._set("trail_current_sl", str(kv.get("trail_current_sl", "-")))
        cE = int(kv.get("count_entry", 0)); cS = int(kv.get("count_skip", 0)); cB = int(kv.get("count_blocked", 0))
        self._set("counts", f"{cE} / {cS} / {cB}")
        ts = kv.get("ts")
        local = "-" if not ts else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ts))
        self._set("ts", local)

    def _set(self, key: str, val: str) -> None:
        lab = self._labels.get(key)
        if lab is not None:
            lab.setText(val)



=== file: app/gui/history_tab.py ===

from __future__ import annotations

from typing import List, Optional

from PyQt6 import QtCore, QtWidgets
from PyQt6.QtWidgets import QHeaderView

from app.services.event_store import EVENT_STORE, UiEvent

_COLUMNS = ["ts", "kind", "symbol", "side", "price", "sl", "tp", "profit_jpy", "reason", "notes"]


class HistoryTab(QtWidgets.QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.table = QtWidgets.QTableWidget(self)
        self.table.setColumnCount(len(_COLUMNS))
        self.table.setHorizontalHeaderLabels(_COLUMNS)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)
        h: Optional[QHeaderView] = self.table.horizontalHeader()
        if h is not None:
            h.setStretchLastSection(True)

        v: Optional[QHeaderView] = self.table.verticalHeader()
        if v is not None:
            v.setVisible(False)

        self.btnExport = QtWidgets.QPushButton("Export CSV")
        self.btnExport.clicked.connect(self._export_csv)

        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.table)
        layout.addWidget(self.btnExport)

        self._timer = QtCore.QTimer(self)
        self._timer.setInterval(1000)
        self._timer.timeout.connect(self.refresh)
        self._timer.start()

        self.refresh()

    def refresh(self) -> None:
        events: List[UiEvent] = EVENT_STORE.recent(300)
        self.table.setRowCount(len(events))
        for r, ev in enumerate(events):
            row = [getattr(ev, col) for col in _COLUMNS]
            for c, val in enumerate(row):
                item = QtWidgets.QTableWidgetItem("" if val is None else str(val))
                self.table.setItem(r, c, item)

    def _export_csv(self) -> None:
        path, _ = QtWidgets.QFileDialog.getSaveFileName(
            self, "Export history to CSV", "history.csv", "CSV Files (*.csv)"
        )
        if not path:
            return
        import csv

        events: List[UiEvent] = EVENT_STORE.recent(1000)
        with open(path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(_COLUMNS)
            for ev in events:
                writer.writerow([getattr(ev, col) for col in _COLUMNS])



=== file: app/gui/main.py ===

import sys
import traceback

from PyQt6.QtCore import QTimer
from PyQt6.QtWidgets import QApplication, QMainWindow, QTabWidget

from app.core import logger as app_logger
from app.gui.control_tab import ControlTab
from app.gui.dashboard_tab_qt import DashboardTab
from app.gui.history_tab import HistoryTab
from app.services.execution_stub import evaluate_and_log_once
from app.gui.ai_tab import AITab
from app.gui.backtest_tab import BacktestTab
from app.gui.settings_tab import SettingsTab

class MainWindow(QMainWindow):
    def __init__(self) -> None:
        super().__init__()
        self.setWindowTitle("FX AI Bot Control Panel")
        self.resize(980, 640)

        tabs = QTabWidget()
        tabs.addTab(DashboardTab(), "Dashboard")
        tabs.addTab(ControlTab(), "Control")
        tabs.addTab(HistoryTab(), "History")
        tabs.addTab(AITab(), "AI")
        tabs.addTab(BacktestTab(), "Backtest")
        tabs.addTab(SettingsTab(), "Settings")
        self.setCentralWidget(tabs)

        app_logger.setup()

        self.timer = QTimer(self)

        def _tick_safe():
            try:
                evaluate_and_log_once()
            except Exception:
                print("[gui.timer] evaluate failed:\n" + traceback.format_exc())

        self.timer.timeout.connect(_tick_safe)
        self.timer.start(3000)
        _tick_safe()


def main() -> None:
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()



=== file: app/gui/settings_tab.py ===

# app/gui/settings_tab.py
from __future__ import annotations

from typing import Optional

from PyQt6.QtCore import Qt
from PyQt6.QtWidgets import (
    QWidget,
    QVBoxLayout,
    QGroupBox,
    QFormLayout,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QComboBox,
    QPushButton,
    QMessageBox,
    QSpacerItem,
    QSizePolicy,
)

from app.services import mt5_account_store, mt5_selftest


class SettingsTab(QWidget):
    """MT5 å£åº§è¨­å®šã‚¿ãƒ–ã€‚

    - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: demo / realï¼‰ã”ã¨ã« login / password / server ã‚’ä¿å­˜
    - ã€Œã“ã®å£åº§ã«åˆ‡ã‚Šæ›¿ãˆã€ãƒœã‚¿ãƒ³ã§ active_profile ã‚’å¤‰æ›´ã—ã€
      ã‚«ãƒ¬ãƒ³ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã®ç’°å¢ƒå¤‰æ•° MT5_LOGIN/PASSWORD/SERVER ã‚‚æ›´æ–°ã™ã‚‹
    """

    def __init__(self, parent: Optional[QWidget] = None) -> None:
        super().__init__(parent)

        self._setup_ui()
        self._load_profiles()

    # ------------------------------------------------------------------
    # UI æ§‹ç¯‰
    # ------------------------------------------------------------------
    def _setup_ui(self) -> None:
        root = QVBoxLayout(self)

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        grp_profile = QGroupBox("MT5 å£åº§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«", self)
        lay_p = QFormLayout(grp_profile)

        self.cmb_profile = QComboBox(grp_profile)
        # æ–°ã—ã„åå‰ï¼ˆä¾‹: demo2, demo_2026 ãªã©ï¼‰ã‚‚å…¥åŠ›ã§ãã‚‹ã‚ˆã† editable ã«
        self.cmb_profile.setEditable(True)
        self.cmb_profile.setInsertPolicy(QComboBox.InsertPolicy.NoInsert)
        self.cmb_profile.currentTextChanged.connect(self._on_profile_changed)

        lay_p.addRow("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: demo / realï¼‰:", self.cmb_profile)

        # èªè¨¼æƒ…å ±
        grp_auth = QGroupBox("ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±", self)
        lay_auth = QFormLayout(grp_auth)

        self.ed_login = QLineEdit(grp_auth)
        self.ed_login.setPlaceholderText("å£åº§ç•ªå·ï¼ˆæ•°å­—ï¼‰")

        self.ed_password = QLineEdit(grp_auth)
        self.ed_password.setEchoMode(QLineEdit.EchoMode.Password)
        self.ed_password.setPlaceholderText("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰")

        self.ed_server = QLineEdit(grp_auth)
        self.ed_server.setPlaceholderText("ã‚µãƒ¼ãƒåï¼ˆä¾‹: GaitameFinest-Demoï¼‰")

        lay_auth.addRow("ãƒ­ã‚°ã‚¤ãƒ³ID:", self.ed_login)
        lay_auth.addRow("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰:", self.ed_password)
        lay_auth.addRow("ã‚µãƒ¼ãƒ:", self.ed_server)

        # ãƒœã‚¿ãƒ³
        btn_row = QHBoxLayout()
        self.btn_save = QPushButton("ä¿å­˜ï¼ˆã“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ï¼‰", self)
        self.btn_switch = QPushButton("ã“ã®å£åº§ã«åˆ‡ã‚Šæ›¿ãˆ", self)

        self.btn_save.clicked.connect(self._on_save_clicked)
        self.btn_switch.clicked.connect(self._on_switch_clicked)

        btn_row.addWidget(self.btn_save)
        btn_row.addWidget(self.btn_switch)
        btn_row.addStretch()

        # æƒ…å ±è¡¨ç¤º
        self.lbl_active = QLabel("", self)
        self.lbl_active.setWordWrap(True)

        # æ¥ç¶šãƒ†ã‚¹ãƒˆãƒœã‚¿ãƒ³ï¼ˆè‡ªå·±è¨ºæ–­ï¼‰
        self.btn_selftest = QPushButton("MT5 æ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆè‡ªå·±è¨ºæ–­ï¼‰", self)
        self.btn_selftest.setToolTip(
            "ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å£åº§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦ MT5 ã¸ã®æ¥ç¶šã¨ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’è‡ªå·±è¨ºæ–­ã—ã¾ã™ã€‚"
        )
        self.btn_selftest.clicked.connect(self._on_selftest_clicked)

        # ãƒ†ã‚¹ãƒˆç™ºæ³¨ãƒœã‚¿ãƒ³ï¼ˆselftest_order_flowï¼‰
        self.btn_orderflow_test = QPushButton("ãƒ†ã‚¹ãƒˆç™ºæ³¨ï¼ˆselftest_order_flowï¼‰", self)
        self.btn_orderflow_test.setToolTip(
            "scripts.selftest_order_flow ã‚’å®Ÿè¡Œã—ã¦ã€0.01 lot ã®æˆè¡Œç™ºæ³¨â†’å³æ±ºæ¸ˆãƒ•ãƒ­ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚\n"
            "å¿…ãšãƒ‡ãƒ¢å£åº§ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )
        self.btn_orderflow_test.clicked.connect(self._on_orderflow_selftest_clicked)

        row_selftest = QHBoxLayout()
        row_selftest.addStretch()
        row_selftest.addWidget(self.btn_selftest)
        row_selftest.addWidget(self.btn_orderflow_test)

        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«ç©ã‚€
        root.addWidget(grp_profile)
        root.addWidget(grp_auth)
        root.addLayout(btn_row)
        root.addWidget(self.lbl_active)
        root.addLayout(row_selftest)

        # ä½™ç™½ã‚’ä¸‹ã«è¿½åŠ 
        root.addItem(QSpacerItem(0, 0, QSizePolicy.Policy.Minimum, QSizePolicy.Policy.Expanding))

    # ------------------------------------------------------------------
    # å†…éƒ¨ãƒ­ã‚¸ãƒƒã‚¯
    # ------------------------------------------------------------------
    def _load_profiles(self) -> None:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’èª­ã¿è¾¼ã¿ã€ã‚³ãƒ³ãƒœãƒœãƒƒã‚¯ã‚¹ã«åæ˜ ã€‚"""
        cfg = mt5_account_store.load_config()
        profiles = sorted(cfg["profiles"].keys())
        active = cfg.get("active_profile") or ""

        self.cmb_profile.blockSignals(True)
        self.cmb_profile.clear()

        # ãƒ‡ãƒ¢ãƒ»æœ¬å£åº§ã®å…¸å‹åã‚’ã‚ã‚‰ã‹ã˜ã‚å€™è£œã«å…¥ã‚Œã¦ãŠã
        base_candidates = ["demo", "real"]
        for name in base_candidates:
            if name not in profiles:
                profiles.append(name)

        for name in profiles:
            self.cmb_profile.addItem(name)

        # active_profile ãŒã‚ã‚Œã°é¸æŠ
        if active and active in profiles:
            self.cmb_profile.setCurrentText(active)
        elif profiles:
            self.cmb_profile.setCurrentIndex(0)

        self.cmb_profile.blockSignals(False)

        # é¸æŠä¸­ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’åæ˜ 
        self._apply_profile_to_fields(self.cmb_profile.currentText())
        self._refresh_active_label()

    def _apply_profile_to_fields(self, profile_name: str) -> None:
        """æŒ‡å®šãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’å…¥åŠ›æ¬„ã«åæ˜ ã€‚"""
        if not profile_name:
            self.ed_login.clear()
            self.ed_password.clear()
            self.ed_server.clear()
            return

        acc = mt5_account_store.get_profile(profile_name)
        if acc is None:
            # æœªä¿å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ç©ºã«
            self.ed_login.clear()
            self.ed_password.clear()
            self.ed_server.clear()
            return

        self.ed_login.setText(str(acc.get("login", "")))
        self.ed_password.setText(acc.get("password", ""))
        self.ed_server.setText(acc.get("server", ""))

    def _on_profile_changed(self, name: str) -> None:
        self._apply_profile_to_fields(name)

    def _on_save_clicked(self) -> None:
        name = self.cmb_profile.currentText().strip()
        if not name:
            QMessageBox.warning(self, "ä¿å­˜ã‚¨ãƒ©ãƒ¼", "ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        login_txt = self.ed_login.text().strip()
        password = self.ed_password.text()
        server = self.ed_server.text().strip()

        if not login_txt or not password or not server:
            QMessageBox.warning(self, "ä¿å­˜ã‚¨ãƒ©ãƒ¼", "ãƒ­ã‚°ã‚¤ãƒ³IDãƒ»ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ»ã‚µãƒ¼ãƒã‚’ã™ã¹ã¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        try:
            login = int(login_txt)
        except ValueError:
            QMessageBox.warning(self, "ä¿å­˜ã‚¨ãƒ©ãƒ¼", "ãƒ­ã‚°ã‚¤ãƒ³ID ã¯æ•°å­—ã®ã¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        mt5_account_store.upsert_profile(name, login=login, password=password, server=server)
        QMessageBox.information(self, "ä¿å­˜å®Œäº†", f"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« '{name}' ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

        # å†èª­è¾¼ã—ã¦ active/profile è¡¨ç¤ºã‚’æ›´æ–°
        self._load_profiles()

    def _on_switch_clicked(self) -> None:
        name = self.cmb_profile.currentText().strip()
        if not name:
            QMessageBox.warning(self, "åˆ‡ã‚Šæ›¿ãˆã‚¨ãƒ©ãƒ¼", "ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é¸æŠã¾ãŸã¯å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        acc = mt5_account_store.get_profile(name)
        if acc is None:
            # æœªä¿å­˜ãªã‚‰ã€Œä¿å­˜ã—ã¦ã‹ã‚‰åˆ‡ã‚Šæ›¿ãˆã‚‹ï¼Ÿã€ã‹ã‚’ç¢ºèª
            res = QMessageBox.question(
                self,
                "æœªä¿å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«",
                "ã“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã¾ã ä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…¥åŠ›ä¸­ã®å†…å®¹ã§ä¿å­˜ã—ã¦ã‹ã‚‰åˆ‡ã‚Šæ›¿ãˆã¾ã™ã‹ï¼Ÿ",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
            )
            if res == QMessageBox.StandardButton.Yes:
                self._on_save_clicked()
                acc = mt5_account_store.get_profile(name)
                if acc is None:
                    return
            else:
                return

        mt5_account_store.set_active_profile(name, apply_env=True)
        self._refresh_active_label()

        QMessageBox.information(
            self,
            "å£åº§åˆ‡ã‚Šæ›¿ãˆ",
            f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å£åº§ã‚’ '{name}' ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸã€‚\n\n"
            "ã“ã®GUIãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã¯ MT5_LOGIN / MT5_PASSWORD / MT5_SERVER ãŒ\n"
            "é¸æŠã—ãŸå£åº§ã®æƒ…å ±ã«æ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚",
        )

    def _on_selftest_clicked(self) -> None:
        """
        ã€ŒMT5 æ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆè‡ªå·±è¨ºæ–­ï¼‰ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ã€‚
        """
        try:
            ok, log = mt5_selftest.run_mt5_selftest()
        except Exception as e:
            # ã‚µãƒ¼ãƒ“ã‚¹å±¤ã§ã‚‚ä¾‹å¤–ã‚’æ¡ã£ã¦ã„ã‚‹ãŒã€GUI å´ã§ã‚‚å¿µã®ãŸã‚ã‚¬ãƒ¼ãƒ‰ã—ã¦ãŠã
            QMessageBox.critical(
                self,
                "MT5 æ¥ç¶šãƒ†ã‚¹ãƒˆ ã‚¨ãƒ©ãƒ¼",
                f"MT5 è‡ªå·±è¨ºæ–­ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n\n{e!r}",
            )
            return

        # æˆåŠŸï¼å¤±æ•—ã§ã‚¢ã‚¤ã‚³ãƒ³ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰ãˆã‚‹
        if ok:
            icon = QMessageBox.Icon.Information
            title = "MT5 æ¥ç¶šãƒ†ã‚¹ãƒˆ æˆåŠŸ"
        else:
            icon = QMessageBox.Icon.Critical
            title = "MT5 æ¥ç¶šãƒ†ã‚¹ãƒˆ å¤±æ•—"

        # è©³ç´°ãƒ­ã‚°ï¼ˆãƒ­ã‚°å…¨æ–‡ï¼‰ã¯ detailedText ã«å…¥ã‚Œã‚‹
        first_line = log.splitlines()[0] if log else ""

        msg_box = QMessageBox(self)
        msg_box.setIcon(icon)
        msg_box.setWindowTitle(title)

        if ok:
            msg_box.setText(
                "MT5 ã¸ã®æ¥ç¶šãƒ»ãƒ­ã‚°ã‚¤ãƒ³ãŒæ­£å¸¸ã«ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚\n"
                "å–å¼•æº–å‚™ã¯å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚"
            )
        else:
            msg_box.setText(
                "MT5 ã¸ã®æ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã§å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n"
                "æ¬¡ã®ç‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š\n"
                "  ãƒ»MT5 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã¯èµ·å‹•ã—ã¦ã„ã‚‹ã‹\n"
                "  ãƒ»è¨­å®šã‚¿ãƒ–ã®å£åº§ID / ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ / ã‚µãƒ¼ãƒãƒ¼ã¯æ­£ã—ã„ã‹\n"
                "  ãƒ»ãƒ‡ãƒ¢å£åº§ã®æœ‰åŠ¹æœŸé™ãŒåˆ‡ã‚Œã¦ã„ãªã„ã‹\n"
                "  ãƒ»åŒæ™‚ãƒ­ã‚°ã‚¤ãƒ³æ•°ã®åˆ¶é™ã«å¼•ã£ã‹ã‹ã£ã¦ã„ãªã„ã‹\n"
                "\nè©³ç´°ã¯ã€Œè©³ç´°ã€ãƒœã‚¿ãƒ³ã‹ã‚‰ç¢ºèªã§ãã¾ã™ã€‚"
            )

        msg_box.setDetailedText(log)
        msg_box.exec()

    def _on_orderflow_selftest_clicked(self) -> None:
        """
        ã€Œãƒ†ã‚¹ãƒˆç™ºæ³¨ï¼ˆselftest_order_flowï¼‰ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ã€‚
        """
        # ã¾ãšã¯ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°
        res = QMessageBox.question(
            self,
            "ãƒ†ã‚¹ãƒˆç™ºæ³¨ã®ç¢ºèª",
            (
                "scripts.selftest_order_flow ã‚’å®Ÿè¡Œã—ã¦ã€\n"
                "ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–MT5å£åº§ã§ 0.01 lot ã®æˆè¡Œæ³¨æ–‡â†’å³æ±ºæ¸ˆãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã€‚\n\n"
                "â€» å¿…ãšãƒ‡ãƒ¢å£åº§ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n\n"
                "ç¶šè¡Œã—ã¦ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ"
            ),
            QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
        )

        if res != QMessageBox.StandardButton.Yes:
            return

        try:
            ok, log = mt5_selftest.run_mt5_orderflow_selftest()
        except Exception as e:
            QMessageBox.critical(
                self,
                "ãƒ†ã‚¹ãƒˆç™ºæ³¨ã‚¨ãƒ©ãƒ¼",
                "selftest_order_flow å®Ÿè¡Œä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n\n"
                f"{e!r}",
            )
            return

        # æˆåŠŸï¼å¤±æ•—ã§ã‚¢ã‚¤ã‚³ãƒ³ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰ãˆã‚‹
        if ok:
            icon = QMessageBox.Icon.Information
            title = "ãƒ†ã‚¹ãƒˆç™ºæ³¨ æˆåŠŸ"
            text = (
                "0.01 lot ã®æˆè¡Œç™ºæ³¨â†’å³æ±ºæ¸ˆãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚\n"
                "è©³ç´°ãƒ­ã‚°ã¯ã€Œè©³ç´°ã€ãƒœã‚¿ãƒ³ã‹ã‚‰ç¢ºèªã§ãã¾ã™ã€‚"
            )
        else:
            icon = QMessageBox.Icon.Critical
            title = "ãƒ†ã‚¹ãƒˆç™ºæ³¨ å¤±æ•—"
            text = (
                "ãƒ†ã‚¹ãƒˆç™ºæ³¨ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‹ã€è‡ªå·±è¨ºæ–­ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚\n"
                "è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ã€å£åº§è¨­å®šã‚„MT5ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®çŠ¶æ…‹ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
            )

        msg_box = QMessageBox(self)
        msg_box.setIcon(icon)
        msg_box.setWindowTitle(title)
        msg_box.setText(text)
        msg_box.setDetailedText(log)
        msg_box.exec()

    def _refresh_active_label(self) -> None:
        cfg = mt5_account_store.load_config()
        active = cfg.get("active_profile") or "(æœªè¨­å®š)"
        acc = mt5_account_store.get_profile(active)

        if acc is None:
            txt = f"ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å£åº§: {active}ï¼ˆè¨­å®šæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰"
        else:
            txt = (
                f"ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å£åº§: {active}\n"
                f"  login={acc.get('login')} / server={acc.get('server')}"
            )
        self.lbl_active.setText(txt)



=== file: app/gui/widgets/feature_importance.py ===

# app/gui/widgets/feature_importance.py
from __future__ import annotations
from typing import Any, Dict, Optional, cast
import json
from pathlib import Path

from PyQt6 import QtWidgets
from PyQt6.QtWidgets import QHeaderView
import pandas as pd
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure

class FeatureImportanceWidget(QtWidgets.QWidget):
    def __init__(self, ai_service, parent: Optional[QtWidgets.QWidget] = None):
        super().__init__(parent)
        self.ai_service = ai_service
        self._df_cache: Optional[pd.DataFrame] = None
        self._alias: Dict[str, str] = self._load_alias()

        self.modelCombo = QtWidgets.QComboBox()
        self.methodCombo = QtWidgets.QComboBox()
        self.methodCombo.addItems(["gain", "split"])
        self.topSpin = QtWidgets.QSpinBox()
        self.topSpin.setRange(3, 100)
        self.topSpin.setValue(20)
        self.refreshBtn = QtWidgets.QPushButton("æ›´æ–°")

        ctrl = QtWidgets.QHBoxLayout()
        ctrl.addWidget(QtWidgets.QLabel("Model"))
        ctrl.addWidget(self.modelCombo, 1)
        ctrl.addWidget(QtWidgets.QLabel("Method"))
        ctrl.addWidget(self.methodCombo)
        ctrl.addWidget(QtWidgets.QLabel("TopN"))
        ctrl.addWidget(self.topSpin)
        ctrl.addWidget(self.refreshBtn)

        self.fig = Figure(figsize=(6, 4))
        self.canvas = FigureCanvas(self.fig)

        self.table = QtWidgets.QTableWidget()
        self.table.setColumnCount(3)
        self.table.setHorizontalHeaderLabels(["feature", "importance(%)", "model"])
        header: Optional[QHeaderView] = self.table.horizontalHeader()
        if header is not None:
            header.setStretchLastSection(True)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)

        lay = QtWidgets.QVBoxLayout(self)
        lay.addLayout(ctrl)
        lay.addWidget(self.canvas, 2)
        lay.addWidget(self.table, 1)

        self.refreshBtn.clicked.connect(self.refresh)
        self.methodCombo.currentTextChanged.connect(self.refresh)
        self.topSpin.valueChanged.connect(self.refresh)
        self.modelCombo.currentTextChanged.connect(self._plot_current)

        self.refresh()

    def refresh(self):
        method = self.methodCombo.currentText()
        top_n = int(self.topSpin.value())
        try:
            df = self.ai_service.get_feature_importance(method=method, top_n=top_n)
        except Exception as e:
            QtWidgets.QMessageBox.critical(self, "FIå–å¾—ã‚¨ãƒ©ãƒ¼", str(e))
            return

        if df is None or df.empty:
            self._df_cache = None
            self.modelCombo.clear()
            self._render_empty()
            return

        df = df.copy()
        if "feature" in df.columns and self._alias:
            df["feature"] = df["feature"].map(lambda x: self._alias.get(str(x), str(x)))

        self._df_cache = df
        models = sorted(df["model"].unique().tolist())
        prev = self.modelCombo.currentText()
        self.modelCombo.blockSignals(True)
        self.modelCombo.clear()
        self.modelCombo.addItems(models)
        self.modelCombo.blockSignals(False)
        if prev in models:
            self.modelCombo.setCurrentIndex(models.index(prev))
        self._plot_current()
        self._fill_table(df)

    def _plot_current(self):
        df = self._df_cache
        if df is None or df.empty:
            self._render_empty()
            return
        model = self.modelCombo.currentText()
        sub = df[df["model"] == model].sort_values("importance", ascending=True)
        self.fig.clear()
        ax = self.fig.add_subplot(111)
        ax.barh(sub["feature"], sub["importance"])
        ax.set_xlabel("importance (%)")
        ax.set_title(f"Feature Importance - {model} ({self.methodCombo.currentText()})")
        self.fig.tight_layout()
        self.canvas.draw_idle()

    def _fill_table(self, df: pd.DataFrame):
        rows = list(df.reset_index(drop=True).itertuples(index=False))
        self.table.setRowCount(len(rows))
        for r, row in enumerate(rows):
            importance_val = float(cast(Any, row.importance))
            self.table.setItem(r, 0, QtWidgets.QTableWidgetItem(str(row.feature)))
            self.table.setItem(r, 1, QtWidgets.QTableWidgetItem(f"{importance_val:.2f}"))
            self.table.setItem(r, 2, QtWidgets.QTableWidgetItem(str(row.model)))
        self.table.resizeColumnsToContents()

    def _render_empty(self):
        self.fig.clear()
        ax = self.fig.add_subplot(111)
        ax.text(0.5, 0.5, "No data", ha="center", va="center", transform=ax.transAxes)
        ax.axis("off")
        self.canvas.draw_idle()
        self.table.setRowCount(0)

    def _load_alias(self) -> Dict[str, str]:
        try:
            root = Path(__file__).resolve().parents[3]
            path = root / "config" / "feature_alias.json"
            if path.exists():
                raw = json.loads(path.read_text(encoding="utf-8"))
                if isinstance(raw, dict):
                    return {str(k): str(v) for k, v in raw.items()}
        except Exception:
            pass
        return {}



=== file: app/gui/widgets/shap_bar.py ===

# app/gui/widgets/shap_bar.py
from __future__ import annotations

from typing import Dict, Optional
import json
from pathlib import Path

import pandas as pd
from PyQt6 import QtWidgets
from PyQt6.QtWidgets import QHeaderView
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure

from app.services.ai_service import AISvc


class ShapBarWidget(QtWidgets.QWidget):
    """
    AISvc.get_shap_top_features() ã®çµæœã‚’æ£’ã‚°ãƒ©ãƒ•ï¼‹ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤ºã™ã‚‹ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã€‚
    """

    def __init__(self, ai_service: AISvc, parent: Optional[QtWidgets.QWidget] = None) -> None:
        super().__init__(parent)
        self.ai_service = ai_service
        self._df_cache: Optional[pd.DataFrame] = None
        self._alias: Dict[str, str] = self._load_alias()

        self.spin_top_n = QtWidgets.QSpinBox()
        self.spin_top_n.setRange(1, 200)
        self.spin_top_n.setValue(20)

        self.spin_cache_sec = QtWidgets.QSpinBox()
        self.spin_cache_sec.setRange(0, 24 * 3600)
        self.spin_cache_sec.setSingleStep(60)
        self.spin_cache_sec.setValue(300)

        self.btn_refresh = QtWidgets.QPushButton("Recalc SHAP")

        ctrl_layout = QtWidgets.QHBoxLayout()
        ctrl_layout.addWidget(QtWidgets.QLabel("Top N:"))
        ctrl_layout.addWidget(self.spin_top_n)
        ctrl_layout.addSpacing(12)
        ctrl_layout.addWidget(QtWidgets.QLabel("Cache TTL (sec):"))
        ctrl_layout.addWidget(self.spin_cache_sec)
        ctrl_layout.addStretch(1)
        ctrl_layout.addWidget(self.btn_refresh)

        self.lbl_top1 = QtWidgets.QLabel("-")
        self.lbl_top2 = QtWidgets.QLabel("-")
        self.lbl_top3 = QtWidgets.QLabel("-")
        for lbl in (self.lbl_top1, self.lbl_top2, self.lbl_top3):
            lbl.setWordWrap(True)

        top_box = QtWidgets.QGroupBox("Top 3 features")
        top_layout = QtWidgets.QVBoxLayout(top_box)
        top_layout.addWidget(self.lbl_top1)
        top_layout.addWidget(self.lbl_top2)
        top_layout.addWidget(self.lbl_top3)

        self.figure = Figure(figsize=(5, 3))
        self.canvas = FigureCanvas(self.figure)

        self.table = QtWidgets.QTableWidget()
        self.table.setColumnCount(4)
        self.table.setHorizontalHeaderLabels(["rank", "feature", "mean|SHAP|", "model"])
        self.table.verticalHeader().setVisible(False)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)
        self.table.horizontalHeader().setSectionResizeMode(QHeaderView.ResizeMode.ResizeToContents)
        self.table.horizontalHeader().setStretchLastSection(True)

        layout = QtWidgets.QVBoxLayout(self)
        layout.addLayout(ctrl_layout)
        layout.addWidget(top_box)
        layout.addWidget(self.canvas, 2)
        layout.addWidget(self.table, 1)

        self.btn_refresh.clicked.connect(self.refresh)
        self.spin_top_n.valueChanged.connect(self.refresh)
        self.spin_cache_sec.valueChanged.connect(self.refresh)

        self.refresh()

    def refresh(self, force: bool = False) -> None:
        """
        AISvc.get_shap_top_features() ã‚’å‘¼ã³å‡ºã—ã¦å†æç”»ã™ã‚‹ã€‚
        """
        from loguru import logger

        top_n = int(self.spin_top_n.value())
        cache_sec = 0 if force else int(self.spin_cache_sec.value())

        try:
            df = self.ai_service.get_shap_top_features(
                top_n=top_n,
                cache_sec=cache_sec,
            )
        except Exception as e:
            logger.exception("failed to compute SHAP top features: %s", e)
            self._render_empty(f"SHAP error: {e}")
            return

        if df is None or df.empty:
            self._render_empty("No SHAP data.")
            return

        self._df_cache = df
        self._plot(df)
        self._fill_table(df)
        self._update_top3(df)

    def _plot(self, df: pd.DataFrame) -> None:
        """
        SHAPã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¦åº¦ã®æ°´å¹³æ£’ã‚°ãƒ©ãƒ•ã‚’æç”»ã€‚
        """
        df_plot = df.copy()
        df_plot = df_plot.sort_values("mean_abs_shap", ascending=True)

        features_raw = df_plot["feature"].astype(str).tolist()
        features = [self._alias.get(f, f) for f in features_raw]
        values = pd.to_numeric(df_plot["mean_abs_shap"], errors="coerce").fillna(0.0).tolist()

        self.figure.clear()
        ax = self.figure.add_subplot(111)

        y_pos = range(len(features))
        ax.barh(y_pos, values)
        ax.set_yticks(list(y_pos))
        ax.set_yticklabels(features)

        ax.set_xlabel("mean |SHAP value|")
        ax.set_title("Global SHAP Feature Importance")

        self.figure.tight_layout()
        self.canvas.draw_idle()

    def _fill_table(self, df: pd.DataFrame) -> None:
        """
        ãƒ†ãƒ¼ãƒ–ãƒ«ã« SHAP é †ä½ã‚’è¡¨ç¤ºã€‚
        """
        df = df.reset_index(drop=True)
        self.table.setRowCount(len(df))

        for row_idx, row in df.iterrows():
            rank = str(row.get("rank", row_idx + 1))
            feat = str(row.get("feature", ""))
            mean_abs = float(row.get("mean_abs_shap", 0.0))
            model = str(row.get("model", ""))

            items = [
                QtWidgets.QTableWidgetItem(rank),
                QtWidgets.QTableWidgetItem(feat),
                QtWidgets.QTableWidgetItem(f"{mean_abs:.6f}"),
                QtWidgets.QTableWidgetItem(model),
            ]

            for col_idx, item in enumerate(items):
                self.table.setItem(row_idx, col_idx, item)

        self.table.resizeColumnsToContents()

    def _update_top3(self, df: Optional[pd.DataFrame]) -> None:
        """
        ä¸Šä½3ç‰¹å¾´é‡ã®ç°¡æ˜“ã‚µãƒãƒªã‚’ãƒ©ãƒ™ãƒ«ã«è¡¨ç¤ºã€‚
        """
        if df is None or df.empty:
            self.lbl_top1.setText("-")
            self.lbl_top2.setText("-")
            self.lbl_top3.setText("-")
            return

        if "rank" in df.columns:
            df_sorted = df.sort_values("rank", ascending=True)
        else:
            df_sorted = df.sort_values("mean_abs_shap", ascending=False)

        top3 = df_sorted.head(3).reset_index(drop=True)

        labels = [self.lbl_top1, self.lbl_top2, self.lbl_top3]
        for idx in range(3):
            if idx < len(top3):
                row = top3.iloc[idx]
                raw_name = str(row.get("feature", ""))
                alias = self._alias.get(raw_name, raw_name)
                mean_abs = float(row.get("mean_abs_shap", 0.0))
                model = str(row.get("model", ""))
                text = f"{idx + 1}. {alias} (|SHAP|={mean_abs:.4f})"
                if model:
                    text += f"  [{model}]"
                labels[idx].setText(text)
            else:
                labels[idx].setText("-")

    def _render_empty(self, message: str) -> None:
        """
        ãƒ‡ãƒ¼ã‚¿ãŒç„¡ã„ or ã‚¨ãƒ©ãƒ¼æ™‚ã®ç°¡å˜ãªè¡¨ç¤ºã€‚
        """
        self.figure.clear()
        ax = self.figure.add_subplot(111)
        ax.text(
            0.5,
            0.5,
            message,
            ha="center",
            va="center",
            transform=ax.transAxes,
        )
        ax.axis("off")
        self.canvas.draw_idle()
        self.table.setRowCount(0)
        self._update_top3(None)

    def _load_alias(self) -> Dict[str, str]:
        """
        configs/feature_alias.json ã‹ã‚‰ feature åã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’èª­ã¿å‡ºã™ã€‚
        FeatureImportanceWidget ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«å…±é€šå‡¦ç†ã‚’æµç”¨ã€‚
        """
        try:
            root = Path(__file__).resolve().parents[3]
            path = root / "configs" / "feature_alias.json"
            if not path.exists():
                return {}
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                return {str(k): str(v) for k, v in data.items()}
        except Exception as e:
            print(f"[ShapBarWidget] _load_alias failed: {e!r}")
        return {}



=== file: app/main_tk.py ===

# app/main.py
import tkinter as tk
from tkinter import ttk
from app.gui.dashboard_tab import DashboardTab

def main() -> None:
    root = tk.Tk()
    root.title("FXBot Dashboard")
    root.geometry("520x540")

    nb = ttk.Notebook(root)
    nb.pack(fill="both", expand=True)

    dash = DashboardTab(nb)
    nb.add(dash, text="Dashboard")

    root.mainloop()

if __name__ == "__main__":
    main()



=== file: app/services/__init__.py ===




=== file: app/services/ai_service.py ===

from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, Optional
import time

import pandas as pd
from loguru import logger

from app.services.feature_importance import compute_feature_importance
from app.services.shap_service import (
    ShapFeatureImpact,
    compute_shap_feature_importance,
    shap_items_to_frame,
)


class AISvc:
    """
    æ—¢å­˜ã®æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹æƒ³å®šã€‚ãƒ¢ãƒ‡ãƒ«ç¾¤ã¯ self.models ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹æƒ³å®šã€‚
    ä¾‹: self.models = {"lgbm_cls": lgb_model, "xgb_cls": xgb_model}
    """

    def __init__(self) -> None:
        self.models: Dict[str, Any] = {}
        self._fi_cache: Optional[pd.DataFrame] = None
        self._fi_cache_key: Optional[str] = None
        self._fi_cache_ts: float = 0.0

        # SHAPç”¨ã®é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._shap_cache: Optional[pd.DataFrame] = None
        self._shap_cache_key: Optional[str] = None
        self._shap_cache_ts: float = 0.0

        self.expected_features: Optional[list[str]] = None
        # ... ï¼ˆæ—¢å­˜ã®åˆæœŸåŒ–ï¼‰

    # ... ï¼ˆæ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼š load_models(), predict(), ãªã©ï¼‰

    def get_feature_importance(
        self,
        method: str = "gain",
        top_n: int = 20,
        cache_sec: int = 300,
    ) -> pd.DataFrame:
        """
        GUI ã‹ã‚‰å‘¼ã³å‡ºã—ã¦ Feature Importance ã‚’å–å¾—ã™ã‚‹ APIã€‚

        ç¾çŠ¶ method å¼•æ•°ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã§ã€
        LightGBM / XGBoost ã®ã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡è¦åº¦ï¼ˆãŠãŠã‚€ã­ gain ãƒ™ãƒ¼ã‚¹ï¼‰ã€ã‚’è¿”ã™ã€‚

        æˆ»ã‚Šå€¤:
            columns = ["model", "feature", "importance"]
            importance ã¯ã€Œå‰²åˆ(%)ã€ã‚’æƒ³å®šã€‚FeatureImportanceWidget å´ã®
            è»¸ãƒ©ãƒ™ãƒ« "importance(%)" ã¨å¯¾å¿œã•ã›ã‚‹ã€‚
        """
        model_key = ",".join(f"{name}:{id(model)}" for name, model in sorted(self.models.items()))
        key = f"{model_key}|{method}|{top_n}"
        now = time.time()

        if (
            self._fi_cache is not None
            and self._fi_cache_key == key
            and (now - self._fi_cache_ts) < cache_sec
        ):
            return self._fi_cache.copy()

        rows: list[dict[str, Any]] = []

        for name, model in self.models.items():
            if model is None:
                continue

            try:
                items = compute_feature_importance(
                    model=model,
                    feature_names=None,
                    top_n=top_n,
                )
            except Exception as e:
                print(f"[AISvc] compute_feature_importance failed for {name}: {e}")
                continue

            for item in items:
                rows.append(
                    {
                        "model": name,
                        "feature": item.name,
                        "importance": item.importance_pct,
                    }
                )

        if not rows:
            df = pd.DataFrame(columns=["model", "feature", "importance"])
        else:
            df = pd.DataFrame(rows)

        self._fi_cache = df.copy()
        self._fi_cache_key = key
        self._fi_cache_ts = now
        return df

    def _load_shap_background_features(
        self,
        max_rows: int = 2000,
        *,
        csv_path: Path | None = None,
    ) -> pd.DataFrame:
        """
        SHAPè¨ˆç®—ç”¨ã®èƒŒæ™¯ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã‚€ãƒ˜ãƒ«ãƒ‘ã€‚

        æš«å®šä»•æ§˜ï¼š
        - data/USDJPY/features_for_shap.csv ã«ç‰¹å¾´é‡CSVãŒã‚ã‚‹å‰æã€‚
          ï¼ˆä»Šå¾Œã€weekly_retrain å´ã‹ã‚‰è‡ªå‹•å‡ºåŠ›ã•ã›ã‚‹äºˆå®šï¼‰
        - self.expected_features ãŒã‚ã‚Œã°ã€ãã®åˆ—é †ã«æƒãˆã‚‹ã€‚
        """
        if csv_path is None:
            csv_path = Path("data") / "USDJPY" / "features_for_shap.csv"

        if not csv_path.exists():
            raise FileNotFoundError(
                f"SHAPç”¨ç‰¹å¾´é‡CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}\n"
                "ä¸€æ™‚çš„ã«ã¯æ‰‹å‹•ã§ç‰¹å¾´é‡CSVã‚’ç”¨æ„ã—ã¦ãã ã•ã„ã€‚"
            )

        logger.info(
            "SHAPèƒŒæ™¯ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿: path={path}", path=csv_path.as_posix()
        )
        df = pd.read_csv(csv_path)

        if self.expected_features:
            missing = set(self.expected_features) - set(df.columns)
            if missing:
                raise ValueError(
                    "SHAPèƒŒæ™¯ç‰¹å¾´é‡ã« expected_features ã®åˆ—ãŒè¶³ã‚Šã¾ã›ã‚“: "
                    f"{sorted(missing)}"
                )
            df = df.loc[:, list(self.expected_features)]

        if len(df) > max_rows:
            df = df.sample(n=max_rows, random_state=42)

        return df

    def get_shap_top_features(
        self,
        *,
        top_n: int = 20,
        max_background: int = 2000,
        csv_path: Path | None = None,
        cache_sec: int = 300,
    ) -> pd.DataFrame:
        """
        LightGBMãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ SHAP ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¦åº¦ï¼ˆå¹³å‡çµ¶å¯¾SHAPï¼‰ã‚’è¨ˆç®—ã—ã€
        DataFrame (rank, feature, mean_abs_shap, model) ã‚’è¿”ã™ã€‚

        - ç¾çŠ¶ã¯ LightGBM ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚­ãƒ¼åã« 'lgb' ã‚’å«ã‚€ã‚‚ã®ï¼‰ã‚’å¯¾è±¡ã€‚
        - èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã¯ _load_shap_background_features() ã§èª­ã¿è¾¼ã‚€ã€‚
        - cache_sec ç§’ä»¥å†…ã«åŒã˜æ¡ä»¶ã§å‘¼ã°ã‚ŒãŸå ´åˆã¯å‰å›çµæœã‚’å†åˆ©ç”¨ã™ã‚‹ã€‚
        """
        model_key = ",".join(
            f"{name}:{id(model)}"
            for name, model in sorted(self.models.items())
        )

        if csv_path is None:
            csv_real = Path("data") / "USDJPY" / "features_for_shap.csv"
        else:
            csv_real = csv_path

        try:
            stat = csv_real.stat()
            csv_sig = f"{csv_real.resolve()}|{int(stat.st_mtime)}|{stat.st_size}"
        except FileNotFoundError:
            csv_sig = f"{csv_real.resolve()}|missing"

        key = f"{model_key}|{csv_sig}|top={top_n}|bg={max_background}"
        now = time.time()

        if (
            self._shap_cache is not None
            and self._shap_cache_key == key
            and (now - self._shap_cache_ts) < cache_sec
        ):
            return self._shap_cache.copy()

        target_name: Optional[str] = None
        target_model: Any | None = None

        for name, model in self.models.items():
            if "lgb" in name.lower():
                target_name = name
                target_model = model
                break

        if target_model is None:
            raise RuntimeError(
                "SHAPè¨ˆç®—å¯¾è±¡ã® LightGBM ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                "AISvc.models ã« 'lgb' ã‚’å«ã‚€ã‚­ãƒ¼ã§ LightGBM ã‚’ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚"
            )

        logger.info(
            "SHAPè¨ˆç®—å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: name={name}, type={typ}",
            name=target_name,
            typ=type(target_model).__name__,
        )

        df_bg = self._load_shap_background_features(
            max_rows=max_background,
            csv_path=csv_real,
        )

        feature_names = (
            list(self.expected_features)
            if self.expected_features
            else list(df_bg.columns)
        )

        items: list[ShapFeatureImpact] = compute_shap_feature_importance(
            target_model,
            df_bg,
            feature_names=feature_names,
            top_n=top_n,
            max_background=max_background,
        )

        df_result = shap_items_to_frame(items)
        df_result.insert(0, "model", target_name)

        self._shap_cache = df_result.copy()
        self._shap_cache_key = key
        self._shap_cache_ts = now

        return df_result



=== file: app/services/aisvc_loader.py ===

# app/services/aisvc_loader.py
import json
from pathlib import Path

ROOT = Path(r"C:\fxbot")  # é‹ç”¨å›ºå®š
ACTIVE = ROOT / "active_model.json"
MODELS = ROOT / "models_store"


class ActiveModelInfo(dict):
    @property
    def model_path(self) -> Path:
        return MODELS / self["model_name"]


def load_active_model_meta() -> ActiveModelInfo | None:
    if not ACTIVE.exists():
        return None
    meta = json.loads(ACTIVE.read_text(encoding="utf-8"))
    return ActiveModelInfo(meta)


def resolve_model_path() -> Path | None:
    meta = load_active_model_meta()
    if not meta:
        return None
    p = meta.model_path
    return p if p.exists() else None


# ä¾‹ï¼šGUIèµ·å‹•æ™‚
def load_model_for_inference():
    p = resolve_model_path()
    if not p:
        print("[AISvc] no active model; fallback to bundled default")
        # ã“ã“ã§åŒæ¢±ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãªã©
        return None
    print(f"[AISvc] loading: {p.name}")
    # å®Ÿéš›ã¯ joblib/pickle/onnxruntime ç­‰ã§ãƒ­ãƒ¼ãƒ‰
    # return joblib.load(p)
    return None



=== file: app/services/circuit_breaker.py ===

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Optional


JST = timezone(timedelta(hours=9))


@dataclass
class CBState:
    tripped: bool = False
    reason: Optional[str] = None
    consecutive_losses: int = 0
    last_trip_ts: Optional[float] = None
    daily_loss_accum_jpy: float = 0.0
    day_key: str = ""


class CircuitBreaker:
    """
    Resettable circuit breaker that combines consecutive-loss and daily-loss budgets
    with a cool-down period.
    """

    def __init__(
        self,
        max_consecutive_losses: int = 5,
        daily_loss_limit_jpy: float = 0.0,
        cooldown_min: int = 30,
    ):
        self.max_consecutive_losses = int(max_consecutive_losses)
        self.daily_loss_limit_jpy = float(daily_loss_limit_jpy)
        self.cooldown_min = int(cooldown_min)
        self.state = CBState()

    # ------------------------------------------------------------------ #
    # Public API
    # ------------------------------------------------------------------ #
    def on_trade_result(self, profit_jpy: float) -> None:
        """Record a trade result and trip if thresholds are violated."""
        self._rollover_if_new_day()
        if profit_jpy <= 0:
            self.state.consecutive_losses += 1
        else:
            self.state.consecutive_losses = 0

        self.state.daily_loss_accum_jpy += float(profit_jpy)

        if self.max_consecutive_losses > 0 and self.state.consecutive_losses >= self.max_consecutive_losses:
            self._trip("consecutive_losses")

        if (
            self.daily_loss_limit_jpy
            and self.state.daily_loss_accum_jpy <= -abs(self.daily_loss_limit_jpy)
        ):
            self._trip("daily_loss_limit")

    def can_trade(self) -> bool:
        """Return True if trading is allowed (not tripped or cool-down finished)."""
        if self.state.tripped and self.state.last_trip_ts:
            elapsed = datetime.now(tz=timezone.utc).timestamp() - self.state.last_trip_ts
            if elapsed < self.cooldown_min * 60:
                return False
            self.reset()
        return True

    def reset(self) -> None:
        """Reset trip status (but keep daily accumulator)."""
        self.state.tripped = False
        self.state.reason = None
        self.state.consecutive_losses = 0
        self.state.last_trip_ts = None

    def status(self) -> dict:
        """Return a serialisable snapshot of the breaker state."""
        return {
            "tripped": self.state.tripped,
            "reason": self.state.reason,
            "consecutive_losses": self.state.consecutive_losses,
            "daily_loss_accum_jpy": self.state.daily_loss_accum_jpy,
            "day_key": self.state.day_key,
            "cooldown_min": self.cooldown_min,
        }

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #
    def _trip(self, reason: str) -> None:
        self.state.tripped = True
        self.state.reason = reason
        self.state.last_trip_ts = datetime.now(tz=timezone.utc).timestamp()

    def _rollover_if_new_day(self) -> None:
        now = datetime.now(JST)
        key = now.strftime("%Y-%m-%d")
        if key != self.state.day_key:
            self.state.day_key = key
            self.state.daily_loss_accum_jpy = 0.0



=== file: app/services/data_guard.py ===

# app/services/data_guard.py
from __future__ import annotations
import subprocess
from pathlib import Path
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parents[2]  # app/services/ â†’ app â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
DATA_DIR = PROJECT_ROOT / "data"

def csv_path(symbol_tag: str, timeframe: str, layout: str="per-symbol") -> Path:
    """
    symbol_tag ã¯æ¥å°¾è¾ãªã—ï¼ˆä¾‹: USDJPYï¼‰
    layout: "flat" or "per-symbol"
    """
    if layout == "per-symbol":
        return DATA_DIR / symbol_tag / "ohlcv" / f"{symbol_tag}_{timeframe}.csv"
    return DATA_DIR / f"{symbol_tag}_{timeframe}.csv"

def ensure_data(symbol_tag: str, timeframe: str, start_date: str, end_date: str,
                env: str="laptop", layout: str="per-symbol") -> Path:
    """
    æŒ‡å®šã® [start_date, end_date] ã‚’æº€ãŸã™CSVãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã€è¶³ã‚Šãªã‘ã‚Œã° scripts.make_csv_from_mt5 ã‚’å‘¼ã‚“ã§è¿½è¨˜ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: CSVã®ãƒ•ãƒ«ãƒ‘ã‚¹
    """
    out_csv = csv_path(symbol_tag, timeframe, layout)
    need_fetch = True

    if out_csv.exists():
        try:
            df = pd.read_csv(out_csv, parse_dates=["time"])
            if not df.empty:
                has_start = (df["time"].min() <= pd.Timestamp(start_date))
                has_end   = (df["time"].max() >= pd.Timestamp(end_date))
                need_fetch = not (has_start and has_end)
        except Exception:
            need_fetch = True

    if need_fetch:
        # make_csv_from_mt5 ã‚’å‘¼ã¶ï¼ˆä¸è¶³åˆ†ã¯è‡ªå‹•è¿½è¨˜ï¼‰
        cmd = [
            str((PROJECT_ROOT / "scripts" / "make_csv_from_mt5.py").resolve()),
            "--symbol", symbol_tag,
            "--timeframes", timeframe,
            "--start", start_date,
            "--layout", layout,
            "--env", env,
        ]
        # Windows ã§ã¯ python çµŒç”±ã§å®Ÿè¡Œ
        subprocess.check_call(["python", *cmd], cwd=str(PROJECT_ROOT))

    # æœ€çµ‚ãƒã‚§ãƒƒã‚¯
    if not out_csv.exists():
        raise FileNotFoundError(f"CSV not found after update: {out_csv}")
    return out_csv



=== file: app/services/decision_log.py ===

# app/services/decision_log.py
from __future__ import annotations

import json
from collections.abc import Mapping, Sequence
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List, Optional

import pandas as pd
import fxbot_path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ = app/services/ ã‹ã‚‰ 2 ã¤ä¸Š
_PROJECT_ROOT = Path(__file__).resolve().parents[2]
_LOG_DIR = _PROJECT_ROOT / "logs" / "decisions"


@dataclass
class DecisionRecord:
    """
    decisions_*.jsonl ã® 1 è¡Œã‚’ã€GUI ã‚„ KPI è¨ˆç®—ã‹ã‚‰ä½¿ã„ã‚„ã™ã„å½¢ã«è–„ããƒ©ãƒƒãƒ—ã—ãŸã‚‚ã®ã€‚

    å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¢—ã‚„ã›ã‚‹ã‚ˆã†ã«ã—ã¦ãŠãã€‚
    """
    ts_jst: str
    symbol: str
    action: str
    side: Optional[str]
    reason: Optional[str]
    meta: Optional[str]
    blocked: Optional[str]
    raw: dict[str, Any]


def _iter_jsonl(path: Path) -> Iterable[dict[str, Any]]:
    """JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ 1 è¡Œãšã¤ dict ã¨ã—ã¦è¿”ã™ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚å£Šã‚ŒãŸè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚"""
    try:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    yield json.loads(line)
                except Exception:
                    # å£Šã‚ŒãŸ1è¡ŒãŒã‚ã£ã¦ã‚‚å…¨ä½“ã¯æ­¢ã‚ãªã„
                    continue
    except FileNotFoundError:
        return


def _extract_decision_record(j: dict[str, Any]) -> DecisionRecord:
    """
    ï¿½ï¿½ JSON ï¿½ï¿½ï¿½ï¿½AAIï¿½^ï¿½u/KPI ï¿½Å‚æ‚­ï¿½gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ DecisionRecord ï¿½ï¿½ï¿½ï¿½ï¿½B
    """
    ts = str(j.get("ts_jst") or j.get("ts") or "")
    symbol = str(j.get("symbol") or "")

    decision_raw = j.get("decision")
    if isinstance(decision_raw, dict):
        decision = decision_raw
    elif isinstance(decision_raw, str):
        decision = {"action": decision_raw}
    else:
        decision = {}

    action = str(decision.get("action") or "")

    inner_dec_raw = decision.get("dec")
    if isinstance(inner_dec_raw, dict):
        inner_dec = inner_dec_raw
    else:
        inner_dec = {}

    side = inner_dec.get("side") or decision.get("side")
    if side is not None:
        side = str(side)

    meta = j.get("meta")
    if meta is None:
        if isinstance(decision_raw, dict):
            meta = decision_raw.get("meta")
        elif isinstance(decision_raw, str):
            meta = decision_raw
    if meta is not None:
        meta = str(meta)

    reason = decision.get("reason")
    if reason is not None:
        reason = str(reason)

    filters_raw = j.get("filters") or {}
    filters = filters_raw if isinstance(filters_raw, dict) else {}
    blocked = filters.get("blocked")
    if blocked is not None:
        blocked = str(blocked)

    return DecisionRecord(
        ts_jst=ts,
        symbol=symbol,
        action=action,
        side=side,
        reason=reason,
        meta=meta,
        blocked=blocked,
        raw=j,
    )


def _find_first_numeric_by_keys(
    container: Any,
    key_candidates: tuple[str, ...],
) -> float | None:
    """
    ä»»æ„ã«ãƒã‚¹ãƒˆã—ãŸ dict/list æ§‹é€ ã®ä¸­ã‹ã‚‰ã€
    æŒ‡å®šã—ãŸã‚­ãƒ¼åã®ã„ãšã‚Œã‹ã«å¯¾å¿œã™ã‚‹ã€Œæœ€åˆã®æ•°å€¤ã€ã‚’è¿”ã™ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° Noneã€‚
    """
    if isinstance(container, Mapping):
        for k in key_candidates:
            if k in container and container[k] is not None:
                try:
                    return float(container[k])
                except (TypeError, ValueError):
                    pass
        for value in container.values():
            val = _find_first_numeric_by_keys(value, key_candidates)
            if val is not None:
                return val
    elif isinstance(container, Sequence) and not isinstance(container, (str, bytes, bytearray)):
        for item in container:
            val = _find_first_numeric_by_keys(item, key_candidates)
            if val is not None:
                return val
    return None


def _ensure_pnl_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    decisions_* ã®ç”Ÿãƒ­ã‚° DataFrame ã«ã€Œpnl åˆ—ã€ãŒç„¡ã‘ã‚Œã°ã€
    exit_plan / decision_detail / ai / meta ãªã©ã® dict ã®ä¸­ã‹ã‚‰
    ãã‚Œã£ã½ã„ã‚­ãƒ¼åã‚’å†å¸°çš„ã«æ¢ã—ã€æœ€åˆã«è¦‹ã¤ã‹ã£ãŸæ•°å€¤ã‚’ pnl ã¨ã™ã‚‹ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã„è¡Œã¯ NaN ã®ã¾ã¾ã€‚
    """
    if "pnl" in df.columns:
        return df

    KEY_CANDIDATES: tuple[str, ...] = (
        "pnl",
        "profit",
        "pl_jpy",
        "pl",
        "pips",
    )

    TARGET_COLS: tuple[str, ...] = (
        "exit_plan",
        "decision_detail",
        "ai",
        "meta",
    )

    def _row_pnl(row: pd.Series) -> float | None:
        for col_name in TARGET_COLS:
            if col_name not in row:
                continue
            container = row[col_name]
            val = _find_first_numeric_by_keys(container, KEY_CANDIDATES)
            if val is not None:
                return val
        return None

    df = df.copy()
    df["pnl"] = df.apply(_row_pnl, axis=1)
    return df


def _get_decision_log_dir() -> Path:
    """
    æ±ºå®šãƒ­ã‚°ã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿”ã™ã€‚

    ä¾‹: <project_root>/logs/decisions
    """
    root = fxbot_path.get_project_root()
    return root / "logs" / "decisions"


def load_recent_decisions(limit: int | None = None) -> pd.DataFrame:
    """
    decisions_*.jsonl ã‹ã‚‰æœ€æ–°ã® N ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ pandas.DataFrame ã§èª­ã¿è¾¼ã‚€ã€‚
    """
    log_dir = _get_decision_log_dir()
    files = sorted(log_dir.glob("decisions_*.jsonl"))
    if not files:
        return pd.DataFrame()

    df_list: list[pd.DataFrame] = []
    for f in files:
        try:
            df_list.append(pd.read_json(f, lines=True))
        except Exception:
            continue

    if not df_list:
        return pd.DataFrame()

    df = pd.concat(df_list, ignore_index=True)

    if "ts_jst" in df.columns:
        df = df.sort_values("ts_jst", ascending=False)
    elif "timestamp" in df.columns:
        df = df.sort_values("timestamp", ascending=False)

    if limit is not None and limit > 0:
        df = df.head(limit)

    df = _ensure_pnl_column(df)

    return df.reset_index(drop=True)



=== file: app/services/event_store.py ===

from __future__ import annotations

import json
import os
import threading
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from typing import Any, Deque, List, Optional
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ = app/services/ ã‹ã‚‰ 2 ã¤ä¸Š
_PROJECT_ROOT = Path(__file__).resolve().parents[2]

_LOG_DIR = _PROJECT_ROOT / "logs"
_LOG_FILE = _LOG_DIR / "ui_events.jsonl"
_LOG_DIR.mkdir(parents=True, exist_ok=True)

_lock = threading.Lock()


@dataclass
class UiEvent:
    ts: str
    kind: str
    symbol: str
    side: Optional[str] = None
    price: Optional[float] = None
    sl: Optional[float] = None
    tp: Optional[float] = None
    profit_jpy: Optional[float] = None
    reason: Optional[str] = None
    notes: Optional[str] = None


def _now() -> str:
    return datetime.now(timezone.utc).astimezone().isoformat(timespec="seconds")


class _EventStore:
    def __init__(self, maxlen: int = 1000):
        self._buf: Deque[UiEvent] = deque(maxlen=maxlen)

    def append(self, ev: UiEvent) -> None:
        with _lock:
            self._buf.appendleft(ev)
            with open(_LOG_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(ev), ensure_ascii=False) + "\n")

    def add(self, **kwargs: Any) -> None:
        kwargs.setdefault("ts", _now())
        self.append(UiEvent(**kwargs))

    def recent(self, n: int = 200) -> List[UiEvent]:
        with _lock:
            return list(list(self._buf)[:n])


EVENT_STORE = _EventStore()



=== file: app/services/execution_stub.py ===

ï»¿from __future__ import annotations

import json
import os
import re
import statistics
from collections import deque, defaultdict
from datetime import datetime
from dataclasses import dataclass
from typing import Any, DefaultDict, Dict, Optional, Tuple
from zoneinfo import ZoneInfo
from pathlib import Path

from loguru import logger

from app.core import market, mt5_client
from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_service, trade_state
from app.services.orderbook_stub import orderbook
from app.services.trailing import AtrTrailer, TrailConfig, TrailState
from app.services.trailing_hook import apply_trailing_update
from core import position_guard
from core.ai.service import AISvc, ProbOut
from core.metrics import METRICS
from core.utils.timeutil import now_jst_iso
from app.services.event_store import EVENT_STORE
from app.services.metrics import publish_metrics

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ = app/services/ ã‹ã‚‰ 2 ã¤ä¸Š
_PROJECT_ROOT = Path(__file__).resolve().parents[2]

LOG_DIR = _PROJECT_ROOT / "logs" / "decisions"
LOG_DIR.mkdir(parents=True, exist_ok=True)

_ATR_MED: deque[float] = deque(maxlen=128)
_ATR_LAST_PASS: bool = False
_ATR_LAST_REF: Optional[float] = None
_ATR_LAST_ENABLE: Optional[float] = None
_ATR_LAST_DISABLE: Optional[float] = None

# Trailing state store shared across dryrun/production per symbol
runtime_trail_states: DefaultDict[str, Dict[str, Any]] = defaultdict(dict)


def _load_runtime_threshold(default: float = 0.5) -> float:
    try:
        meta_path = _PROJECT_ROOT / "models" / "active_model.json"
        if meta_path.exists():
            with open(meta_path, "r", encoding="utf-8") as fh:
                meta = json.load(fh)
            threshold = meta.get("best_threshold")
            if isinstance(threshold, (int, float)) and 0.0 < threshold < 1.0:
                print(f"[exec] using best_threshold from active_model.json: {threshold}")
                return float(threshold)
    except Exception as exc:
        print(f"[exec][warn] failed to load best_threshold: {exc}")
    print(f"[exec] using default threshold: {default}")
    return float(default)


BEST_THRESHOLD = _load_runtime_threshold(0.5)
print(f"[exec] active BEST_THRESHOLD={BEST_THRESHOLD}", flush=True)

def reset_atr_gate_state() -> None:
    """???/????????ATR????????????"""
    global _ATR_MED, _ATR_LAST_PASS
    _ATR_MED.clear()
    _ATR_LAST_PASS = False


def _atr_gate_ok(atr_pct_now: float, runtime_cfg: Dict[str, Any]) -> bool:
    """Hysteresis-enabled ATR gate to avoid rapid flip-flops around thresholds."""
    global _ATR_LAST_PASS, _ATR_LAST_REF, _ATR_LAST_ENABLE, _ATR_LAST_DISABLE

    filters_cfg: Dict[str, Any] = {}
    if isinstance(runtime_cfg, dict):
        filters_cfg = (runtime_cfg.get("filters") or {})

    if not filters_cfg:
        try:
            cfg = load_config()
            filters_cfg = cfg.get("filters", {})
        except Exception:
            filters_cfg = {}

    hy = (filters_cfg.get("atr_hysteresis") or {}) if isinstance(filters_cfg, dict) else {}

    default_min = 0.00055
    if isinstance(runtime_cfg, dict):
        default_min = float(runtime_cfg.get("min_atr_pct", default_min))

    en = float(hy.get("enable_min_pct", default_min))
    de = float(hy.get("disable_min_pct", min(en, 0.00045)))
    _ATR_LAST_ENABLE = en
    _ATR_LAST_DISABLE = de
    lb = int(hy.get("lookback", 12)) or 1
    if lb <= 0:
        lb = 1

    _ATR_MED.append(float(atr_pct_now))
    window = list(_ATR_MED)[-lb:] or [atr_pct_now]
    try:
        ref = statistics.median(window)
    except Exception:
        ref = float(window[-1])
    _ATR_LAST_REF = ref

    if _ATR_LAST_PASS:
        if ref < de:
            _ATR_LAST_PASS = False
        return _ATR_LAST_PASS or ref >= de
    else:
        if ref >= en:
            _ATR_LAST_PASS = True
        return _ATR_LAST_PASS or ref >= en


def _tick_to_dict(tick: Any) -> Optional[Dict[str, float]]:
    if tick is None:
        return None

    if isinstance(tick, dict):
        bid = tick.get("bid")
        ask = tick.get("ask")
    else:
        try:
            bid, ask = tick
        except (TypeError, ValueError):
            return None
    try:
        bid_f = float(bid) if bid is not None else 0.0
        ask_f = float(ask) if ask is not None else 0.0
    except (TypeError, ValueError):
        return None
    return {"bid": bid_f, "ask": ask_f, "mid": (bid_f + ask_f) / 2.0}

def _pip_size_for(symbol: str) -> float:
    return 0.01 if symbol.endswith("JPY") else 0.0001

def _point_for(symbol: str) -> float:
    return 0.001 if symbol.endswith("JPY") else 0.0001

def _mid_price(tick_dict: Optional[Dict[str, float]]) -> Optional[float]:
    if tick_dict is None:
        return None
    return tick_dict.get("mid")

def _current_price_for_side(tick_dict: Optional[Dict[str, float]], side: str, price_source: str) -> Optional[float]:
    if tick_dict is None:
        return None
    ps = (price_source or "mid").lower()
    if ps == "bid":
        return tick_dict.get("bid") if side == "BUY" else tick_dict.get("ask")
    if ps == "ask":
        return tick_dict.get("ask") if side == "BUY" else tick_dict.get("bid")
    return tick_dict.get("mid")

def _register_trailing_state(symbol: str, signal: Dict[str, Any], tick_dict: Optional[Dict[str, float]]) -> None:
    xp = signal.get("exit_plan") or {}
    if xp.get("mode") != "atr":
        runtime_trail_states.pop(symbol, None)
        return

    trailing = xp.get("trailing") or {}
    if not trailing.get("enabled", True):
        runtime_trail_states.pop(symbol, None)
        return

    atr_val = float(xp.get("atr") or 0.0)
    if atr_val <= 0.0:
        return

    side = signal.get("side")
    if not side:
        return

    pip_size = float(_pip_size_for(symbol))
    point = float(_point_for(symbol))
    price_source = (trailing.get("price_source") or "mid").lower()

    entry_price = signal.get("entry_price")
    if entry_price is None and tick_dict is not None:
        entry_price = _current_price_for_side(tick_dict, side, price_source)
    if entry_price is None:
        entry_price = _mid_price(tick_dict) if tick_dict else None
    if entry_price is None:
        return

    state = {
        "mode": "atr",
        "side": side,
        "symbol": symbol,
        "entry": float(entry_price),
        "atr": atr_val,
        "pip_size": pip_size,
        "point": point,
        "activate_atr_mult": float(trailing.get("activate_atr_mult", 0.5)),
        "step_atr_mult": float(trailing.get("step_atr_mult", 0.25)),
        "lock_be_atr_mult": float(trailing.get("lock_be_atr_mult", 0.3)),
        "hard_floor_pips": float(trailing.get("hard_floor_pips", 5.0)),
        "only_in_profit": bool(trailing.get("only_in_profit", True)),
        "max_layers": int(trailing.get("max_layers", 20)),
        "price_source": price_source,
        "activated": False,
        "be_locked": False,
        "layers": 0,
        "current_sl": None,
    }

    trail = runtime_trail_states.setdefault(symbol, {})
    trail.clear()
    trail.update(state)
    signal["trail_state"] = {
        "activated": False,
        "be_locked": False,
        "layers": 0,
        "current_sl": None,
        "atr": atr_val,
        "activate_atr_mult": state["activate_atr_mult"],
        "step_atr_mult": state["step_atr_mult"],
        "lock_be_atr_mult": state["lock_be_atr_mult"],
        "hard_floor_pips": state["hard_floor_pips"],
        "price_source": price_source,
        "max_layers": state["max_layers"],
        "only_in_profit": state["only_in_profit"],
        "side": side,
        "symbol": symbol,
        "entry": float(entry_price),
    }
    publish_metrics({
        "trail_activated": False,
        "trail_be_locked": False,
        "trail_layers":    0,
        "trail_current_sl": None,
    })
    signal["entry_price"] = float(entry_price)

def _update_trailing_state(symbol: str, tick_dict: Optional[Dict[str, float]]) -> Optional[Dict[str, Any]]:
    if tick_dict is None:
        return None

    state = runtime_trail_states.setdefault(symbol, {})
    if not state or state.get("mode") != "atr":
        return None

    side = state.get("side")
    entry = state.get("entry")
    atr_val = float(state.get("atr") or 0.0)
    if not side or entry is None or atr_val <= 0.0:
        return None

    price_source = (state.get("price_source") or "mid").lower()
    current_price = _current_price_for_side(tick_dict, side, price_source)
    if current_price is None:
        return None

    cfg = TrailConfig(
        pip_size=float(state.get("pip_size", _pip_size_for(symbol))),
        point=float(state.get("point", _point_for(symbol))),
        atr=atr_val,
        activate_mult=float(state.get("activate_atr_mult", 0.5)),
        step_mult=float(state.get("step_atr_mult", 0.25)),
        lock_be_mult=float(state.get("lock_be_atr_mult", 0.3)),
        hard_floor_pips=float(state.get("hard_floor_pips", 5.0)),
        only_in_profit=bool(state.get("only_in_profit", True)),
        max_layers=int(state.get("max_layers", 20)),
    )
    trail_state = TrailState(
        side=side,
        entry=float(entry),
        activated=bool(state.get("activated", False)),
        be_locked=bool(state.get("be_locked", False)),
        layers=int(state.get("layers", 0)),
        current_sl=state.get("current_sl"),
    )

    trailer = AtrTrailer(cfg, trail_state)
    new_sl = trailer.suggest_sl(float(current_price))

    state.update(
        {
            "activated": trail_state.activated,
            "be_locked": trail_state.be_locked,
            "layers": trail_state.layers,
            "current_sl": trail_state.current_sl,
        }
    )
    runtime_trail_states[symbol] = state

    if new_sl is None:
        return None

    return {
        "new_sl": new_sl,
        "price": current_price,
        "state": {
            "activated": trail_state.activated,
            "be_locked": trail_state.be_locked,
            "layers": trail_state.layers,
            "current_sl": trail_state.current_sl,
            "price_source": price_source,
            "atr": atr_val,
            "max_layers": int(state.get("max_layers", 20)),
            "only_in_profit": bool(state.get("only_in_profit", True)),
            "side": side,
            "symbol": state.get("symbol", symbol),
        },
    }

def _session_hour_allowed() -> bool:
    """
    config.session.allow_hours_jst ??????????????????????
    ????????/???/???????????
    """
    try:
        from core.config import cfg as _cfg
    except Exception:
        _cfg = {}

    session_cfg = {}
    if isinstance(_cfg, dict):
        raw = _cfg.get("session")
        session_cfg = raw if isinstance(raw, dict) else {}

    allow = session_cfg.get("allow_hours_jst", [])
    if not isinstance(allow, (list, tuple, set)) or len(allow) == 0:
        return True

    try:
        import pytz
        from datetime import datetime
        jst = pytz.timezone("Asia/Tokyo")
        hour = datetime.now(jst).hour
    except Exception:
        return True

    return hour in set(allow)

def _symbol_to_filename(symbol: str) -> str:
    safe = re.sub(r"[^A-Za-z0-9_]+", "_", symbol)
    return safe.strip("_") or "UNKNOWN"


def _write_decision_log(symbol: str, record: Dict[str, Any]) -> None:
    fname = LOG_DIR / f"decisions_{_symbol_to_filename(symbol)}.jsonl"
    with open(fname, "a", encoding="utf-8") as fp:
        fp.write(json.dumps(record, ensure_ascii=False) + "\n")


def _build_decision_trace(
    *,
    ts_jst: str,
    symbol: str,
    ai_out: "ProbOut",
    cb_status: Dict[str, Any],
    filters_ctx: Dict[str, Any],
    decision: Dict[str, Any],
    prob_threshold: float,
    calibrator_name: str,
) -> Dict[str, Any]:
    """Assemble a structured trace record for downstream analysis."""
    if isinstance(decision, dict):
        action = str(decision.get("action") or "").upper()
        if action in {"BUY", "SELL", "LONG", "SHORT"}:
            decision_label = "ENTRY"
        else:
            decision_label = str(decision.get("action") or "")
    else:
        decision_label = str(decision)

    trace = {
        "ts_jst": ts_jst,
        "type": "decision",
        "symbol": symbol,
        "filters": filters_ctx,
        "probs": {
            "buy": round(ai_out.p_buy, 6),
            "sell": round(ai_out.p_sell, 6),
            "skip": round(ai_out.p_skip, 6),
        },
        "calibrator": calibrator_name,
        "meta": ai_out.meta,
        "threshold": float(prob_threshold),
        "decision": decision_label,
        "ai": ai_out.model_dump(),
        "cb": cb_status,
        "features_hash": ai_out.features_hash,
        "model": ai_out.model_name,
    }
    if isinstance(decision, dict):
        trace["decision_detail"] = decision
    exit_plan = decision.get("signal", {}).get("exit_plan") if isinstance(decision, dict) else None
    trace["exit_plan"] = exit_plan or {"mode": "none"}
    return trace

def _collect_features(
    symbol: str,
    base_features: Tuple[str, ...],
    tick: Optional[Tuple[float, float]],
    spread_pips: Optional[float],
    open_positions: int,
) -> Dict[str, float]:
    bid, ask = tick if tick else (None, None)
    mid = (float(bid) + float(ask)) / 2 if bid is not None and ask is not None else 0.0
    spr = float(spread_pips) if spread_pips is not None else 0.0

    features: Dict[str, float] = {}
    if not base_features:
        features["bias"] = 1.0
        return features

    for name in base_features:
        if name == "ema_5":
            features[name] = mid
        elif name == "ema_20":
            features[name] = mid
        elif name == "rsi_14":
            features[name] = 50.0
        elif name == "atr_14":
            features[name] = spr
        elif name == "adx_14":
            features[name] = 20.0 + min(20.0, spr * 5.0)
        elif name == "bbp":
            features[name] = 0.5 if spr == 0 else max(0.0, min(1.0, spr / 5.0))
        elif name == "vol_chg":
            features[name] = float(open_positions)
        elif name == "wick_ratio":
            features[name] = 0.5
        else:
            features[name] = 0.0
    return features


@dataclass
class ExecutionStub:
    """
    ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ç”¨ã®å®Ÿè¡Œã‚¹ã‚¿ãƒ–ï¼š
    - AIç¢ºç‡ï¼ˆAISvc.predictï¼‰ã‚’å‘¼ã³å‡ºã—ã¦æ„æ€æ±ºå®šã ã‘è¡Œã„ã€ç´„å®šã¯ã—ãªã„
    - ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ï¼ˆself.cbï¼‰ç™ºå‹•ä¸­ã¯ BLOCKED ã‚’è¨˜éŒ²
    """
    cb: circuit_breaker.CircuitBreaker
    ai: AISvc

    def __post_init__(self) -> None:
        try:
            self.ai.threshold = float(BEST_THRESHOLD)
        except Exception:
            pass
        try:
            sell_threshold = max(min(1.0 - BEST_THRESHOLD, 1.0), 0.0)
            trade_state.update(
                prob_threshold=float(BEST_THRESHOLD),
                threshold_buy=float(BEST_THRESHOLD),
                threshold_sell=float(sell_threshold),
            )
        except Exception:
            pass


    def on_tick(
        self,
        symbol: str,
        features: Dict[str, float],
        runtime_cfg: Dict[str, Any],
    ) -> Dict[str, Any]:
        ts = now_jst_iso()

        cb_status = self.cb.status()
        ai_out = self.ai.predict(features)

        tick_dict = _tick_to_dict(runtime_cfg.get("tick"))

        spread_limit = float(runtime_cfg.get("spread_limit_pips", 1.5))
        min_adx = float(runtime_cfg.get("min_adx", 15.0))
        min_atr_pct = float(runtime_cfg.get("min_atr_pct", 0.0003))
        disable_adx_gate = bool(runtime_cfg.get("disable_adx_gate", False))
        prob_threshold = float(BEST_THRESHOLD)
        runtime_cfg["prob_threshold"] = prob_threshold
        side_bias = runtime_cfg.get("side_bias")

        raw_spread = runtime_cfg.get("spread_pips", 0.0)
        try:
            cur_spread = float(raw_spread)
        except (TypeError, ValueError):
            cur_spread = 0.0
        cur_spread = round(cur_spread, 5)

        cur_adx = round(float(features.get("adx_14", 0.0)), 5)
        cur_atr_pct = round(float(features.get("atr_14", 0.0)), 8)

        base_filters: Dict[str, Any] = {
            "spread": cur_spread,
            "spread_limit": spread_limit,
            "adx": cur_adx,
            "min_adx": min_adx,
            "adx_disabled": disable_adx_gate,
            "atr_pct": cur_atr_pct,
            "min_atr_pct": min_atr_pct,
            "prob_threshold": prob_threshold,
        }
        if side_bias is not None:
            base_filters["side_bias"] = side_bias

        atr_gate_ok = _atr_gate_ok(cur_atr_pct, runtime_cfg)
        if _ATR_LAST_REF is not None:
            base_filters["atr_ref"] = round(float(_ATR_LAST_REF), 8)
        base_filters["atr_gate_state"] = "open" if _ATR_LAST_PASS else "closed"
        if _ATR_LAST_ENABLE is not None:
            base_filters["atr_enable_min"] = float(_ATR_LAST_ENABLE)
        if _ATR_LAST_DISABLE is not None:
            base_filters["atr_disable_min"] = float(_ATR_LAST_DISABLE)

        grace_active = trade_service.post_fill_grace_active()
        base_filters["post_fill_grace"] = grace_active

        def _emit(decision: Dict[str, Any], filters_ctx: Dict[str, Any], level: str = "info") -> None:
            action = decision.get("action")
            reason = decision.get("reason")

            gate_state = filters_ctx.get("atr_gate_state")
            atr_ref = float(filters_ctx.get("atr_ref", filters_ctx.get("atr_pct", 0.0)) or 0.0)
            post_grace = bool(filters_ctx.get("post_fill_grace", False))

            # --- ã‚«ã‚¦ãƒ³ã‚¿ã¯ KVS ã‹ã‚‰å®‰å…¨ã«èª­ã¿å‡ºã—ã¦åŠ ç®— ---
            cur = METRICS.get()  # dictã‚³ãƒ”ãƒ¼ãŒè¿”ã‚‹æƒ³å®š
            ce = int(cur.get("count_entry", 0))
            cs = int(cur.get("count_skip", 0))
            cb = int(cur.get("count_blocked", 0))
            if action == "ENTRY":
                ce += 1
            elif action == "SKIP":
                cs += 1
            elif action == "BLOCKED":
                cb += 1

            # --- ã¾ã¨ã‚ã¦ publishï¼ˆKVSæ›´æ–°ï¼‹runtime/metrics.jsonåŸå­çš„æ›¸ãæ›ãˆï¼‰ ---
            publish_metrics({
                "last_decision": action,
                "last_reason":   reason,
                "atr_ref":       float(atr_ref),
                "atr_gate_state": gate_state,
                "post_fill_grace": bool(post_grace),
                "spread":          filters_ctx.get("spread"),
                "adx":             filters_ctx.get("adx"),
                "min_adx":         filters_ctx.get("min_adx"),
                "prob_threshold":  filters_ctx.get("prob_threshold"),
                "min_atr_pct":     filters_ctx.get("min_atr_pct"),
                "count_entry":     ce,
                "count_skip":      cs,
                "count_blocked":   cb,
                # ts ã¯ publish_metrics å´ã§ã‚‚è‡ªå‹•ä»˜ä¸ã™ã‚‹ãŒã€ã“ã“ã§å…¥ã‚Œã¦ã‚‚è‰¯ã„
            })


            trail_signal = decision.get("signal") if isinstance(decision, dict) else None
            if isinstance(trail_signal, dict) and "trail_state" in trail_signal:
                trail_state = trail_signal.get("trail_state") or {}
                new_sl_val = trail_state.get("current_sl")
                trail_side = trail_state.get("side") or trail_signal.get("side") or decision.get("side")
                trail_symbol = trail_state.get("symbol") or trail_signal.get("symbol") or symbol
                ticket = trail_state.get("ticket") if isinstance(trail_state, dict) else None
                if new_sl_val is not None and trail_side and trail_symbol:
                    try:
                        apply_trailing_update(
                            ticket=ticket if isinstance(ticket, int) else None,
                            side=str(trail_side),
                            symbol=str(trail_symbol),
                            new_sl=float(new_sl_val),
                            reason=str(action or "trail"),
                        )
                    except Exception as exc:
                        logger.debug(f"[TRAIL][HOOK][ERR] {exc}")

            trace = _build_decision_trace(
                ts_jst=ts,
                symbol=symbol,
                ai_out=ai_out,
                cb_status=cb_status,
                filters_ctx=filters_ctx,
                decision=decision,
                prob_threshold=prob_threshold,
                calibrator_name=self.ai.calibrator_name,
            )
            trace["runtime"] = runtime_cfg
            _write_decision_log(symbol, trace)

            ai_payload = ai_out.model_dump()
            ai_payload["best_threshold"] = BEST_THRESHOLD
            ai_payload.setdefault("threshold", getattr(self.ai, "threshold", prob_threshold))
            payload = {
                "mode": "dryrun",
                "symbol": symbol,
                "decision": decision.get("action"),
                "reason": decision.get("reason"),
                "ai": ai_payload,
                "filters": filters_ctx,
                "cb": cb_status,
            }
            log = logger.bind(event="dryrun", ts=ts)
            if level == "warning":
                log.warning(payload)
            elif level == "error":
                log.error(payload)
            else:
                log.info(payload)

        trail_info = _update_trailing_state(symbol, tick_dict)
        if trail_info:
            filters_ctx = dict(base_filters)
            filters_ctx["trail_state"] = trail_info["state"]
            filters_ctx["trail_new_sl"] = trail_info["new_sl"]
            filters_ctx["trail_price"] = trail_info["price"]
            publish_metrics({
                "trail_activated": bool(trail_info["state"].get("activated")),
                "trail_be_locked": bool(trail_info["state"].get("be_locked")),
                "trail_layers":    int(trail_info["state"].get("layers") or 0),
                "trail_current_sl": trail_info["state"].get("current_sl"),
            })

            decision_payload = {
                "action": "TRAIL_UPDATE",
                "reason": None,
                "signal": {
                    "trail_state": trail_info["state"],
                    "trail_new_sl": trail_info["new_sl"],
                    "trail_price": trail_info["price"],
                    "side": trail_info["state"].get("side"),
                    "symbol": trail_info["state"].get("symbol", symbol),
                },
            }
            _emit(decision_payload, filters_ctx, level="info")

        if not _session_hour_allowed():
            filters_ctx = dict(base_filters)
            filters_ctx["session"] = "closed"
            decision_payload = {"action": "SKIP", "reason": "session_closed"}
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        if not grace_active and cur_spread and cur_spread > spread_limit:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "spread"
            decision_payload = {"action": "BLOCKED", "reason": "spread"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not grace_active and not disable_adx_gate and cur_adx < min_adx:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "adx_low"
            decision_payload = {"action": "BLOCKED", "reason": "adx_low"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not grace_active and not atr_gate_ok:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "atr_low"
            decision_payload = {"action": "BLOCKED", "reason": "atr_low"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not self.cb.can_trade():
            cb_status = self.cb.status()
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "circuit_breaker"
            decision_payload = {"action": "BLOCKED", "reason": cb_status.get("reason", "circuit_breaker")}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        cb_status = self.cb.status()

        if cb_status.get("tripped"):
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "circuit_breaker"
            decision_payload = {"action": "BLOCKED", "reason": cb_status.get("reason", "circuit_breaker")}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        config = load_config()
        entry_cfg = config.get("entry", {}) if isinstance(config, dict) else {}
        edge = float(entry_cfg.get("entry_min_edge", entry_cfg.get("min_edge", 0.0)))
        buy_threshold = prob_threshold
        sell_threshold = max(min(1.0 - prob_threshold, 1.0), 0.0)

        base_filters["threshold_buy"] = buy_threshold
        base_filters["threshold_sell"] = sell_threshold
        base_filters["edge"] = edge

        filters_ctx = dict(base_filters)
        filters_ctx["blocked"] = None

        if side_bias is None:
            side_bias = (entry_cfg.get("side_bias") or "auto").lower()
        else:
            side_bias = str(side_bias).lower()
        filters_ctx["side_bias"] = side_bias

        p_buy = float(ai_out.p_buy)
        p_sell = float(ai_out.p_sell)

        buy_ok = p_buy >= buy_threshold
        sell_ok = p_sell >= sell_threshold

        chosen_side = None
        chosen_prob = 0.0
        other_prob = 0.0

        if buy_ok and (not sell_ok or p_buy >= p_sell):
            chosen_side = "BUY"
            chosen_prob = p_buy
            other_prob = p_sell
        elif sell_ok:
            chosen_side = "SELL"
            chosen_prob = p_sell
            other_prob = p_buy

        if chosen_side is not None and p_buy == p_sell:
            if side_bias == "buy":
                chosen_side = "BUY"
                chosen_prob = p_buy
                other_prob = p_sell
            elif side_bias == "sell":
                chosen_side = "SELL"
                chosen_prob = p_sell
                other_prob = p_buy

        decision_info: Dict[str, Any] = {
            "threshold_buy": buy_threshold,
            "threshold_sell": sell_threshold,
            "edge": edge,
            "prob_buy": p_buy,
            "prob_sell": p_sell,
        }

        if chosen_side is None:
            decision_info.update({"decision": "SKIP", "reason": "ai_threshold"})
            decision_payload = {
                "action": "SKIP",
                "reason": "ai_threshold",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        if (chosen_prob - other_prob) < edge:
            decision_info.update({"decision": "SKIP", "reason": "ai_low_edge"})
            decision_payload = {
                "action": "SKIP",
                "reason": "ai_low_edge",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        decision_info.update(
            {
                "decision": "ENTRY",
                "side": chosen_side,
                "prob": chosen_prob,
                "edge_delta": chosen_prob - other_prob,
            }
        )

        if not trade_service.can_open_new_position(symbol):
            blocked_filters = dict(base_filters)
            blocked_filters["blocked"] = "pos_guard"
            decision_payload = {
                "action": "BLOCKED",
                "reason": "pos_guard",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, blocked_filters, level="warning")
            position_guard.on_order_rejected_or_canceled(symbol)
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        signal = {
            "side": chosen_side,
            "prob": chosen_prob,
            "meta": chosen_side,
            "best_threshold": buy_threshold,
        }

        recent_ohlc = globals().get("get_recent_ohlc")
        ohlc_tail = None
        if callable(recent_ohlc):
            try:
                ohlc_tail = recent_ohlc(symbol, bars=64)
            except Exception:
                ohlc_tail = None

        exit_plan = None
        try:
            exit_plan = trade_service.build_exit_plan(symbol, ohlc_tail)
        except Exception:
            exit_plan = None

        if not exit_plan:
            exit_builder = globals().get("_build_exit_plan")
            decision_exit_builder = globals().get("_build_decision_exit_plan")
            if callable(exit_builder) and ohlc_tail is not None:
                if callable(decision_exit_builder):
                    exit_plan = decision_exit_builder(symbol, ohlc_tail)
                else:
                    exit_plan = exit_builder(symbol, ohlc_tail)

        signal["exit_plan"] = exit_plan or {"mode": "none"}

        _register_trailing_state(symbol, signal, tick_dict)

        trade_service.mark_filled_now()
        filters_ctx = dict(base_filters)
        filters_ctx["blocked"] = None
        decision_payload = {
            "action": "ENTRY",
            "reason": decision_info.get("reason","entry_ok"),
            "ai_meta": ai_out.meta,
            "signal": signal,
            "dec": decision_info,
        }
        _emit(decision_payload, filters_ctx, level="info")
        return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}


def evaluate_and_log_once() -> None:
    """Dry-run evaluation that mirrors the live decision path."""
    cfg = load_config()
    runtime_cfg = cfg.get("runtime", {})
    ai_cfg = cfg.get("ai", {})
    entry_cfg = cfg.get("entry", {})
    filters_cfg = cfg.get("filters", {})

    best_threshold = float(BEST_THRESHOLD)
    sell_threshold = max(min(1.0 - best_threshold, 1.0), 0.0)

    trade_state.update(
        threshold_buy=best_threshold,
        threshold_sell=sell_threshold,
        prob_threshold=best_threshold,
        side_bias=str(entry_cfg.get("side_bias", "auto") or "auto"),
    )
    settings = trade_state.get_settings()

    symbol = runtime_cfg.get("symbol", "USDJPY")
    spread_limit_pips = float(runtime_cfg.get("spread_limit_pips", runtime_cfg.get("spread_limit", 1.5)))
    max_pos = int(runtime_cfg.get("max_positions", 1))
    min_adx = float(filters_cfg.get("adx_min", 15.0))
    disable_adx_gate = bool(filters_cfg.get("adx_disable", False))
    min_atr_pct = float(filters_cfg.get("min_atr_pct", 0.0003))

    if not settings.trading_enabled:
        logger.bind(event="dryrun", ts=now_jst_iso()).info(
            {"mode": "dryrun", "enabled": False, "reason": "trading_disabled"}
        )
        return

    if not mt5_client.initialize():
        logger.bind(event="dryrun", ts=now_jst_iso()).warning(
            {"mode": "dryrun", "enabled": True, "error": "mt5_init_failed"}
        )
        return

    try:
        spr_callable = getattr(market, "spread", None)
        spr = spr_callable(symbol) if callable(spr_callable) else 0.0

        ob_obj = orderbook() if callable(orderbook) else orderbook
        get_maybe = getattr(ob_obj, "get", None)
        ob = get_maybe(symbol) if callable(get_maybe) else None

        open_cnt = 0
        if ob is not None:
            updater = getattr(ob, "update_with_market_and_close_if_hit", None)
            if callable(updater):
                updater(symbol)
            cnt_getter = getattr(ob, "count_open", None)
            if callable(cnt_getter):
                try:
                    open_cnt = int(cnt_getter(symbol))
                except Exception:
                    open_cnt = 0

        tick = market.tick(symbol)
        tick_dict = None
        if tick:
            try:
                bid, ask = tick
                tick_dict = {"bid": float(bid), "ask": float(ask)}
            except (TypeError, ValueError):
                tick_dict = None

        base_features = tuple(ai_cfg.get("features", {}).get("base", []))
        features = _collect_features(symbol, base_features, tick, spr, open_cnt)

        cb_cfg = cfg.get("circuit_breaker", {}) if isinstance(cfg, dict) else {}
        cb = circuit_breaker.CircuitBreaker(
            max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", 5)),
            daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
            cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
        )
        ai = AISvc(threshold=best_threshold)
        print(f"[exec] AISvc model: {getattr(ai, 'model_name', 'unknown')} (threshold={best_threshold})")
        stub = ExecutionStub(cb=cb, ai=ai)

        runtime_payload = {
            "threshold_buy": best_threshold,
            "threshold_sell": sell_threshold,
            "prob_threshold": best_threshold,
            "spread_limit_pips": spread_limit_pips,
            "max_positions": max_pos,
            "spread_pips": spr,
            "open_positions": open_cnt,
            "ai_threshold": stub.ai.threshold,
            "min_adx": min_adx,
            "disable_adx_gate": disable_adx_gate,
            "min_atr_pct": min_atr_pct,
            "tick": tick,
            "side_bias": settings.side_bias,
        }

        result = stub.on_tick(symbol, features, runtime_payload)
        _ = result
    finally:
        mt5_client.shutdown()




=== file: app/services/feature_importance.py ===

# app/services/feature_importance.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Sequence, Tuple

import numpy as np


# LightGBM / XGBoost ã¯ã€Œã‚ã‚‹ãªã‚‰ä½¿ã†ã€ã‚¹ã‚¿ã‚¤ãƒ«ã«ã—ã¦ãŠã
try:  # type: ignore[unused-ignore]
    import lightgbm as lgb
except Exception:  # ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§ LightGBM æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã‚‚æ­»ãªãªã„ã‚ˆã†ã«
    lgb = None  # type: ignore[assignment]


try:  # type: ignore[unused-ignore]
    import xgboost as xgb
except Exception:
    xgb = None  # type: ignore[assignment]


@dataclass
class FeatureImportanceItem:
    """1ã¤ã®ç‰¹å¾´é‡ã«ã¤ã„ã¦ã® FI æƒ…å ±."""

    name: str
    importance: float
    importance_pct: float
    rank: int


def _unwrap_model(model: Any) -> Any:
    """
    CalibratedClassifierCV ã‚„ãƒ©ãƒƒãƒ‘ãƒ¼ã«åŒ…ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€
    ä¸­èº«ã® base_estimator / estimator ã‚’ã§ãã‚‹ã ã‘å‰¥ãŒã™ã€‚
    """
    for attr in ("base_estimator_", "base_estimator", "estimator_", "estimator"):
        inner = getattr(model, attr, None)
        if inner is not None:
            return inner
    return model


def _detect_model_type(model: Any) -> str:
    """
    LightGBM / XGBoost / ãã®ä»– ã‚’ã–ã£ãã‚Šåˆ¤å®šã™ã‚‹ã€‚
    """
    m = _unwrap_model(model)

    # LightGBM
    if lgb is not None:
        try:
            if isinstance(m, lgb.Booster):
                return "lightgbm"
        except Exception:
            pass
        try:
            from lightgbm.sklearn import LGBMModel  # type: ignore
        except Exception:
            LGBMModel = tuple()  # type: ignore[assignment]

        try:
            if isinstance(m, LGBMModel):
                return "lightgbm"
        except Exception:
            pass

    # XGBoost
    if xgb is not None:
        try:
            from xgboost import XGBModel  # type: ignore
        except Exception:
            XGBModel = tuple()  # type: ignore[assignment]

        try:
            if isinstance(m, XGBModel):
                return "xgboost"
        except Exception:
            pass

        try:
            if isinstance(m, xgb.Booster):
                return "xgboost"
        except Exception:
            pass

    return "unknown"


def _fi_lightgbm(
    model: Any,
    feature_names: Optional[Sequence[str]] = None,
) -> Tuple[np.ndarray, List[str]]:
    """
    LightGBM ç”¨ã® raw FI æŠ½å‡º.
    æˆ»ã‚Šå€¤: (importances, names)
    """
    booster = None
    m = _unwrap_model(model)

    # sklearn API: LGBMClassifier / LGBMRegressor ãªã©
    if hasattr(m, "feature_importances_"):
        importances = np.asarray(getattr(m, "feature_importances_"), dtype=float)

        # feature_name_ ãŒã‚ã‚Œã°å„ªå…ˆã€ãã‚ŒãŒãªã‘ã‚Œã°å¼•æ•°ã® feature_names
        names: List[str]
        fn = getattr(m, "feature_name_", None)
        if fn is not None:
            names = list(fn)
        elif feature_names is not None:
            names = list(feature_names)
        else:
            names = [f"f{i}" for i in range(len(importances))]

        return importances, names

    # Booster ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç›´æ¥æŒã£ã¦ã„ã‚‹å ´åˆ
    if hasattr(m, "booster_"):
        booster = getattr(m, "booster_")
    elif lgb is not None and isinstance(m, lgb.Booster):
        booster = m

    if booster is not None:
        try:
            importances = np.asarray(
                booster.feature_importance(importance_type="gain"), dtype=float
            )
        except Exception:
            importances = np.asarray(
                booster.feature_importance(importance_type="split"), dtype=float
            )

        fn = getattr(booster, "feature_name", None)
        names: List[str]
        if callable(fn):
            names = list(fn())
        elif feature_names is not None:
            names = list(feature_names)
        else:
            names = [f"f{i}" for i in range(len(importances))]

        return importances, names

    raise ValueError("LightGBM ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ feature_importance ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")


def _fi_xgboost(
    model: Any,
    feature_names: Optional[Sequence[str]] = None,
) -> Tuple[np.ndarray, List[str]]:
    """
    XGBoost ç”¨ã® raw FI æŠ½å‡º.
    æˆ»ã‚Šå€¤: (importances, names)
    """
    m = _unwrap_model(model)

    booster = None
    # sklearn API: XGBClassifier / XGBRegressor
    if hasattr(m, "get_booster"):
        booster = m.get_booster()
    elif xgb is not None and isinstance(m, xgb.Booster):
        booster = m

    if booster is None:
        raise ValueError("XGBoost ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ Booster ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # gain ãƒ™ãƒ¼ã‚¹ã‚’å„ªå…ˆã€ç„¡ã‘ã‚Œã° weight
    try:
        score_dict: Dict[str, float] = booster.get_score(importance_type="gain")
    except Exception:
        score_dict = {}

    if not score_dict:
        score_dict = booster.get_score(importance_type="weight")

    if not score_dict:
        raise ValueError("XGBoost Booster.get_score() ãŒç©ºã§ã—ãŸã€‚")

    # key ã¯ "f0", "f1" â€¦ ã®ã“ã¨ãŒå¤šã„
    names: List[str] = []
    importances: List[float] = []

    for key, val in score_dict.items():
        # key ãŒ f0 å½¢å¼ãªã‚‰ index ã‚’è§£é‡ˆã—ã¦ feature_names ã¨åˆã‚ã›ã‚‹
        if feature_names is not None and key.startswith("f") and key[1:].isdigit():
            idx = int(key[1:])
            if 0 <= idx < len(feature_names):
                fname = str(feature_names[idx])
            else:
                fname = key
        else:
            fname = key

        names.append(fname)
        importances.append(float(val))

    return np.asarray(importances, dtype=float), names


def compute_feature_importance(
    model: Any,
    feature_names: Optional[Sequence[str]] = None,
    top_n: Optional[int] = 30,
) -> List[FeatureImportanceItem]:
    """
    LightGBM / XGBoost ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ Feature Importance ã‚’å–ã‚Šå‡ºã—ã€
    é™é †ã‚½ãƒ¼ãƒˆï¼‹å‰²åˆä»˜ãã®ãƒªã‚¹ãƒˆã«ã—ã¦è¿”ã™ã€‚

    Parameters
    ----------
    model:
        LightGBM / XGBoost ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        ï¼ˆcalibration ãƒ©ãƒƒãƒ‘ãƒ¼ä»˜ãã§ã‚‚ OKï¼‰
    feature_names:
        ç‰¹å¾´é‡åã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã€‚
        ãƒ¢ãƒ‡ãƒ«å´ã‹ã‚‰å–ã‚Œãªã„å ´åˆã«ã“ã“ã§è£œã†ã€‚
    top_n:
        ä¸Šä½ã„ãã¤ã¾ã§è¿”ã™ã‹ã€‚None ãªã‚‰å…¨ä»¶ã€‚
    """
    model_type = _detect_model_type(model)

    if model_type == "lightgbm":
        importances, names = _fi_lightgbm(model, feature_names)
    elif model_type == "xgboost":
        importances, names = _fi_xgboost(model, feature_names)
    else:
        raise ValueError(
            f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã§ã™: {_detect_model_type(model)} "
            "(LightGBM / XGBoost ä»¥å¤–ã¯ compute_feature_importance() ã§ã¯æ‰±ã„ã¾ã›ã‚“)"
        )

    if len(importances) == 0:
        raise ValueError("feature_importance ã®é•·ã•ãŒ 0 ã§ã™ã€‚")

    # è² ã®å€¤ãªã©ãŒæ¥ã¦ã‚‚ä¸€å¿œæ‰±ãˆã‚‹ã‚ˆã†ã« abs ã‚’å–ã‚‹
    imp = np.asarray(importances, dtype=float)
    imp = np.nan_to_num(imp, nan=0.0)
    total = float(np.sum(np.abs(imp)))
    if total <= 0:
        # ã™ã¹ã¦ 0 ã®å ´åˆã¯ä¸€å¾‹ 0%
        pct = np.zeros_like(imp, dtype=float)
    else:
        pct = (np.abs(imp) / total) * 100.0

    items: List[FeatureImportanceItem] = []
    for name, v, p in zip(names, imp, pct):
        items.append(
            FeatureImportanceItem(
                name=str(name),
                importance=float(v),
                importance_pct=float(p),
                rank=-1,  # ã“ã“ã§ã¯ä»®ã€‚ã‚ã¨ã§ã‚½ãƒ¼ãƒˆã—ã¦ rank ã‚’æŒ¯ã‚‹ã€‚
            )
        )

    # importance é™é †ã§ã‚½ãƒ¼ãƒˆ
    items.sort(key=lambda x: x.importance, reverse=True)

    # rank ã‚’æŒ¯ã‚Šç›´ã—
    for i, it in enumerate(items, start=1):
        it.rank = i

    # top_n ã§åˆ‡ã‚‹
    if top_n is not None and top_n > 0:
        items = items[:top_n]

    return items



=== file: app/services/metrics.py ===

# app/services/metrics.py
from __future__ import annotations
from pathlib import Path
from typing import Dict, Any
import json, os, time, tempfile
import shutil,time
from core.metrics import METRICS_JSON, METRICS  # METRICS_JSON ã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€METRICS ã¯KVS

def publish_metrics(kv: Dict[str, Any]) -> None:
    """
    DashboardãŒèª­ã‚€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ æŒ‡æ¨™ã‚’ KVS ã¨ JSON(atomic write) ã«å‡ºåŠ›ã™ã‚‹ã€‚
    å¿…è¦ãªã‚­ãƒ¼ä¾‹ã¯ä¸‹è¨˜ã®é€šã‚Šï¼ˆå…¨éƒ¨ã§ãªãã¦OKï¼‰:
      last_decision, last_reason, atr_ref, atr_gate_state, post_fill_grace,
      spread, prob_threshold, min_atr_pct, adx, min_adx,
      trail_activated, trail_be_locked, trail_layers, trail_current_sl,
      count_entry, count_skip, count_blocked, cb_tripped, cb_reason, ts
    """
    # KVSï¼ˆåŒä¸€ãƒ—ãƒ­ã‚»ã‚¹å‘ã‘ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    METRICS.update(**kv)

    # JSONï¼ˆåˆ¥ãƒ—ãƒ­ã‚»ã‚¹é€£æºï¼Dashboardæ¨™æº–å…¥åŠ›ï¼‰
    path = Path(METRICS_JSON)
    path.parent.mkdir(parents=True, exist_ok=True)
    data = dict(kv)
    # tsï¼ˆãƒ­ãƒ¼ã‚«ãƒ«æ›´æ–°æ™‚åˆ»ï¼‰ã¯ã“ã“ã§ä¿è¨¼
    data.setdefault("ts", int(time.time()))

    txt = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
    tmp_path = Path(tempfile.mkstemp(prefix="metrics_", suffix=".json", dir=str(path.parent))[1])
    tmp_path.write_text(txt, encoding="utf-8")

    # --- safe replace with retry ---
    for i in range(10):
        try:
            shutil.move(tmp_path, path)
            break
        except PermissionError:
            time.sleep(0.5)
    else:
        print(f"[metrics][warn] could not update {path} (still locked). skipped.")
    


=== file: app/services/mt5_account_store.py ===

# app/services/mt5_account_store.py
from __future__ import annotations

from typing import Dict, Any, Optional
from pathlib import Path
import json
import os

from loguru import logger

import fxbot_path

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹: <project_root>/config/mt5_accounts.json
_CONFIG_DIR = fxbot_path.get_project_root() / "config"
_CONFIG_FILE = _CONFIG_DIR / "mt5_accounts.json"


def _default_config() -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®åˆæœŸå€¤ã€‚"""
    return {
        "active_profile": "",
        "profiles": {},  # name -> {login, password, server}
    }


def load_config() -> Dict[str, Any]:
    """JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ dict ã‚’è¿”ã™ã€‚"""
    try:
        if not _CONFIG_FILE.exists():
            return _default_config()
        with _CONFIG_FILE.open("r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, dict):
            logger.warning(f"[mt5_account_store] config is not dict: {_CONFIG_FILE}")
            return _default_config()
        # å¿…é ˆã‚­ãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        data.setdefault("active_profile", "")
        data.setdefault("profiles", {})
        if not isinstance(data["profiles"], dict):
            data["profiles"] = {}
        return data
    except Exception as e:
        logger.error(f"[mt5_account_store] failed to load config: {e}")
        return _default_config()


def save_config(cfg: Dict[str, Any]) -> None:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    try:
        _CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        with _CONFIG_FILE.open("w", encoding="utf-8") as f:
            json.dump(cfg, f, ensure_ascii=False, indent=2)
        logger.info(f"[mt5_account_store] saved config: {_CONFIG_FILE}")
    except Exception as e:
        logger.error(f"[mt5_account_store] failed to save config: {e}")


def get_profile(name: str) -> Optional[Dict[str, Any]]:
    """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è¨­å®šã‚’å–å¾—ã™ã‚‹ã€‚å­˜åœ¨ã—ãªã‘ã‚Œã° Noneã€‚"""
    cfg = load_config()
    profiles = cfg.get("profiles", {})
    if not isinstance(profiles, dict):
        return None
    acc = profiles.get(name)
    if isinstance(acc, dict):
        return acc
    return None


def upsert_profile(name: str, *, login: int, password: str, server: str) -> None:
    """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã¾ãŸã¯æ›´æ–°ã™ã‚‹ã€‚"""
    cfg = load_config()
    profiles = cfg.setdefault("profiles", {})
    if not isinstance(profiles, dict):
        profiles = {}
        cfg["profiles"] = profiles

    profiles[name] = {
        "login": int(login),
        "password": str(password),
        "server": str(server),
    }
    save_config(cfg)


def set_active_profile(name: str, *, apply_env: bool = True) -> None:
    """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã™ã‚‹ã€‚

    apply_env=True ã®ã¨ãã€os.environ ã® MT5_LOGIN/PASSWORD/SERVER ã‚‚æ›´æ–°ã€‚
    """
    cfg = load_config()
    profiles = cfg.get("profiles", {})
    if not isinstance(profiles, dict) or name not in profiles:
        logger.warning(f"[mt5_account_store] profile {name!r} not found; active_profile not changed")
        return

    cfg["active_profile"] = name
    save_config(cfg)

    if apply_env:
        acc = profiles[name]
        os.environ["MT5_LOGIN"] = str(acc.get("login", ""))
        os.environ["MT5_PASSWORD"] = str(acc.get("password", ""))
        os.environ["MT5_SERVER"] = str(acc.get("server", ""))
        logger.info(f"[mt5_account_store] applied env for profile {name!r}")


def get_active_profile_name() -> str:
    cfg = load_config()
    active = cfg.get("active_profile")
    return str(active or "")



=== file: app/services/mt5_selftest.py ===

# app/services/mt5_selftest.py
from __future__ import annotations

from typing import Any, List, Tuple
import traceback
import subprocess
import sys
from pathlib import Path

from app.core import mt5_client


def _get_attr(obj: Any, name: str, default: Any = "(n/a)") -> Any:
    """
    dict / MT5 ã® AccountInfo ã®ã©ã¡ã‚‰ã§ã‚‚å®‰å…¨ã«å±æ€§ã‚’å–ã‚Šå‡ºã™ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚

    scripts/selftest_mt5.py ã¨åŒã˜æŒ™å‹•ã«ã—ã¦ãŠãã€‚
    """
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def run_mt5_selftest() -> Tuple[bool, str]:
    """
    MT5 è‡ªå·±è¨ºæ–­ã‚’å®Ÿè¡Œã—ã¦ã€(æˆåŠŸãƒ•ãƒ©ã‚°, ãƒ­ã‚°æ–‡å­—åˆ—) ã‚’è¿”ã™ã€‚

    - GUI ã‹ã‚‰å‘¼ã³å‡ºã™ã“ã¨ã‚’å‰æã«ã€print ã§ã¯ãªãæ–‡å­—åˆ—ã‚’çµ„ã¿ç«‹ã¦ã‚‹
    - ä¾‹å¤–ã¯ã“ã“ã§ã‚­ãƒ£ãƒƒãƒã—ã€False ã¨ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¿”ã™
    """
    lines: List[str] = []
    lines.append("=== MT5 self test (GUI) ===")
    lines.append("ã“ã®ãƒ†ã‚¹ãƒˆã¯ã€ç¾åœ¨ã®è¨­å®šã‚¿ãƒ–ã§é¸æŠã•ã‚Œã¦ã„ã‚‹å£åº§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ãã€")
    lines.append("MT5 ã¸ã®æ¥ç¶šã¨ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚")
    lines.append("")

    try:
        # å¿µã®ãŸã‚æ¯å›ã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ã‹ã‚‰å§‹ã‚ã‚‹
        try:
            mt5_client.shutdown()
        except Exception:
            # å¤±æ•—ã—ã¦ã‚‚è‡´å‘½çš„ã§ã¯ãªã„ã®ã§ç„¡è¦–
            pass

        # 1) initialize
        lines.append("[1] mt5_client.initialize() ...")
        ok = mt5_client.initialize()
        lines.append(f"    -> initialize() returned: {ok!r}")
        if not ok:
            lines.append("")
            lines.append("ERROR: MT5 ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            lines.append(" - MT5 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ï¼Ÿ")
            lines.append(" - è¨­å®šã‚¿ãƒ–ã§é¸æŠã—ãŸå£åº§ID / ã‚µãƒ¼ãƒãƒ¼ / ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯æ­£ã—ã„ã‹ï¼Ÿ")
            return False, "\n".join(lines)

        # 2) account_info
        lines.append("")
        lines.append("[2] mt5_client.get_account_info() ...")
        info = mt5_client.get_account_info()
        if not info:
            lines.append("ERROR: get_account_info() ãŒ None / False ã‚’è¿”ã—ã¾ã—ãŸã€‚")
            lines.append("      ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚„ã‚µãƒ¼ãƒãƒ¼è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return False, "\n".join(lines)

        login = _get_attr(info, "login")
        name = _get_attr(info, "name")
        server = _get_attr(info, "server")
        balance = _get_attr(info, "balance")
        equity = _get_attr(info, "equity")
        trade_mode = _get_attr(info, "trade_mode")

        lines.append("  --- Account Info ---")
        lines.append(f"  login      : {login}")
        lines.append(f"  name       : {name}")
        lines.append(f"  server     : {server}")
        lines.append(f"  balance    : {balance}")
        lines.append(f"  equity     : {equity}")
        lines.append(f"  trade_mode : {trade_mode}")
        lines.append("  --------------------")

        # 3) positions (raw)
        lines.append("")
        lines.append("[3] mt5_client.get_positions() ...")
        positions = mt5_client.get_positions()
        n_pos = len(positions) if positions is not None else 0
        lines.append(f"    -> open positions: {n_pos}")
        if positions:
            # å…ˆé ­æ•°ä»¶ã ã‘ã–ã£ãã‚Šè¡¨ç¤º
            lines.append("    sample positions (up to 3):")
            for i, pos in enumerate(positions[:3]):
                lines.append(f"      [{i}] {pos!r}")

        # 4) positions_df (DataFrame)
        lines.append("")
        lines.append("[4] mt5_client.get_positions_df() ...")
        df = mt5_client.get_positions_df()
        if df is None:
            lines.append("    -> positions_df: None")
        else:
            try:
                shape = getattr(df, "shape", None)
                lines.append(f"    -> positions_df.shape = {shape}")
                # è¡Œæ•°ã ã‘è»½ãè¡¨ç¤º
                lines.append(f"    -> positions_df.head():")
                lines.append(df.head(5).to_string())
            except Exception as e:
                lines.append(f"    -> positions_df ã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e!r}")

        lines.append("")
        lines.append("MT5 self test completed successfully.")
        return True, "\n".join(lines)

    except Exception:
        # ã“ã“ã§ä¾‹å¤–ã‚’å…¨éƒ¨é£²ã¿è¾¼ã‚“ã§ã€GUI å´ã«ã¯æ–‡å­—åˆ—ã§è¿”ã™
        lines.append("")
        lines.append("ERROR: MT5 è‡ªå·±è¨ºæ–­ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        lines.append("")
        lines.append(traceback.format_exc())
        return False, "\n".join(lines)

    finally:
        # æ¯å› shutdown ã—ã¦ãŠãã“ã¨ã§ã€æ¬¡å›ãƒ†ã‚¹ãƒˆã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã«å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
        try:
            mt5_client.shutdown()
        except Exception:
            pass

def run_mt5_orderflow_selftest() -> Tuple[bool, str]:
    """
    scripts/selftest_order_flow.py ã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦å®Ÿè¡Œã—ã€
    (æˆåŠŸãƒ•ãƒ©ã‚°, ãƒ­ã‚°æ–‡å­—åˆ—) ã‚’è¿”ã™ã€‚

    - GUI ã‹ã‚‰å‘¼ã³å‡ºã™ã“ã¨ã‚’å‰æã«ã€print ã§ã¯ãªãæ–‡å­—åˆ—ã‚’çµ„ã¿ç«‹ã¦ã‚‹
    - ä¾‹å¤–ã¯ã“ã“ã§ã‚­ãƒ£ãƒƒãƒã—ã€False ã¨ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¿”ã™
    """
    lines: List[str] = []
    lines.append("=== MT5 order flow self test (GUI) ===")
    lines.append(
        "ã“ã®ãƒ†ã‚¹ãƒˆã¯ã€ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–MT5å£åº§ã§ 0.01 lot ã®æˆè¡ŒBUY â†’ å³ã‚¯ãƒ­ãƒ¼ã‚ºã‚’è¡Œã„ã€"
        "ç™ºæ³¨ãƒ•ãƒ­ãƒ¼ãŒæ­£å¸¸ã«å‹•ãã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚"
    )
    lines.append("")
    lines.append("â€» å¿…ãšãƒ‡ãƒ¢å£åº§ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    lines.append("")

    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’æ¨å®šï¼ˆ.../app/services/ ã‹ã‚‰2ã¤ä¸Šï¼‰
        project_root = Path(__file__).resolve().parents[2]

        cmd = [sys.executable, "-m", "scripts.selftest_order_flow"]
        lines.append(f"[INFO] å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
        lines.append(f"[INFO] cwd: {project_root}")
        lines.append("")

        result = subprocess.run(
            cmd,
            cwd=project_root,
            capture_output=True,
            text=True,
            check=False,
        )

        stdout = (result.stdout or "").strip()
        stderr = (result.stderr or "").strip()

        lines.append("--- stdout ---")
        if stdout:
            lines.append(stdout)
        else:
            lines.append("(å‡ºåŠ›ãªã—)")

        if stderr:
            lines.append("")
            lines.append("--- stderr ---")
            lines.append(stderr)

        lines.append("")
        lines.append(f"[INFO] returncode = {result.returncode}")

        ok = (result.returncode == 0)
        return ok, "\n".join(lines)

    except Exception:
        # ã“ã“ã§ä¾‹å¤–ã‚’æ¡ã£ã¦ã€False + ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¿”ã™
        lines.append("")
        lines.append("ERROR: selftest_order_flow å®Ÿè¡Œä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        lines.append(traceback.format_exc())
        return False, "\n".join(lines)



=== file: app/services/mt5_service.py ===

# app/services/mt5_service.py
from __future__ import annotations
import time
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple

import MetaTrader5 as mt5

@dataclass
class BrokerConstraints:
    # ä¾¡æ ¼åˆ»ã¿
    digits: int
    point: float
    tick_size: float
    # åˆ¶ç´„
    stop_level_points: int     # æœ€å°SL/TPè·é›¢ï¼ˆãƒã‚¤ãƒ³ãƒˆï¼‰
    freeze_level_points: int   # å‡çµãƒ¬ãƒ™ãƒ«ï¼ˆãƒã‚¤ãƒ³ãƒˆï¼‰
    trade_stops_level: int     # æ—§ç§°/äº’æ›ï¼ˆãƒã‚¤ãƒ³ãƒˆï¼‰
    # ä¾¡æ ¼åˆ¶ç´„ï¼ˆã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ç­‰ã¯ç™ºæ³¨å´ã§å–ã‚Šå›ã—ï¼‰
    min_sl_step_points: int = 1

def _symbol_props(symbol: str) -> BrokerConstraints:
    info = mt5.symbol_info(symbol)
    if info is None:
        raise RuntimeError(f"symbol_info({symbol}) failed: {mt5.last_error()}")
    # ä¸€éƒ¨ãƒ–ãƒ­ãƒ¼ã‚«ã¯ stop_level ã¨ trade_stops_level ã©ã¡ã‚‰ã‹ã—ã‹æ„å‘³ãŒãªã„ã“ã¨ãŒã‚ã‚‹
    stop_level = getattr(info, "stop_level", 0) or getattr(info, "trade_stops_level", 0) or 0
    trade_stops = getattr(info, "trade_stops_level", 0) or stop_level
    freeze = getattr(info, "freeze_level", 0) or 0
    return BrokerConstraints(
        digits=info.digits,
        point=info.point,
        tick_size=getattr(info, "trade_tick_size", info.point),
        stop_level_points=int(stop_level),
        freeze_level_points=int(freeze),
        trade_stops_level=int(trade_stops),
        min_sl_step_points=1
    )

def _round_to_point(price: float, point: float) -> float:
    # MT5ã«åˆã‚ã›ãŸä¸¸ã‚ï¼ˆãƒã‚¤ãƒ³ãƒˆå˜ä½ï¼‰
    return round(price / point) * point

def _sl_min_distance_ok(side: str, price_now: float, sl_price: float, min_points: int, point: float) -> bool:
    # side=BUY â†’ SLã¯ price_now ã‚ˆã‚Šä¸‹ã€side=SELL â†’ SLã¯ä¸Š
    dist_points = abs(price_now - sl_price) / point
    if side == "BUY":
        return sl_price < price_now and dist_points >= min_points
    else:
        return sl_price > price_now and dist_points >= min_points

def _freeze_level_ok(side: str, price_now: float, sl_price: float, freeze_points: int, point: float) -> bool:
    # FreezeLevelä»¥å†…ã ã¨å¤‰æ›´ä¸å¯
    dist_points = abs(price_now - sl_price) / point
    return dist_points > freeze_points

def _snap_sl_to_rules(side: str, price_now: float, desired_sl: float, bc: BrokerConstraints) -> Optional[float]:
    """
    æœ›ã¾ã—ã„SLã‚’ã€StopLevel/FreezeLevel/ä¸¸ã‚ã«åã¾ã‚‹ã‚ˆã†èª¿æ•´ã€‚
    æ¡ä»¶ã‚’æº€ãŸã›ãªã„å ´åˆã¯ Noneï¼ˆï¼æ›´æ–°ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚
    """
    sl = _round_to_point(desired_sl, bc.point)

    # StopLevelã‚’æº€ãŸã™ã‚ˆã†å°‘ã—é›¢ã™
    if not _sl_min_distance_ok(side, price_now, sl, bc.stop_level_points, bc.point):
        # ä½•ãƒã‚¤ãƒ³ãƒˆãšã‚‰ã›ã°è‰¯ã„ã‹è¨ˆç®—ã—ã¦ã€min_sl_step_pointså˜ä½ã§æŠ¼ã—å‡ºã™
        need = bc.stop_level_points - abs(price_now - sl) / bc.point
        # ç«¯æ•°åˆ‡ã‚Šä¸Šã’
        steps = int(max(0, need)) + 1
        delta = steps * bc.min_sl_step_points * bc.point
        if side == "BUY":
            sl = price_now - delta
        else:
            sl = price_now + delta
        sl = _round_to_point(sl, bc.point)

    # FreezeLevelãƒã‚§ãƒƒã‚¯ï¼ˆæº€ãŸã›ãªã‘ã‚Œã°è«¦ã‚ã‚‹ï¼‰
    if not _freeze_level_ok(side, price_now, sl, bc.freeze_level_points, bc.point):
        return None

    # æ–¹å‘æ€§ã®å®‰å…¨ï¼ˆBUYãªã‚‰ä¸‹ã€SELLãªã‚‰ä¸Šï¼‰
    if side == "BUY" and sl >= price_now:
        return None
    if side == "SELL" and sl <= price_now:
        return None

    return sl

def _price_for_side(tick: Dict[str, float], side: str) -> float:
    # æ›´æ–°åˆ¤å®šã«ä½¿ã†ç¾åœ¨ãƒ¬ãƒ¼ãƒˆï¼ˆSLã¯é€†ã‚µã‚¤ãƒ‰ã«ä»˜ãã®ã§ BUYâ†’bidã€SELLâ†’ask ã‚’åŸºæº–ã«ï¼‰
    if side == "BUY":
        return tick["bid"]
    else:
        return tick["ask"]

def _position_of(ticket: int) -> Any:
    # ticketã‹ã‚‰ç¾åœ¨ã®ãƒã‚¸ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–ã‚‹ï¼ˆãªã‘ã‚Œã°Noneï¼‰
    pos = mt5.positions_get(ticket=ticket)
    if pos is None:
        raise RuntimeError(f"positions_get failed: {mt5.last_error()}")
    return pos[0] if len(pos) > 0 else None

def _current_tick(symbol: str) -> Dict[str, float]:
    t = mt5.symbol_info_tick(symbol)
    if t is None:
        raise RuntimeError(f"symbol_info_tick({symbol}) failed: {mt5.last_error()}")
    return {"bid": t.bid, "ask": t.ask, "last": getattr(t, "last", (t.bid + t.ask)/2)}

class MT5Service:
    """
    æœ¬ç·šï¼šå®‰å…¨ãªSLæ›´æ–°ï¼ˆOrderModifyï¼‰
    """
    def __init__(self, max_retries: int = 3, backoff_sec: float = 0.3, min_change_points: int = 2):
        self.max_retries = max_retries
        self.backoff_sec = backoff_sec
        self.min_change_points = min_change_points  # ç¾SLã‹ã‚‰ã“ã®ãƒã‚¤ãƒ³ãƒˆä»¥ä¸Šãšã‚ŒãŸæ™‚ã®ã¿æ›´æ–°

    def safe_order_modify_sl(self, ticket: int, side: str, symbol: str, desired_sl: float, reason: str = "") -> Tuple[bool, Optional[float], str]:
        """
        è¿”ã‚Šå€¤: (æˆåŠŸ/å¤±æ•—, å®Ÿéš›ã«é€ã£ãŸSL, è©³ç´°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
        """
        pos = _position_of(ticket)
        if pos is None:
            return (False, None, f"no-position ticket={ticket}")

        bc = _symbol_props(symbol)
        tick = _current_tick(symbol)
        price_now = _price_for_side(tick, side)

        # æ—¢å­˜SLã¨ã®å·®åˆ†ãŒå°ã•ã™ãã‚‹ãªã‚‰ä½•ã‚‚ã—ãªã„ï¼ˆãƒãƒ©ã¤ãæŠ‘åˆ¶ï¼‰
        current_sl = float(getattr(pos, "sl", 0.0) or 0.0)
        if current_sl > 0:
            dpoints = abs(current_sl - desired_sl) / bc.point
            if dpoints < self.min_change_points:
                return (True, None, f"skip: delta<{self.min_change_points}pt (current_sl={current_sl}, desired={desired_sl})")

        # è¦ç´„ã«æ²¿ã£ã¦SLã‚’ã‚¹ãƒŠãƒƒãƒ—
        snapped = _snap_sl_to_rules(side, price_now, desired_sl, bc)
        if snapped is None:
            return (False, None, f"reject: violates stop/freeze/side rules (desired={desired_sl}, price_now={price_now})")

        # TRADE_ACTION_SLTP ã§å¤‰æ›´
        request = {
            "action": mt5.TRADE_ACTION_SLTP,
            "position": ticket,
            "symbol": symbol,
            "sl": round(snapped, bc.digits),
            "tp": float(getattr(pos, "tp", 0.0) or 0.0),  # æ—¢å­˜TPã¯ç¶­æŒ
            "deviation": 10,  # å½¢å¼ä¸Šå¿…è¦ã€‚SLTPå¤‰æ›´ã§ã¯é€šå¸¸å½±éŸ¿ã—ãªã„
            "comment": f"trail:{reason}"[:28],
            "type_time": mt5.ORDER_TIME_GTC,
            "type_filling": mt5.ORDER_FILLING_RETURN,
        }

        last_err = ""
        for i in range(self.max_retries + 1):
            res = mt5.order_send(request)
            if res is None:
                last_err = f"order_send None: {mt5.last_error()}"
            else:
                if res.retcode == mt5.TRADE_RETCODE_DONE or res.retcode == mt5.TRADE_RETCODE_DONE_PARTIAL:
                    return (True, request["sl"], f"OK retcode={res.retcode}")
                last_err = f"retcode={res.retcode}, comment={getattr(res, 'comment', '')}"

            time.sleep(self.backoff_sec)

            # ä¾¡æ ¼æ›´æ–°ã—ã¦å†ã‚¹ãƒŠãƒƒãƒ—ï¼ˆFreeze/StopLevelå¤‰åŒ–ã«è¿½éšï¼‰
            tick = _current_tick(symbol)
            price_now = _price_for_side(tick, side)
            snapped = _snap_sl_to_rules(side, price_now, desired_sl, bc)
            if snapped is None:
                break
            request["sl"] = round(snapped, bc.digits)

        return (False, None, f"fail: {last_err}")



=== file: app/services/orderbook_stub.py ===

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Optional
from datetime import datetime, timezone, timedelta
from loguru import logger
from app.core import market

JST = timezone(timedelta(hours=9))
TIMEOUT_SECONDS = 0  # æœ¬ç•ªå¯„ã›ï¼šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«ã‚ˆã‚‹å¼·åˆ¶ã‚¯ãƒ­ãƒ¼ã‚ºã‚’ç„¡åŠ¹åŒ–

@dataclass
class MockPosition:
    id: int
    symbol: str
    side: str        # "BUY" or "SELL"
    lot: float
    entry: float
    sl: float
    tp: float
    open_time: datetime = field(default_factory=lambda: datetime.now(JST))
    close_time: Optional[datetime] = None
    closed: bool = False
    close_price: Optional[float] = None
    profit_pips: Optional[float] = None

class OrderBook:
    def __init__(self) -> None:
        self._next_id = 1
        self._positions: List[MockPosition] = []

    def count_open(self, symbol: Optional[str] = None) -> int:
        return sum(1 for p in self._positions if not p.closed and (symbol is None or p.symbol == symbol))

    def open(self, symbol: str, side: str, lot: float, entry: float, sl: float, tp: float) -> MockPosition:
        pos = MockPosition(
            id=self._next_id, symbol=symbol, side=side, lot=lot, entry=entry, sl=sl, tp=tp
        )
        self._next_id += 1
        self._positions.append(pos)
        logger.bind(event="dryrun_open").info({
            "mode":"dryrun", "action":"open", "id": pos.id, "symbol": symbol, "side": side,
            "lot": lot, "entry": entry, "sl": sl, "tp": tp, "ts": pos.open_time.isoformat(timespec="seconds")
        })
        return pos

    def _close(self, p: MockPosition, price: float, reason: str) -> None:
        if p.closed:
            return
        p.closed = True
        p.close_time = datetime.now(JST)
        p.close_price = price
        pip_delta = market.pips_to_price(p.symbol, 1.0)
        if pip_delta and pip_delta > 0:
            p.profit_pips = (p.close_price - p.entry)/pip_delta if p.side == "BUY" else (p.entry - p.close_price)/pip_delta
        else:
            p.profit_pips = None
        logger.bind(event="dryrun_close").info({
            "mode":"dryrun", "action":"close", "id": p.id, "symbol": p.symbol, "side": p.side,
            "entry": p.entry, "close": p.close_price, "profit_pips": p.profit_pips,
            "reason": reason, "ts": p.close_time.isoformat(timespec="seconds")
        })

    def update_with_market_and_close_if_hit(self, symbol: str) -> None:
        """ç¾åœ¨ã®ä¾¡æ ¼ã§ SL/TP åˆ°é”ã€ã¾ãŸã¯TIMEOUTã§ã‚¯ãƒ­ãƒ¼ã‚ºã€‚"""
        tk = market.tick(symbol)
        for p in list(self._positions):
            if p.closed or p.symbol != symbol:
                continue
            # TIMEOUT
            if TIMEOUT_SECONDS and (datetime.now(JST) - p.open_time).total_seconds() >= TIMEOUT_SECONDS:
                price = (tk[1] if p.side == "BUY" else tk[0]) if tk else p.entry
                self._close(p, price, "TIMEOUT")
                continue
            if not tk:
                continue
            bid, ask = tk
            price = ask if p.side == "BUY" else bid
            hit_tp = (price >= p.tp) if p.side == "BUY" else (price <= p.tp)
            hit_sl = (price <= p.sl) if p.side == "BUY" else (price >= p.sl)
            if hit_tp:
                self._close(p, price, "TP")
            elif hit_sl:
                self._close(p, price, "SL")

    def close_all(self, symbol: Optional[str] = None) -> None:
        """ç¾åœ¨å€¤ã§å…¨ã‚¯ãƒ­ãƒ¼ã‚ºï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼‰ã€‚"""
        tk = None
        if symbol:
            tk = market.tick(symbol)
        for p in list(self._positions):
            if p.closed: 
                continue
            if symbol and p.symbol != symbol: 
                continue
            price = None
            if tk:
                bid, ask = tk
                price = ask if p.side == "BUY" else bid
            self._close(p, price if price is not None else p.entry, "FORCE_CLOSE")

# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³
_orderbook = OrderBook()
def orderbook() -> OrderBook:
    return _orderbook



=== file: app/services/recent_kpi.py ===

# app/services/recent_kpi.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, Mapping, Optional, Sequence, Union

from app.services.decision_log import load_recent_decisions

import math

try:
    import pandas as pd
    from pandas import DataFrame, Series
except ImportError:  # pandas æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç’°å¢ƒå‘ã‘ã®ä¿é™º
    pd = None
    DataFrame = object  # type: ignore
    Series = object  # type: ignore


Number = Union[int, float]


@dataclass
class RecentKpiResult:
    """
    ç›´è¿‘ N ãƒˆãƒ¬ãƒ¼ãƒ‰ã®ç°¡æ˜“ KPI é›†è¨ˆçµæœã€‚

    å˜ä½ã¯ã™ã¹ã¦ã€Œpnl ã®å˜ä½ã€ï¼ˆ= é€šè²¨ or å††ï¼‰ã«æƒãˆã‚‹ã€‚
    """

    n_trades: int
    n_wins: int
    n_losses: int
    win_rate: Optional[float]  # 0.0â€“1.0, ãƒ‡ãƒ¼ã‚¿ä¸è¶³ãªã©ã®å ´åˆã¯ None

    gross_profit: float        # å‹ã¡ãƒˆãƒ¬ãƒ¼ãƒ‰ã®åˆè¨ˆæç›Šï¼ˆ>=0ï¼‰
    gross_loss: float          # è² ã‘ãƒˆãƒ¬ãƒ¼ãƒ‰ã®åˆè¨ˆæç›Šï¼ˆ<=0ï¼‰
    profit_factor: Optional[float]  # gross_profit / abs(gross_loss)

    net_profit: float          # ç·æç›Šï¼ˆ= gross_profit + gross_lossï¼‰

    max_drawdown: float        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ï¼ˆ>0: é‡‘é¡ï¼‰
    max_drawdown_ratio: Optional[float]  # é–‹å§‹æ®‹é«˜ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã®å‰²åˆï¼ˆ0.1 ã§ 10%ï¼‰

    best_win_streak: int       # é€£å‹ã®æœ€å¤§å€¤
    best_loss_streak: int      # é€£æ•—ã®æœ€å¤§å€¤


def _extract_pnl_series(
    trades: Union["DataFrame", Sequence[Mapping[str, Number]]],
    profit_field: str,
) -> Sequence[float]:
    """
    æ±ç”¨çš„ã«ã€Œpnl ã®åˆ—ã€ã‚’å–ã‚Šå‡ºã™ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚

    - pandas.DataFrame ãªã‚‰è©²å½“åˆ—ã‚’ float ã«ã—ã¦ NaN ã‚’é™¤å¤–
    - list[dict] çš„ãªæ§‹é€ ãªã‚‰ profit_field ã‚­ãƒ¼ã§å–ã‚Šå‡ºã™
    """
    if pd is not None and isinstance(trades, pd.DataFrame):
        if profit_field not in trades.columns:
            raise KeyError(f"profit_field '{profit_field}' not in DataFrame columns")
        series = trades[profit_field].astype(float)
        series = series.dropna()
        return series.to_list()

    pnl_list: list[float] = []
    for i, t in enumerate(trades):
        if profit_field not in t:
            raise KeyError(f"profit_field '{profit_field}' not in trade[{i}] keys")
        pnl_list.append(float(t[profit_field]))  # type: ignore[arg-type]
    return pnl_list

def compute_kpi_from_trades(
    trades: Union["DataFrame", Sequence[Mapping[str, Number]]],
    *,
    profit_field: str = "pnl",
    starting_equity: Optional[float] = None,
) -> RecentKpiResult:
    """
    ç›´è¿‘ N ãƒˆãƒ¬ãƒ¼ãƒ‰ã® KPI ã‚’è¨ˆç®—ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚

    Parameters
    ----------
    trades:
        - pandas.DataFrame ã¾ãŸã¯
        - dict ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆå„è¦ç´ ãŒ 1 ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼‰ã‚’æƒ³å®šã€‚
        profit_field ã§æŒ‡å®šã—ãŸã‚­ãƒ¼/åˆ—ã«æç›Šï¼ˆpnlï¼‰ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨ã€‚
    profit_field:
        1 ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æç›Šã‚’è¡¨ã™åˆ—å / ã‚­ãƒ¼åã€‚
        ä¾‹: "pnl", "profit"
    starting_equity:
        æœ€å¤§ DD ã‚’ã€Œæ®‹é«˜ãƒ™ãƒ¼ã‚¹ã€ã§è¦‹ãŸã„å ´åˆã®é–‹å§‹æ®‹é«˜ã€‚
        None ã®å ´åˆã¯ã€ã€Œ0 ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ãŸç´¯ç©æç›Šã€ã«å¯¾ã™ã‚‹ DD ã‚’è¿”ã™ã€‚
    """
    pnl_list = _extract_pnl_series(trades, profit_field)
    n_trades = len(pnl_list)

    if n_trades == 0:
        # å–å¼•ãŒãªã„å ´åˆã¯å…¨éƒ¨ã‚¼ãƒ­/None ã§è¿”ã™
        return RecentKpiResult(
            n_trades=0,
            n_wins=0,
            n_losses=0,
            win_rate=None,
            gross_profit=0.0,
            gross_loss=0.0,
            profit_factor=None,
            net_profit=0.0,
            max_drawdown=0.0,
            max_drawdown_ratio=None,
            best_win_streak=0,
            best_loss_streak=0,
        )

    # --- å‹ã¡ / è² ã‘ / å¼•ãåˆ†ã‘ åˆ¤å®š ---
    wins = [p for p in pnl_list if p > 0]
    losses = [p for p in pnl_list if p < 0]
    n_wins = len(wins)
    n_losses = len(losses)

    # å¼•ãåˆ†ã‘ï¼ˆpnl == 0ï¼‰ã¯ win_rate / PF ã®åˆ†æ¯ã‹ã‚‰ã¯å¤–ã™
    n_effective = n_wins + n_losses

    if n_effective > 0:
        win_rate = n_wins / n_effective
    else:
        win_rate = None

    gross_profit = float(sum(wins)) if wins else 0.0
    gross_loss = float(sum(losses)) if losses else 0.0  # <=0
    net_profit = gross_profit + gross_loss

    if gross_loss < 0.0:
        profit_factor = gross_profit / abs(gross_loss)
    else:
        # è² ã‘ãƒˆãƒ¬ãƒ¼ãƒ‰ãŒç„¡ã„å ´åˆã¯ PF ç„¡é™å¤§ã¨ã¿ãªã™ / None ã¨ã™ã‚‹ã‹ã¯å¥½ã¿ã€‚
        # ã“ã“ã§ã¯ None ã«ã—ã¦ GUI å´ã§ "âˆ" è¡¨ç¤ºãªã©ã«å§”ã­ã‚‹ã€‚
        profit_factor = None

    # --- æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ ---
    # ç´¯ç©æç›Šã‹ã‚‰ DD ã‚’è¨ˆç®—ã™ã‚‹ã€‚starting_equity ãŒã‚ã‚Œã°ãã‚Œã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹ã€‚
    equity_series: list[float] = []
    cumulative = 0.0
    base = starting_equity or 0.0

    for p in pnl_list:
        cumulative += p
        equity_series.append(base + cumulative)

    max_equity = equity_series[0]
    max_dd = 0.0  # æ­£ã®å€¤ï¼ˆä¸‹æ–¹å‘ã®å¹…ï¼‰
    for eq in equity_series:
        if eq > max_equity:
            max_equity = eq
        dd = max_equity - eq  # max_equity >= eq ã®ã¨ã dd >= 0
        if dd > max_dd:
            max_dd = dd

    if starting_equity is not None and starting_equity > 0:
        max_dd_ratio: Optional[float] = max_dd / float(starting_equity)
    else:
        max_dd_ratio = None

    # --- é€£å‹ / é€£æ•— ã‚¹ãƒˆãƒªãƒ¼ã‚¯ ---
    best_win_streak = 0
    best_loss_streak = 0
    current_win_streak = 0
    current_loss_streak = 0

    for p in pnl_list:
        if p > 0:
            current_win_streak += 1
            current_loss_streak = 0
        elif p < 0:
            current_loss_streak += 1
            current_win_streak = 0
        else:
            # å¼•ãåˆ†ã‘ã¯ã©ã¡ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ã‚¯ã‚‚ä¸­æ–­ã•ã›ã‚‹
            current_win_streak = 0
            current_loss_streak = 0

        best_win_streak = max(best_win_streak, current_win_streak)
        best_loss_streak = max(best_loss_streak, current_loss_streak)

    return RecentKpiResult(
        n_trades=n_trades,
        n_wins=n_wins,
        n_losses=n_losses,
        win_rate=win_rate,
        gross_profit=gross_profit,
        gross_loss=gross_loss,
        profit_factor=profit_factor,
        net_profit=net_profit,
        max_drawdown=max_dd,
        max_drawdown_ratio=max_dd_ratio,
        best_win_streak=best_win_streak,
        best_loss_streak=best_loss_streak,
    )


def compute_recent_kpi_from_decisions(
    limit: Optional[int] = None,
    *,
    profit_field: str = "pnl",
    starting_equity: Optional[float] = None,
) -> RecentKpiResult:
    """
    Read logs/decisions/decisions_*.jsonl, filter trades with numeric pnl, and compute recent KPI.

    Parameters
    ----------
    limit : int | None
        Number of pnl-qualified trades to include from the end of the log. None means all trades.
    profit_field : str
        Column name for the profit/loss value. Defaults to "pnl".
    starting_equity : float | None
        Initial equity used for drawdown ratio. None keeps the ratio None.

    Returns
    -------
    RecentKpiResult
        KPI result even when there are zero pnl trades.
    """
    df = load_recent_decisions(limit=None)

    if df.empty or profit_field not in df.columns:
        return compute_kpi_from_trades(
            df,
            profit_field=profit_field,
            starting_equity=starting_equity,
        )

    pnl = pd.to_numeric(df[profit_field], errors="coerce")
    mask = pnl.notna()
    trades = df.loc[mask].copy()

    if trades.empty:
        return compute_kpi_from_trades(
            trades,
            profit_field=profit_field,
            starting_equity=starting_equity,
        )

    if "ts_jst" in trades.columns:
        trades = trades.sort_values("ts_jst")

    if limit is not None:
        trades = trades.tail(limit)

    return compute_kpi_from_trades(
        trades,
        profit_field=profit_field,
        starting_equity=starting_equity,
    )



=== file: app/services/shap_service.py ===

# app/services/shap_service.py

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Sequence, Optional

import numpy as np
import pandas as pd
import shap
from loguru import logger


@dataclass
class ShapFeatureImpact:
    """1ã¤ã®ç‰¹å¾´é‡ã«å¯¾ã™ã‚‹SHAPå½±éŸ¿åº¦æƒ…å ±"""

    name: str
    mean_abs_shap: float
    rank: int


def _normalize_background_frame(
    X: pd.DataFrame,
    max_background: int = 2000,
    feature_names: Optional[Sequence[str]] = None,
) -> pd.DataFrame:
    """
    èƒŒæ™¯ã‚µãƒ³ãƒ—ãƒ«ç”¨ã« DataFrame ã‚’æ•´ç†ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ã€‚
    - åˆ—é †ã‚’ feature_names ã«ãã‚ãˆã‚‹ï¼ˆæŒ‡å®šãŒã‚ã‚Œã°ï¼‰
    - è¡Œæ•°ãŒå¤šã™ãã‚‹å ´åˆã¯ max_background ã¾ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    """
    if X is None or X.empty:
        raise ValueError("SHAPè¨ˆç®—ç”¨ã®èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")

    df = X.copy()

    if feature_names is not None:
        missing = set(feature_names) - set(df.columns)
        if missing:
            raise ValueError(
                f"SHAPèƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã«ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ãŒã‚ã‚Šã¾ã™: {sorted(missing)}"
            )
        # åˆ—é †ã‚’æƒãˆã‚‹
        df = df.loc[:, list(feature_names)]

    if len(df) > max_background:
        logger.info(
            "SHAPèƒŒæ™¯ã‚µãƒ³ãƒ—ãƒ«ã‚’ {orig} è¡Œ â†’ {sub} è¡Œã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚",
            orig=len(df),
            sub=max_background,
        )
        df = df.sample(n=max_background, random_state=42)

    return df


def compute_shap_feature_importance(
    model,
    X: pd.DataFrame,
    *,
    feature_names: Optional[Sequence[str]] = None,
    top_n: int = 20,
    max_background: int = 2000,
) -> List[ShapFeatureImpact]:
    """
    LightGBM ãªã©ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ SHAP (TreeExplainer) ã‚’ä½¿ã£ã¦
    ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡é‡è¦åº¦ï¼ˆå¹³å‡çµ¶å¯¾SHAPå€¤ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚

    Parameters
    ----------
    model : å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMClassifieræƒ³å®šã ãŒã€ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ãªã‚‰æ¦‚ã­OKï¼‰
    X : èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ DataFrame
    feature_names : åˆ—é †ã‚’æ˜ç¤ºã—ãŸã„å ´åˆã®ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
    top_n : ä¸Šä½ä½•å€‹ã¾ã§è¿”ã™ã‹
    max_background : èƒŒæ™¯ã‚µãƒ³ãƒ—ãƒ«ã®æœ€å¤§è¡Œæ•°ï¼ˆé‡ããªã‚‹ã®ã‚’é˜²ããƒ•ã‚£ãƒ«ã‚¿ï¼‰

    Returns
    -------
    List[ShapFeatureImpact]
    """
    df_bg = _normalize_background_frame(
        X, max_background=max_background, feature_names=feature_names
    )

    logger.info(
        "SHAPè¨ˆç®—é–‹å§‹: rows={rows}, cols={cols}, top_n={top_n}",
        rows=len(df_bg),
        cols=df_bg.shape[1],
        top_n=top_n,
    )

    # LightGBM/sklearn äº’æ›ã®ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ãªã‚‰ TreeExplainer ãŒé€Ÿã„
    explainer = shap.TreeExplainer(model)

    shap_values = explainer.shap_values(df_bg)

    # shap_values ã®å½¢ã¯ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦é•ã†ã®ã§é ‘å¼µã£ã¦æ­£è¦åŒ–ã™ã‚‹
    if isinstance(shap_values, list):
        # ã‚¯ãƒ©ã‚¹ã”ã¨ã®é…åˆ—ãƒªã‚¹ãƒˆï¼ˆn_class, n_sample, n_featureï¼‰æƒ³å®š
        arr = np.stack(shap_values, axis=0)  # (n_class, n_sample, n_feature)
        mean_abs = np.mean(np.abs(arr), axis=(0, 1))  # feature æ¬¡å…ƒã«é›†ç´„
    else:
        arr = np.asarray(shap_values)  # (n_sample, n_feature)
        mean_abs = np.mean(np.abs(arr), axis=0)

    # å¿µã®ãŸã‚ shape ã‚’ç¢ºèª
    if mean_abs.shape[0] != df_bg.shape[1]:
        raise RuntimeError(
            f"SHAPé‡è¦åº¦ã®æ¬¡å…ƒæ•° ({mean_abs.shape[0]}) ã¨ç‰¹å¾´é‡æ•° "
            f"({df_bg.shape[1]}) ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚"
        )

    features = list(df_bg.columns)
    # å¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆ
    order = np.argsort(-mean_abs)
    items: List[ShapFeatureImpact] = []

    for idx, feat_idx in enumerate(order):
        if top_n is not None and idx >= top_n:
            break
        items.append(
            ShapFeatureImpact(
                name=features[int(feat_idx)],
                mean_abs_shap=float(mean_abs[int(feat_idx)]),
                rank=idx + 1,
            )
        )

    logger.info("SHAPè¨ˆç®—å®Œäº†: è¿”å´ä»¶æ•°={cnt}", cnt=len(items))
    return items


def shap_items_to_frame(items: List[ShapFeatureImpact]) -> pd.DataFrame:
    """
    ShapFeatureImpact ã®ãƒªã‚¹ãƒˆã‚’ pandas.DataFrame ã«å¤‰æ›ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ã€‚
    GUI ã‚„ CLI ã§æ‰±ã„ã‚„ã™ãã™ã‚‹ãŸã‚ã€‚
    """
    if not items:
        return pd.DataFrame(columns=["rank", "feature", "mean_abs_shap"])

    data = {
        "rank": [it.rank for it in items],
        "feature": [it.name for it in items],
        "mean_abs_shap": [it.mean_abs_shap for it in items],
    }
    return pd.DataFrame(data).sort_values("rank").reset_index(drop=True)



=== file: app/services/trade_service.py ===

from __future__ import annotations

import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, Iterable, Optional
import logging

from app.core import mt5_client
from app.core.config_loader import load_config
from app.services import trade_state
from app.services.circuit_breaker import CircuitBreaker
from app.services.event_store import EVENT_STORE
from core.config import cfg
from core.indicators import atr as _atr
from core.position_guard import PositionGuard
from core.utils.clock import now_jst

from app.core.mt5_client import MT5Client, TickSpec
from app.core.strategy_profile import StrategyProfile, get_profile
from core.risk import LotSizingResult
#from app.core.risk import LotSizingResult


@dataclass
class LotRule:
    base_equity_per_0p01: int = 10_000
    min_lot: float = 0.01
    max_lot: float = 1.00
    step: float = 0.01


def round_to_step(x: float, step: float) -> float:
    return (int(x / step)) * step


def calc_lot(equity: float, rule: LotRule = LotRule()) -> float:
    raw = (equity / rule.base_equity_per_0p01) * 0.01
    lot = max(rule.min_lot, min(rule.max_lot, round_to_step(raw, rule.step)))
    return float(f"{lot:.2f}")


def snapshot_account() -> Optional[dict]:
    if not mt5_client.initialize():
        return None
    try:
        return mt5_client.get_account_info()
    finally:
        mt5_client.shutdown()


class TradeService:
    """Facade that coordinates guards, circuit breaker, and decision helpers."""

    def __init__(
        self,
        mt5_client: MT5Client | None = None,
        profile: StrategyProfile | None = None,
    ) -> None:
        self._mt5 = mt5_client
        self._profile = profile or get_profile()
        self._last_lot_result: LotSizingResult | None = None
        self._logger = logging.getLogger(__name__)
        self.pos_guard = PositionGuard()
        self.cb = CircuitBreaker()
        self._reconcile_interval = 15
        self._desync_fix = True
        self._last_reconcile = 0.0
        self.state = trade_state.get_runtime()
        self.reload()

    def _compute_lot_for_entry(self, symbol: str, atr: float) -> LotSizingResult:
        """
        1ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ãŸã‚Šã®ãƒ­ãƒƒãƒˆæ•°ã‚’ã€ç¾åœ¨ã® equity / ATR / tick æƒ…å ±ã‹ã‚‰è¨ˆç®—ã™ã‚‹ã€‚

        atr: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ç›´å‰ã®è¶³ã§è¨ˆç®—ã—ãŸ ATR å€¤ï¼ˆä¾¡æ ¼å˜ä½ï¼‰
        """
        if atr is None or atr <= 0:
            # ATR ãŒå¤‰ãªå ´åˆã¯ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ­ãƒƒãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            default_lot = getattr(self._profile, "default_lot", None)
            if default_lot is None:
                default_lot = float(self._config.trade.default_lot)  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå´ã®è¨­å®šåã«åˆã‚ã›ã¦ãã ã•ã„

            self._logger.warning(
                "ATR ãŒç„¡åŠ¹ (atr=%s) ã®ãŸã‚ã€default_lot=%.2f ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚",
                atr,
                default_lot,
            )
            return LotSizingResult(
                lot=default_lot,
                capped_by_max_risk=False,
                effective_risk_pct=None,
                note="fallback_default_lot_due_to_invalid_atr",
            )

        equity = self._mt5.get_equity()
        tick_spec: TickSpec = self._mt5.get_tick_spec(symbol)

        result = self._profile.compute_lot_size_from_atr(
            equity=equity,
            atr=atr,
            tick_size=tick_spec.tick_size,
            tick_value=tick_spec.tick_value,
        )

        self._logger.info(
            "ãƒ­ãƒƒãƒˆè¨ˆç®—: equity=%.2f atr=%.5f tick_size=%.5f tick_value=%.5f -> lot=%.2f (capped=%s risk=%.3f)",
            equity,
            atr,
            tick_spec.tick_size,
            tick_spec.tick_value,
            result.lot,
            result.capped_by_max_risk,
            (result.effective_risk_pct or 0.0),
        )

        return result

    # ------------------------------------------------------------------ #
    # Configuration & helpers
    # ------------------------------------------------------------------ #
    def reload(self) -> None:
        conf = cfg
        g = conf.get("guard", {}) or {}
        cb_cfg = conf.get("circuit_breaker", {}) or {}

        max_positions = int(g.get("max_positions", conf.get("runtime", {}).get("max_positions", 1)))
        inflight_timeout = int(g.get("inflight_timeout_sec", 20))
        self.pos_guard = PositionGuard(max_positions=max_positions, inflight_timeout_sec=inflight_timeout)

        self.cb = CircuitBreaker(
            max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", conf.get("risk", {}).get("max_consecutive_losses", 5))),
            daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
            cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
        )
        self._reconcile_interval = int(g.get("reconcile_interval_sec", 15))
        self._desync_fix = bool(g.get("desync_fix", True))
        self._last_reconcile = 0.0
        self.state = trade_state.get_runtime()

    def _periodic_reconcile(self, symbol: str) -> None:
        now = time.time()
        if now - self._last_reconcile >= self._reconcile_interval:
            self._last_reconcile = now
            self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=self._desync_fix)

    # ------------------------------------------------------------------ #
    # Decisions & guards
    # ------------------------------------------------------------------ #
    def can_open(self, symbol: Optional[str]) -> bool:
        if symbol:
            self._periodic_reconcile(symbol)
        return self.pos_guard.can_open()

    def decide_entry_from_probs(self, p_buy: float, p_sell: float) -> Dict:
        conf = load_config()
        entry_cfg = conf.get("entry", {}) if isinstance(conf, dict) else {}
        th = float(entry_cfg.get("prob_threshold", entry_cfg.get("threshold_buy", 0.60)))
        edge = float(entry_cfg.get("entry_min_edge", entry_cfg.get("min_edge", 0.0)))
        bias = (entry_cfg.get("side_bias") or "auto").lower()

        pmax = p_buy if p_buy >= p_sell else p_sell
        p2nd = p_sell if p_buy >= p_sell else p_buy
        side = "BUY" if p_buy >= p_sell else "SELL"

        if pmax < th:
            return {"decision": "SKIP", "meta": "SKIP", "side": None, "reason": "ai_skip", "threshold": th}

        if (pmax - p2nd) < edge:
            return {
                "decision": "SKIP",
                "meta": "SKIP",
                "side": None,
                "reason": "ai_low_edge",
                "threshold": th,
                "edge": edge,
            }

        if p_buy == p_sell:
            if bias == "buy":
                side = "BUY"
            elif bias == "sell":
                side = "SELL"

        return {"decision": "ENTRY", "meta": side, "side": side, "threshold": th, "edge": edge}

    def decide_entry(self, p_buy: float, p_sell: float) -> Optional[str]:
        result = self.decide_entry_from_probs(p_buy, p_sell)
        return result["side"] if result.get("decision") == "ENTRY" else None

    def can_trade(self) -> bool:
        return self.cb.can_trade()

    def open_position(
        self,
        symbol: str,
        direction: str,
        sl: float | None = None,
        tp: float | None = None,
        *,
        atr: float | None = None,
        volume: float | None = None,
        comment: str = "",
    ) -> None:
        """
        ã‚¨ãƒ³ãƒˆãƒªãƒ¼ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ã€‚

        Parameters
        ----------
        symbol : str
            é€šè²¨ãƒšã‚¢ã€‚
        direction : str
            "buy" or "sell"ã€‚
        sl : float | None
            æåˆ‡ã‚Šä¾¡æ ¼ã€‚
        tp : float | None
            åˆ©ç¢ºä¾¡æ ¼ã€‚
        atr : float | None
            ã‚¨ãƒ³ãƒˆãƒªãƒ¼ç›´å‰ã®è¶³ã§è¨ˆç®—ã—ãŸ ATRã€‚
        volume : float | None
            ãƒ­ãƒƒãƒˆæ•°ã€‚None ã®å ´åˆã€atr + å£åº§æƒ…å ±ã‹ã‚‰è‡ªå‹•è¨ˆç®—ã€‚
        comment : str
            MT5 ã‚ªãƒ¼ãƒ€ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã€‚
        """
        if self._mt5 is None:
            raise RuntimeError("MT5 client is not configured on TradeService.")
        if self._profile is None:
            raise RuntimeError("Strategy profile is not configured on TradeService.")

        side = direction.upper()
        if side not in {"BUY", "SELL"}:
            raise ValueError('direction must be "buy" or "sell"')

        # --- ãƒ­ãƒƒãƒˆè¨ˆç®— ---
        if volume is None:
            if atr is None:
                raise ValueError(
                    "volume ã‚‚ atr ã‚‚æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                    "ATRãƒ™ãƒ¼ã‚¹è‡ªå‹•ãƒ­ãƒƒãƒˆè¨ˆç®—ã‚’ä½¿ã†å ´åˆã¯ atr ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚"
                )

            equity = float(self._mt5.get_equity())
            tick_spec: TickSpec = self._mt5.get_tick_spec(symbol)

            lot_result: LotSizingResult = self._profile.compute_lot_size_from_atr(
                equity=equity,
                atr=atr,
                tick_size=tick_spec.tick_size,
                tick_value=tick_spec.tick_value,
            )

            raw_volume = getattr(lot_result, "volume", None)
            if raw_volume is None:
                raw_volume = getattr(lot_result, "lot", None)
            if raw_volume is None:
                raise ValueError("LotSizingResult ã« lot/volume ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            volume = float(raw_volume)
            self._last_lot_result = lot_result
        else:
            # volume ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã‚’å„ªå…ˆ
            self._last_lot_result = None

        self._mt5.order_send(
            symbol=symbol,
            order_type=side,
            lot=float(volume),
            sl=sl,
            tp=tp,
            comment=comment,
        )

    def mark_order_inflight(self, order_id: str) -> None:
        self.pos_guard.mark_inflight(order_id)

    def on_order_result(self, *, order_id: str, ok: bool, symbol: str) -> None:
        self.pos_guard.clear_inflight(order_id)
        if ok:
            self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=True)

    def on_order_success(self, *, ticket: Optional[int], side: str, symbol: str, price: Optional[float] = None) -> None:
        self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=True)
        runtime = self.state
        runtime.last_ticket = ticket
        runtime.last_side = side
        runtime.last_symbol = symbol
        EVENT_STORE.add(kind="ENTRY", symbol=symbol, side=side, price=price, sl=None, notes=f"ticket={ticket}")

    def on_broker_sync(self, symbol: Optional[str], fix: bool = True) -> None:
        self.pos_guard.reconcile_with_broker(symbol, desync_fix=fix)

    def record_trade_result(
        self,
        *,
        symbol: str,
        side: str,
        profit_jpy: float,
        info: Optional[dict[str, Any]] = None,
    ) -> None:
        resolved_symbol = symbol or self.state.last_symbol or "-"
        resolved_side = side or self.state.last_side
        notes = "settled"
        if info:
            if "notes" in info:
                notes = str(info["notes"])
            else:
                notes = str(info)
        EVENT_STORE.add(
            kind="CLOSE",
            symbol=resolved_symbol,
            side=resolved_side,
            profit_jpy=float(profit_jpy),
            notes=notes,
        )
        self.cb.on_trade_result(profit_jpy)


# ------------------------------------------------------------------ #
# Module-level helpers (backwards compatibility)
# ------------------------------------------------------------------ #
SERVICE = TradeService()


def can_open_new_position(symbol: Optional[str] = None) -> bool:
    settings = trade_state.get_settings()
    if not settings.trading_enabled:
        return False
    sym = symbol or load_config().get("runtime", {}).get("symbol")
    return SERVICE.can_open(sym)


def decide_entry(p_buy: float, p_sell: float) -> Optional[str]:
    return SERVICE.decide_entry(p_buy, p_sell)


def decide_entry_from_probs(p_buy: float, p_sell: float) -> dict:
    return SERVICE.decide_entry_from_probs(p_buy, p_sell)


def get_account_summary() -> dict[str, Any] | None:
    return mt5_client.get_account_info()


def build_exit_plan(symbol: str, ohlc_tail: Optional[Iterable[dict[str, Any]]]) -> dict[str, Any]:
    conf = load_config()
    ex_cfg = conf.get("exits", {}) if isinstance(conf, dict) else {}
    mode = (ex_cfg.get("mode") or "fixed").lower()

    if mode == "none":
        return {"mode": "none"}

    if mode == "fixed":
        fx = ex_cfg.get("fixed", {}) or {}
        return {
            "mode": "fixed",
            "tp_pips": float(fx.get("tp_pips", 10)),
            "sl_pips": float(fx.get("sl_pips", 10)),
        }

    if mode == "atr":
        ax = ex_cfg.get("atr", {}) or {}
        period = int(ax.get("period", 14))
        tp_mult = float(ax.get("tp_mult", 1.2))
        sl_mult = float(ax.get("sl_mult", 1.0))
        trailing = ax.get("trailing", {}) or {}

        highs: list[float] = []
        lows: list[float] = []
        closes: list[float] = []
        for row in ohlc_tail or []:
            h = row.get("h") or row.get("high")
            l = row.get("l") or row.get("low")
            c = row.get("c") or row.get("close")
            if h is not None:
                highs.append(float(h))
            if l is not None:
                lows.append(float(l))
            if c is not None:
                closes.append(float(c))

        atr_value = _atr(highs, lows, closes, period)
        return {
            "mode": "atr",
            "atr": atr_value,
            "tp_mult": tp_mult,
            "sl_mult": sl_mult,
            "trailing": {
                "enabled": bool(trailing.get("enabled", True)),
                "activate_atr_mult": float(trailing.get("activate_atr_mult", 0.5)),
                "step_atr_mult": float(trailing.get("step_atr_mult", 0.25)),
                "lock_be_atr_mult": float(trailing.get("lock_be_atr_mult", 0.3)),
                "hard_floor_pips": float(trailing.get("hard_floor_pips", 5)),
                "only_in_profit": bool(trailing.get("only_in_profit", True)),
                "max_layers": int(trailing.get("max_layers", 20)),
                "price_source": (trailing.get("price_source") or "mid").lower(),
            },
        }

    return {"mode": "fixed", "tp_pips": 10, "sl_pips": 10}


_trade_last_fill_ts: Optional[datetime] = None


def mark_filled_now() -> None:
    """Record the timestamp of the latest successful fill."""
    global _trade_last_fill_ts
    _trade_last_fill_ts = now_jst()


def post_fill_grace_active() -> bool:
    """Return True when the post-fill grace window is active."""
    if _trade_last_fill_ts is None:
        return False

    conf = load_config()
    runtime_cfg = conf.get("runtime", {}) if isinstance(conf, dict) else {}
    grace_sec = int((runtime_cfg or {}).get("post_fill_grace_sec", 0) or 0)
    if grace_sec <= 0:
        return False

    return (now_jst() - _trade_last_fill_ts) <= timedelta(seconds=grace_sec)


def mark_order_inflight(order_id: str) -> None:
    SERVICE.mark_order_inflight(order_id)


def on_order_result(order_id: str, ok: bool, symbol: str) -> None:
    SERVICE.on_order_result(order_id=order_id, ok=ok, symbol=symbol)


def reconcile_positions(symbol: Optional[str] = None, desync_fix: bool = True) -> None:
    SERVICE.on_broker_sync(symbol, fix=desync_fix)


def on_order_success(ticket: Optional[int], side: str, symbol: str, price: Optional[float] = None) -> None:
    SERVICE.on_order_success(ticket=ticket, side=side, symbol=symbol, price=price)


def record_trade_result(
    *,
    symbol: str,
    side: str,
    profit_jpy: float,
    info: Optional[dict[str, Any]] = None,
) -> None:
    SERVICE.record_trade_result(symbol=symbol, side=side, profit_jpy=profit_jpy, info=info)


def circuit_breaker_can_trade() -> bool:
    return SERVICE.can_trade()



=== file: app/services/trade_state.py ===

from dataclasses import dataclass, asdict
from typing import Any, Optional

@dataclass
class TradeSettings:
    trading_enabled: bool = False
    threshold_buy: float = 0.60
    threshold_sell: float = 0.60
    prob_threshold: float = 0.60
    side_bias: str = "auto"
    tp_pips: int = 15
    sl_pips: int = 10

# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³çš„ã«ãƒ—ãƒ­ã‚»ã‚¹å†…å…±æœ‰
_settings = TradeSettings()



@dataclass
class TradeRuntime:
    last_ticket: Optional[int] = None
    last_side: Optional[str] = None
    last_symbol: Optional[str] = None


_runtime_state = TradeRuntime()


def get_runtime() -> TradeRuntime:
    return _runtime_state


def update_runtime(**kwargs: Any) -> None:
    for k, v in kwargs.items():
        if hasattr(_runtime_state, k):
            setattr(_runtime_state, k, v)

def get_settings() -> TradeSettings:
    return _settings

def update(**kwargs: Any) -> None:
    for k, v in kwargs.items():
        if hasattr(_settings, k):
            setattr(_settings, k, v)

def as_dict() -> dict[str, Any]:
    return asdict(_settings)



=== file: app/services/trailing.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional


@dataclass
class TrailConfig:
    pip_size: float
    point: float
    atr: float
    activate_mult: float
    step_mult: float
    lock_be_mult: float
    hard_floor_pips: float
    only_in_profit: bool
    max_layers: int


@dataclass
class TrailState:
    side: str  # "BUY" or "SELL"
    entry: float
    activated: bool = False
    be_locked: bool = False
    layers: int = 0
    current_sl: Optional[float] = None


def _round_to_point(price: float, point: float) -> float:
    k = round(price / point)
    return k * point


def _pips(price_diff: float, pip_size: float) -> float:
    return price_diff / pip_size


def _price_from_pips(pips: float, pip_size: float) -> float:
    return pips * pip_size


def _profit_side(side: str, entry: float, price: float) -> float:
    return (price - entry) if side == "BUY" else (entry - price)


class AtrTrailer:
    def __init__(self, cfg: TrailConfig, state: TrailState):
        self.cfg = cfg
        self.st = state

    def activation_threshold(self) -> float:
        return self.cfg.atr * self.cfg.activate_mult

    def step_size(self) -> float:
        return self.cfg.atr * self.cfg.step_mult

    def be_threshold(self) -> float:
        return self.cfg.atr * self.cfg.lock_be_mult

    def suggest_sl(self, current_price: float) -> Optional[float]:
        profit = _profit_side(self.st.side, self.st.entry, current_price)
        if profit <= 0:
            return None

        if not self.st.activated and profit >= self.activation_threshold():
            self.st.activated = True
            sl = self._hard_floor_sl()
            applied = self._apply_if_better(sl)
            if applied is not None:
                return applied

        if not self.st.activated:
            return None

        if (not self.st.be_locked) and (profit >= self.be_threshold()):
            self.st.be_locked = True
            be_sl = self._breakeven_sl()
            hf_sl = self._hard_floor_sl()
            if self.st.side == "BUY":
                new_sl = max(be_sl, hf_sl)
            else:
                new_sl = min(be_sl, hf_sl)
            return self._apply_if_better(new_sl)

        step = self.step_size()
        if step <= 0:
            return None

        layers_should = int(profit // step)
        layers_should = min(layers_should, self.cfg.max_layers)
        if layers_should <= self.st.layers:
            return None

        move_layers = layers_should - self.st.layers
        new_sl = self._layer_sl(move_layers, current_price)
        self.st.layers = layers_should
        return self._apply_if_better(new_sl)

    def _hard_floor_sl(self) -> float:
        delta = _price_from_pips(self.cfg.hard_floor_pips, self.cfg.pip_size)
        if self.st.side == "BUY":
            sl = self.st.entry + delta
        else:
            sl = self.st.entry - delta
        return _round_to_point(sl, self.cfg.point)

    def _breakeven_sl(self) -> float:
        return _round_to_point(self.st.entry, self.cfg.point)

    def _layer_sl(self, move_layers: int, current_price: float) -> float:
        step = self.step_size() * move_layers
        if self.st.side == "BUY":
            sl = current_price - step
        else:
            sl = current_price + step
        sl = self._ensure_profit_side(sl)
        return _round_to_point(sl, self.cfg.point)

    def _ensure_profit_side(self, sl: float) -> float:
        hf = self._hard_floor_sl()
        current = self.st.current_sl
        if self.st.side == "BUY":
            sl = max(sl, hf)
            if self.cfg.only_in_profit and current is not None:
                sl = max(sl, current)
        else:
            sl = min(sl, hf)
            if self.cfg.only_in_profit and current is not None:
                sl = min(sl, current)
        return sl

    def _apply_if_better(self, new_sl: float) -> Optional[float]:
        cur = self.st.current_sl
        if cur is None:
            self.st.current_sl = new_sl
            return new_sl
        if self.st.side == "BUY" and new_sl > cur:
            self.st.current_sl = new_sl
            return new_sl
        if self.st.side == "SELL" and new_sl < cur:
            self.st.current_sl = new_sl
            return new_sl
        return None



=== file: app/services/trailing_hook.py ===

from __future__ import annotations

import logging
from typing import Any, Optional

from app.services.event_store import EVENT_STORE
from core.metrics import METRICS
from core.utils.runtime import is_live

logger = logging.getLogger(__name__)

_mt5svc: Optional[MT5Service] = None

try:
    from app.services.mt5_service import MT5Service
except Exception:
    _mt5svc = None
else:
    _mt5svc = MT5Service(max_retries=3, backoff_sec=0.3, min_change_points=2)


def apply_trailing_update(
    *,
    ticket: Optional[int],
    side: str,
    symbol: str,
    new_sl: float,
    reason: str = "trail",
) -> bool:
    """
    Apply trailing-stop loss updates (dry-run logs or live MT5 OrderModify).
    """

    METRICS.set({"trail_proposed_sl": new_sl, "trail_reason": reason})

    if not is_live():
        EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="DRYRUN")
        logger.info(f"[TRAIL][DRYRUN] side={side} symbol={symbol} new_sl={new_sl} reason={reason}")
        return True

    if _mt5svc is None or ticket is None:
        EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="SKIP")
        logger.warning(
            f"[TRAIL][LIVE][SKIP] ticket={ticket} svc={_mt5svc} side={side} symbol={symbol} new_sl={new_sl}"
        )
        return False

    ok, sent_sl, msg = _mt5svc.safe_order_modify_sl(
        ticket=ticket,
        side=side,
        symbol=symbol,
        desired_sl=new_sl,
        reason=reason,
    )
    sl_val = float(sent_sl) if sent_sl is not None else None

    if ok:
        EVENT_STORE.add(
            kind="TRAIL",
            symbol=symbol,
            side=side,
            sl=sl_val,
            reason=reason,
            notes="OK",
        )
        logger.info(f"[TRAIL][OK] ticket={ticket} side={side} sl={sent_sl} reason={reason} {msg}")
        if sl_val is not None:
            METRICS.set({"trail_current_sl": sl_val})
        METRICS.set({"trail_last_ok": True})
        return True

    if sl_val is not None:
        METRICS.set({"trail_current_sl": sl_val})

    EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="NG")
    logger.warning(f"[TRAIL][NG] ticket={ticket} side={side} desired={new_sl} reason={reason} {msg}")
    METRICS.set({"trail_last_ok": False})
    return False



=== file: app/strategies/__init__.py ===




=== file: app/strategies/ai_strategy.py ===

# app/strategies/ai_strategy.py
from __future__ import annotations
from pathlib import Path
import json
import pandas as pd
import numpy as np
import math
import joblib
import pickle
import binascii
from typing import Tuple, Dict, Any

PROJECT_ROOT = Path(__file__).resolve().parents[2]

def _load_model_generic(path_str: str):
    """
    1) joblib.load()
    2) pickle.load()
    3) LightGBM Boosterï¼ˆ.txtã‚„ãƒã‚¤ãƒŠãƒªï¼‰
       â”” pklãŒå¤±æ•—ã—ãŸã‚‰ã€ŒåŒå.txtã€ã«ã‚‚è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    å…¨æ»…æ™‚ã¯å…ˆé ­ãƒã‚¤ãƒˆã‚’dumpã—ã¦åŸå› ç‰¹å®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã€‚
    """
    p = Path(path_str)

    # 1) joblib
    try:
        import joblib
        m = joblib.load(p)
        print(f"[wfo] model loaded via joblib: {p.name}", flush=True)
        return m
    except Exception as e1:
        e1_msg = str(e1)

    # 2) pickle
    try:
        with open(p, "rb") as f:
            m = pickle.load(f)
        print(f"[wfo] model loaded via pickle: {p.name}", flush=True)
        return m
    except Exception as e2:
        e2_msg = str(e2)

    # 3) Booster
    def _try_booster(mp: Path):
        import lightgbm as lgb
        booster = lgb.Booster(model_file=str(mp))
        class _BoosterWrapper:
            def __init__(self, bst): self.bst = bst
            def predict_proba(self, X):
                prob1 = self.bst.predict(X)
                prob1 = np.asarray(prob1).reshape(-1)
                prob0 = 1.0 - prob1
                return np.vstack([prob0, prob1]).T
        print(f"[wfo] model loaded via booster: {mp.name}", flush=True)
        return _BoosterWrapper(booster)

    e3_msg = ""
    try:
        return _try_booster(p)
    except Exception as e3:
        e3_msg = str(e3)

    # pklâ†’txt ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    alt_txt = p.with_suffix(".txt")
    if alt_txt.exists():
        try:
            return _try_booster(alt_txt)
        except Exception as e4:
            e3_msg += f" | alt_txt='{alt_txt.name}': {e4}"

    # å…ˆé ­ãƒã‚¤ãƒˆã‚’è¡¨ç¤º
    try:
        head = p.read_bytes()[:16]
        head_hex = binascii.hexlify(head).decode("ascii")
    except Exception:
        head_hex = "unreadable"

    raise RuntimeError(
        f"model load failed: joblib='{e1_msg}' | pickle='{e2_msg}' | booster='{e3_msg}' | head={head_hex}"
    )

    
# =====================================================
# active_model.json ã®èª­ã¿è¾¼ã¿
# =====================================================
def load_active_model() -> Tuple[str, str, float, Dict[str, Any]]:
    meta = PROJECT_ROOT / "active_model.json"
    if not meta.exists():
        raise FileNotFoundError(f"{meta} not found.")

    j = json.loads(meta.read_text(encoding="utf-8"))
    model_name = j.get("model_name", "").strip()
    threshold = float(j.get("best_threshold", 0.5))
    params = j.get("params", {}) or {}

    if model_name.startswith("builtin_"):
        return ("builtin", model_name, threshold, params)

    # å¤–éƒ¨ãƒ¢ãƒ‡ãƒ«
    pkl = PROJECT_ROOT / "models" / f"{model_name}.pkl"
    txt = PROJECT_ROOT / "models" / f"{model_name}.txt"

    # ã“ã“ã‚’ txt å„ªå…ˆã«ã™ã‚‹
    if txt.exists():
        return ("pickle", str(txt), threshold, params)
    if pkl.exists():
        return ("pickle", str(pkl), threshold, params)

    raise FileNotFoundError(f"Model file not found: {pkl} nor {txt}")


# =====================================================
# ç‰¹å¾´é‡ãƒ¬ã‚·ãƒ”
# =====================================================
def _rsi(x: pd.Series, period: int = 14) -> pd.Series:
    delta = x.diff()
    up = (delta.clip(lower=0)).rolling(period).mean()
    down = (-delta.clip(upper=0)).rolling(period).mean()
    rs = up / (down + 1e-12)
    return 100 - (100 / (1 + rs))

def _ema(x: pd.Series, span: int) -> pd.Series:
    return x.ewm(span=span, adjust=False).mean()

def _bbands(x: pd.Series, window: int = 20, n_sigma: float = 2.0):
    ma = x.rolling(window).mean()
    sd = x.rolling(window).std(ddof=0)
    upper = ma + n_sigma * sd
    lower = ma - n_sigma * sd
    return upper, lower

def _stoch(high: pd.Series, low: pd.Series, close: pd.Series, k_win: int = 14, d_win: int = 3):
    ll = low.rolling(k_win).min()
    hh = high.rolling(k_win).max()
    k = (close - ll) / (hh - ll + 1e-12) * 100
    d = k.rolling(d_win).mean()
    return k, d

def build_features_recipe(df: pd.DataFrame, name: str) -> pd.DataFrame:
    """
    å†…è”µãƒ¬ã‚·ãƒ”ã§ç‰¹å¾´é‡ã‚’ä½œæˆã€‚timeåˆ—ã¯æ®‹ã—ã¾ã™ã€‚
    name:
      - "ohlcv_tech_v1": ä»£è¡¨çš„ãªãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ç¾¤
    """
    out = df.copy()
    close = out["close"].astype(float)
    high = out["high"].astype(float)
    low  = out["low"].astype(float)

    if name == "ohlcv_tech_v1":
        out["ret1"]  = close.pct_change()
        out["ret5"]  = close.pct_change(5)
        out["ret20"] = close.pct_change(20)

        out["sma_10"] = close.rolling(10, min_periods=1).mean()
        out["sma_50"] = close.rolling(50, min_periods=1).mean()
        out["ema_20"] = _ema(close, 20)

        out["rsi_14"] = _rsi(close, 14)
        u,l = _bbands(close, 20, 2.0)
        out["bb_high_20_2"] = u
        out["bb_low_20_2"]  = l

        k,d = _stoch(high, low, close, 14, 3)
        out["stoch_k_14_3"] = k
        out["stoch_d_14_3"] = d

        # ã‚·ãƒ³ãƒ—ãƒ«ãªATRé¢¨ï¼ˆé«˜ä½å·®ã®EMAï¼‰
        tr = (high - low).abs()
        out["atr_14"] = tr.rolling(14).mean()

        # ãƒœãƒ©æŒ‡æ¨™
        out["vol_pct_20"] = close.pct_change().rolling(20).std() * math.sqrt(20)

    else:
        raise ValueError(f"unknown feature recipe: {name}")

    out = out.dropna().reset_index(drop=True)
    return out

def build_features(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
    """å¤–éƒ¨ãƒ¢ãƒ‡ãƒ«ãƒ»å†…è”µåŒæ–¹ã§ä½¿ã†ç‰¹å¾´é‡ãƒ“ãƒ«ãƒ‰ã®çµ±ä¸€å…¥å£"""
    recipe = (params or {}).get("feature_recipe", "ohlcv_tech_v1")
    feat = build_features_recipe(df, recipe)
    return feat

# =====================================================
# äºˆæ¸¬ã¨ã‚·ã‚°ãƒŠãƒ«
# =====================================================
def _load_scaler_if_any(params: Dict[str, Any]):
    name = params.get("scaler_name")
    if not name:
        return None
    p = PROJECT_ROOT / "models" / "scalers" / f"{name}.pkl"
    if not p.exists():
        raise FileNotFoundError(f"Scaler not found: {p}")

    # ã¾ãšãƒ˜ãƒƒãƒ€ã‚’è¦‹ã¦å½¢å¼æ¨å®š
    head_hex = None
    try:
        b = p.read_bytes()
        head_hex = binascii.hexlify(b[:8]).decode("ascii")
        # NumPy .npy ã®å…¸å‹ãƒ˜ãƒƒãƒ€ã¯ \x93NUMPY (= 93 4e 55 4d 50 59)
        is_npy = b[:6] == b"\x93NUMPY"
    except Exception:
        is_npy = False

    # 1) æ¨™æº–pickle
    if not is_npy:
        try:
            with open(p, "rb") as f:
                scaler = pickle.load(f)
            typename = type(scaler).__name__
            print(f"[wfo] using scaler: {name} ({typename})", flush=True)
            return scaler
        except Exception as e1:
            # 2) joblib
            try:
                import joblib
                scaler = joblib.load(p)
                typename = type(scaler).__name__
                print(f"[wfo] using scaler: {name} ({typename}) via joblib", flush=True)
                return scaler
            except Exception as e2:
                # 3) æœ€å¾Œã« NumPy ãƒ­ãƒ¼ãƒ€
                try:
                    arr = np.load(str(p), allow_pickle=True)
                    # .npz ã®å ´åˆã¯æœ€åˆã®ã‚­ãƒ¼ã‚’å–ã‚Šå‡ºã™
                    if hasattr(arr, "files"):
                        key0 = arr.files[0]
                        arr = arr[key0]
                    print(f"[wfo] using scaler: {name} (ndarray via np.load)", flush=True)
                    return arr
                except Exception as e3:
                    raise RuntimeError(
                        f"Scaler load failed: pickle='{e1}' | joblib='{e2}' | numpy='{e3}' | head={head_hex}"
                    )

    # ãƒ˜ãƒƒãƒ€ã§ .npy ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆã¯æœ€åˆã‹ã‚‰ NumPy
    try:
        arr = np.load(str(p), allow_pickle=True)
        if hasattr(arr, "files"):
            key0 = arr.files[0]
            arr = arr[key0]
        print(f"[wfo] using scaler: {name} (ndarray via np.load)", flush=True)
        return arr
    except Exception as e:
        raise RuntimeError(f"Scaler load failed (npy fast-path): {e} | head={head_hex}")


def _ensure_feature_order(feat_df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
    cols = params.get("feature_cols")
    if cols:
        # æ˜ç¤ºã•ã‚ŒãŸåˆ—é †ã«åˆã‚ã›ã€ä¸è¶³ã¯ã‚¨ãƒ©ãƒ¼ã€ä½™åˆ†ã¯å‰Šé™¤
        missing = [c for c in cols if c not in feat_df.columns]
        if missing:
            raise ValueError(f"Missing features for model: {missing}")
        return feat_df.loc[:, cols]
    # æ˜ç¤ºãªã—ãªã‚‰ã€priceç³»ã‚’é™¤ã„ãŸæ´¾ç”Ÿåˆ—ã‚’ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã§å®‰å®šåŒ–
    skip = {"time","open","high","low","close","tick_volume","real_volume","spread"}
    cols = [c for c in feat_df.columns if c not in skip]
    return feat_df.loc[:, sorted(cols)]

def _sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def _predict_proba_generic(model, X: np.ndarray) -> np.ndarray:
    """
    LightGBM / XGBoost / Sklearn ã„ãšã‚Œã§ã‚‚é™½æ€§ç¢ºç‡ã‚’è¿”ã™æ±ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    """
    import lightgbm as lgb

    # Xã¯å¸¸ã«ndarrayåŒ–ã—ã¦å®‰å…¨å´ã«å€’ã™
    if isinstance(X, pd.DataFrame):
        X = X.values
    elif not isinstance(X, np.ndarray):
        X = np.asarray(X)

    # LightGBM Boosterï¼ˆç”Ÿboosterã¾ãŸã¯ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰
    if isinstance(model, lgb.Booster):
        try:
            prob1 = model.predict(X)
        except Exception as e:
            print(f"[wfo] warn: Booster.predict failed: {e}  -> fallback sigmoid(raw score)", flush=True)
            raw = model.predict(X, raw_score=True)
            prob1 = 1.0 / (1.0 + np.exp(-raw))
        prob1 = np.asarray(prob1).reshape(-1)
        prob0 = 1.0 - prob1
        return np.vstack([prob0, prob1]).T

    # ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆ_BoosterWrapperï¼‰ãªã‚‰ predict_proba ã‚’ç›´æ¥å‘¼ã¶
    if hasattr(model, "bst") and hasattr(model.bst, "predict"):
        try:
            prob1 = model.bst.predict(X)
            prob1 = np.asarray(prob1).reshape(-1)
            prob0 = 1.0 - prob1
            return np.vstack([prob0, prob1]).T
        except Exception as e:
            print(f"[wfo] warn: BoosterWrapper predict failed: {e}", flush=True)

    # Sklearnç³»ï¼špredict_proba
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X)
        if proba.ndim == 1:
            return proba
        if proba.shape[1] == 2:
            return proba[:, 1]
        return proba[:, -1]

    # decision_functionï¼ˆSVMãªã©ï¼‰
    if hasattr(model, "decision_function"):
        score = model.decision_function(X)
        return _sigmoid(score)

    # ãã‚Œä»¥å¤–ã¯ predict() ã®çµæœã‚’ç¢ºç‡æ‰±ã„
    pred = model.predict(X)
    return np.asarray(pred).astype(float)


def predict_signals(kind: str, payload, df_feat: pd.DataFrame, threshold: float = 0.0, params=None) -> pd.Series:
    """
    - builtin_sma: fast/slow ã®ã‚¯ãƒ­ã‚¹ã§ +1/-1 ã‚’è¿”ã™
    - pickle: å¤–éƒ¨ãƒ¢ãƒ‡ãƒ«ã® proba > threshold â†’ +1ã€ãã‚Œä»¥å¤– â†’ -1ï¼ˆlong_shortï¼‰ç­‰ã€params['mode'] ã§åˆ¶å¾¡
    """
    params = params or {}
    mode = params.get("mode", "long_short")  # long_only / short_only / long_flat / long_short

    if kind == "builtin":
        name = str(payload)
        if name == "builtin_sma":
            fast = int(params.get("fast", 10))
            slow = int(params.get("slow", 50))
            sma_fast = df_feat["close"].rolling(fast, min_periods=1).mean()
            sma_slow = df_feat["close"].rolling(slow, min_periods=1).mean()
            raw = np.where(sma_fast > sma_slow, 1, -1)
        else:
            raise ValueError(f"unknown builtin model: {name}")

    else:
        # å¤–éƒ¨ãƒ¢ãƒ‡ãƒ«æ¨è«–
        model = _load_model_generic(payload)
        X = _ensure_feature_order(df_feat, params)
        scaler = _load_scaler_if_any(params)
        if scaler is not None:
            Xv = X.values
            try:
                # æ¨™æº–ã®sklearnç³»ï¼ˆStandardScaler ãªã©ï¼‰
                Xv = scaler.transform(Xv)
            except AttributeError:
                # dict / (mean, scale) / ndarray ã‚’è¨±å®¹
                if isinstance(scaler, dict) and ("mean" in scaler or "scale" in scaler):
                    mean = np.asarray(scaler.get("mean", np.zeros(Xv.shape[1])))
                    scale = np.asarray(scaler.get("scale", np.ones(Xv.shape[1])))
                    Xv = (Xv - mean) / (scale + 1e-12)
                elif isinstance(scaler, (tuple, list)) and len(scaler) >= 2:
                    mean = np.asarray(scaler[0])
                    scale = np.asarray(scaler[1])
                    Xv = (Xv - mean) / (scale + 1e-12)
                elif isinstance(scaler, np.ndarray):
                    # å¹³å‡ã ã‘ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹
                    mean = scaler
                    Xv = (Xv - mean)
                else:
                    # æƒ³å®šå¤–ã®å‹ã¯æœªã‚¹ã‚±ãƒ¼ãƒ«ã§ç¶šè¡Œ
                    print(f"[wfo] warn: unknown scaler type -> skip scaling ({type(scaler).__name__})", flush=True)
            # DataFrameã«æˆ»ã™ï¼ˆåˆ—åã¯ç¶­æŒï¼‰
            X = pd.DataFrame(Xv, index=X.index, columns=X.columns)
        ##
        proba = _predict_proba_generic(model, X.values)

        # 2æ¬¡å…ƒ(=ç¢ºç‡2åˆ—)ãªã‚‰é™½æ€§å´ã ã‘ã‚’æ¡ç”¨
        if proba.ndim == 2 and proba.shape[1] == 2:
            proba = proba[:, 1]

        raw = (proba > float(threshold)).astype(int)  # 1 or 0
        ##
        # raw(1/0) â†’ ãƒ¢ãƒ¼ãƒ‰ã”ã¨ã®æœ€çµ‚signal
        if mode == "long_only" or mode == "long_flat":
            # 1=long, 0=flat
            sig = np.where(raw==1, 1, 0)
            return pd.Series(sig, index=df_feat.index, name="signal")
        elif mode == "short_only":
            # 1=flat, 0=short
            sig = np.where(raw==1, 0, -1)
            return pd.Series(sig, index=df_feat.index, name="signal")
        else:
            # long_short: 1=long, 0=short
            sig = np.where(raw==1, 1, -1)
            return pd.Series(sig, index=df_feat.index, name="signal")

    # builtinã®ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿
    if mode == "long_only" or mode == "long_flat":
        sig = np.where(raw==1, 1, 0)
    elif mode == "short_only":
        sig = np.where(raw==1, 0, -1)
    else:
        sig = raw
    return pd.Series(sig, index=df_feat.index, name="signal")

# =====================================================
# ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆï¼ˆå‹•çš„ã‚µã‚¤ã‚ºãƒ»ãƒ­ãƒ³ã‚°/ã‚·ãƒ§ãƒ¼ãƒˆå¯¾å¿œï¼‰
# =====================================================
def trades_from_signals(df_feat: pd.DataFrame, initial_capital: float, params=None) -> pd.DataFrame:
    """
    signalåˆ—ï¼ˆ+1/-1/0ï¼‰ã«åŸºã¥ã„ã¦ IN/OUT/åè»¢ã€‚
    ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã¯ equity * risk_pct / price ã‚’ lot_step ã§ä¸¸ã‚ã€‚
    PnL = (exit - entry) * dir * units - (spread+commission) * units
    """
    if "signal" not in df_feat.columns:
        raise ValueError("signalåˆ—ãŒå¿…è¦ã§ã™ã€‚")

    p = params or {}
    spread_pips = float(p.get("spread_pips", 0.2))
    risk_pct = float(p.get("risk_pct", 0.01))
    lot_step = int(p.get("lot_step", 1000))
    min_units = int(p.get("min_units", lot_step))
    max_units = int(p.get("max_units", 200000))
    commission_per_unit = float(p.get("commission_per_unit", 0.0))

    spread_yen_per_unit = spread_pips * 0.01
    fee_yen_per_unit = commission_per_unit
    cost_yen_per_unit = spread_yen_per_unit + fee_yen_per_unit

    position = 0           # +1 / -1 / 0
    entry_price = None
    entry_time = None
    units = 0
    equity = float(initial_capital)

    trades = []
    idx_time = df_feat["time"].tolist()
    prices  = df_feat["close"].astype(float).tolist()
    sigs    = df_feat["signal"].astype(int).tolist()

    def _round_units(u: float) -> int:
        if u <= 0:
            return 0
        u = int(u // lot_step * lot_step)
        return max(min_units, min(u, max_units)) if u > 0 else 0

    # entry_time ã®è¡Œç•ªå·æ¤œç´¢ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã®è¾æ›¸
    time_to_index = {t:i for i,t in enumerate(idx_time)}

    for i in range(len(df_feat)):
        sig = sigs[i]
        price = prices[i]
        t = idx_time[i]

        if sig != position:
            if position != 0 and entry_price is not None and units > 0:
                pnl = (price - entry_price) * position * units - cost_yen_per_unit * units
                equity += pnl
                et_idx = time_to_index.get(entry_time, i)
                trades.append({
                    "entry_time": entry_time,
                    "exit_time": t,
                    "pnl": float(pnl),
                    "direction": "LONG" if position > 0 else "SHORT",
                    "units": int(units),
                    "entry_price": float(entry_price),
                    "exit_price": float(price),
                    "holding_bars": i - et_idx,
                    "holding_days": (pd.Timestamp(t) - pd.Timestamp(entry_time)).days,
                    "win": int(pnl > 0),
                    "equity_after": float(equity)
                })

            position = sig
            entry_price = price
            entry_time = t
            if position != 0:
                raw_units = (equity * risk_pct) / max(price, 1e-9)
                units = _round_units(raw_units)
            else:
                units = 0

    if position != 0 and entry_price is not None and units > 0:
        price = prices[-1]
        t = idx_time[-1]
        pnl = (price - entry_price) * position * units - cost_yen_per_unit * units
        equity += pnl
        et_idx = time_to_index.get(entry_time, len(df_feat)-1)
        trades.append({
            "entry_time": entry_time,
            "exit_time": t,
            "pnl": float(pnl),
            "direction": "LONG" if position > 0 else "SHORT",
            "units": int(units),
            "entry_price": float(entry_price),
            "exit_price": float(price),
            "holding_bars": (len(df_feat)-1) - et_idx,
            "holding_days": (pd.Timestamp(t) - pd.Timestamp(entry_time)).days,
            "win": int(pnl > 0),
            "equity_after": float(equity)
        })

    cols = [
        "entry_time","exit_time","pnl","direction","units",
        "entry_price","exit_price","holding_bars","holding_days","win","equity_after"
    ]
    return pd.DataFrame(trades, columns=cols)




=== file: apply_order.py ===

from pathlib import Path

# ã“ã“ã«ã€Œå®Œå…¨ã«å¾©å…ƒã—ãŸã„ mt5_client.py ã®ä¸­èº«ã€ã‚’ãã®ã¾ã¾åŸ‹ã‚è¾¼ã‚€
NEW_MT5_CLIENT_SOURCE = """import time
import MetaTrader5 as MT5
import pandas as pd
from loguru import logger
from typing import Optional, Dict, Any


POSITION_COLUMNS = [
    "ticket",
    "time",
    "time_msc",
    "time_update",
    "time_update_msc",
    "symbol",
    "magic",
    "volume",
    "price_open",
    "sl",
    "tp",
    "price_current",
    "swap",
    "profit",
    "comment",
]


class MT5Client:
    \"\"\"MT5 ç™ºæ³¨ãƒ»æ¥ç¶šãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆæœ€å°æ§‹æˆï¼‰\"\"\"

    def __init__(self, login: int, password: str, server: str, timeout: float = 5.0):
        self.login = login
        self.password = password
        self.server = server
        self.timeout = timeout
        self.connected = False
        self.logger = logger

    # ------------------------
    # æ¥ç¶šç³»
    # ------------------------
    def initialize(self) -> bool:
        \"\"\"MT5ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®åˆæœŸåŒ–ï¼ˆãƒ­ã‚°ã‚¤ãƒ³ã¯ login_account()ï¼‰\"\"\"
        logger.info("MT5 initialize() called...")

        if not MT5.initialize():
            err = MT5.last_error()
            logger.error(f"MT5 initialize() failed: {err}")
            self.connected = False
            return False

        logger.info("MT5 initialize() succeeded")
        self.connected = True
        return True

    def login_account(self) -> bool:
        \"\"\"è¨­å®šã•ã‚ŒãŸãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã§ MT5.login() ã‚’å®Ÿè¡Œ\"\"\"
        logger.info(
            f"MT5 login() called with login={self.login}, server={self.server}"
        )

        ok = MT5.login(
            self.login,
            password=self.password,
            server=self.server,
        )
        if not ok:
            err = MT5.last_error()
            logger.error(f"MT5 login() failed: {err}")
            return False

        logger.info("MT5 login() succeeded")
        return True

    def shutdown(self):
        \"\"\"MT5 ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³\"\"\"
        logger.info("MT5 shutdown()")
        MT5.shutdown()
        self.connected = False

    # ------------------------
    # ç™ºæ³¨
    # ------------------------
    def order_send(
        self,
        symbol: str,
        order_type: str,
        lot: float,
        sl: Optional[float] = None,
        tp: Optional[float] = None,
        retries: int = 3,
    ) -> Optional[int]:
        \"\"\"æˆè¡Œç™ºæ³¨ï¼ˆBUY / SELLï¼‰

        Parameters
        ----------
        symbol : str
        order_type : "BUY" or "SELL"
        lot : float
        sl, tp : Optional[float]
        retries : int

        Returns
        -------
        Optional[int]
            æˆåŠŸ: ãƒã‚±ãƒƒãƒˆç•ªå·ï¼ˆintï¼‰
            å¤±æ•—: None
        \"\"\"

        if order_type not in ("BUY", "SELL"):
            raise ValueError(f"order_type must be BUY/SELL: got {order_type}")

        # --- 1) ã‚·ãƒ³ãƒœãƒ«æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯ ---
        info = MT5.symbol_info(symbol)
        if info is None:
            logger.error(f"[order_send] symbol_info({symbol}) ãŒ Noneã€‚ã‚·ãƒ³ãƒœãƒ«ãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§")
            return None

        if not info.visible:
            logger.info(f"[order_send] {symbol} ãŒéè¡¨ç¤ºãªã®ã§ symbol_select() ã—ã¾ã™")
            if not MT5.symbol_select(symbol, True):
                logger.error(f"[order_send] symbol_select({symbol}, True) ã«å¤±æ•—")
                return None

        # --- 2) æœ€æ–°ãƒ†ã‚£ãƒƒã‚¯ ---
        tick = MT5.symbol_info_tick(symbol)
        if tick is None:
            logger.error(f"[order_send] symbol_info_tick({symbol}) ãŒ Noneã€‚ãƒ†ã‚£ãƒƒã‚¯ãŒå–å¾—ã§ããªã„")
            return None

        # --- 3) æ³¨æ–‡ç¨®åˆ¥ã¨ä¾¡æ ¼ ---
        if order_type == "BUY":
            mt_type = MT5.ORDER_TYPE_BUY
            price = tick.ask
        else:
            mt_type = MT5.ORDER_TYPE_SELL
            price = tick.bid

        # --- 4) æ³¨æ–‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ ---
        request: Dict[str, Any] = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": float(lot),
            "type": mt_type,
            "price": float(price),
            "sl": float(sl) if sl is not None else 0.0,
            "tp": float(tp) if tp is not None else 0.0,
            "magic": 123456,
            "comment": "fxbot_test_order",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        last_error: Optional[tuple[int, str]] = None

        # --- 5) ãƒªãƒˆãƒ©ã‚¤ä»˜ã order_send ---
        for attempt in range(1, retries + 1):
            logger.info(
                f"[order_send] Try {attempt}/{retries}: {order_type} {lot} lot @ {price} {symbol}"
            )

            result = MT5.order_send(request)

            if result is None:
                last_error = MT5.last_error()
                logger.error(f"[order_send] result is None, last_error={last_error}")

            else:
                logger.info(
                    "[order_send] retcode=%s, order=%s, deal=%s, comment=%s",
                    getattr(result, "retcode", None),
                    getattr(result, "order", None),
                    getattr(result, "deal", None),
                    getattr(result, "comment", None),
                )

                # æˆè¡Œãªã®ã§ DONE = æˆåŠŸ
                if result.retcode == MT5.TRADE_RETCODE_DONE:
                    ticket = int(result.order or result.deal or 0)
                    if ticket > 0:
                        logger.info(f"[order_send] æˆåŠŸ: ticket={ticket}")
                        return ticket
                    else:
                        logger.warning(f"[order_send] DONE ã ãŒ ticket ãŒå–å¾—ã§ããªã„: {result}")

                else:
                    logger.warning(
                        f"[order_send] å¤±æ•— retcode={result.retcode}ã€‚å†è©¦è¡Œã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™"
                    )

            if attempt < retries:
                time.sleep(1.0)

        logger.error(f"[order_send] å…¨ {retries} å›ãƒªãƒˆãƒ©ã‚¤ã—ã¦ã‚‚å¤±æ•—ã€‚last_error={last_error}")
        return None

    # ------------------------
    # æ±ºæ¸ˆï¼ˆã‚¯ãƒ­ãƒ¼ã‚ºï¼‰
    # ------------------------
    def close_position(self, ticket: int, symbol: str, retries: int = 3) -> bool:
        \"\"\"æŒ‡å®šãƒã‚±ãƒƒãƒˆã®æˆè¡Œã‚¯ãƒ­ãƒ¼ã‚º\"\"\"

        pos = MT5.positions_get(ticket=ticket)
        if not pos:
            logger.error(f"ticket={ticket} ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return False

        position = pos[0]
        lot = position.volume

        # position.type: 0=BUY, 1=SELL
        order_type = MT5.ORDER_TYPE_SELL if position.type == 0 else MT5.ORDER_TYPE_BUY

        # ã‚¯ãƒ­ãƒ¼ã‚ºä¾¡æ ¼
        t = MT5.symbol_info_tick(symbol)
        if t is None:
            logger.error(f"[close_position] symbol_info_tick({symbol}) ãŒ None")
            return False

        price = t.bid if order_type == MT5.ORDER_TYPE_SELL else t.ask

        request = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": lot,
            "type": order_type,
            "position": ticket,
            "price": price,
            "magic": 123456,
            "comment": "fxbot_test_close",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        for attempt in range(1, retries + 1):
            logger.info(f"[close_position] Try {attempt}: ticket={ticket}")
            result = MT5.order_send(request)

            if result and result.retcode == MT5.TRADE_RETCODE_DONE:
                logger.info(f"ã‚¯ãƒ­ãƒ¼ã‚ºæˆåŠŸ: ticket={ticket}")
                return True

            logger.error(
                f"retcode={result.retcode if result else None}, err={MT5.last_error()}"
            )
            time.sleep(1.0)

        logger.error("[close_position] å…¨ãƒªãƒˆãƒ©ã‚¤å¤±æ•—")
        return False

    # ------------------------
    # ãƒã‚¸ã‚·ãƒ§ãƒ³ä¸€è¦§
    # ------------------------
    def get_positions(self):
        try:
            pos = MT5.positions_get()
            if pos is None:
                self.logger.warning("positions_get() returned None")
                return []
            return list(pos)
        except Exception as exc:
            self.logger.exception(f"positions_get() failed: {exc}")
            return []

    def get_positions_by_symbol(self, symbol: str):
        rows = self.get_positions()
        out = [p for p in rows if getattr(p, "symbol", None) == symbol]
        self.logger.info(f"get_positions_by_symbol: {symbol} count={len(out)}")
        return out

    def get_positions_df(self, symbol: Optional[str] = None):
        rows = self.get_positions()
        if symbol:
            rows = [p for p in rows if getattr(p, "symbol", None) == symbol]

        if not rows:
            return pd.DataFrame(columns=POSITION_COLUMNS)

        data = []
        for p in rows:
            data.append(
                {
                    "ticket": p.ticket,
                    "time": p.time,
                    "time_msc": p.time_msc,
                    "time_update": p.time_update,
                    "time_update_msc": p.time_update_msc,
                    "symbol": p.symbol,
                    "magic": p.magic,
                    "volume": p.volume,
                    "price_open": p.price_open,
                    "sl": p.sl,
                    "tp": p.tp,
                    "price_current": p.price_current,
                    "swap": p.swap,
                    "profit": p.profit,
                    "comment": p.comment,
                }
            )

        return pd.DataFrame(data, columns=POSITION_COLUMNS)
"""


def main() -> None:
    """mt5_client.py ã‚’å®Œå…¨å¾©å…ƒã™ã‚‹ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""
    path = Path("app/core/mt5_client.py")
    path.write_text(NEW_MT5_CLIENT_SOURCE, encoding="utf-8")
    print("mt5_client.py ã‚’ NEW_MT5_CLIENT_SOURCE ã§ä¸Šæ›¸ãã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()



=== file: config/mt5_accounts.json ===

{
  "active_profile": "demo",
  "profiles": {
    "demo": {
      "login": 22004581,
      "password": "eXSIDTUCV4&",
      "server": "GaitameFinest-Demo"
    },
    "real": {
      "login": 3002973,
      "password": "QpNa@w7s",
      "server": "GaitameFinest-LIVE"
    }
  }
}


=== file: configs/config copy.yaml ===

account:
  profile: demo
  server:
    demo: OANDA-Demo
    live: OANDA-Live
  login_env_key: MT5_LOGIN
  password_env_key: MT5_PASSWORD
runtime:
  timezone: Asia/Tokyo
  symbol: USDJPY-
  timeframe_exec: M5
  max_positions: 1
  spread_limit: 0.015
  slip_limit: 0.015
  spread_limit_pips: 0.2
  post_fill_grace_sec: 60
lot:
  base_equity_per_0p01: 10000
  min_lot: 0.01
  max_lot: 1.0
  lot_step: 0.01
risk:
  daily_loss_stop: 0.05
  dd_stop: 0.1
  max_consecutive_losses: 2
entry:
  prob_threshold: 0.56     # step back to production range gradually
  threshold_buy: 0.6
  threshold_sell: 0.6
  side_bias: "auto"
  min_edge: 0.01
exit:
  mode: fixed
  tp_pips: 15
  sl_pips: 10
exits:
  mode: "fixed"        # fixed | atr | none
  fixed:
    tp_pips: 12
    sl_pips: 10
  atr:
    period: 14
    tp_mult: 1.2
    sl_mult: 1.0
    trailing:
      enabled: true
      activate_atr_mult: 0.50
      step_atr_mult: 0.25
      lock_be_atr_mult: 0.30
      hard_floor_pips: 5
      only_in_profit: true
      max_layers: 20
      price_source: "mid"
ai:
  stacking: false
  models:
  - name: lgbm_cls
  features:
    base:
    - ema_5
    - ema_20
    - rsi_14
    - atr_14
    - adx_14
    - bbp
    - vol_chg
    - wick_ratio
  retrain:
    schedule_cron: sat 03:00
filters:
  adx_min: 15.0
  adx_disable: false
  min_atr_pct: 0.0005      # ATR floor 0.05%
  atr_hysteresis:
    enable_min_pct: 0.00055
    disable_min_pct: 0.00045
    lookback: 12
session:
  allow_hours_jst:
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    - 20
    - 21
    - 22
    - 23





=== file: configs/config.local.yaml ===

runtime:
  symbol: "USDJPY-"
  spread_limit_pips: 1.5
  max_positions: 1

risk:
  max_consecutive_losses: 5



=== file: configs/config.yaml ===

account:
  profile: demo
  server:
    demo: OANDA-Demo
    live: OANDA-Live
  login_env_key: MT5_LOGIN
  password_env_key: MT5_PASSWORD
runtime:
  mode: "dryrun"   # "live" ã«ã™ã‚‹ã¨å®Ÿå£²è²·ï¼ˆMT5 OrderModifyï¼‰ãŒæœ‰åŠ¹ã«ãªã‚‹
  enabled: true     #trueã§æœ¬ç•ª
  timezone: Asia/Tokyo
  symbol: USDJPY-
  timeframe_exec: M5
  max_positions: 1
  spread_limit: 0.015
  slip_limit: 0.015
  spread_limit_pips: 0.2
  post_fill_grace_sec: 60
guard:
  reconcile_interval_sec: 15
  desync_fix: true
  inflight_timeout_sec: 20
  max_positions: 1
circuit_breaker:
  enable: true
  max_consecutive_losses: 5
  daily_loss_limit_jpy: 30000
  cooldown_min: 30
lot:
  base_equity_per_0p01: 10000
  min_lot: 0.01
  max_lot: 1.0
  lot_step: 0.01
risk:
  daily_loss_stop: 0.05
  dd_stop: 0.1
  max_consecutive_losses: 2
entry:
  prob_threshold: 0.52     # step back to production range gradually
  threshold_buy: 0.6
  threshold_sell: 0.6
  side_bias: "auto"
  min_edge: 0.01
exit:
  mode: fixed
  tp_pips: 15
  sl_pips: 10
exits:
  mode: "fixed"        # fixed | atr | none
  fixed:
    tp_pips: 12
    sl_pips: 10
  atr:
    period: 14
    tp_mult: 1.2
    sl_mult: 1.0
    trailing:
      enabled: true
      activate_atr_mult: 0.50
      step_atr_mult: 0.25
      lock_be_atr_mult: 0.30
      hard_floor_pips: 5
      only_in_profit: true
      max_layers: 20
      price_source: "mid"
ai:
  recent_kpi_trades: 100   # ç›´è¿‘ä½•ãƒˆãƒ¬ãƒ¼ãƒ‰ã§KPIè¨ˆç®—ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
  stacking: false
  models:
  - name: lgbm_cls
  features:
    base:
    - ema_5
    - ema_20
    - rsi_14
    - atr_14
    - adx_14
    - bbp
    - vol_chg
    - wick_ratio
  retrain:
    schedule_cron: sat 03:00
filters:
  adx_min: 15.0
  adx_disable: false
  min_atr_pct: 0.0002      # ATR floor 0.05%
  atr_hysteresis:
    enable_min_pct: 0.00025
    disable_min_pct: 0.00020
    lookback: 12
session:
  allow_hours_jst: []

training:
  data_dir: "data/usdjpy"         # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿CSVç½®ãå ´ï¼ˆä¾‹ï¼‰
  file_glob: "USDJPY_M5_*.csv"    # å–ã‚Šè¾¼ã‚€CSVã®ãƒ‘ã‚¿ãƒ¼ãƒ³
  model_out_dir: "models"         # æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ç½®ãå ´
  staging_dir: "models/_staging"  # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç½®ãå ´ï¼ˆæ˜‡æ ¼å¾…ã¡ï¼‰
  reports_dir: "logs/retrain"     # å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›å…ˆ
  algo: "lightgbm"                # "lightgbm" / "logreg"
  wf:
    train_days: 5*365             # å­¦ç¿’çª“ï¼šéå»5å¹´
    test_days: 30                 # ç›´è¿‘30æ—¥ã®ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆ
    step_days: 7                  # 1é€±é–“ã”ã¨ã«ç¹°ã‚Šè¿”ã™ï¼ˆä»Šå›ã¯å›ºå®šã§1å›åˆ†ï¼‰
  metrics:
    min_auc: 0.54                 # æ˜‡æ ¼ã®æœ€ä½æ¡ä»¶ï¼ˆä¾‹ï¼‰
    min_f1: 0.50                  # ã€ƒ
  calibrators: ["platt", "isotonic"]
  random_seed: 42

data:
  # è¤‡æ•°PCã§é•ã†ãƒ•ã‚©ãƒ«ãƒ€ã§ã‚‚OKï¼šä¸Šã‹ã‚‰é †ã«æ¢ã—ã¾ã™ï¼ˆç’°å¢ƒå¤‰æ•°ã‚‚å±•é–‹ï¼‰
  search_paths:
    - "${FXBOT_DATA}"                      # ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šï¼ˆæ¨å¥¨ï¼‰
    - "data"                               # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…CSV
    # ä»£è¡¨ä¾‹ï¼šMT5ã®ãƒ’ã‚¹ãƒˆãƒªæ ¼ç´ï¼ˆç«¯æœ«ãƒãƒƒã‚·ãƒ¥ãƒ»ã‚µãƒ¼ãƒåã¯ * ã§ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ï¼‰
    - "%APPDATA%/MetaQuotes/Terminal/*/bases/*/history"
    - "%LOCALAPPDATA%/Programs/MetaTrader 5/bases/*/history"



=== file: core/__init__.py ===




=== file: core/ai/__init__.py ===

# Core AI package initializer.



=== file: core/ai/calibration.py ===

from __future__ import annotations

import pickle
from dataclasses import dataclass
from typing import Any, Literal, Optional, Sequence, Tuple, cast

import numpy as np
from numpy.typing import NDArray

FloatArray = NDArray[np.float64]

from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import log_loss

CalibMethod = Literal["platt", "isotonic"]

# äº’æ›API: æ—¢å­˜ã‚³ãƒ¼ãƒ‰ï¼ˆcore.ai.service ç­‰ï¼‰ã‹ã‚‰å‘¼ã°ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š
def load_calibrator(path: str) -> Any:
    """
    äº’æ›ãƒ­ãƒ¼ãƒ€ãƒ¼ã€‚æ˜”ã®ã‚³ãƒ¼ãƒ‰ãŒæœŸå¾…ã—ã¦ã„ã‚‹ã‚·ã‚°ãƒãƒãƒ£ã«åˆã‚ã›ã‚‹ã€‚
    core.ai.service ã‹ã‚‰ import ã•ã‚Œã‚‹æƒ³å®šã€‚
    """
    with open(path, "rb") as f:
        return pickle.load(f)

def apply_calibration(p: Sequence[float] | FloatArray, calib: Calibrator | None) -> FloatArray:
    """
    ï¿½İŠï¿½ï¿½Kï¿½pï¿½wï¿½ï¿½ï¿½pï¿½Bï¿½mï¿½ï¿½ï¿½zï¿½ï¿½ p ï¿½É‘Î‚ï¿½ï¿½ÄAcalibrator ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ transform ï¿½ï¿½Kï¿½pï¿½B
    calibrator ï¿½ï¿½ None ï¿½È‚ï¿½ p ï¿½ï¿½ï¿½ï¿½ï¿½Ì‚Ü‚Ü•Ô‚ï¿½ï¿½B
    """
    data = cast(FloatArray, np.asarray(p, dtype=float))
    if calib is None:
        return data
    # Calibrator.transform ï¿½ï¿½ 1ï¿½ï¿½ï¿½ï¿½ï¿½mï¿½ï¿½ï¿½xï¿½Nï¿½gï¿½ï¿½ï¿½ï¿½ï¿½ó‚¯ï¿½ï¿½ï¿½ï¿½ 1ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ô‚ï¿½ï¿½İŒv
    return calib.transform(data)


@dataclass
class Calibrator:
    method: CalibMethod
    model: LogisticRegression | IsotonicRegression

    def transform(self, p: FloatArray) -> FloatArray:
        # LogisticRegression ã¯å…¥åŠ› x=logit(p)
        if self.method == "platt":
            eps = 1e-12
            x = np.clip(p, eps, 1 - eps)
            logit = np.log(x / (1 - x)).reshape(-1, 1)
            probs = self.model.predict_proba(logit)[:, 1]
            return cast(FloatArray, np.asarray(probs, dtype=float))
        elif self.method == "isotonic":
            transformed = self.model.transform(p)
            return cast(FloatArray, np.asarray(transformed, dtype=float))
        else:
            raise ValueError(f"unknown method: {self.method}")


def fit_platt(y_valid: np.ndarray, p_valid: np.ndarray) -> Calibrator:
    eps = 1e-12
    x = np.clip(p_valid, eps, 1 - eps)
    logit = np.log(x / (1 - x)).reshape(-1, 1)
    lr = LogisticRegression(solver="liblinear")
    lr.fit(logit, y_valid.astype(int))
    return Calibrator(method="platt", model=lr)


def fit_isotonic(y_valid: np.ndarray, p_valid: np.ndarray) -> Calibrator:
    ir = IsotonicRegression(out_of_bounds="clip")
    ir.fit(p_valid, y_valid.astype(float))
    return Calibrator(method="isotonic", model=ir)


def choose_best_calibrator(
    y_valid: np.ndarray, p_valid: np.ndarray
) -> Tuple[Optional[Calibrator], dict[str, Any]]:
    """
    Compare logloss across available calibration strategies and return the best.
    """
    scores: dict[str, float] = {}
    y_arr = np.asarray(y_valid, dtype=float)
    p_arr = cast(FloatArray, np.asarray(p_valid, dtype=float))
    base_ll = log_loss(y_arr, p_arr, labels=[0, 1])
    scores["none"] = base_ll

    # platt
    try:
        platt = fit_platt(y_arr, p_arr)
        p_platt = platt.transform(p_arr)
        scores["platt"] = log_loss(y_arr, p_platt, labels=[0, 1])
    except Exception:
        scores["platt"] = np.inf
        platt = None

    # isotonic
    try:
        iso = fit_isotonic(y_arr, p_arr)
        p_iso = iso.transform(p_arr)
        scores["isotonic"] = log_loss(y_arr, p_iso, labels=[0, 1])
    except Exception:
        scores["isotonic"] = np.inf
        iso = None

    best = min(scores, key=lambda k: scores[k])
    meta: dict[str, Any] = {"valid_logloss": scores, "baseline": base_ll, "selected": best}

    if best == "none":
        return None, meta
    if best == "platt" and platt is not None and scores["platt"] < base_ll:
        return platt, meta
    if best == "isotonic" and iso is not None and scores["isotonic"] < base_ll:
        return iso, meta
    return None, meta


def save_calibrator(path: str, calib: Calibrator) -> None:
    with open(path, "wb") as f:
        pickle.dump(calib, f)



=== file: core/ai/features.py ===

import numpy as np
import pandas as pd


def _rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = delta.clip(lower=0)
    down = -delta.clip(upper=0)
    roll_up = up.ewm(alpha=1 / period, adjust=False).mean()
    roll_down = down.ewm(alpha=1 / period, adjust=False).mean()
    rs = roll_up / (roll_down + 1e-12)
    rsi = 100 - (100 / (1 + rs))
    return rsi


def _true_range(high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:
    prev_close = close.shift(1)
    tr1 = high - low
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)


def _adx(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    up_arr = np.asarray(high.diff(), dtype=float)
    down_arr = np.asarray(-low.diff(), dtype=float)
    plus_dm = np.where((up_arr > down_arr) & (up_arr > 0), up_arr, 0.0)
    minus_dm = np.where((down_arr > up_arr) & (down_arr > 0), down_arr, 0.0)

    tr = _true_range(high, low, close).astype(float)
    tr_smooth = tr.rolling(period).sum()
    plus_series = pd.Series(plus_dm, index=df.index)
    minus_series = pd.Series(minus_dm, index=df.index)
    plus_di = 100 * plus_series.rolling(period).sum() / (tr_smooth + 1e-12)
    minus_di = 100 * minus_series.rolling(period).sum() / (tr_smooth + 1e-12)

    dx = ((plus_di - minus_di).abs() / ((plus_di + minus_di) + 1e-12)) * 100
    adx = dx.rolling(period).mean()
    return adx


def _bb_percent_b(close: pd.Series, period: int = 20, k: float = 2.0) -> pd.Series:
    ma = close.rolling(period).mean()
    sd = close.rolling(period).std(ddof=0)
    upper = ma + k * sd
    lower = ma - k * sd
    bbp = (close - lower) / ((upper - lower) + 1e-12)
    return bbp.clip(0, 1)


def _wick_body_ratios(df: pd.DataFrame) -> pd.DataFrame:
    open_, high, low, close = df["open"], df["high"], df["low"], df["close"]
    body = (close - open_).abs()
    upper_wick = (high - np.maximum(open_, close)).clip(lower=0)
    lower_wick = (np.minimum(open_, close) - low).clip(lower=0)
    total = (high - low).replace(0, np.nan)
    return pd.DataFrame(
        {
            "upper_wick_ratio": (upper_wick / total).fillna(0),
            "lower_wick_ratio": (lower_wick / total).fillna(0),
            "body_ratio": (body / total).fillna(0),
        }
    )


def _zscore(series: pd.Series, win: int = 20) -> pd.Series:
    mean = series.rolling(win).mean()
    sd = series.rolling(win).std(ddof=0)
    return (series - mean) / (sd + 1e-12)


def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…¥åŠ›: df ã«ã¯ ["open","high","low","close","volume"] ãŒå¿…é ˆ
    å‡ºåŠ›: å…ƒã®OHLCV + è¿½åŠ ç‰¹å¾´åˆ—ï¼ˆNaNã¯dropnaã§æœ€çµ‚çš„ã«è½ã¨ã—ã¦ãã ã•ã„ï¼‰
    """
    out = df.copy()

    for p in (1, 3, 5, 10):
        out[f"ret_{p}"] = df["close"].pct_change(p)

    out["ret_std_10"] = out["ret_1"].rolling(10).std(ddof=0)
    out["ret_std_20"] = out["ret_1"].rolling(20).std(ddof=0)

    out["tr"] = _true_range(df["high"], df["low"], df["close"])
    out["atr_14"] = out["tr"].rolling(14).mean()

    out["rsi_14"] = _rsi(df["close"], 14)
    out["adx_14"] = _adx(df, 14)
    out["bbp_20"] = _bb_percent_b(df["close"], 20, 2.0)

    wick = _wick_body_ratios(df)
    out = pd.concat([out, wick], axis=1)

    out["vol_zscore_20"] = _zscore(df["volume"], 20)

    return out

# --- ã“ã“ã‹ã‚‰è¿½è¨˜ï¼ˆä»»æ„ï¼‰ ---
def build_Xy(df_raw: pd.DataFrame, label_col: str = "label") -> tuple[pd.DataFrame, pd.Series, list[str]]:
    """
    build_features() ã§ä½œã£ãŸç‰¹å¾´ç¾¤ã‹ã‚‰ã€å­¦ç¿’ç”¨ã® X(DF) ã¨ y(Series) ã‚’è¿”ã™ã€‚
    - label_col ã¯ã‚ãªãŸã®æ—¢å­˜ãƒ©ãƒ™ãƒ«åˆ—åã«åˆã‚ã›ã¦å¤‰æ›´
    """
    feat_df = build_features(df_raw)

    # å­¦ç¿’ã«ä½¿ã†åˆ—ã‚’ã“ã“ã§æ˜ç¤ºåŒ–ï¼ˆã‚ãªãŸã®å®Ÿéš›ã®ç‰¹å¾´åˆ—ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
    feature_cols = [
        "open","high","low","close","volume",
        "ret_1","ret_3","ret_5","ret_10",
        "ret_std_10","ret_std_20",
        "tr","atr_14",
        "rsi_14","adx_14","bbp_20",
        "upper_wick_ratio","lower_wick_ratio","body_ratio",
        "vol_zscore_20",
    ]

    # ãƒ©ãƒ™ãƒ«ãŒã¾ã ç„¡ã„å ´åˆã¯å¤–ã§ä½œã£ã¦ã‹ã‚‰æ¸¡ã™æƒ³å®š
    if label_col not in feat_df.columns:
        raise ValueError(f"label col '{label_col}' not in DataFrame. å…ˆã«ãƒ©ãƒ™ãƒ«ä½œæˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")

    # æ¬ æè½ã¨ã—ï¼†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒæœŸ
    X_df = feat_df[feature_cols].copy()
    y_ser = feat_df[label_col].copy()
    mask = ~X_df.isna().any(axis=1)
    X_df = X_df.loc[mask]
    y_ser = y_ser.loc[X_df.index]

    return X_df, y_ser, feature_cols
# --- è¿½è¨˜ã“ã“ã¾ã§ ---



=== file: core/ai/loader.py ===

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, cast, Sequence, Union

import json
import joblib
import numpy as np
import pandas as pd
from numpy.typing import NDArray

FloatArray = NDArray[np.float64]

ArrayLike = Union[np.ndarray, Sequence[float]]

class ModelWrapper:
    """dict/ãƒ©ãƒƒãƒ‘ãƒ¼ã®å¤šæ®µãƒã‚¹ãƒˆã‚’å†å¸°ã§â€œã»ã©ãâ€ã€predict_proba/predictã‚’å®‰å®šæä¾›ã™ã‚‹è–„ã„ãƒ©ãƒƒãƒ‘ãƒ¼ã€‚"""

    def __init__(self, obj_or_path: Union[str, Path, Any]) -> None:
        if isinstance(obj_or_path, (str, Path)):
            loaded = joblib.load(str(obj_or_path))
        else:
            loaded = obj_or_path

        self.base_model = self._unwrap(loaded, depth=0)
        self.classes_ = getattr(self.base_model, "classes_", None)
        self.model_name = getattr(self.base_model, "model_name", None) or \
                          getattr(self.base_model, "__class__", type("X",(object,),{})).__name__
        if isinstance(self.base_model, dict):
            try:
                print(f"[ModelWrapper][warn] still dict after unwrap. keys={list(self.base_model.keys())[:10]}")
            except Exception:
                print("[ModelWrapper][warn] still dict after unwrap (keys unavailable).")

    def _unwrap(self, obj: Any, depth: int = 0) -> Any:
        if depth > 5:
            return obj
        if hasattr(obj, "predict_proba") or hasattr(obj, "predict") or hasattr(obj, "decision_function"):
            return obj
        if isinstance(obj, dict):
            for key in ("model", "estimator", "clf", "base_model", "wrapped", "inner", "object"):
                if key in obj and obj[key] is not None:
                    out = self._unwrap(obj[key], depth + 1)
                    if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                        return out
            for v in obj.values():
                out = self._unwrap(v, depth + 1)
                if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                    return out
        if isinstance(obj, (list, tuple)):
            for v in obj:
                out = self._unwrap(v, depth + 1)
                if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                    return out
        return obj

    def predict_proba(self, X: ArrayLike) -> np.ndarray:
        X_arr = np.asarray(X)
        return self.base_model.predict_proba(X_arr)

    def predict(self, X: ArrayLike) -> np.ndarray:
        X_arr = np.asarray(X)
        if hasattr(self.base_model, "predict"):
            return self.base_model.predict(X_arr)
        if hasattr(self.base_model, "decision_function"):
            scores = np.asarray(self.base_model.decision_function(X_arr), dtype=float)
            probs = 1.0 / (1.0 + np.exp(-scores))
            return (probs >= 0.5).astype(int)
        raise AttributeError("The underlying model has neither predict nor decision_function.")

def _load_pickle_or_joblib(path: str) -> Any:
    return joblib.load(path)

def _apply_calibration(calibrator: Any, p1: FloatArray) -> FloatArray:
    data = np.asarray(p1, dtype=float)
    if calibrator is None:
        return data
    if hasattr(calibrator, "transform"):
        transformed = calibrator.transform(data)
        return np.asarray(transformed, dtype=float)
    if hasattr(calibrator, "predict_proba"):
        proba = calibrator.predict_proba(data)
        return np.asarray(proba, dtype=float)
    return data

# ------------------------------------------------------------
# ãƒ¢ãƒ‡ãƒ«ãƒãƒ³ãƒ‰ãƒ«ï¼ˆå¿…è¦ãªã‚‰ä½¿ã†ã€‚æœªä½¿ç”¨ãªã‚‰æ®‹ã—ã¦ã‚‚å®³ãªã—ï¼‰
# ------------------------------------------------------------
@dataclass
class LGBBundle:
    name: str
    version: str
    clf: object            # predict_proba ã‚’æŒã¤æ¨è«–å™¨
    feature_order: List[str]
    ready: bool = True

# ------------------------------------------------------------
# æ ¡æ­£ä»˜ããƒ©ãƒƒãƒ‘ï¼ˆâ€»å˜ä¸€å®šç¾©ãƒ»predict_probaã‚ã‚Šï¼‰
# ------------------------------------------------------------
class _CalibratedWrapper:
    """Thin wrapper that applies optional calibration to predict_proba outputs."""

    def __init__(self, base_model: Any, calibrator: Any, model_name: str = "(unknown)") -> None:
        self.base_model = base_model
        self.calibrator = calibrator
        self.model_name = model_name
        self.calibrator_name = getattr(calibrator, "method", "none") if calibrator else "none"
        self.expected_features: Optional[list[str]] = None

    def __getattr__(self, item: str) -> Any:
        return getattr(self.base_model, item)

    def predict_proba(self, X: Any) -> FloatArray:
        X_arr = np.asarray(X, dtype=float)
        raw = self.base_model.predict_proba(X_arr)
        raw_arr = np.asarray(raw, dtype=float)
        calibrated = _apply_calibration(self.calibrator, raw_arr)
        return np.asarray(calibrated, dtype=float)

    @property
    def classes_(self) -> NDArray[np.int_]:
        base_classes = getattr(self.base_model, "classes_", None)
        if base_classes is None:
            return np.asarray([0, 1], dtype=int)
        return np.asarray(base_classes, dtype=int)

def _read_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as fh:
        data = json.load(fh)
    if isinstance(data, dict):
        return cast(Dict[str, Any], data)
    raise ValueError(f"JSON at {path} is not an object")

# ------------------------------------------------------------
# active_model.json / æ ¡æ­£ãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æ±º
# ------------------------------------------------------------
def _load_active_meta() -> Dict[str, Any]:
    meta_path = Path("models") / "active_model.json"
    if meta_path.exists():
        try:
            return _read_json(str(meta_path))
        except Exception as exc:
            print(f"[core.ai.loader][warn] active_model.json read failed: {exc}")
    return {}

def _resolve_calib_path(calib_path: str | None) -> Optional[str]:
    return calib_path if calib_path else None

def _maybe_load_calibrator_from_meta(meta: Dict[str, Any]) -> Any:
    cpath = _resolve_calib_path(meta.get("calibrator_path"))
    if cpath:
        try:
            return _load_pickle_or_joblib(cpath)
        except Exception as exc:
            print(f"[core.ai.loader][warn] calibrator load failed: {exc}")
    return None

# ------------------------------------------------------------
# ãƒ‘ãƒ–ãƒªãƒƒã‚¯APIï¼šãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€
# ------------------------------------------------------------
def load_lgb_clf(model_path: str | None = None, *, meta_path: str | None = None) -> Any:
    model_file = model_path or "models/LightGBM_clf.pkl"
    base_model = _load_pickle_or_joblib(model_file)
    # â˜…ã“ã“ã‚’è¿½åŠ ï¼šä¿å­˜ç‰©ãŒ dict/ãƒ©ãƒƒãƒ‘ã§ã‚‚æ¨å®šå™¨æœ¬ä½“ã‚’å–ã‚Šå‡ºã™
    try:
        # ModelWrapper ã‚’ä¸€æ™‚çš„ã«ä½¿ã£ã¦ã€Œä¸­èº«ã®æ¨å®šå™¨ã€ã‚’å–ã‚Šå‡ºã™
        _tmp = ModelWrapper(base_model)        # â† æ—¢ã«åŒãƒ•ã‚¡ã‚¤ãƒ«å†…ã§å®šç¾©æ¸ˆã¿ã‚¯ãƒ©ã‚¹
        base_model = _tmp.base_model           # â† predict_proba ã‚’æŒã¤ã¯ãš
    except Exception:
        # å¤±æ•—ã—ã¦ã‚‚ãã®ã¾ã¾é€²ã‚ã‚‹ï¼ˆã‚ã¨ã§ _CalibratedWrapper ã§ã¾ãŸæ‹¾ã†ï¼‰
        pass
    meta: Dict[str, Any] = {}
    if meta_path and Path(meta_path).is_file():
        try:
            meta = _read_json(meta_path)
        except Exception as exc:
            print(f"[core.ai.loader][warn] meta read failed: {exc}")
    else:
        try:
            meta = _load_active_meta()
        except Exception as exc:
            print(f"[core.ai.loader][warn] active meta read failed: {exc}")
            meta = {}

    if "calibration" in meta and "calibrator_path" not in meta:
        calib_meta = meta.get("calibration") or {}
        meta["calibrator_path"] = calib_meta.get("path")

    calibrator = _maybe_load_calibrator_from_meta(meta or {})
    wrapper = _CalibratedWrapper(base_model, calibrator, model_name=Path(model_file).name)

    expected = meta.get("feature_order") or meta.get("features")
    if expected:
        wrapper.expected_features = list(expected)

    return wrapper

# ------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ------------------------------------------------------------
def build_feature_vector(features: dict, order: list[str]) -> pd.DataFrame:
    row = [features.get(k, 0.0) for k in order]
    return pd.DataFrame([row], columns=order)



=== file: core/ai/service.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional

import glob
import json
import os
from pathlib import Path

import numpy as np
import pandas as pd
from loguru import logger

from core.ai.loader import ModelWrapper, load_lgb_clf


def _read_active_model_path(default: str = "models/LightGBM_clf.pkl") -> str:
    """Resolve model path from active_model.json (target_path -> source_path -> default)."""
    meta_path = Path("models") / "active_model.json"
    if meta_path.exists():
        try:
            with meta_path.open("r", encoding="utf-8") as fh:
                meta = json.load(fh)
            return meta.get("target_path") or meta.get("source_path") or default
        except Exception as exc:
            logger.warning(f"failed to read active_model.json: {exc}")
    return default


def _as_2d_frame(X: Any) -> pd.DataFrame:
    """ä»»æ„Xã‚’LightGBMã«æ¸¡ã›ã‚‹2D DataFrameã«æ•´å½¢ã™ã‚‹ã€‚"""
    if isinstance(X, pd.DataFrame):
        return X.copy()
    if isinstance(X, dict):
        return pd.DataFrame([X])
    if isinstance(X, (list, tuple, np.ndarray)):
        arr = np.asarray(X, dtype=float)
        if arr.ndim == 1:
            arr = arr.reshape(1, -1)
        return pd.DataFrame(arr)
    return pd.DataFrame([[X]])

class _ProbOut:
    __slots__ = ("p_buy", "p_sell", "p_skip", "meta", "model_name", "version", "features_hash")

    def __init__(
        self,
        p_buy: float,
        p_sell: float,
        *,
        model_name: str = "(unknown)",
        version: str = "na",
        features_hash: str = "",
    ) -> None:
        buy_val = float(p_buy)
        sell_val = float(p_sell)
        self.p_buy = buy_val
        self.p_sell = sell_val
        self.p_skip = float(min(buy_val, sell_val))
        self.meta = "BUY" if buy_val >= sell_val else "SELL"
        self.model_name = model_name
        self.version = version
        self.features_hash = features_hash

    def model_dump(self) -> Dict[str, float | str]:
        return {
            "model_name": self.model_name,
            "version": self.version,
            "features_hash": self.features_hash,
            "p_buy": self.p_buy,
            "p_sell": self.p_sell,
            "p_skip": self.p_skip,
            "meta": self.meta,
        }


ProbOut = _ProbOut  # backward compatibility for external imports

__all__ = ["AISvc", "ProbOut", "_as_2d_frame", "_read_active_model_path"]


@dataclass
class AISvc:
    threshold: float = 0.52
    model: Optional[object] = None
    model_name: str = "unknown"
    calibrator_name: str = "none"
    is_dummy: bool = False
    expected_features: Optional[list[str]] = None

    def __post_init__(self) -> None:
        self._initialize_model()

    def _resolve_model_path(self) -> str:
        """
        å„ªå…ˆé †ï¼š
          1) self.model_pathï¼ˆã‚ã‚Œã°ï¼‰
          2) runtime/active_model.json ã® "path"
          3) æ—¢å®š: models/LightGBM_clf.pkl
        """
        if getattr(self, "model_path", None):
            return str(self.model_path)

        try:
            p = Path("runtime/active_model.json")
            if p.exists():
                d = json.loads(p.read_text(encoding="utf-8"))
                m = d.get("path")
                if m:
                    return str(m)
        except Exception:
            pass

        return "models/LightGBM_clf.pkl"

    def _initialize_model(self, model_path: str | None = None):
        from core.ai.loader import ModelWrapper
        from pathlib import Path

        model_path = model_path or self._resolve_model_path()

        # ä½•ãŒè¿”ã£ã¦ã‚‚æœ€çµ‚çš„ã«æ¨å®šå™¨ã¸åˆ°é”ã§ãã‚‹ã‚ˆã† ModelWrapper ã§çµ±ä¸€
        bundle = load_lgb_clf(model_path)
        self.model = ModelWrapper(bundle)

        # è¡¨ç¤ºå
        self.model_name = getattr(self.model, "model_name", None) or Path(model_path).name or "(unknown)"
        print(f"[AISvc] loaded model: {self.model_name}")

        # æœŸå¾…ç‰¹å¾´é‡ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        self._load_expected_features()

    def _load_expected_features(self) -> None:
        """æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆã® features ã‚’ expected_features ã¨ã—ã¦ä¿æŒ"""
        try:
            meta_path = os.path.join("models", "active_model.json")
            report = None
            if os.path.isfile(meta_path):
                with open(meta_path, encoding="utf-8") as f:
                    meta = json.load(f)
                report = meta.get("best_threshold_source_report")
            if not report or not os.path.isfile(report or ""):
                candidates = sorted(glob.glob(os.path.join("logs", "retrain", "report_*.json")))
                if candidates:
                    report = candidates[-1]
            if report and os.path.isfile(report):
                with open(report, encoding="utf-8") as f:
                    data = json.load(f)
                feats = data.get("features")
                if isinstance(feats, list) and feats:
                    self.expected_features = feats
                    print(f"[AISvc] expected_features loaded ({len(feats)} cols) from {report}")
                    return
        except Exception as exc:
            print(f"[AISvc][warn] expected_features load failed: {exc}")
        self.expected_features = None

    def predict(self, X: Any) -> _ProbOut:
        if self.model is None:
            return _ProbOut(0.5, 0.5, model_name=self.model_name, version="na", features_hash="")

        df = _as_2d_frame(X)
        if self.expected_features:
            for col in self.expected_features:
                if col not in df.columns:
                    df[col] = 0.0
            df = df[self.expected_features]

        values = df.to_numpy(dtype=float, copy=False)
        if hasattr(self.model, "predict_proba"):
            probs = np.asarray(self.model.predict_proba(values), dtype=float)
        elif hasattr(self.model, "predict"):
            preds = np.asarray(self.model.predict(values), dtype=float).reshape(-1, 1)
            probs = np.column_stack([1.0 - preds, preds])
        else:
            probs = np.zeros((len(values), 2), dtype=float)

        features_hash = ""
        try:
            if not df.empty:
                features_hash = str(hash(tuple(df.iloc[0].astype(float).values.tolist())))
        except Exception:
            features_hash = ""

        if probs.ndim == 2:
            p_buy = probs[0, 1]
            p_sell = probs[0, 0]
        else:
            p_buy = float(probs)
            p_sell = 1.0 - p_buy

        version = getattr(getattr(self.model, "clf", self.model), "version", "na")
        return _ProbOut(
            p_buy,
            p_sell,
            model_name=self.model_name,
            version=str(version),
            features_hash=features_hash,
        )



=== file: core/config.py ===

from __future__ import annotations

from functools import lru_cache
from typing import Any, Dict

from app.core.config_loader import load_config


@lru_cache(maxsize=1)
def _load() -> Dict[str, Any]:
    return load_config()


cfg: Dict[str, Any] = _load()


def reload() -> Dict[str, Any]:
    """
    Reload configuration from disk and update cached reference.
    """
    global cfg
    cfg = load_config()
    _load.cache_clear()
    return cfg



=== file: core/indicators.py ===

import math
from typing import Sequence


def true_range(h: float, l: float, prev_close: float) -> float:
    """Return Wilder's true range for a single bar."""
    return max(h - l, abs(h - prev_close), abs(prev_close - l))


def atr(highs: Sequence[float], lows: Sequence[float], closes: Sequence[float], period: int) -> float:
    """Compute a simple average true range over the trailing window."""
    n = len(closes)
    if n < period + 1:
        return math.nan

    trs = []
    for i in range(n - period, n):
        trs.append(true_range(highs[i], lows[i], closes[i - 1]))
    return sum(trs) / len(trs)



=== file: core/metrics.py ===

# core/metrics.py
from __future__ import annotations
from dataclasses import dataclass, field
from threading import Lock
from typing import Any, Dict
import time, os, json, tempfile

RUNTIME_DIR = os.path.join(os.getcwd(), "runtime")
METRICS_JSON = os.path.join(RUNTIME_DIR, "metrics.json")

def _atomic_write_json(path: str, obj: dict):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    fd, tmp = tempfile.mkstemp(prefix="metrics_", suffix=".json", dir=os.path.dirname(path))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, separators=(",", ":"))
        os.replace(tmp, path)
    except Exception:
        try:
            os.remove(tmp)
        except Exception:
            pass

@dataclass
class _MetricsStore:
    _lock: Lock = field(default_factory=Lock)
    _kv: Dict[str, Any] = field(default_factory=dict)

    def set(self, **kwargs):
        with self._lock:
            self._kv.update(kwargs)
            self._kv["ts"] = time.time()
            _atomic_write_json(METRICS_JSON, self._kv)

    def inc(self, key: str, by: int = 1):
        with self._lock:
            self._kv[key] = int(self._kv.get(key, 0)) + by
            self._kv["ts"] = time.time()
            _atomic_write_json(METRICS_JSON, self._kv)

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._kv)

METRICS = _MetricsStore()



=== file: core/metrics/__init__.py ===

# core/metrics/__init__.py
from .registry import METRICS_JSON, METRICS



=== file: core/metrics/fi_extractor.py ===

from __future__ import annotations
from typing import Any, Dict, Iterable, List, Optional, cast

import numpy as np
import pandas as pd
from numpy.typing import NDArray


def _norm_importance(vals: Iterable[float]) -> List[float]:
    arr: NDArray[np.float64] = np.asarray(list(vals), dtype=float)
    if arr.size == 0:
        return []
    total = float(arr.sum())
    if total == 0.0:
        zero_dist = (np.ones_like(arr, dtype=float) / len(arr) * 100.0).astype(float).tolist()
        return cast(List[float], zero_dist)
    normalized = (arr / total * 100.0).astype(float).tolist()
    return cast(List[float], normalized)


def _lgbm_importance(model: Any, method: str = "gain") -> Optional[pd.DataFrame]:
    # LightGBM sklearn API or Booster ã‚’æƒ³å®š
    # method: "gain" or "split"
    booster = None
    feature_names = None
    if hasattr(model, "booster_"):
        booster = model.booster_
    elif hasattr(model, "booster"):
        booster = model.booster()
    elif hasattr(model, "model"):
        booster = getattr(model.model, "booster_", None)

    if booster is None:
        # fallback: sklearn APIã® feature_importances_ï¼ˆ=splitç›¸å½“ãŒå¤šã„ï¼‰
        if hasattr(model, "feature_importances_") and hasattr(model, "feature_name_"):
            vals = list(map(float, getattr(model, "feature_importances_")))
            feature_names = list(getattr(model, "feature_name_"))
            imp = _norm_importance(vals)
            return pd.DataFrame({"feature": feature_names, "importance": imp})
        return None

    # Booster ã‹ã‚‰
    try:
        vals = booster.feature_importance(importance_type=method)
        feature_names = booster.feature_name()
        imp = _norm_importance(vals)
        return pd.DataFrame({"feature": feature_names, "importance": imp})
    except Exception:
        return None


def _xgb_importance(model: Any, method: str = "gain") -> Optional[pd.DataFrame]:
    # XGBoostï¼šbooster.get_score(importance_type=method) -> dict {feat: score}
    booster = None
    if hasattr(model, "get_booster"):
        booster = model.get_booster()
    elif hasattr(model, "booster"):
        booster = model.booster
    if booster is None:
        # fallback: sklearn APIã® feature_importances_ï¼ˆä»•æ§˜ä¸Šgainç›¸å½“ã§ã¯ãªã„æ™‚ã‚‚ã‚ã‚‹ï¼‰
        if hasattr(model, "feature_importances_"):
            vals = list(map(float, getattr(model, "feature_importances_")))
            # XGBã®sklearnãƒ©ãƒƒãƒ‘ã¯ feature_names_in_ ã‚’æŒã¤
            feats = getattr(model, "feature_names_in_", None)
            if feats is None:
                feats = [f"f{i}" for i in range(len(vals))]
            imp = _norm_importance(vals)
            return pd.DataFrame({"feature": feats, "importance": imp})
        return None

    try:
        score: Dict[str, float] = booster.get_score(importance_type=method)
        if not score:
            return None
        feats = list(score.keys())
        vals = list(score.values())
        imp = _norm_importance(vals)
        return pd.DataFrame({"feature": feats, "importance": imp})
    except Exception:
        return None


def extract_feature_importance(
    models: Dict[str, Any],
    method: str = "gain",
    top_n: int = 30,
) -> pd.DataFrame:
    """
    models: {"lgbm_cls": model_obj, "xgb_cls": model_obj, ...}
    method: "gain" | "split"ï¼ˆLightGBM/XGBoostä¸¡å¯¾å¿œã€‚XGBã¯"weight"=splitç›¸å½“ï¼‰
    top_n : ä¸Šä½Nã®ã¿è¿”ã™ï¼ˆãƒ¢ãƒ‡ãƒ«ã”ã¨ã«top_nã‚’æŠ½å‡ºâ†’ç¸¦çµåˆï¼‰

    return columns: ["feature","importance","model","method"]
    importanceã¯å„ãƒ¢ãƒ‡ãƒ«å†…ã§æ­£è¦åŒ–ï¼ˆåˆè¨ˆ=100ï¼‰å¾Œã€top_næŠ½å‡ºã€‚
    """
    frames: List[pd.DataFrame] = []
    for name, m in models.items():
        df = None
        # åˆ¤åˆ¥ã–ã£ãã‚Šï¼šæ–‡å­—åˆ—ã«"lightgbm" or "xgboost"ãŒå«ã¾ã‚Œã‚‹ã‹ã€å±æ€§ã§åˆ¤å®š
        module_name = type(m).__module__.lower()
        if "lightgbm" in module_name:
            df = _lgbm_importance(m, method=method)
        elif "xgboost" in module_name:
            # XGBoostã®"split"ç›¸å½“ã¯importance_type="weight"
            xgb_method = method
            if method == "split":
                xgb_method = "weight"
            df = _xgb_importance(m, method=xgb_method)
        else:
            # æœ€å¾Œã®æ‰‹æ®µï¼šfeature_importances_ãŒã‚ã‚Œã°ä½¿ã†
            if hasattr(m, "feature_importances_"):
                vals = list(map(float, getattr(m, "feature_importances_")))
                feats = getattr(m, "feature_names_in_", None)
                if feats is None:
                    feats = [f"f{i}" for i in range(len(vals))]
                imp = _norm_importance(vals)
                df = pd.DataFrame({"feature": feats, "importance": imp})

        if df is None or df.empty:
            continue

        df = df.sort_values("importance", ascending=False).head(top_n).copy()
        df["model"] = name
        df["method"] = method
        frames.append(df)

    if not frames:
        return pd.DataFrame(columns=["feature", "importance", "model", "method"])

    out = pd.concat(frames, axis=0, ignore_index=True)
    # è¡¨ç¤ºå®‰å®šåŒ–ã®ãŸã‚importanceã‚’ä¸¸ã‚ã‚‹
    out["importance"] = out["importance"].round(2)
    return out.sort_values(["model", "importance"], ascending=[True, False]).reset_index(drop=True)



=== file: core/metrics/registry.py ===

# core/metrics/registry.py
from __future__ import annotations
from pathlib import Path
import json
import threading
from typing import Any, Dict

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ .../fxbot
ROOT = Path(__file__).resolve().parents[2]

# ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å‡ºåŠ›: .../fxbot/runtime/metrics.json
_RUNTIME_DIR = ROOT / "runtime"
_RUNTIME_DIR.mkdir(parents=True, exist_ok=True)
METRICS_JSON = str(_RUNTIME_DIR / "metrics.json")  # GUIå´ãŒ open(METRICS_JSON) ã™ã‚‹å‰æ

class _MetricsKV:
    """
    ç°¡æ˜“KVS: GUIå´ãŒãƒ•ã‚¡ã‚¤ãƒ«èª­ã‚ãªã‹ã£ãŸã¨ãã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
    trade_service ç­‰ãŒåŒãƒ—ãƒ­ã‚»ã‚¹å†…ã§ set()/update() ã‚’å‘¼ã¹ã° get() ã§è¿”ã‚‹ã€‚
    """
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self._kv: Dict[str, Any] = {}

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._kv)

    def set(self, kv: Dict[str, Any]) -> None:
        with self._lock:
            self._kv.update(kv)

    def update(self, **kwargs: Any) -> None:
        with self._lock:
            self._kv.update(kwargs)

# å¤–éƒ¨å…¬é–‹
METRICS = _MetricsKV()



=== file: core/position_guard.py ===

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import Dict, Optional

try:
    import MetaTrader5 as mt5
except Exception:  # pragma: no cover - MT5 unavailable in dryrun/tests
    mt5 = None


@dataclass
class PositionGuardState:
    inflight_orders: Dict[str, float] = field(default_factory=dict)
    last_reconcile_ts: float = 0.0
    open_count: int = 0
    last_fix_reason: Optional[str] = None


class PositionGuard:
    """
    Track live positions/in-flight orders and reconcile with broker state.
    The guard focuses on aggregate count (per account) to keep logic simple.
    """

    def __init__(self, max_positions: int = 1, inflight_timeout_sec: int = 20):
        self.max_positions = max_positions
        self.inflight_timeout_sec = inflight_timeout_sec
        self.state = PositionGuardState()

    # === public API ===

    def mark_inflight(self, order_id: str) -> None:
        """Mark an order as in-flight (before send)."""
        self.state.inflight_orders[str(order_id)] = time.time()

    def clear_inflight(self, order_id: str) -> None:
        """Clear an order from in-flight tracking."""
        self.state.inflight_orders.pop(str(order_id), None)

    def can_open(self) -> bool:
        """Return True when within allowed max positions (after GC)."""
        self._gc_inflight()
        return self.state.open_count < self.max_positions

    def reset(self) -> None:
        """Reset guard state."""
        self.state = PositionGuardState()

    def reconcile_with_broker(self, symbol: Optional[str], desync_fix: bool = True) -> None:
        """
        Sync the local open count with broker positions.
        In dryrun (no MT5), it sticks with current open_count.
        """
        now = time.time()
        self.state.last_reconcile_ts = now
        count = self.state.open_count
        try:
            if mt5 is not None:
                if symbol:
                    poss = mt5.positions_get(symbol=symbol) or []
                else:
                    poss = mt5.positions_get() or []
                count = len(poss)
        except Exception:
            return

        if count != self.state.open_count:
            reason = f"desync(open_count={self.state.open_count} -> {count})"
            self.state.last_fix_reason = reason
            if desync_fix:
                self.state.open_count = count
            self._gc_inflight()

    # === helpers ===

    def _gc_inflight(self) -> None:
        now = time.time()
        dead = [k for k, ts in list(self.state.inflight_orders.items()) if now - ts > self.inflight_timeout_sec]
        for k in dead:
            self.state.inflight_orders.pop(k, None)


# ----------------------------------------------------------------------
# Backwards-compatible procedural helpers (legacy call sites still expect these)
# ----------------------------------------------------------------------
_DEFAULT_GUARD = PositionGuard()


def get_default_guard() -> PositionGuard:
    return _DEFAULT_GUARD


def can_open_new(symbol: Optional[str], max_positions: int) -> bool:
    guard = get_default_guard()
    if guard.max_positions != max_positions:
        guard.max_positions = max_positions
    guard._gc_inflight()
    return guard.can_open()


def mark_inflight(symbol: Optional[str], flag: bool) -> None:
    guard = get_default_guard()
    key = symbol or "GLOBAL"
    if flag:
        guard.mark_inflight(key)
    else:
        guard.clear_inflight(key)


def reset() -> None:
    get_default_guard().reset()


def on_order_rejected_or_canceled(symbol: Optional[str] = None, ticket: Optional[int] = None) -> None:
    mark_inflight(symbol, False)



=== file: core/risk.py ===

# core/risk.py
from __future__ import annotations

from dataclasses import dataclass
from math import sqrt
from typing import Optional


@dataclass
class LotSizingResult:
    """
    target_monthly_return / max_monthly_dd ã¨ ATR ã‚¹ãƒˆãƒƒãƒ—ã‹ã‚‰
    ã€Œæ¨å¥¨ãƒ­ãƒƒãƒˆã€ã¨ã€Œæœˆé–“æƒ³å®šãƒœãƒ©ã€ã‚’è¨ˆç®—ã—ãŸçµæœã‚’ã¾ã¨ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚
    """

    lot: float  # å®Ÿéš›ã«ä½¿ã†ãƒ­ãƒƒãƒˆï¼ˆmin/max ã§ã‚¯ãƒ©ãƒ³ãƒ—æ¸ˆã¿ï¼‰
    per_trade_risk_pct: float  # 1ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ãŸã‚Šã®ãƒªã‚¹ã‚¯ï¼ˆï¼…ï¼‰
    est_monthly_volatility_pct: float  # æœˆé–“æƒ³å®šãƒœãƒ©ï¼ˆã–ã£ãã‚Šæ¨™æº–åå·®ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼…ï¼‰
    est_max_monthly_dd_pct: float  # æƒ³å®šæœ€å¤§DDï¼ˆï¼…ã€max_monthly_dd ã«è¿‘ã„å€¤ã«ãªã‚‹ã‚ˆã†è¨­è¨ˆï¼‰


def compute_lot_size_from_atr(
    *,
    equity: float,
    atr: float,
    atr_mult_sl: float,
    target_monthly_return: float,
    max_monthly_dd: float,
    tick_value: float,
    tick_size: float,
    expected_trades_per_month: int = 40,
    worst_case_trades_for_dd: int = 10,
    avg_r_multiple: float = 0.6,
    min_lot: float = 0.01,
    max_lot: float = 1.0,
) -> LotSizingResult:
    """
    target_monthly_return / max_monthly_dd ã¨ ATR ã‚¹ãƒˆãƒƒãƒ—ã‹ã‚‰
    è‡ªå‹•ãƒ­ãƒƒãƒˆã‚’è¨ˆç®—ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚

    Parameters
    ----------
    equity:
        ç¾åœ¨ã®å£åº§æ®‹é«˜ or æœ‰åŠ¹è¨¼æ‹ é‡‘ï¼ˆå£åº§é€šè²¨ï¼‰ã€‚
    atr:
        ATR å€¤ï¼ˆä¾¡æ ¼å˜ä½ï¼‰ã€‚ä¾‹: USDJPY ãªã‚‰ 0.25 ãªã©ã€‚
    atr_mult_sl:
        ã‚¹ãƒˆãƒƒãƒ—å¹…ã®ä¿‚æ•°ã€‚SL è·é›¢ = atr_mult_sl * atrã€‚
    target_monthly_return:
        ç›®æ¨™æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ (ä¾‹: 0.03)ã€‚
    max_monthly_dd:
        è¨±å®¹æœ€å¤§æœˆæ¬¡ DD (ä¾‹: -0.20)ã€‚ç¬¦å·ä»˜ãã§ã‚‚ã‚ˆã„ãŒçµ¶å¯¾å€¤ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    tick_value:
        1ãƒ†ã‚£ãƒƒã‚¯å‹•ã„ãŸã¨ãã®æç›Šï¼ˆ1 ãƒ­ãƒƒãƒˆã‚ãŸã‚Šã€å£åº§é€šè²¨ï¼‰ã€‚
        MT5 ã® symbol_info(...).trade_tick_value ã‚’æƒ³å®šã€‚
    tick_size:
        1ãƒ†ã‚£ãƒƒã‚¯ã®ä¾¡æ ¼å¹…ã€‚symbol_info(...).trade_tick_size ã‚’æƒ³å®šã€‚
    expected_trades_per_month:
        æœˆã‚ãŸã‚Šæƒ³å®šãƒˆãƒ¬ãƒ¼ãƒ‰å›æ•°ã€‚
    worst_case_trades_for_dd:
        ã€Œã“ã®å›æ•°é€£ç¶šã§è² ã‘ãŸã‚‰ max_dd ã«é”ã™ã‚‹ã€ã¨ã¿ãªã™å›æ•°ã€‚
    avg_r_multiple:
        1ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ãŸã‚Šã®å¹³å‡ Rï¼ˆãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰ï¼‰ã€‚0.6 ãªã©ã®çµŒé¨“å€¤ã€‚
    min_lot, max_lot:
        ãƒ­ãƒƒãƒˆã®ä¸‹é™ãƒ»ä¸Šé™ï¼ˆãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼ä»•æ§˜ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰ã€‚

    Returns
    -------
    LotSizingResult
    """

    if equity <= 0:
        raise ValueError("equity ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    if atr <= 0:
        raise ValueError("atr ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    if atr_mult_sl <= 0:
        raise ValueError("atr_mult_sl ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    if tick_value <= 0 or tick_size <= 0:
        raise ValueError("tick_value / tick_size ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    if expected_trades_per_month <= 0:
        raise ValueError("expected_trades_per_month ã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    if worst_case_trades_for_dd <= 0:
        raise ValueError("worst_case_trades_for_dd ã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
    if avg_r_multiple <= 0:
        raise ValueError("avg_r_multiple ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")

    # DD ã¯çµ¶å¯¾å€¤ã‚’ä½¿ã†ï¼ˆä»•æ§˜æ›¸ã§ã¯ -0.20 ãªã©ã«ãªã£ã¦ã„ã‚‹æƒ³å®šï¼‰
    max_dd_abs = abs(max_monthly_dd)

    # --- 1. DD åˆ¶ç´„ã‹ã‚‰è¦‹ãŸ 1ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ãŸã‚Šè¨±å®¹ãƒªã‚¹ã‚¯ ---
    risk_from_dd = max_dd_abs / float(worst_case_trades_for_dd)

    # --- 2. ç›®æ¨™ãƒªã‚¿ãƒ¼ãƒ³ã‹ã‚‰è¦‹ãŸ 1ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ãŸã‚Šå¿…è¦ãƒªã‚¹ã‚¯ ---
    expected_return_per_trade = target_monthly_return / float(expected_trades_per_month)
    risk_from_return = expected_return_per_trade / avg_r_multiple

    # --- 3. å®Ÿéš›ã«ä½¿ã† 1ãƒˆãƒ¬ãƒ¼ãƒ‰ã®ãƒªã‚¹ã‚¯ï¼… ---
    # å®‰å…¨å´ã«æŒ¯ã‚‹ãŸã‚ã€DD åˆ¶ç´„ã¨ 3% ç›®æ¨™ã®ã†ã¡ã€Œå°ã•ã„æ–¹ã€ã‚’æ¡ç”¨ã™ã‚‹ã€‚
    per_trade_risk_pct = min(risk_from_dd, risk_from_return)

    # å¿µã®ãŸã‚ã€æ¥µç«¯ãªå€¤ã‚’ã‚¯ãƒ©ãƒ³ãƒ—ï¼ˆ0.01%ã€œ10% ã®ç¯„å›²ã«åã‚ã‚‹ï¼‰
    per_trade_risk_pct = max(0.0001, min(per_trade_risk_pct, 0.10))

    # --- 4. ATR ã‚¹ãƒˆãƒƒãƒ—ã‹ã‚‰ 1ãƒ­ãƒƒãƒˆã‚ãŸã‚Šã®æå¤±é¡ã‚’è¨ˆç®— ---
    # 1ãƒã‚¤ãƒ³ãƒˆã‚ãŸã‚Šæç›Šï¼ˆ1 ãƒ­ãƒƒãƒˆï¼‰
    value_per_point_per_lot = tick_value / tick_size

    # ã‚¹ãƒˆãƒƒãƒ—ã¾ã§ã®ãƒã‚¤ãƒ³ãƒˆæ•°
    sl_points = (atr_mult_sl * atr) / tick_size

    # 1ãƒ­ãƒƒãƒˆã‚ãŸã‚Šã®æå¤±é¡ï¼ˆå£åº§é€šè²¨ï¼‰
    risk_per_lot = sl_points * value_per_point_per_lot

    if risk_per_lot <= 0:
        raise ValueError("risk_per_lot ãŒ 0 ä»¥ä¸‹ã§ã™ã€‚tick_value / tick_size / atr ã®æŒ‡å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # --- 5. å£åº§æ®‹é«˜ã‹ã‚‰ãƒ­ãƒƒãƒˆã‚’é€†ç®— ---
    risk_per_trade_money = equity * per_trade_risk_pct
    raw_lot = risk_per_trade_money / risk_per_lot

    # ãƒ­ãƒƒãƒˆã‚’ã‚¯ãƒ©ãƒ³ãƒ—ï¼ˆ0.01ã€œmax_lotï¼‰
    lot = max(min_lot, min(raw_lot, max_lot))

    # --- 6. æœˆé–“æƒ³å®šãƒœãƒ©ã¨ DD ã®ã–ã£ãã‚Šæ¨å®š ---
    # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã£ã½ã„è¿‘ä¼¼ã§ã€ã€Œæ¨™æº–åå·® ~ sqrt(N) * per_trade_risk_pctã€ã¨ç½®ãã€‚
    est_monthly_volatility_pct = per_trade_risk_pct * sqrt(float(expected_trades_per_month))

    # ã€Œworst_case_trades_for_dd å›è² ã‘ãŸã‚‰ã“ã® DDã€ã¨ã¿ãªã™ã€‚
    est_max_monthly_dd_pct = per_trade_risk_pct * float(worst_case_trades_for_dd)

    return LotSizingResult(
        lot=lot,
        per_trade_risk_pct=per_trade_risk_pct,
        est_monthly_volatility_pct=est_monthly_volatility_pct,
        est_max_monthly_dd_pct=est_max_monthly_dd_pct,
    )



=== file: core/utils/__init__.py ===

# Core utils package initializer.



=== file: core/utils/clock.py ===

from __future__ import annotations

from datetime import datetime, timedelta, timezone

JST = timezone(timedelta(hours=9), name="Asia/Tokyo")


def now_jst() -> datetime:
    """Return current datetime in JST (timezone-aware)."""
    return datetime.now(JST)



=== file: core/utils/hashing.py ===

# core/utils/hashing.py
from __future__ import annotations

import hashlib
import json
from typing import Any, Mapping, Sequence


def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def hash_features(features: Mapping[str, Any], order: Sequence[str] | None = None) -> str:
    """
    ç‰¹å¾´é‡è¾æ›¸ã‚’å®‰å®šãƒãƒƒã‚·ãƒ¥åŒ–ã™ã‚‹ã€‚
    order ãŒä¸ãˆã‚‰ã‚ŒãŸã‚‰ãã®é †ã€ç„¡ã‘ã‚Œã°ã‚­ãƒ¼ã§ã‚½ãƒ¼ãƒˆã€‚
    å€¤ã¯ JSON ã«ã—ã¦ã‹ã‚‰ sha256ã€‚
    """
    if order is None:
        order = sorted(features.keys())
    snapshot = {key: features.get(key, None) for key in order}
    payload = json.dumps(snapshot, ensure_ascii=False, sort_keys=True, separators=(",", ":"))
    return sha256_text(payload)



=== file: core/utils/runtime.py ===

from __future__ import annotations

import os
from functools import lru_cache


@lru_cache(maxsize=1)
def is_live() -> bool:
    """
    Returns True when the environment variable FXBOT_RUNTIME indicates live mode.
    """
    return os.environ.get("FXBOT_RUNTIME", "").strip().lower() in {"live", "prod", "production"}



=== file: core/utils/timeutil.py ===

# core/utils/timeutil.py
from __future__ import annotations

from datetime import datetime, timedelta, timezone

JST = timezone(timedelta(hours=9), name="Asia/Tokyo")


def now_jst_iso() -> str:
    return datetime.now(JST).isoformat(timespec="seconds")



=== file: fxbot_path.py ===

from __future__ import annotations

import os
from pathlib import Path


def get_project_root() -> Path:
    """
    fxbot ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¿”ã™ã€‚
    ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«è‡ªèº«ï¼ˆfxbot_path.pyï¼‰ãŒç½®ã‹ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ«ãƒ¼ãƒˆã¨ã¿ãªã™ã€‚
    ä¾‹:
      C:\\Users\\macht\\OneDrive\\fxbot
      D:\\macht\\OneDrive\\fxbot
      C:\\fxbot
    """
    return Path(__file__).resolve().parent


def get_data_root(cli_data_dir: str | os.PathLike | None = None) -> Path:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å€™è£œã‚’è¤‡æ•°è©¦ã—ã¦ã€æœ€åˆã«å­˜åœ¨ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¡ç”¨ã™ã‚‹ã€‚
    å„ªå…ˆé †ä½:
      1) --data-dir å¼•æ•°ã§æ˜ç¤ºã•ã‚ŒãŸãƒ‘ã‚¹
      2) ç’°å¢ƒå¤‰æ•° FXBOT_DATA
      3) ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆé…ä¸‹ã® data/
      4) ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã® data/
    ã©ã‚Œã‚‚å­˜åœ¨ã—ãªã„å ´åˆã¯ã€æœ€å¾Œã« project_root/data ã‚’è¿”ã™ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ã«ã¯ä½¿ãˆã‚‹ï¼‰ã€‚
    """
    candidates: list[Path] = []

    # 1) CLI å¼•æ•°
    if cli_data_dir:
        candidates.append(Path(cli_data_dir))

    # 2) ç’°å¢ƒå¤‰æ•°
    env_dir = os.getenv("FXBOT_DATA")
    if env_dir:
        candidates.append(Path(env_dir))

    # 3) ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã® data
    root = get_project_root()
    candidates.append(root / "data")

    # 4) ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® data
    candidates.append(Path.cwd() / "data")

    # å®Ÿåœ¨ã™ã‚‹ã‚‚ã®ã®ã†ã¡å…ˆé ­
    for p in candidates:
        try:
            if p.is_dir():
                return p.resolve()
        except Exception:
            # ãƒ‘ã‚¹å‚ç…§ã§å•é¡ŒãŒèµ·ããŸå ´åˆã¯æ¬¡ã¸
            continue

    # å…¨æ»…ãªã‚‰ project_root/data ã‚’ã¨ã‚Šã‚ãˆãšè¿”ã™ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã§ã‚‚ä½œæˆã•ã‚Œã†ã‚‹ï¼‰
    return (root / "data").resolve()


def _ensure_dir(p: Path) -> Path:
    """
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„ãªã‚‰ä½œæˆã—ã¦ Path ã‚’è¿”ã™ã€‚
    Windows/OneDrive ç­‰ã§ãƒ‘ã‚¹ãŒç‰¹æ®Šã§ã‚‚ä¾‹å¤–ã‚’ä¸Šã’ãšã«æ¸ˆã‚€ã‚ˆã†ã«ã™ã‚‹ã€‚
    """
    try:
        p.mkdir(parents=True, exist_ok=True)
    except Exception:
        # ä½œæˆã§ããªã„å ´åˆã§ã‚‚ Path ã‚’è¿”ã™ï¼ˆå‘¼ã³å‡ºã—å´ã§ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
        pass
    return p


def get_file_tag(symbol: str) -> str:
    """
    CSV ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ã†æ¥å°¾è¾ãªã—ã‚¿ã‚°ã‚’æ±ºå®šã™ã‚‹ï¼ˆè‹±å­—ã®ã¿ï¼‰ã€‚
    ä¾‹: "USDJPY-" -> "USDJPY" , "USDJPY.m" -> "USDJPY"
    """
    tag = "".join([c for c in (symbol or "") if c.isalpha()])
    return tag or symbol


def get_ohlcv_csv_path(
    symbol: str,
    timeframe: str,
    data_root: str | os.PathLike | Path | None = None,
    layout: str = "per-symbol",
) -> Path:
    """
    çµ±ä¸€ã•ã‚ŒãŸå ´æ‰€ã¸ OHLCV CSV ã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä½œæˆã™ã‚‹ã€‚

    - `symbol`: ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼æ¥å°¾è¾ã‚’å«ã‚€å¯èƒ½æ€§ã‚ã‚Šï¼ˆã“ã“ã§ã¯ãã®ã¾ã¾å—ã‘å–ã‚‹ï¼‰
    - `timeframe`: ä¾‹ 'M5', 'H1'ï¼ˆãã®ã¾ã¾ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ã†ï¼‰
    - `data_root`: None ã®å ´åˆã¯ `get_data_root()` ã«å§”è­²
    - `layout`: 'per-symbol' ã¾ãŸã¯ 'flat'

    è¿”ã‚Šå€¤ä¾‹ï¼ˆper-symbolï¼‰: <data_root>/USDJPY/ohlcv/USDJPY_M5.csv
    è¿”ã‚Šå€¤ä¾‹ï¼ˆflatï¼‰: <data_root>/USDJPY_M5.csv
    """
    # data_root ã‚’æ±ºå®š
    if data_root is None:
        root = get_data_root()
    else:
        root = Path(data_root) if not isinstance(data_root, Path) else data_root
        if not root.is_absolute():
            root = (get_project_root() / root).resolve()

    root = root.resolve()

    # file tag
    tag = get_file_tag(symbol.upper())

    if layout == "per-symbol":
        ohlcv_dir = root / tag / "ohlcv"
        _ensure_dir(ohlcv_dir)
        csv_path = ohlcv_dir / f"{tag}_{timeframe}.csv"
    else:
        _ensure_dir(root)
        csv_path = root / f"{tag}_{timeframe}.csv"

    return csv_path.resolve()



=== file: mypy.ini ===

[mypy]
python_version = 3.13
warn_unused_configs = True

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆMetaTrader5 ãªã©ï¼‰ã®å‹å®šç¾©ãŒãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ã—ãªã„
ignore_missing_imports = True

# ãã“ãã“å³ã—ã‚ã ã‘ã©å…¨å£Šã¯ã—ãªã„ãƒãƒ©ãƒ³ã‚¹
no_implicit_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_return_any = True
strict_equality = True

# å°†æ¥çš„ã« app.* ã ã‘ã¯ã¡ã‚‡ã£ã¨å³ã—ã‚ã«ã™ã‚‹
[mypy-app.*]
disallow_untyped_defs = True
check_untyped_defs = True

# scripts ã¯ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒå¤šã„ã®ã§å°‘ã—ç·©ã‚
[mypy-scripts.*]
disallow_untyped_defs = False
check_untyped_defs = False



=== file: project_tree.txt ===

fxbot/
  .env.example
  .gitignore
  .importlinter.ini
  7C9B198C-ADA6-4308-A321-C13EA50E0707.jpg
  active_model.json
  Agent HQ ç”¨ã€Œç´°åˆ†åŒ–ç‰ˆã€.txt
  apply_order.py
  fxbot_path.py
  mypy.ini
  PROJECT_MAP.md
  project_snapshot-DESKTOP-KK77.txt
  project_snapshot.txt
  project_tree.txt
  pyproject.toml
  pytest.ini
  README.md
  requirements.txt
  ruff.toml
  setup_weekly_job.ps1
  temp_equity_check.py
  tmp_view.py
  To Doãƒªã‚¹ãƒˆ.txt
  To Doãƒªã‚¹ãƒˆ2.txt
  To Doãƒªã‚¹ãƒˆv4.txt
  To Doãƒªã‚¹ãƒˆv5.txt
  ToDoãƒªã‚¹ãƒˆ Masterç‰ˆ.txt
  ToDoãƒªã‚¹ãƒˆ ä¸‹ä½ç‰ˆ.txt
  ã‚„ã‚‹ã¹ãã“ã¨.txt
  ãƒ•ã‚§ãƒ¼ã‚ºA~Hã¾ã§ã®çµæœ.txt
  ãƒŸãƒãƒ“ã‚­ FX è‡ªå‹•å£²è²·ã‚·ã‚¹ãƒ†ãƒ ï¼šä»•æ§˜æ›¸ v4ï¼ˆæ­£å¼ç‰ˆï¼‰.txt
  ä»•æ§˜ã®çµ±åˆã¾ã¨ã‚.docx
  ä»•æ§˜æ›¸.txt
  ä»•æ§˜æ›¸v3.txt
  ä½œæ¥­6ï½8.txt
  ä½œæ¥­9.txt
  å‚™å¿˜éŒ².txt
  è³ªå•.txt
  .git/
    COMMIT_EDITMSG
    config
    description
    FETCH_HEAD
    HEAD
    index
    ORIG_HEAD
    packed-refs
    hooks/
      applypatch-msg.sample
      commit-msg.sample
      fsmonitor-watchman.sample
      post-update.sample
      pre-applypatch.sample
      pre-commit.sample
      pre-merge-commit.sample
      pre-push.sample
      pre-rebase.sample
      pre-receive.sample
      prepare-commit-msg.sample
      push-to-checkout.sample
      sendemail-validate.sample
      update.sample
    info/
      exclude
    logs/
      HEAD
      refs/
        heads/
          master
        remotes/
          origin/
            HEAD
            master
    objects/
      06/
        425b3657b93fc01e1d8fbaff947aad746ee33c
      08/
        35fc00560ad9e7108594ec24d5becf1c126078
      0b/
        db24735e619c1a8577f981159e1c39f8751994
      0f/
        c114262374e3d672dadc6afa2b991e4f6c689b
        ee13da1c3397fa33041e4a91031f702b56ecae
      16/
        a03a706eddf6ce5c7212b8058f8296390ed199
      2d/
        c2d9eb0861384aead82ee94b669ffecd7e6613
      31/
        feea61368deedca4bbd4a928698b3540cd5258
      33/
        fa6e9a3a910bdaceb1a320e0c44c70b178dbde
      3a/
        39953223bca508d41b067546c44eae33cc77d5
      47/
        b97783f66d735cbe7a3a7073a62a76080de3c7
      48/
        4dc31f9cac7be77b7b2f1a34fa1f4fe862f214
      50/
        62019735f64f9a539ad8ab097d24838f53d80f
      57/
        3db7466e3f7d157f70e6741b499f70f2da9d39
      5c/
        034b81e195bdd3e98a7ee73154a5622b8ce89a
      5d/
        f340f11dd1b171ab57f27ebe75fad47bc3f7f8
      65/
        480c84e085b24fda67fabdd48acec874947ecd
      67/
        278fb3e10eab5386539eaa7a29bdf86f306b2e
      69/
        36786aadd934f728a2c06b7a9521ac897eef84
      6b/
        cc773c80bcd22bc5b4207d5dc62145490477bb
      77/
        0ffb062b2463534f5ef6b777e7c48a871d28f0
      78/
        a4e99ab1f760126d1d7f5c41426478665ac392
      7a/
        a5ecac2b1de7ad5de88d9dc173bd4fe7f7457f
      7c/
        07fc1869e186866b741d9ee51f0a4b1f4b62c1
      7e/
        fc71bcff730e132162a1a4d73e181474d7aa02
      82/
        f7512b210a54ea81231e2e21be5177c8b8f0fd
      85/
        e5a61cffbb987edcfd2a1150bf7d5d0693fbd1
      88/
        4eebe2f269b80fe23b16d6dcfa66cffeda8853
      97/
        ecea0268d144e5a65b86bba34c977671169da6
      a1/
        765334d21dbbb3c76f58588bba42ee46539399
      a7/
        1c36872b306846e4e95ce758bca8e2f0b1a62e
      b3/
        acab41d43907963f5dafd4481848861fc9b9c7
      b5/
        05174fb161cc773fca7db77c1653b7fb487c3a
        b178511621238433e50822db295c6d82b472cb
      bf/
        74d40947b88c5af841dfae36233c4d0d379e7e
      c2/
        5e861b5523e19f640926b8adbbd08c9e3a141d
      cb/
        1fd5f93257436256990a115132d7e4a8639110
      d2/
        7cd39775ed7be77cbec2eec12166b09ce3d54b
      d7/
        053cd4ac4136202519be5128077cc50dde2c08
      d9/
        1e6f2b6be49a6cfa32919c57cf0f012905203b
      de/
        5c51330c4457010905d380ebfc3f3456ffe2aa
      e1/
        50ca817673139fcd4d1cb7a1002270b09f4271
      e4/
        118c6879c6a728601f1bc3921389a2cb76c9b6
      e5/
        c7051fd249b54729e6e640631dd055d17f2583
      ff/
        096b06a36a656530caf87349ecf7dd797a11f2
        717f39ea33ddadd68191d928e85a1480799419
      info/
      pack/
        pack-9467dc01728b5d62cd5eb249c8846616378e21f3.idx
        pack-9467dc01728b5d62cd5eb249c8846616378e21f3.pack
        pack-9467dc01728b5d62cd5eb249c8846616378e21f3.rev
    refs/
      heads/
        master
      remotes/
        origin/
          HEAD
          master
      tags/
  .mypy_cache/
    .gitignore
    CACHEDIR.TAG
    missing_stubs
    3.13/
      @plugins_snapshot.json
      abc.data.json
      abc.meta.json
      argparse.data.json
      argparse.meta.json
      array.data.json
      array.meta.json
      ast.data.json
      ast.meta.json
      atexit.data.json
      atexit.meta.json
      binascii.data.json
      binascii.meta.json
      builtins.data.json
      builtins.meta.json
      bz2.data.json
      bz2.meta.json
      codecs.data.json
      codecs.meta.json
      contextlib.data.json
      contextlib.meta.json
      contextvars.data.json
      contextvars.meta.json
      copy.data.json
      copy.meta.json
      copyreg.data.json
      copyreg.meta.json
      csv.data.json
      csv.meta.json
      dataclasses.data.json
      dataclasses.meta.json
      datetime.data.json
      datetime.meta.json
      decimal.data.json
      decimal.meta.json
      dis.data.json
      dis.meta.json
      enum.data.json
      enum.meta.json
      fractions.data.json
      fractions.meta.json
      functools.data.json
      functools.meta.json
      fxbot_path.data.json
      fxbot_path.meta.json
      genericpath.data.json
      genericpath.meta.json
      glob.data.json
      glob.meta.json
      gzip.data.json
      gzip.meta.json
      inspect.data.json
      inspect.meta.json
      io.data.json
      io.meta.json
      itertools.data.json
      itertools.meta.json
      math.data.json
      math.meta.json
      mmap.data.json
      mmap.meta.json
      ntpath.data.json
      ntpath.meta.json
      numbers.data.json
      numbers.meta.json
      opcode.data.json
      opcode.meta.json
      operator.data.json
      operator.meta.json
      pickle.data.json
      pickle.meta.json
      platform.data.json
      platform.meta.json
      posixpath.data.json
      posixpath.meta.json
      pprint.data.json
      pprint.meta.json
      queue.data.json
      queue.meta.json
      re.data.json
      re.meta.json
      selectors.data.json
      selectors.meta.json
      shlex.data.json
      shlex.meta.json
      shutil.data.json
      shutil.meta.json
      signal.data.json
      signal.meta.json
      socket.data.json
      socket.meta.json
      sre_compile.data.json
      sre_compile.meta.json
      sre_constants.data.json
      sre_constants.meta.json
      sre_parse.data.json
      sre_parse.meta.json
      ssl.data.json
      ssl.meta.json
      statistics.data.json
      statistics.meta.json
      struct.data.json
      struct.meta.json
      subprocess.data.json
      subprocess.meta.json
      tarfile.data.json
      tarfile.meta.json
      tempfile.data.json
      tempfile.meta.json
      textwrap.data.json
      textwrap.meta.json
      threading.data.json
      threading.meta.json
      time.data.json
      time.meta.json
      traceback.data.json
      traceback.meta.json
      types.data.json
      types.meta.json
      typing.data.json
      typing.meta.json
      typing_extensions.data.json
      typing_extensions.meta.json
      unicodedata.data.json
      unicodedata.meta.json
      uuid.data.json
      uuid.meta.json
      warnings.data.json
      warnings.meta.json
      weakref.data.json
      weakref.meta.json
      zipimport.data.json
      zipimport.meta.json
      zlib.data.json
      zlib.meta.json
      _ast.data.json
      _ast.meta.json
      _asyncio.data.json
      _asyncio.meta.json
      _bz2.data.json
      _bz2.meta.json
      _codecs.data.json
      _codecs.meta.json
      _collections_abc.data.json
      _collections_abc.meta.json
      _compression.data.json
      _compression.meta.json
      _contextvars.data.json
      _contextvars.meta.json
      _csv.data.json
      _csv.meta.json
      _ctypes.data.json
      _ctypes.meta.json
      _decimal.data.json
      _decimal.meta.json
      _frozen_importlib.data.json
      _frozen_importlib.meta.json
      _frozen_importlib_external.data.json
      _frozen_importlib_external.meta.json
      _io.data.json
      _io.meta.json
      _operator.data.json
      _operator.meta.json
      _pickle.data.json
      _pickle.meta.json
      _queue.data.json
      _queue.meta.json
      _sitebuiltins.data.json
      _sitebuiltins.meta.json
      _socket.data.json
      _socket.meta.json
      _ssl.data.json
      _ssl.meta.json
      _struct.data.json
      _struct.meta.json
      _thread.data.json
      _thread.meta.json
      _tkinter.data.json
      _tkinter.meta.json
      _warnings.data.json
      _warnings.meta.json
      _weakref.data.json
      _weakref.meta.json
      _weakrefset.data.json
      _weakrefset.meta.json
      _winapi.data.json
      _winapi.meta.json
      __future__.data.json
      __future__.meta.json
      app/
        core.data.json
        core.meta.json
        __init__.data.json
        __init__.meta.json
        core/
          config_loader.data.json
          config_loader.meta.json
          data_finder.data.json
          data_finder.meta.json
          market.data.json
          market.meta.json
        gui/
          dashboard_tab_qt.data.json
          dashboard_tab_qt.meta.json
          widgets.data.json
          widgets.meta.json
          __init__.data.json
          __init__.meta.json
          widgets/
            feature_importance.data.json
            feature_importance.meta.json
        services/
          circuit_breaker.data.json
          circuit_breaker.meta.json
          data_guard.data.json
          data_guard.meta.json
          decision_log.data.json
          decision_log.meta.json
          event_store.data.json
          event_store.meta.json
          metrics.data.json
          metrics.meta.json
          mt5_account_store.data.json
          mt5_account_store.meta.json
          mt5_service.data.json
          mt5_service.meta.json
          orderbook_stub.data.json
          orderbook_stub.meta.json
          trade_state.data.json
          trade_state.meta.json
          trailing.data.json
          trailing.meta.json
          trailing_hook.data.json
          trailing_hook.meta.json
          __init__.data.json
          __init__.meta.json
        strategies/
          __init__.data.json
          __init__.meta.json
      asyncio/
        base_events.data.json
        base_events.meta.json
        base_futures.data.json
        base_futures.meta.json
        constants.data.json
        constants.meta.json
        coroutines.data.json
        coroutines.meta.json
        events.data.json
        events.meta.json
        exceptions.data.json
        exceptions.meta.json
        futures.data.json
        futures.meta.json
        locks.data.json
        locks.meta.json
        mixins.data.json
        mixins.meta.json
        proactor_events.data.json
        proactor_events.meta.json
        protocols.data.json
        protocols.meta.json
        queues.data.json
        queues.meta.json
        runners.data.json
        runners.meta.json
        selector_events.data.json
        selector_events.meta.json
        streams.data.json
        streams.meta.json
        subprocess.data.json
        subprocess.meta.json
        taskgroups.data.json
        taskgroups.meta.json
        tasks.data.json
        tasks.meta.json
        threads.data.json
        threads.meta.json
        timeouts.data.json
        timeouts.meta.json
        transports.data.json
        transports.meta.json
        unix_events.data.json
        unix_events.meta.json
        windows_events.data.json
        windows_events.meta.json
        windows_utils.data.json
        windows_utils.meta.json
        __init__.data.json
        __init__.meta.json
      collections/
        abc.data.json
        abc.meta.json
        __init__.data.json
        __init__.meta.json
      concurrent/
        __init__.data.json
        __init__.meta.json
        futures/
          process.data.json
          process.meta.json
          thread.data.json
          thread.meta.json
          _base.data.json
          _base.meta.json
          __init__.data.json
          __init__.meta.json
      core/
        config.data.json
        config.meta.json
        indicators.data.json
        indicators.meta.json
        position_guard.data.json
        position_guard.meta.json
        __init__.data.json
        __init__.meta.json
        ai/
          __init__.data.json
          __init__.meta.json
        metrics/
          registry.data.json
          registry.meta.json
          __init__.data.json
          __init__.meta.json
        utils/
          clock.data.json
          clock.meta.json
          runtime.data.json
          runtime.meta.json
          timeutil.data.json
          timeutil.meta.json
          __init__.data.json
          __init__.meta.json
      ctypes/
        _endian.data.json
        _endian.meta.json
        __init__.data.json
        __init__.meta.json
      cycler/
        __init__.data.json
        __init__.meta.json
      email/
        charset.data.json
        charset.meta.json
        contentmanager.data.json
        contentmanager.meta.json
        errors.data.json
        errors.meta.json
        header.data.json
        header.meta.json
        message.data.json
        message.meta.json
        policy.data.json
        policy.meta.json
        _policybase.data.json
        _policybase.meta.json
        __init__.data.json
        __init__.meta.json
      html/
        entities.data.json
        entities.meta.json
        __init__.data.json
        __init__.meta.json
      importlib/
        abc.data.json
        abc.meta.json
        machinery.data.json
        machinery.meta.json
        readers.data.json
        readers.meta.json
        util.data.json
        util.meta.json
        _abc.data.json
        _abc.meta.json
        _bootstrap.data.json
        _bootstrap.meta.json
        _bootstrap_external.data.json
        _bootstrap_external.meta.json
        __init__.data.json
        __init__.meta.json
        metadata/
          _meta.data.json
          _meta.meta.json
          __init__.data.json
          __init__.meta.json
        resources/
          abc.data.json
          abc.meta.json
          _common.data.json
          _common.meta.json
          _functional.data.json
          _functional.meta.json
          __init__.data.json
          __init__.meta.json
      json/
        decoder.data.json
        decoder.meta.json
        encoder.data.json
        encoder.meta.json
        __init__.data.json
        __init__.meta.json
      lightgbm/
        basic.data.json
        basic.meta.json
        callback.data.json
        callback.meta.json
        compat.data.json
        compat.meta.json
        dask.data.json
        dask.meta.json
        engine.data.json
        engine.meta.json
        libpath.data.json
        libpath.meta.json
        plotting.data.json
        plotting.meta.json
        sklearn.data.json
        sklearn.meta.json
        __init__.data.json
        __init__.meta.json
      logging/
        __init__.data.json
        __init__.meta.json
      loguru/
        __init__.data.json
        __init__.meta.json
      matplotlib/
        artist.data.json
        artist.meta.json
        axis.data.json
        axis.meta.json
        backend_bases.data.json
        backend_bases.meta.json
        backend_managers.data.json
        backend_managers.meta.json
        backend_tools.data.json
        backend_tools.meta.json
        bezier.data.json
        bezier.meta.json
        cbook.data.json
        cbook.meta.json
        cm.data.json
        cm.meta.json
        collections.data.json
        collections.meta.json
        colorbar.data.json
        colorbar.meta.json
        colorizer.data.json
        colorizer.meta.json
        colors.data.json
        colors.meta.json
        container.data.json
        container.meta.json
        contour.data.json
        contour.meta.json
        dates.data.json
        dates.meta.json
        figure.data.json
        figure.meta.json
        font_manager.data.json
        font_manager.meta.json
        ft2font.data.json
        ft2font.meta.json
        gridspec.data.json
        gridspec.meta.json
        image.data.json
        image.meta.json
        inset.data.json
        inset.meta.json
        layout_engine.data.json
        layout_engine.meta.json
        legend.data.json
        legend.meta.json
        legend_handler.data.json
        legend_handler.meta.json
        lines.data.json
        lines.meta.json
        markers.data.json
        markers.meta.json
        mathtext.data.json
        mathtext.meta.json
        mlab.data.json
        mlab.meta.json
        offsetbox.data.json
        offsetbox.meta.json
        patches.data.json
        patches.meta.json
        path.data.json
        path.meta.json
        patheffects.data.json
        patheffects.meta.json
        quiver.data.json
        quiver.meta.json
        scale.data.json
        scale.meta.json
        spines.data.json
        spines.meta.json
        stackplot.data.json
        stackplot.meta.json
        streamplot.data.json
        streamplot.meta.json
        table.data.json
        table.meta.json
        texmanager.data.json
        texmanager.meta.json
        text.data.json
        text.meta.json
        textpath.data.json
        textpath.meta.json
        ticker.data.json
        ticker.meta.json
        transforms.data.json
        transforms.meta.json
        typing.data.json
        typing.meta.json
        units.data.json
        units.meta.json
        widgets.data.json
        widgets.meta.json
        _afm.data.json
        _afm.meta.json
        _enums.data.json
        _enums.meta.json
        _mathtext.data.json
        _mathtext.meta.json
        _mathtext_data.data.json
        _mathtext_data.meta.json
        _pylab_helpers.data.json
        _pylab_helpers.meta.json
        _tri.data.json
        _tri.meta.json
        __init__.data.json
        __init__.meta.json
        axes/
          _axes.data.json
          _axes.meta.json
          _base.data.json
          _base.meta.json
          _secondary_axes.data.json
          _secondary_axes.meta.json
          __init__.data.json
          __init__.meta.json
        backends/
          backend_agg.data.json
          backend_agg.meta.json
          backend_qt.data.json
          backend_qt.meta.json
          backend_qtagg.data.json
          backend_qtagg.meta.json
          qt_compat.data.json
          qt_compat.meta.json
          registry.data.json
          registry.meta.json
          _backend_agg.data.json
          _backend_agg.meta.json
          __init__.data.json
          __init__.meta.json
          qt_editor/
            figureoptions.data.json
            figureoptions.meta.json
            _formlayout.data.json
            _formlayout.meta.json
            __init__.data.json
            __init__.meta.json
        projections/
          geo.data.json
          geo.meta.json
          polar.data.json
          polar.meta.json
          __init__.data.json
          __init__.meta.json
        tri/
          _triangulation.data.json
          _triangulation.meta.json
          _tricontour.data.json
          _tricontour.meta.json
          _trifinder.data.json
          _trifinder.meta.json
          _triinterpolate.data.json
          _triinterpolate.meta.json
          _tripcolor.data.json
          _tripcolor.meta.json
          _triplot.data.json
          _triplot.meta.json
          _trirefine.data.json
          _trirefine.meta.json
          _tritools.data.json
          _tritools.meta.json
          __init__.data.json
          __init__.meta.json
        _api/
          deprecation.data.json
          deprecation.meta.json
          __init__.data.json
          __init__.meta.json
      multiprocessing/
        connection.data.json
        connection.meta.json
        context.data.json
        context.meta.json
        managers.data.json
        managers.meta.json
        pool.data.json
        pool.meta.json
        popen_fork.data.json
        popen_fork.meta.json
        popen_forkserver.data.json
        popen_forkserver.meta.json
        popen_spawn_posix.data.json
        popen_spawn_posix.meta.json
        popen_spawn_win32.data.json
        popen_spawn_win32.meta.json
        process.data.json
        process.meta.json
        queues.data.json
        queues.meta.json
        reduction.data.json
        reduction.meta.json
        sharedctypes.data.json
        sharedctypes.meta.json
        shared_memory.data.json
        shared_memory.meta.json
        spawn.data.json
        spawn.meta.json
        synchronize.data.json
        synchronize.meta.json
        util.data.json
        util.meta.json
        __init__.data.json
        __init__.meta.json
      numpy/
        dtypes.data.json
        dtypes.meta.json
        exceptions.data.json
        exceptions.meta.json
        matlib.data.json
        matlib.meta.json
        version.data.json
        version.meta.json
        _array_api_info.data.json
        _array_api_info.meta.json
        _expired_attrs_2_0.data.json
        _expired_attrs_2_0.meta.json
        _globals.data.json
        _globals.meta.json
        _pytesttester.data.json
        _pytesttester.meta.json
        __config__.data.json
        __config__.meta.json
        __init__.data.json
        __init__.meta.json
        char/
          __init__.data.json
          __init__.meta.json
        core/
          __init__.data.json
          __init__.meta.json
        ctypeslib/
          _ctypeslib.data.json
          _ctypeslib.meta.json
          __init__.data.json
          __init__.meta.json
        f2py/
          auxfuncs.data.json
          auxfuncs.meta.json
          cfuncs.data.json
          cfuncs.meta.json
          f2py2e.data.json
          f2py2e.meta.json
          __init__.data.json
          __init__.meta.json
          __version__.data.json
          __version__.meta.json
        fft/
          _helper.data.json
          _helper.meta.json
          _pocketfft.data.json
          _pocketfft.meta.json
          __init__.data.json
          __init__.meta.json
        lib/
          array_utils.data.json
          array_utils.meta.json
          format.data.json
          format.meta.json
          introspect.data.json
          introspect.meta.json
          mixins.data.json
          mixins.meta.json
          npyio.data.json
          npyio.meta.json
          scimath.data.json
          scimath.meta.json
          stride_tricks.data.json
          stride_tricks.meta.json
          _arraypad_impl.data.json
          _arraypad_impl.meta.json
          _arraysetops_impl.data.json
          _arraysetops_impl.meta.json
          _arrayterator_impl.data.json
          _arrayterator_impl.meta.json
          _array_utils_impl.data.json
          _array_utils_impl.meta.json
          _datasource.data.json
          _datasource.meta.json
          _format_impl.data.json
          _format_impl.meta.json
          _function_base_impl.data.json
          _function_base_impl.meta.json
          _histograms_impl.data.json
          _histograms_impl.meta.json
          _index_tricks_impl.data.json
          _index_tricks_impl.meta.json
          _iotools.data.json
          _iotools.meta.json
          _nanfunctions_impl.data.json
          _nanfunctions_impl.meta.json
          _npyio_impl.data.json
          _npyio_impl.meta.json
          _polynomial_impl.data.json
          _polynomial_impl.meta.json
          _scimath_impl.data.json
          _scimath_impl.meta.json
          _shape_base_impl.data.json
          _shape_base_impl.meta.json
          _stride_tricks_impl.data.json
          _stride_tricks_impl.meta.json
          _twodim_base_impl.data.json
          _twodim_base_impl.meta.json
          _type_check_impl.data.json
          _type_check_impl.meta.json
          _ufunclike_impl.data.json
          _ufunclike_impl.meta.json
          _utils_impl.data.json
          _utils_impl.meta.json
          _version.data.json
          _version.meta.json
          __init__.data.json
          __init__.meta.json
        linalg/
          linalg.data.json
          linalg.meta.json
          _linalg.data.json
          _linalg.meta.json
          _umath_linalg.data.json
          _umath_linalg.meta.json
          __init__.data.json
          __init__.meta.json
        ma/
          core.data.json
          core.meta.json
          extras.data.json
          extras.meta.json
          mrecords.data.json
          mrecords.meta.json
          __init__.data.json
          __init__.meta.json
        matrixlib/
          defmatrix.data.json
          defmatrix.meta.json
          __init__.data.json
          __init__.meta.json
        polynomial/
          chebyshev.data.json
          chebyshev.meta.json
          hermite.data.json
          hermite.meta.json
          hermite_e.data.json
          hermite_e.meta.json
          laguerre.data.json
          laguerre.meta.json
          legendre.data.json
          legendre.meta.json
          polynomial.data.json
          polynomial.meta.json
          polyutils.data.json
          polyutils.meta.json
          _polybase.data.json
          _polybase.meta.json
          _polytypes.data.json
          _polytypes.meta.json
          __init__.data.json
          __init__.meta.json
        random/
          bit_generator.data.json
          bit_generator.meta.json
          mtrand.data.json
          mtrand.meta.json
          _generator.data.json
          _generator.meta.json
          _mt19937.data.json
          _mt19937.meta.json
          _pcg64.data.json
          _pcg64.meta.json
          _philox.data.json
          _philox.meta.json
          _sfc64.data.json
          _sfc64.meta.json
          __init__.data.json
          __init__.meta.json
        rec/
          __init__.data.json
          __init__.meta.json
        strings/
          __init__.data.json
          __init__.meta.json
        testing/
          overrides.data.json
          overrides.meta.json
          __init__.data.json
          __init__.meta.json
          _private/
            utils.data.json
            utils.meta.json
            __init__.data.json
            __init__.meta.json
        typing/
          __init__.data.json
          __init__.meta.json
        _core/
          arrayprint.data.json
          arrayprint.meta.json
          defchararray.data.json
          defchararray.meta.json
          einsumfunc.data.json
          einsumfunc.meta.json
          fromnumeric.data.json
          fromnumeric.meta.json
          function_base.data.json
          function_base.meta.json
          multiarray.data.json
          multiarray.meta.json
          numeric.data.json
          numeric.meta.json
          numerictypes.data.json
          numerictypes.meta.json
          records.data.json
          records.meta.json
          shape_base.data.json
          shape_base.meta.json
          strings.data.json
          strings.meta.json
          _asarray.data.json
          _asarray.meta.json
          _internal.data.json
          _internal.meta.json
          _type_aliases.data.json
          _type_aliases.meta.json
          _ufunc_config.data.json
          _ufunc_config.meta.json
          __init__.data.json
          __init__.meta.json
        _typing/
          _add_docstring.data.json
          _add_docstring.meta.json
          _array_like.data.json
          _array_like.meta.json
          _char_codes.data.json
          _char_codes.meta.json
          _dtype_like.data.json
          _dtype_like.meta.json
          _extended_precision.data.json
          _extended_precision.meta.json
          _nbit.data.json
          _nbit.meta.json
          _nbit_base.data.json
          _nbit_base.meta.json
          _nested_sequence.data.json
          _nested_sequence.meta.json
          _scalars.data.json
          _scalars.meta.json
          _shape.data.json
          _shape.meta.json
          _ufunc.data.json
          _ufunc.meta.json
          __init__.data.json
          __init__.meta.json
      os/
        path.data.json
        path.meta.json
        __init__.data.json
        __init__.meta.json
      packaging/
        version.data.json
        version.meta.json
        _structures.data.json
        _structures.meta.json
        __init__.data.json
        __init__.meta.json
      pathlib/
        __init__.data.json
        __init__.meta.json
      PIL/
        ExifTags.data.json
        ExifTags.meta.json
        GimpGradientFile.data.json
        GimpGradientFile.meta.json
        GimpPaletteFile.data.json
        GimpPaletteFile.meta.json
        Image.data.json
        Image.meta.json
        ImageCms.data.json
        ImageCms.meta.json
        ImageColor.data.json
        ImageColor.meta.json
        ImageFile.data.json
        ImageFile.meta.json
        ImageFilter.data.json
        ImageFilter.meta.json
        ImageMode.data.json
        ImageMode.meta.json
        ImageOps.data.json
        ImageOps.meta.json
        ImagePalette.data.json
        ImagePalette.meta.json
        ImageQt.data.json
        ImageQt.meta.json
        ImageShow.data.json
        ImageShow.meta.json
        PaletteFile.data.json
        PaletteFile.meta.json
        TiffImagePlugin.data.json
        TiffImagePlugin.meta.json
        TiffTags.data.json
        TiffTags.meta.json
        _binary.data.json
        _binary.meta.json
        _deprecate.data.json
        _deprecate.meta.json
        _imaging.data.json
        _imaging.meta.json
        _imagingcms.data.json
        _imagingcms.meta.json
        _typing.data.json
        _typing.meta.json
        _util.data.json
        _util.meta.json
        _version.data.json
        _version.meta.json
        __init__.data.json
        __init__.meta.json
      pyexpat/
        errors.data.json
        errors.meta.json
        model.data.json
        model.meta.json
        __init__.data.json
        __init__.meta.json
      pyparsing/
        actions.data.json
        actions.meta.json
        common.data.json
        common.meta.json
        core.data.json
        core.meta.json
        exceptions.data.json
        exceptions.meta.json
        helpers.data.json
        helpers.meta.json
        results.data.json
        results.meta.json
        testing.data.json
        testing.meta.json
        unicode.data.json
        unicode.meta.json
        util.data.json
        util.meta.json
        __init__.data.json
        __init__.meta.json
        diagram/
          __init__.data.json
          __init__.meta.json
      PyQt6/
        QtCore.data.json
        QtCore.meta.json
        QtGui.data.json
        QtGui.meta.json
        QtWidgets.data.json
        QtWidgets.meta.json
        sip.data.json
        sip.meta.json
        __init__.data.json
        __init__.meta.json
      string/
        __init__.data.json
        __init__.meta.json
      sys/
        _monitoring.data.json
        _monitoring.meta.json
        __init__.data.json
        __init__.meta.json
      tkinter/
        constants.data.json
        constants.meta.json
        font.data.json
        font.meta.json
        ttk.data.json
        ttk.meta.json
        __init__.data.json
        __init__.meta.json
      unittest/
        async_case.data.json
        async_case.meta.json
        case.data.json
        case.meta.json
        loader.data.json
        loader.meta.json
        main.data.json
        main.meta.json
        mock.data.json
        mock.meta.json
        result.data.json
        result.meta.json
        runner.data.json
        runner.meta.json
        signals.data.json
        signals.meta.json
        suite.data.json
        suite.meta.json
        _log.data.json
        _log.meta.json
        __init__.data.json
        __init__.meta.json
      urllib/
        parse.data.json
        parse.meta.json
        __init__.data.json
        __init__.meta.json
      xgboost/
        callback.data.json
        callback.meta.json
        collective.data.json
        collective.meta.json
        compat.data.json
        compat.meta.json
        config.data.json
        config.meta.json
        core.data.json
        core.meta.json
        data.data.json
        data.meta.json
        libpath.data.json
        libpath.meta.json
        plotting.data.json
        plotting.meta.json
        sklearn.data.json
        sklearn.meta.json
        tracker.data.json
        tracker.meta.json
        training.data.json
        training.meta.json
        _data_utils.data.json
        _data_utils.meta.json
        _typing.data.json
        _typing.meta.json
        __init__.data.json
        __init__.meta.json
      xml/
        __init__.data.json
        __init__.meta.json
        etree/
          ElementTree.data.json
          ElementTree.meta.json
          __init__.data.json
          __init__.meta.json
        parsers/
          __init__.data.json
          __init__.meta.json
          expat/
            __init__.data.json
            __init__.meta.json
      zipfile/
        __init__.data.json
        __init__.meta.json
        _path/
          __init__.data.json
          __init__.meta.json
      zoneinfo/
        _common.data.json
        _common.meta.json
        _tzpath.data.json
        _tzpath.meta.json
        __init__.data.json
        __init__.meta.json
      _typeshed/
        importlib.data.json
        importlib.meta.json
        __init__.data.json
        __init__.meta.json
  .ruff_cache/
    .gitignore
    CACHEDIR.TAG
    0.14.5/
      12600658311990167276
      16257144851545386692
      17965227139474717685
      1875680279843845063
      4652391773985594464
      5620828212296045680
      6443226264972188389
      7095227949590206593
  .vscode/
    settings.json
  app/
    main_tk.py
    __init__.py
    core/
      config_loader.py
      data_finder.py
      logger.py
      market.py
      mt5_client.py
      strategy_profile.py
      __pycache__/
        mt5_client.cpython-313.pyc
        strategy_profile.cpython-313.pyc
    gui/
      ai_tab.py
      backtest_tab.py
      control_tab.py
      dashboard_tab.py
      dashboard_tab_qt.py
      history_tab.py
      main.py
      settings_tab.py
      __init__.py
      widgets/
        feature_importance.py
        shap_bar.py
    services/
      aisvc_loader.py
      ai_service.py
      circuit_breaker.py
      data_guard.py
      decision_log.py
      event_store.py
      execution_stub.py
      feature_importance.py
      metrics.py
      mt5_account_store.py
      mt5_selftest.py
      mt5_service.py
      orderbook_stub.py
      recent_kpi.py
      shap_service.py
      trade_service.py
      trade_state.py
      trailing.py
      trailing_hook.py
      __init__.py
    strategies/
      ai_strategy.py
      __init__.py
      __pycache__/
        ai_strategy.cpython-313.pyc
        __init__.cpython-313.pyc
    __pycache__/
      __init__.cpython-313.pyc
  backtests/
    michibiki_std/
  config/
    mt5_accounts.json
  configs/
    .env.example
    config copy.yaml
    config.local.yaml
    config.yaml
  core/
    config.py
    indicators.py
    metrics.py
    position_guard.py
    risk.py
    __init__.py
    ai/
      calibration.py
      features.py
      loader.py
      service.py
      __init__.py
    metrics/
      fi_extractor.py
      registry.py
      __init__.py
    utils/
      clock.py
      hashing.py
      runtime.py
      timeutil.py
      __init__.py
    __pycache__/
      risk.cpython-313.pyc
      __init__.cpython-313.pyc
  data/
    USDJPY_H1.csv
    USDJPY_M15.csv
    USDJPY_M5.csv
    USDJPY/
      features_for_shap.csv
      ohlcv/
        USDJPY_H1.csv
        USDJPY_M15-DESKTOP-KK77.csv
        USDJPY_M15.csv
        USDJPY_M5-DESKTOP-KK77.csv
        USDJPY_M5.csv
    ä»®/
      USDJPY_M5.csv
      USDJPY_M5_big.csv
      usdjpy/
        USDJPY_M5_mt5.csv
  logs/
    app-DESKTOP-KK77.log
    app.log
    events.jsonl
    ui_events.jsonl
    val_export_meta.json
    val_p_buy_raw.npy
    val_y_true.npy
    weekly_retrain_20251113.log
    weekly_retrain_20251116.log
    weekly_retrain_20251117.log
    backtest/
      equity_curve.csv
      USDJPY/
        M15/
          equity_curve.csv
          metrics.json
        M5/
          equity_curve.csv
          equity_test.csv
          equity_train.csv
          metrics.json
          metrics_wfo.json
          monthly_returns.csv
          monthly_returns_test.csv
          monthly_returns_train.csv
          signals_test.csv
          signals_train.csv
          trades.csv
          trades_test.csv
          trades_train.csv
          backtest_2024-01-01_to_2024-12-31/
            equity_curve.csv
            equity_test.csv
            equity_train.csv
            metrics.json
            metrics_wfo.json
            monthly_returns.csv
            monthly_returns_test.csv
            monthly_returns_train.csv
            trades.csv
            trades_test.csv
            trades_train.csv
          backtest_2024-01-01_to_2025-11-16/
            equity_curve.csv
            equity_test.csv
            equity_train.csv
            metrics.json
            metrics_wfo.json
            monthly_returns.csv
            monthly_returns_test.csv
            monthly_returns_train.csv
            trades.csv
            trades_test.csv
            trades_train.csv
          backtest_2024-01-01_to_2025-11-17/
            equity_curve.csv
            equity_test.csv
            equity_train.csv
            metrics.json
            metrics_wfo.json
            monthly_returns.csv
            monthly_returns_test.csv
            monthly_returns_train.csv
            trades.csv
            trades_test.csv
            trades_train.csv
          backtest_2024-07-01_to_2024-12-31/
            equity_curve.csv
            equity_test.csv
            equity_train.csv
            metrics.json
            metrics_wfo.json
            monthly_returns.csv
            monthly_returns_test.csv
            monthly_returns_train.csv
            trades.csv
            trades_test.csv
            trades_train.csv
          backtest_2024-11-07_to_2024-12-31/
            equity_curve.csv
            equity_test.csv
            equity_train.csv
            metrics.json
            metrics_wfo.json
            monthly_returns.csv
            monthly_returns_test.csv
            monthly_returns_train.csv
            trades.csv
            trades_test.csv
            trades_train.csv
          backtest_2025-05-15_to_2025-11-15/
            equity_curve.csv
            metrics.json
            monthly_returns.csv
            trades.csv
          backtest_2025-06-21_to_2025-11-15/
            equity_curve.csv
            metrics.json
            monthly_returns.csv
            trades.csv
          backtest_2025-10-15_to_2025-11-15/
            equity_curve.csv
            metrics.json
            monthly_returns.csv
            trades.csv
          backtest_ALL_to_ALL/
            equity_curve.csv
            equity_test.csv
            equity_train.csv
            metrics.json
            metrics_wfo.json
            monthly_returns.csv
            monthly_returns_test.csv
            monthly_returns_train.csv
            trades.csv
            trades_test.csv
            trades_train.csv
    decisions/
      decisions_USDJPY.jsonl
    reports/
      retrain_report_20251113_0914.json
    retrain/
      equity_test_1763292931.csv
      equity_test_1763370025.csv
      equity_train_1763292931.csv
      equity_train_1763370025.csv
      feat_importance_lk10_20251107_175557.csv
      feat_importance_lk15_20251107_162055.csv
      feat_importance_lk15_20251107_162243.csv
      feat_importance_lk15_20251107_163007.csv
      feat_importance_lk15_20251107_163044.csv
      job_20251105_173052_251f5624.log
      job_20251105_173113_df2776b1.log
      job_20251105_174004_2dd5540f.log
      job_20251105_174018_d4741638.log
      job_20251105_175145_673a797f.log
      job_20251105_180545_a53ced47.log
      job_20251106_075546_f4036ef5.log
      job_20251106_171821_5f3f029d.log
      job_20251107_124035_a081c214.log
      job_20251107_162053_b7252f19.log
      job_20251107_163005_f87f9cc2.log
      job_20251107_175555_4ab13e4d.log
      report_1762327557.2895734.json
      report_1762328795.2841146.json
      report_1762329166.0924582.json
      report_1762330372.625119.json
      report_1762330422.048195.json
      report_1762330462.918168.json
      report_17623314611667528.json
      report_17623314813159773.json
      report_17623320128869925.json
      report_17623320267762287.json
      report_17623327167915437.json
      report_17623335580532284.json
      report_17623833702376008.json
      report_17624171072965233.json
      report_17624869548320115.json
      report_17625000562076344.json
      report_17625006073549628.json
      report_1762505757473982.json
      report_1763292931.json
      report_1763370025.json
      weekly_retrain_20251113_091458.log
  models/
    active_model.json
    LightGBM_clf.classes.json
    LightGBM_clf.features.json
    LightGBM_clf.txt
    LightGBM_clf.version.json
    LightGBM_clf_20251108_183507.meta.json
    LightGBM_clf_20251109_151424.meta.json
    LightGBM_clf_20251113_020721.pkl.meta.json
    LightGBM_clf_20251113_021127.pkl.meta.json
    LightGBM_clf_20251113_154952.meta.json
    LightGBM_clf_20251113_155649.meta.json
    LightGBM_clf_20251114_064505.meta.json
    LightGBM_clf_20251116_112204.pkl.meta.json
    LightGBM_clf_20251117_090029.pkl.meta.json
    LightGBM_info.json
    PROMOTE.json
    VERSION.txt
    _backup_20251105_174012/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
    _backup_20251105_174026/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
      PROMOTE.json
      VERSION.txt
    _backup_20251105_180558/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
      PROMOTE.json
      VERSION.txt
    _backup_20251106_075609/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
      PROMOTE.json
      VERSION.txt
    _backup_20251106_171826/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
      PROMOTE.json
      VERSION.txt
    _backup_20251107_124234/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
      PROMOTE.json
      VERSION.txt
    _backup_20251107_162055/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
      PROMOTE.json
      VERSION.txt
    _backup_20251107_163007/
      active_model.json
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      LightGBM_clf.version.json
      PROMOTE.json
      VERSION.txt
    _staging/
      LightGBM_clf.classes.json
      LightGBM_clf.features.json
      PROMOTE.json
      VERSION.txt
  runtime/
    metrics.json
    metrics_044ho80o.json
    metrics_04i6g_bm.json
    metrics_08lb8guj.json
    metrics_0fdsym9v.json
    metrics_0ks5iv74.json
    metrics_0m3pzxsa.json
    metrics_0s0y1mq_.json
    metrics_0so3t4e4.json
    metrics_0t6las4q.json
    metrics_13d1atob.json
    metrics_1ce8in0z.json
    metrics_1f8wcifb.json
    metrics_1nqq6c9g.json
    metrics_1y_j88c6.json
    metrics_1_r7f8xe.json
    metrics_2b5irl55.json
    metrics_2llsmgld.json
    metrics_2wh_r543.json
    metrics_32ej_3qs.json
    metrics_384xt0qj.json
    metrics_3op0smhm.json
    metrics_3pck01qi.json
    metrics_3pw7kppo.json
    metrics_3w4e28bl.json
    metrics_3wwdl7sf.json
    metrics_3xjcntun.json
    metrics_3_z7j5dq.json
    metrics_4fx6u97k.json
    metrics_4jkcw838.json
    metrics_4jwlaw79.json
    metrics_4sqd0ccq.json
    metrics_4u83ovxn.json
    metrics_51fzibz0.json
    metrics_56tbpp98.json
    metrics_5c1keku_.json
    metrics_5_uus6ny.json
    metrics_681i8dz0.json
    metrics_6g2ygykl.json
    metrics_6jx0f61y.json
    metrics_6mv45wka.json
    metrics_6n99dteb.json
    metrics_6oizo1ba.json
    metrics_6u8fjy4e.json
    metrics_751tqi7x.json
    metrics_78ji6p4u.json
    metrics_7bbmsyvw.json
    metrics_7caerxlw.json
    metrics_7pcppci4.json
    metrics_7upf8_is.json
    metrics_7wtig7zq.json
    metrics_7yx4f9p_.json
    metrics_8i0ku3__.json
    metrics_8iecfllz.json
    metrics_8s427o_h.json
    metrics_8t5wcv0v.json
    metrics_8wbixgeh.json
    metrics_9clf0tik.json
    metrics_9wqc4wjf.json
    metrics_9y2xjt1a.json
    metrics_a4qsce4c.json
    metrics_abcuwh_8.json
    metrics_aen0sw0t.json
    metrics_avlwot5z.json
    metrics_a_jw5ekj.json
    metrics_beh8fq83.json
    metrics_bkm4j_2_.json
    metrics_bmocgusp.json
    metrics_bsejl6ul.json
    metrics_b_e39daa.json
    metrics_c19f7jhr.json
    metrics_c6dlu7j3.json
    metrics_c7srgc6l.json
    metrics_cfvm2vuv.json
    metrics_cs6cyrr4.json
    metrics_ctdk6hr_.json
    metrics_d0zio68n.json
    metrics_d2fneb6c.json
    metrics_d83v0qrg.json
    metrics_d_7cu4co.json
    metrics_e55ccqf4.json
    metrics_efewdoso.json
    metrics_es10kbkj.json
    metrics_exyc2aoi.json
    metrics_f4ba8yj0.json
    metrics_fusrtchw.json
    metrics_fvkjx3i3.json
    metrics_fwgrd7ek.json
    metrics_fx59gkw6.json
    metrics_gctnecas.json
    metrics_gdrsfom2.json
    metrics_gh0exqfy.json
    metrics_gn3r368k.json
    metrics_gre0l2vv.json
    metrics_gu2jj4x6.json
    metrics_gy7yzni_.json
    metrics_h2ke1que.json
    metrics_h2zbzsi6.json
    metrics_i5pwbdy8.json
    metrics_i9ixee1p.json
    metrics_iegm05ma.json
    metrics_jeul60z4.json
    metrics_jldkp6oj.json
    metrics_joyqy1yz.json
    metrics_k452r_bd.json
    metrics_keyuq093.json
    metrics_kijozz54.json
    metrics_l0nrt8ax.json
    metrics_l4qkqhvl.json
    metrics_l9a3g7f_.json
    metrics_lg3z_ojj.json
    metrics_lij48idf.json
    metrics_mfx0u92s.json
    metrics_ml129ig1.json
    metrics_mziwmuji.json
    metrics_mzkihnhl.json
    metrics_n92kltdx.json
    metrics_naa4pb8_.json
    metrics_o0bwp99t.json
    metrics_oe0gx0ow.json
    metrics_og6_x57t.json
    metrics_okatezba.json
    metrics_olwp427j.json
    metrics_oy5jty8q.json
    metrics_pbfm5_s8.json
    metrics_pguw1jt8.json
    metrics_q1mfycvq.json
    metrics_q32vexb9.json
    metrics_qd9sdimy.json
    metrics_qmjxkoxo.json
    metrics_qygbv_em.json
    metrics_r1diayz3.json
    metrics_r1oyukcw.json
    metrics_r2iljktq.json
    metrics_r4cxpbus.json
    metrics_ri5kzye8.json
    metrics_smrtxq4q.json
    metrics_sunrm50l.json
    metrics_szni51cl.json
    metrics_szy5o819.json
    metrics_s_yeu75a.json
    metrics_t09rkk69.json
    metrics_tnyuui1p.json
    metrics_tvk5w34y.json
    metrics_u85k0bty.json
    metrics_ue987r_m.json
    metrics_ugsv8j1h.json
    metrics_uhoep47z.json
    metrics_uiqkfigp.json
    metrics_ujl1qfgc.json
    metrics_v30su220.json
    metrics_v372j6bp.json
    metrics_vd8t6g2k.json
    metrics_vi859rsu.json
    metrics_vlx06rwe.json
    metrics_vm46jhw3.json
    metrics_vzimyuwy.json
    metrics_w88v4g45.json
    metrics_wcsasy33.json
    metrics_x7rwrpb_.json
    metrics_xbjh179p.json
    metrics_xcr4m91b.json
    metrics_xcyd97h5.json
    metrics_xe7x_sje.json
    metrics_xjeelk50.json
    metrics_xqnkdh58.json
    metrics_xuw93ulk.json
    metrics_xxx7__b9.json
    metrics_y1mt7hlm.json
    metrics_y5_m62os.json
    metrics_yehnyu4c.json
    metrics_yuncl873.json
    metrics_yv04iu3_.json
    metrics_z4d75mrh.json
    metrics_zn9wzer_.json
    metrics_zrz0v00o.json
    metrics_zskuik5i.json
    metrics__7n81rbm.json
    metrics__9tk6o8d.json
    metrics__cx1yvq5.json
    metrics__g0tzwco.json
    metrics__g6j76yc.json
    metrics__l9umv1h.json
    metrics__nrd_y1k.json
    metrics__rjbfcpd.json
    metrics__s4hfi2b.json
    metrics__tv_40i0.json
    metrics__vtagxy_.json
    metrics___1o6ki2.json
    trade_state.json
  scripts/
    cb_smoke.py
    diagnose_symbol.py
    dryrun_smoke.py
    export_mt5_history.py
    export_val_probs.py
    make_csv_from_mt5.py
    make_project_snapshot.py
    make_toy_model.py
    mt5_export_csv.py
    mt5_smoke.py
    print_runtime.py
    promote_model.py
    register_weekly_task.ps1
    rollback_model.py
    selftest_mt5.py
    selftest_order_flow.py
    sim_trailing.py
    swap_model.py
    train_calibrator.py
    verify_smoke.ps1
    walkforward_retrain.py
    walkforward_train.py
    weekly_retrain.py
    weekly_wf.ps1
  tests/
    test_sanity.py
  tools/
    backtest_equity_curve.py
    backtest_run.py
    dump_feature_importance.py
    export_tree_clean.ps1
    inspect_report.py
    list_wfo_reports.py
    train_lightgbm.py
    train_scaler.py
    __init__.py
    __pycache__/
      backtest_run.cpython-313.pyc
      __init__.cpython-313.pyc
  è¨­å®š/
    ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ.txt
    ä»®.txt
    ä»®æƒ³ç’°å¢ƒ.txt



=== file: pyproject.toml ===

[tool.ruff]
target-version = "py313"
line-length = 88

[tool.ruff.lint]
select = ["E", "F", "I", "UP", "ARG", "PT", "PL"]
ignore = ["E501"]



=== file: pytest.ini ===

[pytest]
minversion = 8.0
addopts = -q -ra --maxfail=1 --durations=10
testpaths =
    tests



=== file: README.md ===

ï»¿# FX AI Bot (Python + MT5 + PyQt6)
Python 3.13 / PyQt6 / MetaTrader5

## ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ä»®æƒ³ç’°å¢ƒã®ä½œæˆã¨æœ‰åŠ¹åŒ–

```powershell
# Windows PowerShell
python -m venv venv
.\venv\Scripts\Activate.ps1

# WSL / Linux / macOS
python3 -m venv venv
source venv/bin/activate
```

### ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```powershell
pip install -r requirements.txt
```

## ãƒ‡ãƒ¼ã‚¿æ›´æ–°æ‰‹é †

### æ¦‚è¦

MT5 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ OHLCVï¼ˆUSDJPYï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’ CSV ã¸ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

**3å°ã® PCï¼ˆé–‹ç™ºA / B / é‹ç”¨VPSï¼‰ã§ä¸€è²«ã—ãŸå‡ºåŠ›å…ˆã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°è¨­å®šã‚’å¿…é ˆã¨ã—ã¾ã™ã€‚**

### ã‚¹ãƒ†ãƒƒãƒ—1: ç’°å¢ƒå¤‰æ•° `FXBOT_DATA` ã®è¨­å®š

ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ã€‚

#### æ–¹æ³•A: é–‹ç™ºç’°å¢ƒï¼ˆ.env ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã« `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã‚’è¨˜è¿°ï¼š

```bash
# .env
FXBOT_DATA=C:\Users\macht\OneDrive\fxbot\data
```

**ãƒ¡ãƒªãƒƒãƒˆ**ï¼šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¯ã«è¨­å®šå¯èƒ½ã€gitç®¡ç†å¤–ï¼ˆ`.gitignore` ã«è¨˜è¼‰ï¼‰

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**ï¼šã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚ã« python-dotenv ã§èª­ã¿è¾¼ã‚€å¿…è¦ãŒã‚ã‚‹

#### æ–¹æ³•B: æœ¬ç•ªç’°å¢ƒï¼ˆsetx ã‚³ãƒãƒ³ãƒ‰ / ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒå¤‰æ•°ï¼‰

PowerShell ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã«æ°¸ç¶šåŒ–ï¼š

```powershell
# é‹ç”¨VPS ã®å ´åˆ
setx FXBOT_DATA "C:\fxbot\data"

# ã¾ãŸã¯ OneDrive ãƒ‘ã‚¹ã®å ´åˆ
setx FXBOT_DATA "C:\Users\macht\OneDrive\fxbot\data"
```

**ãƒ¡ãƒªãƒƒãƒˆ**ï¼šæ°¸ç¶šåŒ–ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¯ã‚¤ãƒ‰ã§æœ‰åŠ¹ï¼ˆå†ãƒ­ã‚°ã‚¤ãƒ³å¾Œï¼‰

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**ï¼šç®¡ç†è€…æ¨©é™ãŒå¿…è¦ã€å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¦‹ãˆã‚‹å¯èƒ½æ€§ã‚ã‚Š

#### æ–¹æ³•C: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç’°å¢ƒå¤‰æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰

PowerShell ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ã®ã¿æœ‰åŠ¹ï¼š

```powershell
$env:FXBOT_DATA = 'C:\Users\macht\OneDrive\fxbot\data'
```

**ç”¨é€”**ï¼šãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ç”¨ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«æ¶ˆãˆã‚‹ï¼‰

### ã‚¹ãƒ†ãƒƒãƒ—2: MT5 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã®æº–å‚™

- MetaTrader 5 ã‚’èµ·å‹•
- å£åº§ã«ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å / ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ ã¯ `config.yaml` ã® `mt5_login` / `mt5_password` ã§æŒ‡å®šï¼‰
- ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’èµ·å‹•ã—ãŸã¾ã¾ã«ã—ã¦ãŠãï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¸­ã«ä½¿ç”¨ï¼‰

### ã‚¹ãƒ†ãƒƒãƒ—3: CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ USDJPY ã®è¤‡æ•°ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆM5, M15, H1 ç­‰ï¼‰ã‚’å–å¾—ï¼š

#### åŸºæœ¬çš„ãªå®Ÿè¡Œæ–¹æ³•

```powershell
# ç’°å¢ƒå¤‰æ•° FXBOT_DATA ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆæ¨å¥¨ï¼‰
python scripts\make_csv_from_mt5.py --symbol USDJPY --timeframes M5 M15 H1 --start 2020-11-01

# ã¾ãŸã¯ --data-dir ã§æ˜ç¤ºæŒ‡å®š
python scripts\make_csv_from_mt5.py --symbol USDJPY --timeframes M5 M15 H1 --data-dir C:\Users\macht\OneDrive\fxbot\data
```

#### å‡ºåŠ›å…ˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæŒ‡å®š

`--layout` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ä¿å­˜å…ˆæ§‹é€ ã‚’é¸æŠï¼š

```powershell
# per-symbol: é€šè²¨ãƒšã‚¢æ¯ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ¨å¥¨ï¼‰
python scripts\make_csv_from_mt5.py --symbol USDJPY --timeframes M5 M15 H1 --layout per-symbol
# å‡ºåŠ›å…ˆ: <FXBOT_DATA>/USDJPY/ohlcv/USDJPY_M5.csv ãªã©

# flat: data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹
python scripts\make_csv_from_mt5.py --symbol USDJPY --timeframes M5 M15 H1 --layout flat
# å‡ºåŠ›å…ˆ: <FXBOT_DATA>/USDJPY_M5.csv ãªã©
```

#### MT5 terminal.exe ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¹æŒ‡å®š

GaitameFinestç­‰ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’ä½¿ã†å ´åˆï¼š

```powershell
python scripts\make_csv_from_mt5.py \
  --symbol USDJPY \
  --timeframes M5 M15 H1 \
  --terminal "C:\Program Files\MetaTrader 5\terminal64.exe"
```

### ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª

ç”Ÿæˆã•ã‚ŒãŸ CSV ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚ŒãŸã‹ç¢ºèªï¼š

```powershell
# per-symbol ã®å ´åˆ
python -c "import pandas as pd; df = pd.read_csv('data/USDJPY/ohlcv/USDJPY_M5.csv', parse_dates=['time']); print(f'Rows: {len(df)}, From: {df[\"time\"].min()}, To: {df[\"time\"].max()}')"

# ã¾ãŸã¯ fxbot_path ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨
python -c "import pandas as pd; from fxbot_path import get_ohlcv_csv_path; p = get_ohlcv_csv_path('USDJPY','M5'); df = pd.read_csv(p, parse_dates=['time']); print(f'Rows: {len(df)}, From: {df[\"time\"].min()}, To: {df[\"time\"].max()}')"
```

### ãƒ‘ã‚¹è§£æ±ºã®å„ªå…ˆé †ä½

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®é †åºã§ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®šã—ã¾ã™ï¼ˆå…ˆè¡Œã™ã‚‹ã‚‚ã®ãŒå„ªå…ˆï¼‰ï¼š

1. `--data-dir` ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
2. `FXBOT_DATA` ç’°å¢ƒå¤‰æ•°
3. `<ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ>/data`
4. `<ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª>/data`

æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ã€ãã‚Œã‚’ä½¿ç”¨ã—ã¾ã™ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã® `data` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆãƒ»ä½¿ç”¨ã—ã¾ã™ã€‚

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### MT5 æ¥ç¶šã‚¨ãƒ©ãƒ¼
```
[fatal] MT5 initialize å¤±æ•—: ...
```
**è§£æ±ºæ–¹æ³•**ï¼š
- MetaTrader 5 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
- ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å£åº§ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ã‚‹ã‹ç¢ºèª
- `--terminal` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãƒ‘ã‚¹ã‚’æ˜ç¤ºã—ã¦ã¿ã‚‹

#### CSV ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„
```
FileNotFoundError: data/USDJPY/ohlcv/USDJPY_M5.csv
```
**è§£æ±ºæ–¹æ³•**ï¼š
- `FXBOT_DATA` ç’°å¢ƒå¤‰æ•°ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- `python -c "from fxbot_path import get_ohlcv_csv_path; print(get_ohlcv_csv_path('USDJPY','M5'))"` ã§ãƒ‘ã‚¹ã‚’ç¢ºèª
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚‹ã‹ç¢ºèªï¼ˆOneDrive ã®åŒæœŸçŠ¶æ…‹ã‚‚ç¢ºèªï¼‰

#### æ—¢å­˜ CSV ã¸ã®è¿½è¨˜ã«å¤±æ•—ã™ã‚‹
```
ValueError: time column not found
```
**è§£æ±ºæ–¹æ³•**ï¼š
- æ—¢å­˜ CSV ã®å½¢å¼ãŒæ­£ã—ã„ã‹ç¢ºèªï¼ˆã‚«ãƒ©ãƒ å: time, open, high, low, close ãªã©ï¼‰
- ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å†åº¦å®Ÿè¡Œã™ã‚Œã°ã€æ–°è¦ CSV ã¨ã—ã¦å†ä½œæˆã•ã‚Œã¾ã™

## ãƒ‘ã‚¹ç®¡ç†ï¼ˆfxbot_path.pyï¼‰

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã§å…±é€šçš„ã«ä½¿ç”¨ã™ã‚‹ `fxbot_path.py` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒä»¥ä¸‹ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’æä¾›ã—ã¾ã™ï¼š

- `get_project_root()`: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼ˆfxbot_path.py ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰ã‚’è¿”ã™
- `get_data_root(cli_data_dir=None)`: ãƒ‡ãƒ¼ã‚¿ãƒ«ãƒ¼ãƒˆã‚’å„ªå…ˆé †ä½ä»˜ãã§æ±ºå®š
- `get_ohlcv_csv_path(symbol, timeframe, data_root=None, layout='per-symbol')`: OHLCV CSV ã®ãƒ‘ã‚¹ã‚’çµ±ä¸€çš„ã«ç”Ÿæˆ

ã“ã‚Œã‚‰ã‚’ä»–ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€3å°ã®PCé–“ã§ã®ãƒ‘ã‚¹ä¸çµ±ä¸€ã‚’é˜²ãã¾ã™ã€‚

### ä½¿ç”¨ä¾‹

```python
from fxbot_path import get_ohlcv_csv_path
import pandas as pd

# USDJPY M5 ã® CSV ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆè‡ªå‹•çš„ã« FXBOT_DATA ç’°å¢ƒå¤‰æ•°ã‚’å‚ç…§ï¼‰
csv_path = get_ohlcv_csv_path('USDJPY', 'M5')
df = pd.read_csv(csv_path)
print(df.head())
```




=== file: requirements.txt ===

annotated-types==0.7.0
black==25.11.0
cfgv==3.4.0
click==8.3.1
cloudpickle==3.1.2
colorama==0.4.6
contourpy==1.3.3
cycler==0.12.1
distlib==0.4.0
filelock==3.20.0
fonttools==4.60.1
identify==2.6.15
joblib==1.5.2
kiwisolver==1.4.9
lightgbm==4.6.0
llvmlite==0.45.1
loguru==0.7.3
matplotlib==3.10.7
metatrader5==5.0.5430
mypy==1.18.2
mypy_extensions==1.1.0
nodeenv==1.9.1
numba==0.62.1
numpy==2.3.5
packaging==25.0
pandas==2.3.3
pathspec==0.12.1
pillow==12.0.0
platformdirs==4.5.0
pre_commit==4.4.0
pydantic==2.12.4
pydantic_core==2.41.5
pyparsing==3.2.5
PyQt6==6.10.0
PyQt6-Qt6==6.10.0
PyQt6_sip==13.10.2
pyqtgraph==0.14.0
python-dateutil==2.9.0.post0
python-dotenv==1.2.1
pytokens==0.3.0
pytz==2025.2
PyYAML==6.0.3
ruff==0.14.5
scikit-learn==1.7.2
scipy==1.16.3
shap==0.50.0
six==1.17.0
slicer==0.0.8
threadpoolctl==3.6.0
tqdm==4.67.1
typing-inspection==0.4.2
typing_extensions==4.15.0
tzdata==2025.2
virtualenv==20.35.4
win32_setctime==1.2.0
xgboost==3.1.1



=== file: ruff.toml ===

line-length = 88
target-version = "py311"

# .venv ã‚„çµæœãƒ•ã‚©ãƒ«ãƒ€ã¯ãƒªãƒ³ãƒ†ã‚£ãƒ³ã‚°å¯¾è±¡å¤–
extend-exclude = [
    ".venv",
    "backtests",
    "models",
    "logs",
    "dist",
    "build",
]

[lint]
# ãƒ™ãƒ¼ã‚·ãƒƒã‚¯ãª lint + ãƒã‚°æ¤œå‡º + import æ•´ç† + è¿‘ä»£åŒ– + ã‚·ãƒ³ãƒ—ãƒ«åŒ–
select = [
    "E",  # pycodestyle (PEP8)
    "F",  # pyflakes
    "W",  # ä¸€èˆ¬çš„ãªè­¦å‘Š
    "B",  # flake8-bugbearï¼ˆãƒã‚°ã«ãªã‚Šãã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    "I",  # isortï¼ˆimport ä¸¦ã¹æ›¿ãˆï¼‰
    "UP", # pyupgradeï¼ˆæœ€æ–°æ§‹æ–‡ã¸ï¼‰
    "SIM" # flake8-simplifyï¼ˆæ›¸ãæ–¹ã®ç°¡ç•¥åŒ–ï¼‰
]

# è¡Œé•·ã¯ black ã«ä»»ã›ã‚‹ã®ã§ E501 ã¯ç„¡è¦–
ignore = ["E501"]

[lint.isort]
known-first-party = ["app"]
combine-as-imports = true



=== file: runtime/metrics.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100885,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.70719,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":5,"count_skip":0,"count_blocked":2,"ts":1763172594}


=== file: runtime/metrics_044ho80o.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0002159,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":24.25421,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1763017407}


=== file: runtime/metrics_04i6g_bm.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00070439,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.05378,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":3,"count_skip":0,"count_blocked":2,"ts":1763017422}


=== file: runtime/metrics_08lb8guj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.23212,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":158,"count_skip":0,"count_blocked":2,"ts":1763018216}


=== file: runtime/metrics_0fdsym9v.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00171831,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.10776,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":67,"count_skip":0,"count_blocked":2,"ts":1763017750}


=== file: runtime/metrics_0ks5iv74.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00101968,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.34686,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":58,"count_skip":0,"count_blocked":2,"ts":1763017704}


=== file: runtime/metrics_0m3pzxsa.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00106303,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.68062,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":39,"count_skip":0,"count_blocked":2,"ts":1763017606}


=== file: runtime/metrics_0s0y1mq_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.51373,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":101,"count_skip":0,"count_blocked":2,"ts":1763017924}


=== file: runtime/metrics_0so3t4e4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.0778,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":63,"count_skip":0,"count_blocked":2,"ts":1763017729}


=== file: runtime/metrics_0t6las4q.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00145723,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.05805,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":160,"count_skip":0,"count_blocked":2,"ts":1763018226}


=== file: runtime/metrics_13d1atob.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.15756,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":147,"count_skip":0,"count_blocked":2,"ts":1763018159}


=== file: runtime/metrics_1_r7f8xe.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.87851,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":81,"count_skip":0,"count_blocked":2,"ts":1763017822}


=== file: runtime/metrics_1ce8in0z.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00151429,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.66906,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":66,"count_skip":0,"count_blocked":2,"ts":1763017745}


=== file: runtime/metrics_1f8wcifb.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.96455,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":79,"count_skip":0,"count_blocked":2,"ts":1763017811}


=== file: runtime/metrics_1nqq6c9g.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.14061,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":120,"count_skip":0,"count_blocked":2,"ts":1763018021}


=== file: runtime/metrics_1y_j88c6.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.74186,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":26,"count_skip":0,"count_blocked":2,"ts":1763017540}


=== file: runtime/metrics_2b5irl55.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00135831,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.30994,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":151,"count_skip":0,"count_blocked":2,"ts":1763018180}


=== file: runtime/metrics_2llsmgld.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.56017,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":44,"count_skip":0,"count_blocked":2,"ts":1763017632}


=== file: runtime/metrics_2wh_r543.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00147993,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.35101,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":47,"count_skip":0,"count_blocked":2,"ts":1763017647}


=== file: runtime/metrics_32ej_3qs.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00133374,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.39766,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":38,"count_skip":0,"count_blocked":2,"ts":1763017601}


=== file: runtime/metrics_384xt0qj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.69095,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":54,"count_skip":0,"count_blocked":2,"ts":1763017683}


=== file: runtime/metrics_3_z7j5dq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.5398,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":91,"count_skip":0,"count_blocked":2,"ts":1763017873}


=== file: runtime/metrics_3op0smhm.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.10852,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":118,"count_skip":0,"count_blocked":2,"ts":1763018011}


=== file: runtime/metrics_3pck01qi.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":4,"ts":1762671546}


=== file: runtime/metrics_3pw7kppo.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762671540}


=== file: runtime/metrics_3w4e28bl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.34986,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":144,"count_skip":0,"count_blocked":2,"ts":1763018144}


=== file: runtime/metrics_3wwdl7sf.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.40584,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":156,"count_skip":0,"count_blocked":2,"ts":1763018206}


=== file: runtime/metrics_3xjcntun.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":3,"ts":1762671543}


=== file: runtime/metrics_4fx6u97k.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.37357,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":134,"count_skip":0,"count_blocked":2,"ts":1763018093}


=== file: runtime/metrics_4jkcw838.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.65645,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":157,"count_skip":0,"count_blocked":2,"ts":1763018211}


=== file: runtime/metrics_4jwlaw79.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.99009,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":32,"count_skip":0,"count_blocked":2,"ts":1763017571}


=== file: runtime/metrics_4sqd0ccq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.57861,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":89,"count_skip":0,"count_blocked":2,"ts":1763017862}


=== file: runtime/metrics_4u83ovxn.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100885,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.70719,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":5,"count_skip":0,"count_blocked":2,"ts":1763172594}


=== file: runtime/metrics_51fzibz0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00038179,"atr_gate_state":"open","post_fill_grace":false,"spread":0.5,"adx":27.35757,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":1,"count_skip":0,"count_blocked":2,"ts":1763172574}


=== file: runtime/metrics_56tbpp98.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.37376,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":90,"count_skip":0,"count_blocked":2,"ts":1763017868}


=== file: runtime/metrics_5_uus6ny.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.42191,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":61,"count_skip":0,"count_blocked":2,"ts":1763017719}


=== file: runtime/metrics_5c1keku_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.10247,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":71,"count_skip":0,"count_blocked":2,"ts":1763017770}


=== file: runtime/metrics_681i8dz0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00054309,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.10884,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":2,"count_skip":0,"count_blocked":2,"ts":1763172579}


=== file: runtime/metrics_6g2ygykl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00153604,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.61205,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":56,"count_skip":0,"count_blocked":2,"ts":1763017693}


=== file: runtime/metrics_6jx0f61y.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00120359,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.95706,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":159,"count_skip":0,"count_blocked":2,"ts":1763018221}


=== file: runtime/metrics_6mv45wka.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.8547,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":62,"count_skip":0,"count_blocked":2,"ts":1763017724}


=== file: runtime/metrics_6n99dteb.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00176191,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.41532,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":36,"count_skip":0,"count_blocked":2,"ts":1763017591}


=== file: runtime/metrics_6oizo1ba.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00158023,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.98339,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":104,"count_skip":0,"count_blocked":2,"ts":1763017939}


=== file: runtime/metrics_6u8fjy4e.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.31557,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":100,"count_skip":0,"count_blocked":2,"ts":1763017919}


=== file: runtime/metrics_751tqi7x.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0002159,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":24.25421,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1763172569}


=== file: runtime/metrics_78ji6p4u.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00125571,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.0775,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":76,"count_skip":0,"count_blocked":2,"ts":1763017796}


=== file: runtime/metrics_7bbmsyvw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00153035,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.10197,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":9,"count_skip":0,"count_blocked":2,"ts":1763017453}


=== file: runtime/metrics_7caerxlw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.55136,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":128,"count_skip":0,"count_blocked":2,"ts":1763018062}


=== file: runtime/metrics_7pcppci4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00167187,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.75221,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":161,"count_skip":0,"count_blocked":2,"ts":1763018231}


=== file: runtime/metrics_7upf8_is.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.05466,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":45,"count_skip":0,"count_blocked":2,"ts":1763017637}


=== file: runtime/metrics_7wtig7zq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0014214,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":38.18061,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":141,"count_skip":0,"count_blocked":2,"ts":1763018129}


=== file: runtime/metrics_7yx4f9p_.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762729397}


=== file: runtime/metrics_8i0ku3__.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":5e-05,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":21.32509,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1763172564}


=== file: runtime/metrics_8iecfllz.json ===

{"last_decision":"TEST","adx":12.3,"min_adx":20,"ts":1762510822}


=== file: runtime/metrics_8s427o_h.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.73688,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":42,"count_skip":0,"count_blocked":2,"ts":1763017622}


=== file: runtime/metrics_8t5wcv0v.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.43208,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":129,"count_skip":0,"count_blocked":2,"ts":1763018067}


=== file: runtime/metrics_8wbixgeh.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.78984,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":70,"count_skip":0,"count_blocked":2,"ts":1763017765}


=== file: runtime/metrics_9clf0tik.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00165111,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.72461,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":112,"count_skip":0,"count_blocked":2,"ts":1763017980}


=== file: runtime/metrics_9wqc4wjf.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.30769,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":138,"count_skip":0,"count_blocked":2,"ts":1763018113}


=== file: runtime/metrics_9y2xjt1a.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":7,"ts":1762729427}


=== file: runtime/metrics__7n81rbm.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":3,"ts":1762729403}


=== file: runtime/metrics__9tk6o8d.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.49746,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":127,"count_skip":0,"count_blocked":2,"ts":1763018057}


=== file: runtime/metrics___1o6ki2.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.37609,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":33,"count_skip":0,"count_blocked":2,"ts":1763017576}


=== file: runtime/metrics__cx1yvq5.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00162085,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.46151,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":131,"count_skip":0,"count_blocked":2,"ts":1763018078}


=== file: runtime/metrics__g0tzwco.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.46993,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":23,"count_skip":0,"count_blocked":2,"ts":1763017524}


=== file: runtime/metrics__g6j76yc.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00129511,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.0365,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":57,"count_skip":0,"count_blocked":2,"ts":1763017699}


=== file: runtime/metrics__l9umv1h.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180397,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.21146,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":162,"count_skip":0,"count_blocked":2,"ts":1763018236}


=== file: runtime/metrics__nrd_y1k.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00138471,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.46285,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":122,"count_skip":0,"count_blocked":2,"ts":1763018031}


=== file: runtime/metrics__rjbfcpd.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.19516,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":80,"count_skip":0,"count_blocked":2,"ts":1763017816}


=== file: runtime/metrics__s4hfi2b.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00113303,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.14893,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":133,"count_skip":0,"count_blocked":2,"ts":1763018088}


=== file: runtime/metrics__tv_40i0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.20587,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":64,"count_skip":0,"count_blocked":2,"ts":1763017734}


=== file: runtime/metrics__vtagxy_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.38751,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":34,"count_skip":0,"count_blocked":2,"ts":1763017581}


=== file: runtime/metrics_a4qsce4c.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.85332,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":14,"count_skip":0,"count_blocked":2,"ts":1763017478}


=== file: runtime/metrics_a_jw5ekj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.15562,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":35,"count_skip":0,"count_blocked":2,"ts":1763017586}


=== file: runtime/metrics_abcuwh_8.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":5,"ts":1762729415}


=== file: runtime/metrics_aen0sw0t.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00143202,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.52959,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":113,"count_skip":0,"count_blocked":2,"ts":1763017985}


=== file: runtime/metrics_avlwot5z.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00168037,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.43589,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":93,"count_skip":0,"count_blocked":2,"ts":1763017883}


=== file: runtime/metrics_b_e39daa.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.08087,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":116,"count_skip":0,"count_blocked":2,"ts":1763018001}


=== file: runtime/metrics_beh8fq83.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.85148,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":16,"count_skip":0,"count_blocked":2,"ts":1763017489}


=== file: runtime/metrics_bkm4j_2_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0012697,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.46397,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":65,"count_skip":0,"count_blocked":2,"ts":1763017740}


=== file: runtime/metrics_bmocgusp.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00161176,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.48796,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":123,"count_skip":0,"count_blocked":2,"ts":1763018037}


=== file: runtime/metrics_bsejl6ul.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.67972,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":82,"count_skip":0,"count_blocked":2,"ts":1763017827}


=== file: runtime/metrics_c19f7jhr.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0011478,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.53105,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":6,"count_skip":0,"count_blocked":2,"ts":1763017437}


=== file: runtime/metrics_c6dlu7j3.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.6403,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":60,"count_skip":0,"count_blocked":2,"ts":1763017714}


=== file: runtime/metrics_c7srgc6l.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100885,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.70719,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":5,"count_skip":0,"count_blocked":2,"ts":1763017432}


=== file: runtime/metrics_cfvm2vuv.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00134718,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.03312,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":103,"count_skip":0,"count_blocked":2,"ts":1763017934}


=== file: runtime/metrics_cs6cyrr4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.75231,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":115,"count_skip":0,"count_blocked":2,"ts":1763017996}


=== file: runtime/metrics_ctdk6hr_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.70205,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":119,"count_skip":0,"count_blocked":2,"ts":1763018016}


=== file: runtime/metrics_d0zio68n.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.05924,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":139,"count_skip":0,"count_blocked":2,"ts":1763018118}


=== file: runtime/metrics_d2fneb6c.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.45621,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":153,"count_skip":0,"count_blocked":2,"ts":1763018190}


=== file: runtime/metrics_d83v0qrg.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.51296,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":88,"count_skip":0,"count_blocked":2,"ts":1763017857}


=== file: runtime/metrics_d_7cu4co.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00150227,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":23.88217,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":75,"count_skip":0,"count_blocked":2,"ts":1763017791}


=== file: runtime/metrics_e55ccqf4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00146759,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.57701,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":94,"count_skip":0,"count_blocked":2,"ts":1763017888}


=== file: runtime/metrics_efewdoso.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.1878,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":108,"count_skip":0,"count_blocked":2,"ts":1763017960}


=== file: runtime/metrics_es10kbkj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.1606,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":126,"count_skip":0,"count_blocked":2,"ts":1763018052}


=== file: runtime/metrics_exyc2aoi.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00107818,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.16007,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":102,"count_skip":0,"count_blocked":2,"ts":1763017929}


=== file: runtime/metrics_f4ba8yj0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.3132,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":49,"count_skip":0,"count_blocked":2,"ts":1763017658}


=== file: runtime/metrics_fusrtchw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.3179,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":107,"count_skip":0,"count_blocked":2,"ts":1763017955}


=== file: runtime/metrics_fvkjx3i3.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00179554,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.45145,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":124,"count_skip":0,"count_blocked":2,"ts":1763018042}


=== file: runtime/metrics_fwgrd7ek.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.50361,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":117,"count_skip":0,"count_blocked":2,"ts":1763018006}


=== file: runtime/metrics_fx59gkw6.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.61258,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":125,"count_skip":0,"count_blocked":2,"ts":1763018047}


=== file: runtime/metrics_gctnecas.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00177087,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.98795,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":105,"count_skip":0,"count_blocked":2,"ts":1763017944}


=== file: runtime/metrics_gdrsfom2.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.09115,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":111,"count_skip":0,"count_blocked":2,"ts":1763017975}


=== file: runtime/metrics_gh0exqfy.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.59901,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":12,"count_skip":0,"count_blocked":2,"ts":1763017468}


=== file: runtime/metrics_gn3r368k.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":4,"ts":1762729409}


=== file: runtime/metrics_gre0l2vv.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":5e-05,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":21.32509,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1763017402}


=== file: runtime/metrics_gu2jj4x6.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762671193}


=== file: runtime/metrics_gy7yzni_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00085662,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.21134,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":4,"count_skip":0,"count_blocked":2,"ts":1763172589}


=== file: runtime/metrics_h2ke1que.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.78668,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":96,"count_skip":0,"count_blocked":2,"ts":1763017898}


=== file: runtime/metrics_h2zbzsi6.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00164232,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.56873,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":142,"count_skip":0,"count_blocked":2,"ts":1763018134}


=== file: runtime/metrics_i5pwbdy8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00137155,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.6944,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":19,"count_skip":0,"count_blocked":2,"ts":1763017504}


=== file: runtime/metrics_i9ixee1p.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.63559,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":24,"count_skip":0,"count_blocked":2,"ts":1763017530}


=== file: runtime/metrics_iegm05ma.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.34749,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":43,"count_skip":0,"count_blocked":2,"ts":1763017627}


=== file: runtime/metrics_jeul60z4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00154773,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.27046,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":85,"count_skip":0,"count_blocked":2,"ts":1763017842}


=== file: runtime/metrics_jldkp6oj.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":8,"ts":1762729433}


=== file: runtime/metrics_joyqy1yz.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.12183,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":73,"count_skip":0,"count_blocked":2,"ts":1763017781}


=== file: runtime/metrics_k452r_bd.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762671537}


=== file: runtime/metrics_keyuq093.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.27232,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":15,"count_skip":0,"count_blocked":2,"ts":1763017483}


=== file: runtime/metrics_kijozz54.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.82456,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":87,"count_skip":0,"count_blocked":2,"ts":1763017852}


=== file: runtime/metrics_l0nrt8ax.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.27612,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":21,"count_skip":0,"count_blocked":2,"ts":1763017514}


=== file: runtime/metrics_l4qkqhvl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.88215,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":92,"count_skip":0,"count_blocked":2,"ts":1763017878}


=== file: runtime/metrics_l9a3g7f_.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762729391}


=== file: runtime/metrics_lg3z_ojj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00140854,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.97584,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":8,"count_skip":0,"count_blocked":2,"ts":1763017448}


=== file: runtime/metrics_lij48idf.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.69698,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":154,"count_skip":0,"count_blocked":2,"ts":1763018195}


=== file: runtime/metrics_mfx0u92s.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.5493,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":30,"count_skip":0,"count_blocked":2,"ts":1763017560}


=== file: runtime/metrics_ml129ig1.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.15362,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":51,"count_skip":0,"count_blocked":2,"ts":1763017668}


=== file: runtime/metrics_mziwmuji.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00166155,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.83636,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":29,"count_skip":0,"count_blocked":2,"ts":1763017555}


=== file: runtime/metrics_mzkihnhl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.02933,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":31,"count_skip":0,"count_blocked":2,"ts":1763017565}


=== file: runtime/metrics_n92kltdx.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.83382,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":97,"count_skip":0,"count_blocked":2,"ts":1763017903}


=== file: runtime/metrics_naa4pb8_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.83045,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":69,"count_skip":0,"count_blocked":2,"ts":1763017760}


=== file: runtime/metrics_o0bwp99t.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.67833,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":53,"count_skip":0,"count_blocked":2,"ts":1763017678}


=== file: runtime/metrics_oe0gx0ow.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00144468,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.68122,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":28,"count_skip":0,"count_blocked":2,"ts":1763017550}


=== file: runtime/metrics_og6_x57t.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.45723,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":52,"count_skip":0,"count_blocked":2,"ts":1763017673}


=== file: runtime/metrics_okatezba.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.33992,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":72,"count_skip":0,"count_blocked":2,"ts":1763017775}


=== file: runtime/metrics_olwp427j.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.07399,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":146,"count_skip":0,"count_blocked":2,"ts":1763018154}


=== file: runtime/metrics_oy5jty8q.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00038179,"atr_gate_state":"open","post_fill_grace":false,"spread":0.5,"adx":27.35757,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":1,"count_skip":0,"count_blocked":2,"ts":1763017412}


=== file: runtime/metrics_pbfm5_s8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.85726,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":106,"count_skip":0,"count_blocked":2,"ts":1763017949}


=== file: runtime/metrics_pguw1jt8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00156887,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.49224,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":37,"count_skip":0,"count_blocked":2,"ts":1763017596}


=== file: runtime/metrics_q1mfycvq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.36631,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":68,"count_skip":0,"count_blocked":2,"ts":1763017755}


=== file: runtime/metrics_q32vexb9.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00054309,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.10884,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":2,"count_skip":0,"count_blocked":2,"ts":1763017417}


=== file: runtime/metrics_qd9sdimy.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.28033,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":98,"count_skip":0,"count_blocked":2,"ts":1763017909}


=== file: runtime/metrics_qmjxkoxo.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.12556,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":25,"count_skip":0,"count_blocked":2,"ts":1763017535}


=== file: runtime/metrics_qygbv_em.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00128674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.18961,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":7,"count_skip":0,"count_blocked":2,"ts":1763017442}


=== file: runtime/metrics_r1diayz3.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762671779}


=== file: runtime/metrics_r1oyukcw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.51121,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":130,"count_skip":0,"count_blocked":2,"ts":1763018072}


=== file: runtime/metrics_r2iljktq.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762671785}


=== file: runtime/metrics_r4cxpbus.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.72554,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":78,"count_skip":0,"count_blocked":2,"ts":1763017806}


=== file: runtime/metrics_ri5kzye8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.47894,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":59,"count_skip":0,"count_blocked":2,"ts":1763017709}


=== file: runtime/metrics_s_yeu75a.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.3389,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":13,"count_skip":0,"count_blocked":2,"ts":1763017473}


=== file: runtime/metrics_smrtxq4q.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00158961,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.15592,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":150,"count_skip":0,"count_blocked":2,"ts":1763018175}


=== file: runtime/metrics_sunrm50l.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.88573,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":109,"count_skip":0,"count_blocked":2,"ts":1763017965}


=== file: runtime/metrics_szni51cl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00173578,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":23.25067,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":55,"count_skip":0,"count_blocked":2,"ts":1763017688}


=== file: runtime/metrics_szy5o819.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.35016,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":41,"count_skip":0,"count_blocked":2,"ts":1763017617}


=== file: runtime/metrics_t09rkk69.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00109075,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.86342,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":152,"count_skip":0,"count_blocked":2,"ts":1763018185}


=== file: runtime/metrics_tnyuui1p.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.37394,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":148,"count_skip":0,"count_blocked":2,"ts":1763018165}


=== file: runtime/metrics_tvk5w34y.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00169045,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.05573,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":48,"count_skip":0,"count_blocked":2,"ts":1763017652}


=== file: runtime/metrics_u85k0bty.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00163165,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.93238,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":10,"count_skip":0,"count_blocked":2,"ts":1763017458}


=== file: runtime/metrics_ue987r_m.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.6835,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":145,"count_skip":0,"count_blocked":2,"ts":1763018149}


=== file: runtime/metrics_ugsv8j1h.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.49403,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":22,"count_skip":0,"count_blocked":2,"ts":1763017519}


=== file: runtime/metrics_uhoep47z.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00178696,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.24436,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":17,"count_skip":0,"count_blocked":2,"ts":1763017494}


=== file: runtime/metrics_uiqkfigp.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00174512,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.47869,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":86,"count_skip":0,"count_blocked":2,"ts":1763017847}


=== file: runtime/metrics_ujl1qfgc.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00118915,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":38.29087,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":27,"count_skip":0,"count_blocked":2,"ts":1763017545}


=== file: runtime/metrics_v30su220.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.96378,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":143,"count_skip":0,"count_blocked":2,"ts":1763018139}


=== file: runtime/metrics_v372j6bp.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.0253,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":77,"count_skip":0,"count_blocked":2,"ts":1763017801}


=== file: runtime/metrics_vd8t6g2k.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00121554,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.12527,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":95,"count_skip":0,"count_blocked":2,"ts":1763017893}


=== file: runtime/metrics_vi859rsu.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00116248,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.62738,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":140,"count_skip":0,"count_blocked":2,"ts":1763018124}


=== file: runtime/metrics_vlx06rwe.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00160074,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.77141,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":18,"count_skip":0,"count_blocked":2,"ts":1763017499}


=== file: runtime/metrics_vm46jhw3.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":6,"ts":1762729421}


=== file: runtime/metrics_vzimyuwy.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.20493,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":110,"count_skip":0,"count_blocked":2,"ts":1763017970}


=== file: runtime/metrics_w88v4g45.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00130883,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.77716,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":84,"count_skip":0,"count_blocked":2,"ts":1763017837}


=== file: runtime/metrics_wcsasy33.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00177824,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.41911,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":149,"count_skip":0,"count_blocked":2,"ts":1763018170}


=== file: runtime/metrics_x7rwrpb_.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762671190}


=== file: runtime/metrics_xbjh179p.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00070439,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.05378,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":3,"count_skip":0,"count_blocked":2,"ts":1763172584}


=== file: runtime/metrics_xcr4m91b.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017086,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.91885,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":74,"count_skip":0,"count_blocked":2,"ts":1763017786}


=== file: runtime/metrics_xcyd97h5.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.86567,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":50,"count_skip":0,"count_blocked":2,"ts":1763017663}


=== file: runtime/metrics_xe7x_sje.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.10578,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":11,"count_skip":0,"count_blocked":2,"ts":1763017463}


=== file: runtime/metrics_xjeelk50.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.81588,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":137,"count_skip":0,"count_blocked":2,"ts":1763018108}


=== file: runtime/metrics_xqnkdh58.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00103505,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.76621,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":83,"count_skip":0,"count_blocked":2,"ts":1763017832}


=== file: runtime/metrics_xuw93ulk.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00110574,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.39791,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":20,"count_skip":0,"count_blocked":2,"ts":1763017509}


=== file: runtime/metrics_xxx7__b9.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.35727,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":40,"count_skip":0,"count_blocked":2,"ts":1763017611}


=== file: runtime/metrics_y1mt7hlm.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00139558,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":23.96779,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":132,"count_skip":0,"count_blocked":2,"ts":1763018083}


=== file: runtime/metrics_y5_m62os.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.63395,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":136,"count_skip":0,"count_blocked":2,"ts":1763018103}


=== file: runtime/metrics_yehnyu4c.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0012298,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.9624,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":46,"count_skip":0,"count_blocked":2,"ts":1763017642}


=== file: runtime/metrics_yuncl873.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.11364,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":135,"count_skip":0,"count_blocked":2,"ts":1763018098}


=== file: runtime/metrics_yv04iu3_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.24244,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":99,"count_skip":0,"count_blocked":2,"ts":1763017914}


=== file: runtime/metrics_z4d75mrh.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.42979,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":155,"count_skip":0,"count_blocked":2,"ts":1763018200}


=== file: runtime/metrics_zn9wzer_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00085662,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.21134,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":4,"count_skip":0,"count_blocked":2,"ts":1763017427}


=== file: runtime/metrics_zrz0v00o.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00112066,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.19923,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":121,"count_skip":0,"count_blocked":2,"ts":1763018026}


=== file: runtime/metrics_zskuik5i.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00117464,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.23949,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":114,"count_skip":0,"count_blocked":2,"ts":1763017990}


=== file: runtime/trade_state.json ===

{"trading_enabled": true}



=== file: scripts/cb_smoke.py ===

from __future__ import annotations

from typing import Any, Optional

import importlib


def main() -> None:
    cb_mod = importlib.import_module("app.services.circuit_breaker")
    scan_and_update = getattr(cb_mod, "scan_and_update", None)
    status = getattr(cb_mod, "status", None)

    if callable(scan_and_update):
        scan_and_update()
    if callable(status):
        s: Optional[dict[str, Any]] = status()
        print(s)
    else:
        print({"circuit": "unknown"})


if __name__ == "__main__":
    main()



=== file: scripts/diagnose_symbol.py ===

# scripts/diagnose_symbol.py
import MetaTrader5 as mt5


def main() -> None:
    if not mt5.initialize():
        print("MT5 init failed:", mt5.last_error())
        return
    try:
        # USDJPYã§å§‹ã¾ã‚‹å…¨å€™è£œã‚’åˆ—æŒ™
        cands = mt5.symbols_get("USDJPY*")
        print("Candidates:", len(cands))
        for s in cands:
            print(f"- {s.name}  (select={s.select}, bid={s.bid}, ask={s.ask}, point={s.point})")
    finally:
        mt5.shutdown()


if __name__ == "__main__":
    main()



=== file: scripts/dryrun_smoke.py ===

# scripts/dryrun_smoke.py
from __future__ import annotations

import argparse
import math
import random
import sys
import time
from pathlib import Path
from typing import Dict

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from loguru import logger

from app.core import logger as app_logger
from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_state
from app.services.execution_stub import ExecutionStub, reset_atr_gate_state
from core.ai.service import AISvc
from core.utils.hashing import hash_features
from core.utils.timeutil import now_jst_iso


def parse_args(argv: list[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Dry-run smoke simulator")
    parser.add_argument("--sim", action="store_true", help="Use synthetic tick stream")
    parser.add_argument("--atr-open", action="store_true", help="Force ATR gate open")
    parser.add_argument("--n", type=int, default=200, help="Number of ticks to simulate")
    parser.add_argument("--dt", type=int, default=50, help="Tick interval in milliseconds")
    parser.add_argument("--base", type=float, default=150.20, help="Base mid price")
    parser.add_argument("--spread", type=float, default=0.5, help="Spread in pips")
    parser.add_argument("--atrpct", type=float, default=0.0005, help="ATR percentage baseline")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument("--symbol", type=str, default=None, help="Override runtime symbol")
    return parser.parse_args(argv)


def _build_features(base_price: float, spread_pips: float, tick_idx: int, rng: random.Random) -> Dict[str, float]:
    drift = math.sin(tick_idx / 6.0)
    noise = rng.uniform(-0.02, 0.02)
    ema_5 = base_price * (1 + drift * 0.002) + noise
    ema_20 = base_price * (1 - drift * 0.001) - noise
    rsi_14 = 60.0 + drift * 25.0 + rng.uniform(-3, 3)
    atr_14 = abs(drift) * 0.002 + spread_pips * 0.0001
    adx_14 = 22.0 + abs(drift) * 15.0 + rng.uniform(-1.5, 1.5)
    bbp = 0.5 + drift * 0.35 + rng.uniform(-0.05, 0.05)
    vol_chg = drift * 0.08 + rng.uniform(-0.02, 0.02)
    wick_ratio = 0.5 + drift * 0.3 + rng.uniform(-0.05, 0.05)
    return {
        "ema_5": float(ema_5),
        "ema_20": float(ema_20),
        "rsi_14": float(max(0.0, min(100.0, rsi_14))),
        "atr_14": float(abs(atr_14)),
        "adx_14": float(max(5.0, adx_14)),
        "bbp": float(max(0.0, min(1.0, bbp))),
        "vol_chg": float(vol_chg),
        "wick_ratio": float(max(0.0, min(1.0, wick_ratio))),
    }


def _ensure_config_overrides(cfg: dict, args: argparse.Namespace) -> None:
    filters_cfg = cfg.setdefault("filters", {})
    hy = filters_cfg.setdefault("atr_hysteresis", {})
    if args.atr_open:
        filters_cfg["min_atr_pct"] = 0.0
        hy["enable_min_pct"] = 0.0
        hy["disable_min_pct"] = 0.0


def prepare_state(cfg: dict, args: argparse.Namespace) -> ExecutionStub:
    runtime_cfg = cfg.get("runtime", {})
    entry_cfg = cfg.get("entry", {})
    prob_threshold = float(entry_cfg.get("prob_threshold", entry_cfg.get("threshold_buy", 0.60)))
    trade_state.update(
        trading_enabled=True,
        threshold_buy=float(entry_cfg.get("threshold_buy", prob_threshold)),
        threshold_sell=float(entry_cfg.get("threshold_sell", prob_threshold)),
        prob_threshold=prob_threshold,
        side_bias=str(entry_cfg.get("side_bias", "auto") or "auto"),
    )

    cb_cfg = cfg.get("circuit_breaker", {}) if isinstance(cfg, dict) else {}
    cb = circuit_breaker.CircuitBreaker(
        max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", cfg.get("risk", {}).get("max_consecutive_losses", 5))),
        daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
        cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
    )
    ai = AISvc(threshold=prob_threshold)
    try:
        reset_atr_gate_state()
    except Exception:
        pass
    return ExecutionStub(cb=cb, ai=ai)


def main(argv: list[str]) -> None:
    app_logger.setup()
    args = parse_args(argv)
    if not args.sim:
        args.sim = True  # default to simulation for smoke test

    rng = random.Random(args.seed)
    cfg = load_config()
    _ensure_config_overrides(cfg, args)

    import core.config as core_config

    core_config.cfg = cfg
    stub = prepare_state(cfg, args)

    runtime_cfg = cfg.get("runtime", {})
    symbol = args.symbol or runtime_cfg.get("symbol", "USDJPY")

    spread_limit = float(runtime_cfg.get("spread_limit_pips", runtime_cfg.get("spread_limit", 1.5)))
    max_positions = int(runtime_cfg.get("max_positions", 1))

    trail_logged = False

    for idx in range(args.n):
        features = _build_features(args.base, args.spread, idx, rng)
        runtime_payload = {
            "spread_pips": float(args.spread),
            "spread_limit_pips": spread_limit,
            "max_positions": max_positions,
            "open_positions": 0,
            "ai_threshold": stub.ai.threshold,
            "min_atr_pct": cfg.get("filters", {}).get("min_atr_pct", 0.0),
            "filters": cfg.get("filters", {}),
        }
        result = stub.on_tick(symbol, features, runtime_payload)
        if not trail_logged:
            logger.info("[TRAIL][DRYRUN] smoke trail ping features_hash={}", hash_features(features))
            trail_logged = True
        if args.dt > 0:
            time.sleep(min(args.dt / 1000.0, 0.1))

    logger.info(
        "[SMOKE] completed n={} dt_ms={} symbol={} time={}",
        args.n,
        args.dt,
        symbol,
        now_jst_iso(),
    )


if __name__ == "__main__":
    main(sys.argv[1:])



=== file: scripts/export_mt5_history.py ===

# --- project root on sys.path ---
import os, sys
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)
# --------------------------------

import MetaTrader5 as mt5
import pandas as pd
from datetime import datetime, timedelta
from typing import NoReturn

SYMBOL = os.environ.get("FXBOT_SYMBOL", "USDJPY-")
TIMEFRAME = mt5.TIMEFRAME_M5

# ç«¯æœ«ãƒ‘ã‚¹ã‚’æ˜ç¤ºã—ãŸã„å ´åˆã¯ç’°å¢ƒå¤‰æ•° FXBOT_MT5_TERMINAL ã‚’ä½¿ã†
# ä¾‹: setx FXBOT_MT5_TERMINAL "C:\Program Files\MetaTrader 5\terminal64.exe"
TERM_PATH = os.environ.get("FXBOT_MT5_TERMINAL")

def die(msg: str) -> NoReturn:
    print(msg)
    mt5.shutdown()
    raise SystemExit(1)

def ensure_init() -> None:
    ok = mt5.initialize() if not TERM_PATH else mt5.initialize(path=TERM_PATH)
    if not ok:
        die(f"MT5 initialize() failed: last_error={mt5.last_error()} term_path={TERM_PATH!r}")
    ver = mt5.version()
    print(f"MT5 initialized. version={ver} term_path={TERM_PATH!r}")

def ensure_logged_in() -> None:
    ai = mt5.account_info()
    if ai is None:
        die(f"Not logged in or terminal not ready. last_error={mt5.last_error()}")
    print(f"Account: {ai.login} / {ai.server}")

def ensure_symbol(symbol: str) -> None:
    info = mt5.symbol_info(symbol)
    if info is None:
        die(f"symbol_info({symbol}) is None. last_error={mt5.last_error()}")
    if not info.visible:
        if not mt5.symbol_select(symbol, True):
            die(f"symbol_select({symbol}) failed. last_error={mt5.last_error()}")
    # è©¦ã—ã«æœ€æ–°ãƒ†ã‚£ãƒƒã‚¯ã‚‚è§¦ã£ã¦ãŠã
    _ = mt5.symbol_info_tick(symbol)
    print(f"Symbol {symbol} ready (visible={mt5.symbol_info(symbol).visible})")

def try_copy_small(symbol: str, timeframe: int, count: int = 1000) -> int:
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None:
        return 0
    return len(rates)

def export_range(symbol: str, timeframe: int, days: int = 365 * 5) -> str:
    to = datetime.now()
    frm = to - timedelta(days=days)

    # ã¾ãšå°ã•ãå–ã‚Œã‚‹ã‹è¨ºæ–­
    small = try_copy_small(symbol, timeframe, 1000)
    print(f"diagnostic: copy_rates_from_pos count={small}")

    # --- å®‰å…¨å–å¾—ãƒ¢ãƒ¼ãƒ‰ ---
    print(f"fetching {symbol} {days}days range in chunks ...")
    chunk_days = 30   # 1ã‹æœˆå˜ä½ã§é¡ã‚‹
    frames = []
    cursor_to = to
    while cursor_to > frm:
        cursor_from = cursor_to - timedelta(days=chunk_days)
        rates = mt5.copy_rates_range(symbol, timeframe, cursor_from, cursor_to)
        if rates is None or len(rates) == 0:
            print(f"chunk {cursor_from.date()}~{cursor_to.date()} => no data (skip)")
        else:
            df = pd.DataFrame(rates)
            frames.append(df)
            print(f"chunk {cursor_from.date()}~{cursor_to.date()} => {len(df)} bars")
        cursor_to = cursor_from

    if not frames:
        die("no data returned even by chunked fetch. Try shorter days or different symbol/timeframe.")

    df = pd.concat(frames).drop_duplicates(subset=["time"]).sort_values("time")
    df["Date"] = pd.to_datetime(df["time"], unit="s")
    df = df.rename(columns={
        "open": "open", "high": "high", "low": "low", "close": "close",
        "tick_volume": "volume"
    })
    df = df[["Date", "open", "high", "low", "close", "volume"]]
    df["label"] = (df["close"].shift(-1) > df["close"]).map({True: "BUY", False: "SELL"})

    outdir = os.path.join("data", "usdjpy")
    os.makedirs(outdir, exist_ok=True)
    out = os.path.join(outdir, "USDJPY_M5_mt5.csv")
    df.to_csv(out, index=False)
    print(f"wrote {out} rows:{len(df)}")
    return out


def main() -> None:
    ensure_init()
    ensure_logged_in()
    ensure_symbol(SYMBOL)
    export_range(SYMBOL, TIMEFRAME, days=365*5)
    mt5.shutdown()

if __name__ == "__main__":
    main()



=== file: scripts/export_val_probs.py ===

# scripts/export_val_probs.py
from __future__ import annotations

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import atexit, math, json
from datetime import datetime, timezone, timedelta
from typing import Any

import numpy as np
import pandas as pd
import MetaTrader5 as mt5
from loguru import logger

from core.ai.loader import load_lgb_clf  # æ—¢å­˜ãƒ­ãƒ¼ãƒ€ã‚’åˆ©ç”¨
from pathlib import Path

# ====== è¨­å®š ======
SYMBOL = "USDJPY"                 # ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼æ¥å°¾è¾ã¯è‡ªå‹•å¸åã—ã¾ã™
TIMEFRAME = mt5.TIMEFRAME_M5
START = "2025-08-01 00:00:00"     # æ¤œè¨¼é–‹å§‹ï¼ˆJSTï¼‰
END   = "2025-10-01 00:00:00"     # æ¤œè¨¼çµ‚äº†ï¼ˆJSTï¼‰
BARS_MIN = 400                    # æœ€ä½å¿…è¦ãƒãƒ¼æ•°ï¼ˆEMA/BBç­‰ã®ãŸã‚ï¼‰
OUT_PBUY = Path("logs/val_p_buy_raw.npy")
OUT_Y    = Path("logs/val_y_true.npy")
OUT_META = Path("logs/val_export_meta.json")

# ====== MT5åˆæœŸåŒ– ======
def _ensure_symbol(symbol: str) -> str:
    if mt5.symbol_select(symbol, True):
        return symbol
    upper = symbol.upper()
    cands = [s.name for s in mt5.symbols_get() if s.name.upper().startswith(upper)]
    if not cands:
        raise RuntimeError(f"no candidates for '{symbol}'")
    best = sorted(cands, key=len)[0]
    best = str(best)
    if not mt5.symbol_select(best, True):
        raise RuntimeError(f"symbol_select failed for '{best}'")
    return best


def _read_feature_order(meta_path: str) -> list[str]:
    try:
        with open(meta_path, "r", encoding="utf-8") as fh:
            meta: dict[str, Any] = json.load(fh)
        return list(meta.get("feature_order", []))
    except Exception:
        return []

def _to_utc(jst_str: str) -> datetime:
    jst = datetime.strptime(jst_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone(timedelta(hours=9)))
    return jst.astimezone(timezone.utc)

# ====== ç‰¹å¾´é‡ï¼ˆdryrunã¨åŒã˜å®šç¾©ï¼‰ ======
def make_features_df(df: pd.DataFrame) -> pd.DataFrame:
    close, high, low, open_ = df["close"], df["high"], df["low"], df["open"]

    ema_5  = close.ewm(span=5, adjust=False).mean()
    ema_20 = close.ewm(span=20, adjust=False).mean()

    delta = close.diff()
    up = delta.clip(lower=0.0)
    down = -delta.clip(upper=0.0)
    roll_up = up.ewm(alpha=1/14, adjust=False).mean()
    roll_down = down.ewm(alpha=1/14, adjust=False).mean()
    rs = roll_up / roll_down.replace(0, np.nan)
    rsi_14 = (100.0 - (100.0 / (1.0 + rs))).fillna(50.0)

    tr1 = (high - low).abs()
    tr2 = (high - close.shift()).abs()
    tr3 = (low - close.shift()).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr_14_abs = tr.ewm(alpha=1/14, adjust=False).mean()
    atr_14 = (atr_14_abs / close.replace(0, np.nan)).fillna(0.0)

    plus_dm  = (high.diff()).clip(lower=0.0)
    minus_dm = (-low.diff()).clip(lower=0.0)
    plus_dm[plus_dm < minus_dm] = 0.0
    minus_dm[minus_dm <= plus_dm] = 0.0
    tr_smooth = tr.ewm(alpha=1/14, adjust=False).mean()
    plus_di = 100 * (plus_dm.ewm(alpha=1/14, adjust=False).mean() / tr_smooth.replace(0, np.nan))
    minus_di = 100 * (minus_dm.ewm(alpha=1/14, adjust=False).mean() / tr_smooth.replace(0, np.nan))
    dx = 100 * ((plus_di - minus_di).abs() / (plus_di + minus_di).replace(0, np.nan))
    adx_14 = dx.ewm(alpha=1/14, adjust=False).mean().fillna(20.0)

    bb_ma = close.rolling(20).mean()
    bb_std = close.rolling(20).std(ddof=0)
    bb_upper = bb_ma + 2 * bb_std
    bb_lower = bb_ma - 2 * bb_std
    bbp = ((close - bb_lower) / (bb_upper - bb_lower)).replace([np.inf, -np.inf], np.nan).clip(0.0, 1.0).fillna(0.5)

    # volumeç”±æ¥ã®ç‰¹å¾´ã¯çœç•¥ï¼ˆå­¦ç¿’æ™‚ã«ä½¿ã£ã¦ã„ã‚Œã°è¿½åŠ ï¼‰
    body_high = np.maximum(open_, close)
    body_low  = np.minimum(open_, close)
    upper_wick = (high - body_high).clip(lower=0.0)
    lower_wick = (body_low - low).clip(lower=0.0)
    rng = (high - low).replace(0, np.nan)
    wick_ratio = ((upper_wick + lower_wick) / rng).clip(0.0, 1.0).fillna(0.0)

    out = pd.DataFrame({
        "ema_5": ema_5 - ema_20,
        "ema_20": (ema_20 - close) / close.replace(0, np.nan),
        "rsi_14": rsi_14,
        "atr_14": atr_14,
        "adx_14": adx_14,
        "bbp": bbp,
        "wick_ratio": wick_ratio,
    })
    return out.replace([np.inf, -np.inf], 0.0).fillna(0.0)

def main() -> None:
    if not mt5.initialize():
        raise SystemExit(f"MT5 init failed: {mt5.last_error()}")
    atexit.register(mt5.shutdown)

    symbol = _ensure_symbol(SYMBOL)

    # ä½™è£•ã‚’æŒã£ã¦éå»ã‹ã‚‰å–å¾—
    utc_from = _to_utc(START) - timedelta(days=7)
    utc_to   = _to_utc(END)
    rates = mt5.copy_rates_range(symbol, TIMEFRAME, utc_from, utc_to)
    if rates is None or len(rates) < BARS_MIN:
        raise SystemExit(f"not enough bars: {len(rates) if rates is not None else 0}")

    df = pd.DataFrame(rates)
    df["ts"] = pd.to_datetime(df["time"], unit="s", utc=True).dt.tz_convert("Asia/Tokyo")
    df = df[ (df["ts"] >= pd.Timestamp(START, tz="Asia/Tokyo")) & (df["ts"] < pd.Timestamp(END, tz="Asia/Tokyo")) ].copy()
    df = df.reset_index(drop=True)

    feats = make_features_df(df)
    # ãƒ©ãƒ™ãƒ«å®šç¾©ï¼ˆæ¬¡ãƒãƒ¼ã®ä¸Šã’ä¸‹ã’ã€åŒå€¤ã¯0ã¨ã™ã‚‹ï¼‰
    y_true = (df["close"].shift(-1) > df["close"]).astype(int)[:-1].values
    feats = feats.iloc[:-1, :].copy()
#
    # ===== ãƒ¢ãƒ‡ãƒ«ã¨åˆ—é †ï¼ˆè¿”ã‚Šå€¤ã®å‹å·®ã‚’å¸åï¼‰ =====
    lm = load_lgb_clf()  # LoadedModel / sklearn LGBMClassifier / lightgbm.Booster ç­‰

    # ãƒ¢ãƒ‡ãƒ«æœ¬ä½“å€™è£œã‚’åºƒãæ¢ç´¢
    candidates = [lm]
    for attr in ("model", "clf", "estimator", "inner", "wrapped", "lgbm", "booster", "booster_"):
        if hasattr(lm, attr):
            candidates.append(getattr(lm, attr))

    model = None
    for cand in candidates:
        if hasattr(cand, "predict_proba"):
            model = cand  # sklearn äº’æ›
            use_proba = "predict_proba"
            break
        if hasattr(cand, "predict"):
            model = cand  # Booster ç­‰ï¼ˆpredictã§ç¢ºç‡ãŒè¿”ã‚‹æƒ³å®šï¼‰
            use_proba = "predict"
            # break ã—ãªã„ã§ç¶šã‘ã¦ã‚‚ã„ã„ãŒã€ã¾ãšã¯ã“ã‚Œã§æ¡ç”¨
            break

    if model is None:
        raise TypeError("No usable model found: neither predict_proba nor predict detected.")

    # ç‰¹å¾´é‡é †ï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°ç¾åœ¨åˆ—ã§é€²ã‚€ï¼‰
    try:
        order = _read_feature_order("models/LightGBM_clf.features.json")
    except Exception:
        order = None
    if not order:
        order = list(feats.columns)

    # æ¬ ã‘åˆ—ã¯0ã§è£œã„ã€ä½™åˆ†ã¯è½ã¨ã™
    for col in order:
        if col not in feats.columns:
            feats[col] = 0.0
    X = feats[order].astype(float)

    # ===== ç”ŸBUYç¢ºç‡ã®å–å¾— =====
    if use_proba == "predict_proba":
        proba = model.predict_proba(X)  # (N,2 or 3)
    else:
        # LightGBM Booster ç­‰: predict ã§ç¢ºç‡ãŒè¿”ã‚‹ï¼ˆbinary ã¯é™½æ€§ç¢ºç‡ã€multiclass ã¯ (N, K)ï¼‰
        # raw_score=False ã‚’æ¸¡ã›ã‚‹å ´åˆã¯æ¸¡ã™ï¼ˆç„¡ãã¦ã‚‚OKï¼‰
        try:
            proba = model.predict(X, raw_score=False)
        except TypeError:
            proba = model.predict(X)

    # shape ã‚’æ­£è¦åŒ–
    proba = np.asarray(proba)
    if proba.ndim == 1:
        # binary ã®é™½æ€§ç¢ºç‡ãŒ 1 åˆ—ã§è¿”ã£ãŸã‚±ãƒ¼ã‚¹
        p_buy_raw = proba
    elif proba.ndim == 2:
        # (N,2) or (N,K) ã‚’æƒ³å®šã€‚BUYåˆ—ï¼ˆé™½æ€§ï¼‰ã‚’åˆ—1ã¨ä»®å®šï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰
        if proba.shape[1] >= 2:
            p_buy_raw = proba[:, 1]
        else:
            # ã‚ˆã»ã©ã®ç‰¹æ®Šå½¢çŠ¶ã€‚å®‰å…¨å´ã§æœ€å¤§ã‚¹ã‚³ã‚¢åˆ—ã‚’BUYã¨ã¿ãªã™
            idx = np.argmax(proba, axis=1)
            p_buy_raw = (idx == 1).astype(float)
    else:
        raise ValueError(f"Unsupported prediction output shape: {proba.shape}")

    OUT_PBUY.parent.mkdir(parents=True, exist_ok=True)
    np.save(OUT_PBUY, p_buy_raw.astype(float))
    np.save(OUT_Y,    y_true.astype(int))
    OUT_META.write_text(json.dumps({
        "symbol": symbol,
        "timeframe": "M5",
        "start": START,
        "end": END,
        "N": int(len(p_buy_raw)),
        "feature_order_used": order,
        "inferred_api": use_proba,
    }, ensure_ascii=False, indent=2), encoding="utf-8")

    logger.info(f"wrote: {OUT_PBUY} ({len(p_buy_raw)}), {OUT_Y} ({len(y_true)}), api={use_proba}")

#
if __name__ == "__main__":
    main()



=== file: scripts/make_csv_from_mt5.py ===

# scripts/make_csv_from_mt5.py
"""
MT5 ã‹ã‚‰ USDJPY ã® M5/M15/H1 ã‚’ 2020-11-01 ä»¥é™ã§ CSV åŒ–ã—ã€
ä»¥é™ã¯ä¸è¶³åˆ†ã®ã¿ã‚’è‡ªå‹•è¿½è¨˜ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚

- ä¿å­˜å…ˆ: <ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ>/data ï¼ˆç›¸å¯¾æŒ‡å®šã§ã‚‚æœ€çµ‚çš„ã«çµ¶å¯¾ãƒ‘ã‚¹ã¸è§£æ±ºï¼‰
- ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³: JSTï¼ˆAsia/Tokyoï¼‰ã§ time åˆ—ã‚’ naive datetime64[ns] ã¨ã—ã¦ä¿å­˜
- æ—¢å­˜CSVãŒã‚ã‚Œã°æœ«å°¾æ™‚åˆ»ä»¥é™ã‚’è‡ªå‹•ã§è¿½è¨˜ï¼ˆé‡è¤‡ã¯é™¤å»ï¼‰
- ã‚·ãƒ³ãƒœãƒ«æ¥å°¾è¾ï¼ˆä¾‹: USDJPY-ï¼‰ã¯è‡ªå‹•è§£æ±º
- GaitameFinest ç­‰ã§ copy_rates_range() ãŒå¤±æ•—ã™ã‚‹ç’°å¢ƒã«å¯¾ã—ã¦
  copy_rates_from() ã®ã€Œç¾åœ¨â†’éå»ã¸ãƒšãƒ¼ã‚¸ãƒ³ã‚°ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ­è¼‰
- å®Ÿè¡Œç’°å¢ƒåã¯ --env ã§æ˜ç¤ºã§ãã€æœªæŒ‡å®šæ™‚ã¯ HOST_MAP ã¨ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§æ¨å®š
- ä¿å­˜ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¯ --layout ã§åˆ‡æ›¿ï¼ˆflat | per-symbolï¼‰
"""

from __future__ import annotations

import argparse
import os
import socket
import sys
from datetime import UTC
from datetime import datetime as pdt
from pathlib import Path

import pandas as pd

try:
    import MetaTrader5 as mt5
except Exception as e:
    raise SystemExit(
        "[fatal] MetaTrader5 ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»®æƒ³ç’°å¢ƒã§ `pip install MetaTrader5 pandas` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    ) from e


# =========================
# è¨­å®šï¼ˆå¿…è¦ãªã‚‰ç·¨é›†ï¼‰
# =========================

SYMBOL_DEFAULT = "USDJPY"
TIMEFRAMES_DEFAULT = ["M5", "M15", "H1"]
START_DATE_DEFAULT = "2020-11-01"  # ã“ã“ä»¥å‰ã¯å–å¾—ã—ãªã„

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼ˆã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®1ã¤ä¸Šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
PROJECT_ROOT = Path(__file__).resolve().parents[1]
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæœ€çµ‚çš„ã« PROJECT_ROOT/data ã«è§£æ±ºï¼‰
DATA_DIR_DEFAULT = "data"

# allow importing fxbot_path from project root
sys.path.insert(0, str(PROJECT_ROOT))
try:
    from fxbot_path import get_data_root, get_ohlcv_csv_path
except Exception:
    # import error will be surfaced when running main; keep module import-safe
    get_data_root = None  # type: ignore
    get_ohlcv_csv_path = None  # type: ignore

# MT5 ã® timeframe å®šæ•°ãƒãƒƒãƒ—
TF_MAP = {
    "M1": mt5.TIMEFRAME_M1,
    "M5": mt5.TIMEFRAME_M5,
    "M15": mt5.TIMEFRAME_M15,
    "M30": mt5.TIMEFRAME_M30,
    "H1": mt5.TIMEFRAME_H1,
    "H4": mt5.TIMEFRAME_H4,
    "D1": mt5.TIMEFRAME_D1,
}

# CSV ã‚«ãƒ©ãƒ é †ï¼ˆMT5ã®æˆ»ã‚Šå€¤ã«æº–æ‹ ï¼‰
CSV_COLS = [
    "time",
    "open",
    "high",
    "low",
    "close",
    "tick_volume",
    "spread",
    "real_volume",
]

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®1å›ã‚ãŸã‚Šå–å¾—æœ¬æ•°ã¨æœ€å¤§ãƒ«ãƒ¼ãƒ—å›æ•°ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´å¯èƒ½ï¼‰
PAGE = 20000
MAX_LOOPS = 300

# CSVãƒ•ã‚¡ã‚¤ãƒ«åã§ä½¿ã† â€œæ¥å°¾è¾ãªã—ã‚¿ã‚°â€ ã‚’ main() å†…ã§è¨­å®š
FILE_TAG: str | None = None


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================


def log(msg: str):
    host = os.environ.get("COMPUTERNAME", socket.gethostname())
    ts = pdt.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}][{host}] {msg}")


def ensure_mt5_initialized(terminal_path: str | None = None):
    """
    MT5 ã‚’åˆæœŸåŒ–ã€‚terminal_path ã‚’æŒ‡å®šã™ã‚Œã°ãã® exe ã«ç´ã¥ã‘ã€‚
    Windows ã§ MT5 ãŒèµ·å‹•ã—ã¦ã„ãªãã¦ã‚‚ã€é€šå¸¸ã¯è‡ªå‹•ã§èµ·å‹•ãƒ»æ¥ç¶šå¯èƒ½ã€‚
    """
    ok = mt5.initialize(path=terminal_path) if terminal_path else mt5.initialize()
    if not ok:
        code, details = mt5.last_error()
        raise SystemExit(f"[fatal] MT5 initialize å¤±æ•—: {code} {details}")

    info = mt5.account_info()
    if info is None:
        log(
            "[warn] account_info() ãŒ Noneã€‚æœªãƒ­ã‚°ã‚¤ãƒ³ã®å¯èƒ½æ€§ã€‚ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å´ã§å£åº§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚"
        )
    else:
        log(f"connected login={info.login} server={info.server} balance={info.balance}")


def resolve_symbol(base: str) -> str:
    """
    ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼æ¥å°¾è¾é•ã„ã«å¯¾å¿œã—ã¦ã‚·ãƒ³ãƒœãƒ«åã‚’è§£æ±ºã€‚
    æˆåŠŸã—ãŸå®Ÿåœ¨ã‚·ãƒ³ãƒœãƒ«åã‚’è¿”ã™ã€‚å…¨ã¦å¤±æ•—ãªã‚‰å…ƒã® base ã‚’è¿”ã™ã€‚
    """
    candidates = [
        base,
        base + "-",
        base + ".",
        base + ".r",
        base + ".m",
        base + ".mini",
        base + "_",
    ]
    tried = []
    for sym in candidates:
        tried.append(sym)
        info = mt5.symbol_info(sym)
        if info is not None:
            if not info.visible:
                mt5.symbol_select(sym, True)
            return sym
    log(f"[warn] symbol resolve failed. tried={tried}")
    return base


def jst_from_mt5_epoch(series):
    """
    MT5ã® 'time' (Unixç§’, UTC) ã‚’ JST ã® naive datetime64[ns] ã«å¤‰æ›ã€‚
    series ã¯ pandas Series ã§ã‚‚ DatetimeIndex ã§ã‚‚ä¸¡å¯¾å¿œã€‚
    """
    s = pd.to_datetime(series, unit="s", utc=True)
    if isinstance(s, pd.DatetimeIndex):
        return s.tz_convert("Asia/Tokyo").tz_localize(None)
    else:
        return s.dt.tz_convert("Asia/Tokyo").dt.tz_localize(None)


def merge_and_dedup(old: pd.DataFrame | None, new: pd.DataFrame) -> pd.DataFrame:
    """
    old ã¨ new ã‚’ç¸¦çµåˆã—ã¦ time é‡è¤‡ã‚’é™¤å»ï¼ˆå¾Œå‹ã¡ï¼‰â†’ time æ˜‡é †ã«æƒãˆã‚‹ã€‚
    """
    if old is None or old.empty:
        out = new.copy()
    else:
        out = pd.concat([old, new], axis=0, ignore_index=True)
        out = out.drop_duplicates(subset=["time"], keep="last")
    return out.sort_values("time").reset_index(drop=True)


# =========================
# ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚³ã‚¢
# =========================


def _to_utc_naive(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> UTC naiveï¼ˆtzinfoãªã—ï¼‰"""
    return (
        ts_jst.tz_localize("Asia/Tokyo")
        .tz_convert("UTC")
        .to_pydatetime()
        .replace(tzinfo=None)
    )


def _to_local_naive(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> JST naiveï¼ˆãƒ­ãƒ¼ã‚«ãƒ«naiveã‚’è¦æ±‚ã™ã‚‹ç’°å¢ƒå‘ã‘ï¼‰"""
    return ts_jst.to_pydatetime().replace(tzinfo=None)


def _to_utc_aware(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> UTC awareï¼ˆtimezone.utcï¼‰"""
    return (
        ts_jst.tz_localize("Asia/Tokyo")
        .tz_convert("UTC")
        .to_pydatetime()
        .replace(tzinfo=UTC)
    )


def _range_attempts(
    symbol: str, tf: int, start_ts: pd.Timestamp, end_ts: pd.Timestamp
) -> tuple[pd.DataFrame | None, str]:
    """
    copy_rates_range() ã‚’ 3 æ–¹å¼ï¼ˆUTC-naive / local-naive / UTC-awareï¼‰ã§è©¦ã™ã€‚
    æˆåŠŸæ™‚ã¯ (DataFrame, "ok:<tag>")ã€å…¨æ»…ãªã‚‰ (None, "fail")
    """
    variants = [
        ("utc_naive", _to_utc_naive(start_ts), _to_utc_naive(end_ts)),
        ("local_naive", _to_local_naive(start_ts), _to_local_naive(end_ts)),
        ("utc_aware", _to_utc_aware(start_ts), _to_utc_aware(end_ts)),
    ]
    for tag, dfrom, dto in variants:
        rates = mt5.copy_rates_range(symbol, tf, dfrom, dto)
        if rates is None:
            code, details = mt5.last_error()
            log(f"[try:{tag}] copy_rates_range returned None: {code} {details}")
            continue
        df = pd.DataFrame(rates)
        if len(df) == 0:
            log(f"[ok:{tag}] fetched rows=0")
            return pd.DataFrame(columns=CSV_COLS), f"ok:{tag}"
        df["time"] = jst_from_mt5_epoch(df["time"])
        df = df.sort_values("time").reset_index(drop=True)
        log(f"[ok:{tag}] fetched rows={len(df)}")
        return df[CSV_COLS], f"ok:{tag}"
    return None, "fail"


def fetch_rates(
    symbol: str, tf: int, start_ts: pd.Timestamp, end_ts: pd.Timestamp
) -> pd.DataFrame:
    """
    æŒ‡å®šæœŸé–“ã®ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—ã—ã¦ DataFrame ã§è¿”ã™ï¼ˆJSTã«å¤‰æ›ï¼‰ã€‚
    ã¾ãš copy_rates_range() ã‚’è©¦ã—ã€å…¨æ»…ã—ãŸã‚‰ copy_rates_from() ã®ãƒãƒƒã‚¯ãƒšãƒ¼ã‚¸ãƒ³ã‚°ã§è£œã†ã€‚
    """
    if not isinstance(start_ts, pd.Timestamp):
        start_ts = pd.Timestamp(start_ts)
    if not isinstance(end_ts, pd.Timestamp):
        end_ts = pd.Timestamp(end_ts)
    if end_ts <= start_ts:
        raise ValueError(f"start >= end: {start_ts} .. {end_ts}")

    # 1) range è©¦è¡Œ
    df_range, status = _range_attempts(symbol, tf, start_ts, end_ts)
    if status.startswith("ok"):
        return df_range

    # 2) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: from ã§éå»ã«ãƒšãƒ¼ã‚¸ãƒ³ã‚°
    log("[fallback] using copy_rates_from() paging backward")
    dt_to = _to_utc_naive(end_ts)  # MT5ã¯ tzinfo ãªã—ã® UTC ã‚’å¥½ã‚€

    frames = []
    safety_loops = 0

    while safety_loops < MAX_LOOPS:
        safety_loops += 1
        rates = mt5.copy_rates_from(symbol, tf, dt_to, PAGE)
        if rates is None:
            code, details = mt5.last_error()
            log(f"[fallback] copy_rates_from returned None: {code} {details}")
            break
        if len(rates) == 0:
            log("[fallback] no more bars returned")
            break

        # ç”Ÿã®DataFrameï¼ˆUTC epochç§’ï¼‰
        df_raw = pd.DataFrame(rates)

        # ã¾ãšã¯JSTã¸
        df = df_raw.copy()
        df["time"] = jst_from_mt5_epoch(df["time"])
        df = df.sort_values("time").reset_index(drop=True)

        # ç›®æ¨™æœŸé–“ã«é‡ãªã‚‹åˆ†ã ã‘ä¿æŒï¼ˆJSTåŸºæº–ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰
        df_keep = df[(df["time"] >= start_ts) & (df["time"] <= end_ts)]
        if len(df_keep):
            frames.append(df_keep[CSV_COLS])

        # æ¬¡ãƒšãƒ¼ã‚¸ã®çµ‚ç«¯ï¼ˆã•ã‚‰ã«éå»ã¸ï¼‰
        oldest_utc_epoch = int(df_raw["time"].min())  # epochç§’
        # DeprecationWarning å›é¿ï¼šUTC aware ã§ä½œã£ã¦ã‹ã‚‰ tzinfo=None ã§ naive UTC ã¸
        dt_to = pdt.fromtimestamp(oldest_utc_epoch - 1, tz=UTC).replace(tzinfo=None)

        # ã‚‚ã†ååˆ†é¡ã‚ŒãŸã‹ï¼Ÿ
        if len(df) and df["time"].min() <= start_ts:
            break

    if not frames:
        log("[fallback] collected 0 rows")
        return pd.DataFrame(columns=CSV_COLS)

    out = (
        pd.concat(frames, axis=0, ignore_index=True)
        .drop_duplicates(subset=["time"])
        .sort_values("time")
        .reset_index(drop=True)
    )
    log(
        f"[fallback] fetched rows={len(out)} (min={out['time'].min()} .. max={out['time'].max()})"
    )
    return out[CSV_COLS]


# =========================
# CSV ä½œæˆãƒ»æ›´æ–°
# =========================


def ensure_csv_for_timeframe(
    symbol: str,
    tf_name: str,
    start_date: str,
    data_dir: Path,
    end_date: str | None = None,
    layout: str = "per-symbol",
) -> Path:
    """
    å˜ä¸€ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®CSVã‚’ä½œæˆ/æ›´æ–°ã™ã‚‹ã€‚
    - start_date ï½ end_date ã®ç¯„å›²ã§ä½œæˆï¼ˆend_date ãŒ None ã®å ´åˆã¯ç¾åœ¨ã¾ã§ï¼‰
    - æ—¢å­˜ãŒã‚ã‚Œã°æœ«å°¾ä»¥é™ã®ã¿è¿½è¨˜
    - è¿”ã‚Šå€¤: ä¿å­˜ã—ãŸ CSV ã®ãƒ‘ã‚¹
    """
    log(f"=== begin timeframe={tf_name} ===")
    if tf_name not in TF_MAP:
        raise ValueError(f"æœªçŸ¥ã®ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ : {tf_name}")

    tf_const = TF_MAP[tf_name]
    assert FILE_TAG is not None, "FILE_TAG ãŒæœªè¨­å®šã§ã™ï¼ˆmain() ã§è¨­å®šã•ã‚Œã¾ã™ï¼‰"
    # çµ±ä¸€ãƒ‘ã‚¹ç”Ÿæˆ
    if get_ohlcv_csv_path is not None:
        csv_path = get_ohlcv_csv_path(
            symbol, tf_name, data_root=data_dir, layout=layout
        )
    else:
        # fall back to legacy behavior (data_dir / FILE_TAG_tf.csv)
        csv_path = data_dir / f"{FILE_TAG}_{tf_name}.csv"
        csv_path.parent.mkdir(parents=True, exist_ok=True)

    start_ts = pd.Timestamp(start_date)  # JST naive

    if end_date is not None:
        end_ts = pd.Timestamp(end_date)
    else:
        end_ts = pd.Timestamp.now(tz="Asia/Tokyo").tz_localize(None)

    # æ—¢å­˜CSVã®èª­ã¿è¾¼ã¿
    if csv_path.exists():
        old = pd.read_csv(csv_path, parse_dates=["time"])
        old = old[CSV_COLS]
        last_time = old["time"].max()
        fetch_from = max(start_ts, last_time)
        log(
            f"{csv_path.name}: existing rows={len(old)} last={last_time} -> fetch_from={fetch_from}"
        )
    else:
        old = None
        fetch_from = start_ts
        log(f"{csv_path.name}: not found -> fresh export from {fetch_from}")

    # end_ts ã‚ˆã‚Šé€²ã‚“ã§ã„ãŸã‚‰ã€æ–°è¦å–å¾—ã¯è¡Œã‚ãªã„
    if end_ts <= fetch_from:
        log(
            f"{csv_path.name}: end_ts <= fetch_from ({end_ts} <= {fetch_from}) -> no new fetch"
        )
        merged = old if old is not None else pd.DataFrame(columns=CSV_COLS)
    else:
        # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå°‘ã—ä½™åˆ†ã«å–ã‚Šç›´ã—ã¦é‡è¤‡ã§å¸åï¼‰
        df_new = fetch_rates(symbol, tf_const, fetch_from, end_ts)
        log(f"{csv_path.name}: fetched rows={len(df_new)} [{fetch_from} .. {end_ts}]")

        # ãƒãƒ¼ã‚¸ï¼†é‡è¤‡é™¤å»
        merged = merge_and_dedup(old, df_new)

    # å‹æœ€é©åŒ–ï¼ˆçœãƒ¡ãƒ¢ãƒªï¼‰
    if not merged.empty:
        for c in ["open", "high", "low", "close"]:
            merged[c] = merged[c].astype("float32")
        for c in ["tick_volume", "spread", "real_volume"]:
            merged[c] = merged[c].astype("int32")

    merged.to_csv(csv_path, index=False)
    log(f"{csv_path.name}: wrote rows={len(merged)}")
    log(f"filepath: {csv_path.resolve()}")
    log(f"=== end timeframe={tf_name} ===")
    return csv_path


# =========================
# ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# =========================


def main():
    parser = argparse.ArgumentParser(
        description="Export/Update MT5 rates to CSV per timeframe."
    )
    parser.add_argument(
        "--symbol", default=SYMBOL_DEFAULT, help="ã‚·ãƒ³ãƒœãƒ«ï¼ˆä¾‹: USDJPYï¼‰"
    )
    parser.add_argument(
        "--timeframes", nargs="+", default=TIMEFRAMES_DEFAULT, help="ä¾‹: M5 M15 H1"
    )
    parser.add_argument(
        "--start", default=START_DATE_DEFAULT, help="é–‹å§‹æ—¥ï¼ˆä¾‹: 2020-11-01ï¼‰"
    )
    parser.add_argument(
        "--end",
        default=None,
        help="çµ‚äº†æ—¥ï¼ˆä¾‹: 2024-07-10ã€çœç•¥æ™‚ã¯ç¾åœ¨æ™‚åˆ»ã¾ã§ï¼‰",
    )
    parser.add_argument(
        "--data-dir",
        default=DATA_DIR_DEFAULT,
        help="ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆç›¸å¯¾ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆåŸºæº–ï¼‰",
    )
    parser.add_argument(
        "--terminal", default=None, help="MT5 terminal.exe ã®ãƒ•ãƒ«ãƒ‘ã‚¹ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰"
    )

    # ç’°å¢ƒã¨ä¿å­˜ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    parser.add_argument(
        "--env",
        choices=["laptop", "desktop", "vps"],
        default=None,
        help="ç’°å¢ƒåã‚’æ˜ç¤ºï¼ˆlaptop/desktop/vpsï¼‰ã€‚æœªæŒ‡å®šãªã‚‰ãƒ›ã‚¹ãƒˆåãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã€‚",
    )
    parser.add_argument(
        "--layout",
        choices=["flat", "per-symbol"],
        default="per-symbol",
        help="CSVä¿å­˜ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€‚flat= data/ç›´ä¸‹, per-symbol= data/<SYMBOL>/ohlcv/ ä¸‹ã«ä¿å­˜",
    )

    args = parser.parse_args()

    symbol = args.symbol.upper()
    tfs = [tf.upper() for tf in args.timeframes]

    # data_root ã‚’æ±ºå®šï¼ˆFXBOT_DATA ç’°å¢ƒå¤‰æ•°ã€--data-dir ã‚’è€ƒæ…®ï¼‰
    if get_data_root is not None:
        data_root = get_data_root(cli_data_dir=args.data_dir)
    else:
        raw_data_dir = Path(args.data_dir)
        data_root = (
            raw_data_dir
            if raw_data_dir.is_absolute()
            else (PROJECT_ROOT / raw_data_dir)
        )
        data_root = data_root.resolve()

    end_display = args.end or "NOW"
    log(
        f"start export: symbol={symbol} tfs={tfs} start={args.start} end={end_display} data_root={data_root}"
    )
    log(f"cwd={Path.cwd()} project_root={PROJECT_ROOT}")

    # ç’°å¢ƒæ¨å®šï¼ˆæ˜ç¤ºæŒ‡å®šå„ªå…ˆ â†’ HOST_MAP â†’ ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
    host = os.environ.get("COMPUTERNAME", socket.gethostname()).lower()
    HOST_MAP = {
        # å¿…è¦ã«å¿œã˜ã¦å›ºå®šãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¿½åŠ 
        # ä¾‹: "desktop-8rrd83d": "laptop",
        # "sakura-vps": "vps",
    }
    if args.env:
        env_resolved = args.env
    else:
        env_resolved = HOST_MAP.get(host)
        if not env_resolved:
            if (
                "vps" in host
                or "sakura" in host
                or "administrator" in str(Path.home()).lower()
            ):
                env_resolved = "vps"
            elif "desk" in host:
                env_resolved = "desktop"
            else:
                env_resolved = "laptop"
    log(f"ç’°å¢ƒ: {env_resolved} (host={host})")

    # MT5 åˆæœŸåŒ–
    ensure_mt5_initialized(terminal_path=args.terminal)

    # ã‚·ãƒ³ãƒœãƒ«è§£æ±ºï¼ˆUSDJPY / USDJPY- ãªã©ï¼‰
    resolved_symbol = resolve_symbol(symbol)
    if resolved_symbol != symbol:
        log(f"symbol resolved: {symbol} -> {resolved_symbol}")
    symbol = resolved_symbol
    mt5.symbol_select(symbol, True)

    # CSV ç”¨ã® â€œæ¥å°¾è¾ãªã—ã‚¿ã‚°â€ ã‚’ä½œæˆï¼ˆè‹±å­—ã®ã¿æŠ½å‡ºï¼‰
    global FILE_TAG
    FILE_TAG = "".join([c for c in symbol if c.isalpha()]) or symbol

    # ä¿å­˜å…ˆãƒ«ãƒ¼ãƒˆã¯ data_rootï¼ˆä¸€ã¤ã®å ´æ‰€ã‹ã‚‰ get_ohlcv_csv_path ã§å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ±ºå®šï¼‰
    save_root = data_root
    log(f"save_root={save_root}")

    # å–å¾—ãƒ»ä¿å­˜
    created: list[Path] = []
    for tf_name in tfs:
        path = ensure_csv_for_timeframe(
            symbol,
            tf_name,
            args.start,
            save_root,
            end_date=args.end,
            layout=args.layout,
        )
        created.append(path)

    mt5.shutdown()
    log("done.")


if __name__ == "__main__":
    main()



=== file: scripts/make_project_snapshot.py ===

#!/usr/bin/env python
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã¨ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’
1ã¤ã® project_snapshot.txt ã«ã¾ã¨ã‚ã¦å‡ºåŠ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ = ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã® 1 ã¤ä¸Šã®ãƒ•ã‚©ãƒ«ãƒ€
- é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚„å¯¾è±¡æ‹¡å¼µå­ã¯ CONFIG ã®ã¨ã“ã‚ã§èª¿æ•´å¯èƒ½
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ„ãƒªãƒ¼ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ã‚‚è¡¨ç¤ºã—ã¾ã™
"""

from __future__ import annotations

import os
from pathlib import Path
from datetime import datetime
from typing import Iterable, List

# ========================
# è¨­å®š
# ========================

# é™¤å¤–ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
EXCLUDE_DIR_NAMES = {
    ".git",
    ".venv",
    "__pycache__",
    ".mypy_cache",
    ".pytest_cache",
    ".idea",
    ".vscode",
    "logs",
    "data",
    "models",
    "dist",
    "build",
    ".ruff_cache",
}

# ä¸­èº«ã‚’æ›¸ãå‡ºã™å¯¾è±¡ã®æ‹¡å¼µå­
INCLUDE_EXTENSIONS = {
    ".py",
    ".txt",
    ".md",
    ".rst",
    ".yaml",
    ".yml",
    ".ini",
    ".toml",
    ".json",
    ".ps1",
    ".bat",
    ".sh",
}

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ä¸Šé™ï¼ˆãƒã‚¤ãƒˆï¼‰ã€‚ã“ã‚Œã‚’è¶…ãˆã‚‹ã¨ä¸­èº«ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒ˜ãƒƒãƒ€ã ã‘æ›¸ã
MAX_FILE_SIZE_BYTES = 100_000  # 100KB

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
SNAPSHOT_FILENAME = "project_snapshot.txt"


# ========================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ========================

def iter_tree(root: Path) -> Iterable[Path]:
    """root é…ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ walk ã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
    é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
    """
    for dirpath, dirnames, filenames in os.walk(root):
        # dirnames ã‚’ in-place ã§ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹ã¨ os.walk ãŒãã‚Œã‚’è¾¿ã‚‰ãªããªã‚‹
        dirnames[:] = [
            d for d in dirnames
            if d not in EXCLUDE_DIR_NAMES
        ]
        current_dir = Path(dirpath)
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªèº«
        yield current_dir
        # ãƒ•ã‚¡ã‚¤ãƒ«
        for fname in filenames:
            yield current_dir / fname


def make_tree_text(root: Path) -> str:
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ„ãƒªãƒ¼æ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯æœ€çµ‚æ›´æ–°æ—¥æ™‚ä»˜ãï¼‰ã€‚"""
    lines: List[str] = []

    root_str = root.name
    lines.append(f"{root_str}/")

    # root ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã§ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º
    paths = sorted(
        (p for p in iter_tree(root)),
        key=lambda p: str(p.relative_to(root)).lower(),
    )

    seen_dirs = set()

    for p in paths:
        rel = p.relative_to(root)
        parts = rel.parts

        # ãƒ«ãƒ¼ãƒˆè‡ªèº«ã¯ã‚‚ã†æ›¸ã„ã¦ã„ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
        if rel == Path("."):
            continue

        indent = "  " * (len(parts) - 1)
        name = parts[-1]

        if p.is_dir():
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            dir_key = rel
            if dir_key in seen_dirs:
                continue
            seen_dirs.add(dir_key)
            lines.append(f"{indent}{name}/")
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€çµ‚æ›´æ–°æ—¥æ™‚ã‚’ä»˜ã‘ã‚‹ï¼‰
            try:
                mtime = datetime.fromtimestamp(p.stat().st_mtime)
                mtime_str = mtime.strftime("%Y-%m-%d %H:%M:%S")
            except OSError:
                mtime_str = "unknown"
            lines.append(f"{indent}{name} (updated: {mtime_str})")

    return "\n".join(lines)


def should_dump_content(path: Path) -> bool:
    """ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«å«ã‚ã‚‹ã‹åˆ¤å®šã€‚"""
    if not path.is_file():
        return False

    if path.suffix.lower() not in INCLUDE_EXTENSIONS:
        return False

    try:
        size = path.stat().st_size
    except OSError:
        return False

    if size > MAX_FILE_SIZE_BYTES:
        return False

    return True


def read_text_safely(path: Path) -> str:
    """UTF-8 ã§èª­ã¿ã¤ã¤ã€èª­ã‚ãªã„æ–‡å­—ã¯ç½®ãæ›ãˆã‚‹ã€‚"""
    try:
        return path.read_text(encoding="utf-8", errors="replace")
    except Exception as e:  # noqa: BLE001
        return f"<< FAILED TO READ FILE: {e} >>"


def collect_files_for_dump(root: Path) -> List[Path]:
    """ä¸­èº«ã‚’æŠœãå‡ºã™å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¿”ã™ã€‚"""
    files: List[Path] = []
    for p in iter_tree(root):
        if p.is_file() and should_dump_content(p):
            files.append(p)
    files.sort(key=lambda p: str(p.relative_to(root)).lower())
    return files


# ========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================

def main() -> int:
    # ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã® 1 ã¤ä¸Šã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨ã¿ãªã™
    script_path = Path(__file__).resolve()
    project_root = script_path.parent.parent
    snapshot_path = project_root / SNAPSHOT_FILENAME

    print(f"[INFO] script_path   = {script_path}")
    print(f"[INFO] project_root  = {project_root}")
    print(f"[INFO] snapshot_path = {snapshot_path}")

    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # 1) ãƒ˜ãƒƒãƒ€
    header_lines = [
        "#" * 60,
        "# project_snapshot",
        "#" * 60,
        f"generated_at: {now}",
        f"project_root: {project_root}",
        "",
        "NOTE:",
        "  - ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ ChatGPT ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã¨ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¼ãˆã‚‹ãŸã‚ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§ã™ã€‚",
        "  - ãƒ­ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ».gitãƒ».venv ãªã©ã¯é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚",
        "  - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ„ãƒªãƒ¼ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ (updated: ...) ã‚’å«ã¿ã¾ã™ã€‚",
        "",
        "========================================",
        "=== DIRECTORY TREE =====================",
        "========================================",
        "",
    ]

    tree_text = make_tree_text(project_root)

    # 2) ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
    files = collect_files_for_dump(project_root)

    content_lines: List[str] = []
    content_lines.append("")
    content_lines.append("")
    content_lines.append("========================================")
    content_lines.append("=== FILE CONTENTS ======================")
    content_lines.append("========================================")
    content_lines.append("")

    for fpath in files:
        rel = fpath.relative_to(project_root)
        content_lines.append("")
        content_lines.append(f"=== file: {rel.as_posix()} ===")
        content_lines.append("")
        txt = read_text_safely(fpath)
        content_lines.append(txt)
        content_lines.append("")  # åŒºåˆ‡ã‚Š

    # 3) ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æ›¸ãå‡ºã—
    all_text = "\n".join(header_lines) + "\n" + tree_text + "\n" + "\n".join(content_lines)

    try:
        snapshot_path.write_text(all_text, encoding="utf-8")
    except Exception as e:  # noqa: BLE001
        print(f"[ERROR] failed to write snapshot: {e}")
        return 1

    print(f"[OK] snapshot written: {snapshot_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/make_toy_model.py ===

from __future__ import annotations

import json
import os
from pathlib import Path

import joblib
import numpy as np
import pandas as pd

USE_LGB = True
try:
    from lightgbm import LGBMClassifier
except Exception:
    USE_LGB = False
    from sklearn.linear_model import LogisticRegression

MODELS_DIR = Path("models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)

FEATURES = [
    "ema_5",
    "ema_20",
    "rsi_14",
    "atr_14",
    "adx_14",
    "bbp",
    "vol_chg",
    "wick_ratio",
]


def make_synthetic(n: int = 5000, seed: int = 42) -> tuple[pd.DataFrame, np.ndarray]:
    rng = np.random.default_rng(seed)
    X = pd.DataFrame(
        {
            "ema_5": rng.normal(0.0, 0.2, n),
            "ema_20": rng.normal(0.0, 0.2, n),
            "rsi_14": rng.uniform(0, 100, n),
            "atr_14": rng.uniform(0, 1, n),
            "adx_14": rng.uniform(5, 40, n),
            "bbp": rng.uniform(-0.5, 1.5, n),
            "vol_chg": rng.normal(0.0, 0.05, n),
            "wick_ratio": rng.uniform(0, 1, n),
        }
    )
    score = (
        0.8 * X["ema_5"]
        - 0.5 * X["ema_20"]
        + 0.01 * (X["rsi_14"] - 50)
        + 0.4 * (X["bbp"] - 0.5)
        - 0.2 * X["vol_chg"]
        + 0.15 * (X["adx_14"] > 20).astype(float)
    )
    p = 1 / (1 + np.exp(-score))
    y = (rng.uniform(0, 1, n) < p).astype(int)
    return X[FEATURES], y


def main() -> None:
    X, y = make_synthetic()

    if USE_LGB:
        model = LGBMClassifier(
            n_estimators=200,
            max_depth=-1,
            num_leaves=31,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=123,
            n_jobs=1,
        )
    else:
        model = LogisticRegression(max_iter=1000, n_jobs=1)

    model.fit(X, y)

    joblib.dump(model, MODELS_DIR / "LightGBM_clf.pkl")
    with open(MODELS_DIR / "LightGBM_clf.features.json", "w", encoding="utf-8") as f:
        json.dump(list(X.columns), f, ensure_ascii=False, indent=2)

    classes = getattr(model, "classes_", None)
    if classes is not None:
        with open(MODELS_DIR / "LightGBM_clf.classes.json", "w", encoding="utf-8") as f:
            json.dump([str(c) for c in classes], f, ensure_ascii=False, indent=2)

    print("[OK] Exported:")
    print(" - models/LightGBM_clf.pkl")
    print(" - models/LightGBM_clf.features.json")
    print(" - models/LightGBM_clf.classes.json (optional)")


if __name__ == "__main__":
    main()



=== file: scripts/mt5_export_csv.py ===

# scripts/mt5_export_csv.py
# MT5ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ’ã‚¹ãƒˆãƒªã‹ã‚‰ rates ã‚’å–å¾—â†’ data/{SYMBOL}_{TF}.csv ã«ä¿å­˜
from __future__ import annotations
import argparse
from datetime import datetime, timedelta, timezone
from pathlib import Path
import pandas as pd

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--symbol", required=True)
    ap.add_argument("--timeframe", default="M5", help="M1/M5/M15/M30/H1/H4/D1")
    ap.add_argument("--days", type=int, default=365*2, help="éå»næ—¥ã‚’å–å¾—")
    args = ap.parse_args()

    import MetaTrader5 as mt5  # pip install MetaTrader5

    if not mt5.initialize():
        raise SystemExit("MT5 initialize failed")

    tf_map = {
        "M1": mt5.TIMEFRAME_M1, "M5": mt5.TIMEFRAME_M5, "M15": mt5.TIMEFRAME_M15,
        "M30": mt5.TIMEFRAME_M30, "H1": mt5.TIMEFRAME_H1, "H4": mt5.TIMEFRAME_H4,
        "D1": mt5.TIMEFRAME_D1
    }
    tf = tf_map.get(args.timeframe.upper(), mt5.TIMEFRAME_M5)

    utc_to = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(days=args.days)

    rates = mt5.copy_rates_range(args.symbol, tf, utc_from, utc_to)
    mt5.shutdown()
    if rates is None or len(rates) == 0:
        raise SystemExit("no rates from MT5")

    df = pd.DataFrame(rates)
    # MT5ã® time ã¯unixç§’
    df["time"] = pd.to_datetime(df["time"], unit="s")
    df.rename(columns={"real_volume":"tick_volume"}, inplace=True)

    out_dir = Path(__file__).resolve().parents[1] / "data"
    out_dir.mkdir(parents=True, exist_ok=True)
    out = out_dir / f"{args.symbol.upper()}_{args.timeframe.upper()}.csv"
    df[["time","open","high","low","close","tick_volume"]].to_csv(out, index=False)
    print(f"wrote: {out} rows={len(df)}")

if __name__ == "__main__":
    main()



=== file: scripts/mt5_smoke.py ===

# scripts/mt5_smoke.py
#
# ç›®çš„:
#   - MetaTrader5 ã®åˆæœŸåŒ–ãŒæˆåŠŸã™ã‚‹ã‹
#   - ç¾åœ¨ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ã‚‹å£åº§æƒ…å ±ãŒå–ã‚Œã‚‹ã‹
# ã‚’ç¢ºèªã™ã‚‹ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã€‚

import MetaTrader5 as mt5


def main() -> None:
    print("[mt5_smoke] initialize() ...")
    if not mt5.initialize():
        print(f"[mt5_smoke] initialize() FAILED: last_error={mt5.last_error()}")
        print("  -> MT5 ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    print("[mt5_smoke] initialize() OK")

    info = mt5.account_info()
    if info is None:
        print("[mt5_smoke] account_info() is None.")
        print("  -> MT5 ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€ãƒ‡ãƒ¢å£åº§ãªã©ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        print("[mt5_smoke] account_info():")
        print(f"  login   = {info.login}")
        print(f"  name    = {info.name}")
        print(f"  balance = {info.balance}")
        print(f"  equity  = {info.equity}")

    mt5.shutdown()
    print("[mt5_smoke] shutdown() done.")


if __name__ == "__main__":
    main()



=== file: scripts/print_runtime.py ===

from __future__ import annotations

from core.config import cfg
from core.utils.runtime import is_live

print("is_live:", is_live())
print("filters:", cfg.get("filters"))
print("entry:", cfg.get("entry"))
print("session:", cfg.get("session"))



=== file: scripts/promote_model.py ===

# scripts/promote_model.py
import os, shutil, json, sys
from core.config import cfg
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)


def main() -> None:
    tr = cfg.get("training", {})
    staging = tr.get("staging_dir", "models/_staging")
    prod = tr.get("model_out_dir", "models")
    if not os.path.isdir(staging):
        raise SystemExit("staging not found")
    prom = os.path.join(staging, "PROMOTE.json")
    if os.path.exists(prom):
        st = json.load(open(prom, "r", encoding="utf-8"))
        print("PROMOTE.json:", st)
    # ä¸Šæ›¸ãæ˜‡æ ¼
    os.makedirs(prod, exist_ok=True)
    ts = __import__("datetime").datetime.now().strftime("%Y%m%d_%H%M%S")
    bk = os.path.join(prod, f"_backup_{ts}")
    os.makedirs(bk, exist_ok=True)
    for fn in os.listdir(prod):
        if fn.startswith("_backup_"): continue
        shutil.copy2(os.path.join(prod, fn), os.path.join(bk, fn))
    for fn in os.listdir(staging):
        shutil.copy2(os.path.join(staging, fn), os.path.join(prod, fn))
    print("PROMOTED. backup:", bk)


if __name__ == "__main__":
    main()



=== file: scripts/register_weekly_task.ps1 ===

param(
  [string]$TaskName = "FXBot_WeeklyRetrain",
  [string]$PythonExe = "$env:USERPROFILE\AppData\Local\Programs\Python\Python313\python.exe",
  [string]$ProjectDir = "C:\fxbot",
  [string]$StartTime = "03:05",   # JST
  [ValidateSet("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")]
  [string]$DayOfWeek = "Sunday"
)

$Action = New-ScheduledTaskAction -Execute $PythonExe -Argument "scripts/walkforward_retrain.py" -WorkingDirectory $ProjectDir
$Trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek $DayOfWeek -At $StartTime
$Settings = New-ScheduledTaskSettingsSet -StartWhenAvailable -AllowStartIfOnBatteries -DontStopOnIdleEnd
Register-ScheduledTask -TaskName $TaskName -Action $Action -Trigger $Trigger -Settings $Settings -Description "Weekly walk-forward retrain & promote if metrics pass"
Write-Host "Registered task:" $TaskName



=== file: scripts/rollback_model.py ===

# scripts/rollback_model.py
import os, shutil, sys
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)


def main() -> None:
    prod = "models"
    bks = sorted([d for d in os.listdir(prod) if d.startswith("_backup_")])
    if not bks:
        raise SystemExit("no backups found")
    last = os.path.join(prod, bks[-1])
    for fn in os.listdir(last):
        shutil.copy2(os.path.join(last, fn), os.path.join(prod, fn))
    print("ROLLED BACK to:", last)


if __name__ == "__main__":
    main()



=== file: scripts/selftest_mt5.py ===

from __future__ import annotations

import sys
import traceback
from pathlib import Path
from typing import Any

# ============================================
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ sys.path ã«è¿½åŠ 
# ï¼ˆscripts/ ã® 1 å€‹ä¸ŠãŒ fxbot ãƒ«ãƒ¼ãƒˆï¼‰
# ============================================
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from app.core import mt5_client  # noqa: E402


def _get_attr(obj: Any, name: str, default: Any = "(n/a)") -> Any:
    """dict / MT5 ã® AccountInfo ã®ã©ã¡ã‚‰ã§ã‚‚å®‰å…¨ã«å±æ€§ã‚’å–ã‚Šå‡ºã™ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚"""
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def main() -> int:
    print("=== MT5 self test ===")
    print("ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ç¾åœ¨ã®è¨­å®šã‚¿ãƒ–ã§é¸æŠã•ã‚Œã¦ã„ã‚‹å£åº§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦æ¥ç¶šç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚")
    print(f"PROJECT_ROOT: {PROJECT_ROOT}")
    print()

    try:
        # 1) initialize
        print("[1] mt5_client.initialize() ...")
        ok = mt5_client.initialize()
        print(f"    -> initialize() returned: {ok!r}")
        if not ok:
            print("ERROR: MT5 ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            print(" - MT5 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ï¼Ÿ")
            print(" - è¨­å®šã‚¿ãƒ–ã§é¸æŠã—ãŸå£åº§ID / ã‚µãƒ¼ãƒãƒ¼ / ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯æ­£ã—ã„ã‹ï¼Ÿ")
            return 1

        # 2) account_info
        print()
        print("[2] mt5_client.get_account_info() ...")
        info = mt5_client.get_account_info()
        if not info:
            print("ERROR: get_account_info() ãŒ None / False ã‚’è¿”ã—ã¾ã—ãŸã€‚")
            print("      ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚„ã‚µãƒ¼ãƒãƒ¼è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return 2

        login = _get_attr(info, "login")
        name = _get_attr(info, "name")
        server = _get_attr(info, "server")
        balance = _get_attr(info, "balance")
        equity = _get_attr(info, "equity")
        trade_mode = _get_attr(info, "trade_mode")

        print("  --- Account Info ---")
        print(f"  login      : {login}")
        print(f"  name       : {name}")
        print(f"  server     : {server}")
        print(f"  balance    : {balance}")
        print(f"  equity     : {equity}")
        print(f"  trade_mode : {trade_mode}")
        print("  --------------------")

        # 3) positions (raw)
        print()
        print("[3] mt5_client.get_positions() ...")
        positions = mt5_client.get_positions()
        n_pos = len(positions) if positions is not None else 0
        print(f"    -> open positions: {n_pos}")
        if positions:
            # å…ˆé ­æ•°ä»¶ã ã‘ã–ã£ãã‚Šè¡¨ç¤º
            print("    sample positions (up to 5):")
            for i, pos in enumerate(positions[:5]):
                print(f"      [{i}] {pos!r}")

        # 4) positions_df (DataFrame)
        print()
        print("[4] mt5_client.get_positions_df() ...")
        try:
            df = mt5_client.get_positions_df()
        except Exception as e:  # noqa: BLE001
            print(f"    WARN: get_positions_df() ã§ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e!r}")
        else:
            if df is None:
                print("    -> DataFrame: None ï¼ˆãƒã‚¸ã‚·ãƒ§ãƒ³ç„¡ã—ã‹ã€æœªå¯¾å¿œã®å¯èƒ½æ€§ï¼‰")
            else:
                try:
                    print("    -> DataFrame å½¢å¼ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ä¸€è¦§:")
                    print(df)
                except Exception:
                    print("    -> DataFrame ã® print ä¸­ã«ä¾‹å¤–ãŒå‡ºãŸãŸã‚ç°¡æ˜“è¡¨ç¤ºã«åˆ‡æ›¿ãˆã¾ã™ã€‚")
                    print(repr(df))

        print()
        print("=== MT5 self test finished: SUCCESS ===")
        return 0

    except Exception:
        print()
        print("=== MT5 self test crashed ===")
        traceback.print_exc()
        return 99

    finally:
        # shutdown ã¯å¿µã®ãŸã‚ä¾‹å¤–æ¡ã‚Šã¤ã¶ã—ã§
        try:
            print()
            print("[*] mt5_client.shutdown() ...")
            mt5_client.shutdown()
            print("    -> shutdown() å®Œäº†")
        except Exception as e:  # noqa: BLE001
            print(f"    WARN: shutdown() ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e!r}")


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/selftest_order_flow.py ===

from __future__ import annotations

import os
import sys
import time
from typing import Any

import MetaTrader5 as MT5  # type: ignore[import]
from core.config import cfg  # noqa: F401  # ensure configuration loads
from app.core.mt5_client import MT5Client


def _get_env(name: str) -> str:
    v = os.getenv(name, "")
    if not v:
        raise SystemExit(
            f"ç’°å¢ƒå¤‰æ•° {name} ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
            "GUI ã®è¨­å®šã‚¿ãƒ–ã§å£åº§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ã‹ã‚‰ã€å†åº¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )
    return v


def _get_attr(obj: Any, name: str, default: Any = "(n/a)") -> Any:
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def _wait_for_position(ticket: int, timeout_sec: float = 10.0, interval: float = 0.5) -> bool:
    """æŒ‡å®š ticket ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ãŒ MT5.positions_get ã§è¦‹ãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã¾ã§å¾…ã¤ã€‚"""
    deadline = time.time() + timeout_sec
    while time.time() < deadline:
        pos = MT5.positions_get(ticket=ticket)
        if pos:
            return True
        time.sleep(interval)
    return False


def main() -> int:
    print("=== MT5 order flow self test ===")
    print("ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ DEMO å£åº§ã§ã€")
    print("  1) æˆè¡Œ BUY ã§ 0.01 lot ã‚¨ãƒ³ãƒˆãƒªãƒ¼")
    print("  2) ç´„å®šç¢ºèª")
    print("  3) ã™ãæˆè¡Œã‚¯ãƒ­ãƒ¼ã‚º")
    print("ã‚’è¡Œã„ã¾ã™ã€‚å¿…ãšãƒ‡ãƒ¢å£åº§ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    print()

    # --- èªè¨¼æƒ…å ± & ãƒ†ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
    login = int(_get_env("MT5_LOGIN"))
    password = _get_env("MT5_PASSWORD")
    server = _get_env("MT5_SERVER")

    symbol = os.getenv("FXBOT_TEST_SYMBOL", "USDJPY-")
    lot = float(os.getenv("FXBOT_TEST_LOT", "0.01"))

    print(f"[config] login={login} server={server} symbol={symbol} lot={lot}")
    print()

    client = MT5Client(login=login, password=password, server=server)

    try:
        # 1) initialize
        print("[1] client.initialize()")
        if not client.initialize():
            print("    -> initialize() å¤±æ•—")
            return 1
        print("    -> OK")

        # 2) login_account
        print("[2] client.login_account()")
        if not client.login_account():
            print("    -> login_account() å¤±æ•—")
            return 2
        print("    -> OK")

        # 3) account_info è¡¨ç¤º
        info = MT5.account_info()
        print()
        print("[3] account_info()")
        if info is None:
            print("    -> account_info() ãŒ None")
        else:
            print(f"    login   : {_get_attr(info, 'login')}")
            print(f"    name    : {_get_attr(info, 'name')}")
            print(f"    balance : {_get_attr(info, 'balance')}")
            print(f"    equity  : {_get_attr(info, 'equity')}")

        # 4) æˆè¡Œ BUY ã‚¨ãƒ³ãƒˆãƒªãƒ¼
        print()
        print(f"[4] order_send() ã§ BUY ã‚¨ãƒ³ãƒˆãƒªãƒ¼: symbol={symbol}, lot={lot}")
        ticket = client.order_send(symbol=symbol, order_type="BUY", lot=lot)
        if not ticket:
            print("    -> order_send() ãŒ ticket ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return 3
        print(f"    -> ç™ºæ³¨æˆåŠŸ: ticket={ticket}")

        # 5) ãƒã‚¸ã‚·ãƒ§ãƒ³å‡ºç¾å¾…ã¡
        print()
        print("[5] positions_get(ticket) ã§ãƒã‚¸ã‚·ãƒ§ãƒ³å‡ºç¾ã‚’å¾…æ©Ÿä¸­...")
        if not _wait_for_position(ticket):
            print("    -> ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ãƒã‚¸ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return 4
        print("    -> ãƒã‚¸ã‚·ãƒ§ãƒ³æ¤œå‡º OK")

        # 6) æˆè¡Œã‚¯ãƒ­ãƒ¼ã‚º
        print()
        print("[6] close_position() ã§å³ã‚¯ãƒ­ãƒ¼ã‚º")
        ok = client.close_position(ticket=ticket, symbol=symbol)
        print(f"    -> close_position() returned {ok!r}")
        if not ok:
            print("    -> ã‚¯ãƒ­ãƒ¼ã‚ºå¤±æ•—")
            return 5

        # 7) ã‚¯ãƒ­ãƒ¼ã‚ºå¾Œã®ç¢ºèª
        time.sleep(1.0)
        remaining = MT5.positions_get(ticket=ticket)
        print()
        print(f"[7] close å¾Œã® positions_get(ticket={ticket}) = {remaining}")
        if remaining:
            print("    WARN: ã‚¯ãƒ­ãƒ¼ã‚ºå¾Œã‚‚ãƒã‚¸ã‚·ãƒ§ãƒ³ãŒæ®‹ã£ã¦ã„ã¾ã™ã€‚")
            return 6

        print()
        print("=== MT5 order flow self test finished: SUCCESS ===")
        return 0

    finally:
        print()
        print("[*] client.shutdown()")
        try:
            client.shutdown()
            print("    -> shutdown() å®Œäº†")
        except Exception as e:  # noqa: BLE001
            print(f"    WARN: shutdown() ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e!r}")


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/sim_trailing.py ===

import argparse

from app.services.trailing import AtrTrailer, TrailConfig, TrailState


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--side", choices=["BUY", "SELL"], default="BUY")
    parser.add_argument("--entry", type=float, required=True)
    parser.add_argument(
        "--atr",
        type=float,
        required=True,
        help="ATR in price units, e.g., 0.12 for 12 pips on USDJPY",
    )
    parser.add_argument("--pip", type=float, default=0.01, help="pip size, USDJPY=0.01")
    parser.add_argument("--point", type=float, default=0.001, help="point size, USDJPY=0.001")
    parser.add_argument("--activate", type=float, default=0.5)
    parser.add_argument("--step", type=float, default=0.25)
    parser.add_argument("--lockbe", type=float, default=0.3)
    parser.add_argument("--floor", type=float, default=5.0)
    args = parser.parse_args()

    cfg = TrailConfig(
        pip_size=args.pip,
        point=args.point,
        atr=args.atr,
        activate_mult=args.activate,
        step_mult=args.step,
        lock_be_mult=args.lockbe,
        hard_floor_pips=args.floor,
        only_in_profit=True,
        max_layers=20,
    )
    state = TrailState(side=args.side, entry=args.entry)
    trailer = AtrTrailer(cfg, state)

    steps = []
    for i in range(0, 31):
        delta = cfg.atr * 0.1 * i
        if args.side == "BUY":
            steps.append(args.entry + delta)
        else:
            steps.append(args.entry - delta)

    print(f"# side={args.side} entry={args.entry} atr={args.atr}")
    print("# price, activated, be_locked, layers, current_sl, new_sl")
    for px in steps:
        new_sl = trailer.suggest_sl(px)
        print(f"{px:.5f}, {state.activated}, {state.be_locked}, {state.layers}, {state.current_sl}, {new_sl}")


if __name__ == "__main__":
    main()



=== file: scripts/swap_model.py ===

from __future__ import annotations

import glob
import hashlib
import json
import os
import shutil
import sys
import time
from pathlib import Path

ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

MODELS_DIR = "models"
LIVE_LINK = os.path.join(MODELS_DIR, "live")


def _is_ready(bundle_dir: str) -> bool:
    ready = os.path.exists(os.path.join(bundle_dir, "READY"))
    manifest = os.path.exists(os.path.join(bundle_dir, "manifest.json"))
    return ready and manifest


def _latest_ready() -> str:
    candidates: list[tuple[float, str]] = []
    for bundle in glob.glob(os.path.join(MODELS_DIR, "*")):
        if (
            os.path.isdir(bundle)
            and _is_ready(bundle)
            and os.path.basename(bundle) not in {"live", "prev"}
        ):
            candidates.append((os.path.getmtime(bundle), bundle))
    if not candidates:
        raise SystemExit("no READY bundles found.")
    candidates.sort(reverse=True)
    return candidates[0][1]


def _sha256(path: Path) -> str:
    digest = hashlib.sha256()
    with open(path, "rb") as fh:
        for chunk in iter(lambda: fh.read(1024 * 1024), b""):
            digest.update(chunk)
    return digest.hexdigest()


def _load_latest_best_threshold() -> tuple[float | None, str | None]:
    try:
        report_dir = os.path.join("logs", "retrain")
        pattern = os.path.join(report_dir, "report_*.json")
        reports = sorted(glob.glob(pattern))
        if not reports:
            return None, None
        latest = reports[-1]
        with open(latest, "r", encoding="utf-8") as fh:
            data = json.load(fh)
        metrics = data.get("metrics_test") or {}
        best_t = metrics.get("best_threshold")
        if isinstance(best_t, (int, float)):
            return float(best_t), latest
        return None, latest
    except Exception:
        return None, None


def activate_model_from_pkl(
    pkl_path: Path, models_dir: Path, target_name: str = "LightGBM_clf.pkl"
) -> int:
    models_dir.mkdir(parents=True, exist_ok=True)
    active_pkl = models_dir / target_name
    tmp = active_pkl.with_suffix(".tmp.pkl")
    shutil.copy2(pkl_path, tmp)
    if active_pkl.exists():
        backup = models_dir / f"backup_{int(time.time())}.pkl"
        shutil.copy2(active_pkl, backup)
    os.replace(tmp, active_pkl)

    best_t, source_report = _load_latest_best_threshold()

    meta = {
        "activated_at_jst": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
        "source": "direct-pkl",
        "source_path": str(pkl_path.resolve()),
        "target_path": str(active_pkl.resolve()),
        "sha256": _sha256(active_pkl),
        "version": str(time.time()),
        "features_hash": None,
    }
    meta["best_threshold"] = None if best_t is None else f"{float(best_t):.6f}"
    if source_report:
        meta["best_threshold_source_report"] = source_report
    with open(models_dir / "active_model.json", "w", encoding="utf-8") as fh:
        json.dump(meta, fh, ensure_ascii=False, indent=2)
    print(f"[swap_model] ACTIVATED direct pkl -> {active_pkl}")
    return 0


def _fallback_main(args: list[str]) -> int:
    # READYæ¢ç´¢ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚å˜ç‹¬å®Ÿè¡Œã«ã‚‚å¯¾å¿œã€‚
    if args:
        candidate = Path(args[0])
        if candidate.suffix.lower() == ".pkl" and candidate.exists():
            return activate_model_from_pkl(candidate, Path("models"))
    print("no READY bundles found and no valid .pkl path provided.")
    return 1


def main(args: list[str] | None = None) -> int:
    if args is None:
        args = sys.argv[1:]
    try:
        target = _latest_ready()
    except SystemExit as exc:
        if exc.code:
            print(exc.code)
        return _fallback_main(args)

    prev = os.path.join(MODELS_DIR, "prev")
    if os.path.exists(prev):
        shutil.rmtree(prev)
    if os.path.exists(LIVE_LINK):
        shutil.move(LIVE_LINK, prev)
    shutil.copytree(target, LIVE_LINK)
    print(f"[SWAP] live -> {os.path.basename(target)} (prev saved)")
    return 0


if __name__ == "__main__":
    # æ—¢å­˜ã®READYåˆ‡æ›¿ãƒ­ã‚¸ãƒƒã‚¯ãŒsys.exitã™ã‚‹å‰ã«ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‘¼ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹ä¿é™ºã€‚
    sys.exit(main())



=== file: scripts/train_calibrator.py ===

# scripts/train_calibrator.py
from __future__ import annotations
import numpy as np, joblib
from pathlib import Path
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression

import sys
if not (Path("logs/val_p_buy_raw.npy").exists() and Path("logs/val_y_true.npy").exists()):
    sys.exit("val_p_buy_raw.npy / val_y_true.npy ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«æ¤œè¨¼æ¨è«–ã‚’å®Ÿè¡Œã—ã¦ä¿å­˜ã—ã¦ãã ã•ã„ã€‚")
    
# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ã€Œç”ŸBUYç¢ºç‡ã€ã¨ã€Œæ­£è§£ãƒ©ãƒ™ãƒ«ã€ã‚’ç”¨æ„ã—ã¦ä¿å­˜ã—ã¦ãŠã
# ä¾‹: logs/val_p_buy_raw.npy (shape (N,)), logs/val_y_true.npy (0/1)
p_raw = np.load("logs/val_p_buy_raw.npy").astype(float).ravel()
y_true = np.load("logs/val_y_true.npy").astype(int).ravel()

models = Path("models"); models.mkdir(exist_ok=True, parents=True)

# Platt
lr = LogisticRegression(max_iter=1000)
lr.fit(p_raw.reshape(-1,1), y_true)
joblib.dump(lr, models / "calib_platt.pkl")

# Isotonic
iso = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds="clip")
iso.fit(p_raw, y_true)
joblib.dump(iso, models / "calib_isotonic.pkl")

print("wrote: models/calib_platt.pkl, models/calib_isotonic.pkl")



=== file: scripts/verify_smoke.ps1 ===

param(
  [ValidateSet("smoke","strict","live")]
  [string]$Mode = "smoke",
  [int]$Ticks = 600,
  [int]$DtMs = 50,
  [double]$Base = 150.20,
  [double]$Spread = 0.5,
  [double]$AtrPct = 0.00050
)

$ErrorActionPreference = "Stop"

$root = Split-Path -Parent $PSCommandPath
Set-Location (Join-Path $root "..")

$python = "python"

function Invoke-Py {
  param([string]$ArgsLine)
  & $python -c $ArgsLine
}

function PyMod {
  param(
    [string]$Module,
    [string[]]$Arguments
  )
  & $python -m $Module @Arguments
}

Write-Host "== Step 1: Compile =="
PyMod "compileall" @("app","core","scripts") | Out-Null
Write-Host "OK: compile"

Write-Host "== Step 2: Runtime mode check =="
$live = Invoke-Py "from core.utils.runtime import is_live; print('LIVE' if is_live() else 'DRYRUN')"
Write-Host "Runtime:" $live

Write-Host "== Step 3: Show key filters =="
Invoke-Py "from core.config import cfg; f=cfg.get('filters',{}); hy=f.get('atr_hysteresis',{}); print('min_atr_pct=', f.get('min_atr_pct'), 'enable_min=', hy.get('enable_min_pct'), 'disable_min=', hy.get('disable_min_pct')); entry=cfg.get('entry',{}); print('prob_threshold=', entry.get('prob_threshold')); print('session.allow_hours_jst=', cfg.get('session',{}).get('allow_hours_jst'))"

Write-Host "== Step 4: Ensure logs dir =="
$logDir = "logs\decisions"
if (!(Test-Path $logDir)) {
  New-Item -ItemType Directory -Force -Path $logDir | Out-Null
}

Write-Host "== Step 5: Launch GUI =="
Start-Process -WindowStyle Minimized powershell -ArgumentList "-NoLogo -NoExit -Command `"$python -m app.gui.main`""
Start-Sleep -Seconds 2

Write-Host "== Step 6: Dryrun / Replay =="
$extra = @()
switch ($Mode) {
  "smoke"  { $extra = @("--atr-open") }
  "strict" { $extra = @() }
  "live"   { $extra = @() }
}

$argsList = @("--sim") + $extra + @(
  "--n", "$Ticks",
  "--dt", "$DtMs",
  "--base", "$Base",
  "--spread", "$Spread",
  "--atrpct", "$AtrPct"
)
Write-Host "Running: python -m scripts.dryrun_smoke $($argsList -join ' ')"
PyMod "scripts.dryrun_smoke" $argsList

Write-Host "== Step 7: Log scan =="
$decisionLog = Get-ChildItem -Path $logDir -Filter "decisions_*.jsonl" | Sort-Object LastWriteTime | Select-Object -Last 1
if ($null -eq $decisionLog) {
  Write-Warning "No decisions_*.jsonl found."
  $entryCount = 0
} else {
  $entryCount = (Select-String -Path $decisionLog.FullName -Pattern '"decision": "ENTRY"' | Measure-Object).Count
}
$trailFlags = Get-Content "logs\app.log" -ErrorAction SilentlyContinue | Select-String "\[TRAIL\]\[(DRYRUN|OK|NG)\]"
$trailCount = ($trailFlags | Measure-Object).Count

Write-Host "ENTRY count =" $entryCount
Write-Host "TRAIL events =" $trailCount

Write-Host "== Step 8: PASS/FAIL =="
$pass = $false
if ($Mode -eq "smoke") {
  if ($trailCount -ge 1) { $pass = $true }
} else {
  if ($entryCount -ge 1 -or $trailCount -ge 1) { $pass = $true }
}

if ($pass) {
  Write-Host "RESULT: PASS" -ForegroundColor Green
  exit 0
} else {
  Write-Host "RESULT: FAIL" -ForegroundColor Red
  Write-Host "Hints:"
  Write-Host " - smokeãƒ¢ãƒ¼ãƒ‰ã§ã¯ --atr-open ãŒæœ‰åŠ¹ã‹ç¢ºèªã—ã¦ãã ã•ã„"
  Write-Host " - [TRAIL] ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
  Write-Host " - strict ãƒ¢ãƒ¼ãƒ‰ã§ã¯ atrpct ã‚’èª¿æ•´ã—ã¦ã‚²ãƒ¼ãƒˆã‚’è·¨ãã‹æ¤œè¨ã—ã¦ãã ã•ã„"
  exit 1
}



=== file: scripts/walkforward_retrain.py ===

from __future__ import annotations

import argparse
import json
import os
import sys
import warnings
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path

import lightgbm as lgbm
import numpy as np
import pandas as pd
from joblib import dump
from sklearn.metrics import log_loss, precision_recall_curve, roc_auc_score
from sklearn.model_selection import train_test_split

# ------------------------------------------------------------
# åŸºæœ¬è¨­å®šï¼ˆãƒ•ã‚©ãƒ«ãƒ€ãªã©ï¼‰
# ------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"
MODELS_DIR = PROJECT_ROOT / "models"
LOGS_DIR = PROJECT_ROOT / "logs"
MODELS_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)


def resolve_data_root(cli_data_dir: str | None) -> Path:
    """
    ãƒ‡ãƒ¼ã‚¿ã®ãƒ«ãƒ¼ãƒˆå€™è£œã‚’è¤‡æ•°è©¦ã—ã¦ã€æœ€åˆã«å­˜åœ¨ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¡ç”¨ã™ã‚‹ã€‚
    å„ªå…ˆé †ä½:
      1) --data-dir å¼•æ•°
      2) ç’°å¢ƒå¤‰æ•° FXBOT_DATA
      3) ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆé…ä¸‹ã® data/
      4) ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã® data/
    """
    candidates: list[Path] = []

    # 1) CLI å¼•æ•°
    if cli_data_dir:
        candidates.append(Path(cli_data_dir))

    # 2) ç’°å¢ƒå¤‰æ•°
    env_dir = os.getenv("FXBOT_DATA")
    if env_dir:
        candidates.append(Path(env_dir))

    # 3) ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã® data (C:\Users\...\fxbot\data / D:\...\fxbot\data / C:\fxbot\data)
    candidates.append(DATA_DIR)

    # 4) å¿µã®ãŸã‚ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® data
    candidates.append(Path.cwd() / "data")

    existing = [p for p in candidates if p.is_dir()]
    if existing:
        return existing[0].resolve()

    # ã©ã‚Œã‚‚ãªã‘ã‚Œã°æœ€å¾Œã« DATA_DIR ã‚’è¿”ã™ï¼ˆå­˜åœ¨ã—ãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ï¼‰
    return DATA_DIR.resolve()


RNG = np.random.default_rng(42)
pd.options.display.width = 200
warnings.filterwarnings("ignore", category=UserWarning)


# ------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ------------------------------------------------------------
def jst_now_str() -> str:
    return datetime.now(UTC).astimezone().isoformat(timespec="seconds")


def safe_log(msg: str):
    ts = jst_now_str()
    print(f"{ts} | {msg}", flush=True)


def find_csv(symbol: str, timeframe: str, data_dir: str | None = None) -> Path:
    """
    CSVãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆä¸¡å¯¾å¿œ:
      - flat:       data/USDJPY_M5.csv
      - per-symbol: data/USDJPY/ohlcv/  å†…ã®  {symbol}_{tf}.csv ã‚‚ã—ãã¯  {tf}.csv
    å„ªå…ˆé †: æ˜ç¤ºä¸€è‡´ â†’ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒæ–°ã—ã„ã‚‚ã®
    """
    # ãƒ«ãƒ¼ãƒˆæ±ºå®šï¼ˆ--data-dir / FXBOT_DATA / PROJECT_ROOT/data / ./data ã®é †ã§å­˜åœ¨ã‚’ç¢ºèªï¼‰
    root = resolve_data_root(data_dir)

    symU = symbol.upper()
    symL = symbol.lower()
    tf = timeframe.upper()

    # è¨˜å·ä»˜ãã‚·ãƒ³ãƒœãƒ«ï¼ˆUSDJPY- ç­‰ï¼‰ã‹ã‚‰è‹±å­—ã ã‘ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚‚ä½œã‚‹
    symU_clean = "".join(ch for ch in symU if ch.isalpha())
    symL_clean = symU_clean.lower()

    candidates: list[Path] = []

    # --- flat layout (data/ç›´ä¸‹)
    candidates += list(root.glob(f"{symU}_{tf}.csv"))
    candidates += list(root.glob(f"{symL}_{tf}.csv"))
    candidates += list(root.glob(f"{symU_clean}_{tf}.csv"))
    candidates += list(root.glob(f"{symL_clean}_{tf}.csv"))
    candidates += list(root.glob(f"*_{tf}.csv"))  # ä¾‹: ANYTHING_M5.csv

    # --- per-symbol layoutï¼ˆæ¨å¥¨: data/USDJPY/ohlcv/ï¼‰
    base_dirs = [
        root / symU / "ohlcv",
        root / symL / "ohlcv",
        root / symU_clean / "ohlcv",
        root / symL_clean / "ohlcv",
        root / symU,
        root / symL,
        root / symU_clean,
        root / symL_clean,
    ]

    for b in base_dirs:
        candidates += list(b.glob(f"{symU}_{tf}.csv"))
        candidates += list(b.glob(f"{symL}_{tf}.csv"))
        candidates += list(b.glob(f"{symU_clean}_{tf}.csv"))
        candidates += list(b.glob(f"{symL_clean}_{tf}.csv"))
        candidates += list(b.glob(f"*_{tf}.csv"))  # ä¾‹: anyprefix_M5.csv
        candidates += list(b.glob(f"{tf}.csv"))    # ä¾‹: M5.csv

    # å®Ÿåœ¨ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã€é‡è¤‡é™¤å»
    uniq: list[Path] = []
    seen = set()
    for p in candidates:
        if p.is_file():
            try:
                key = p.resolve()
            except Exception:
                key = p
            if key not in seen:
                seen.add(key)
                uniq.append(p)

    if not uniq:
        tried = [
            root / f"{symU}_{tf}.csv",
            root / f"{symU_clean}_{tf}.csv",
            root / symU / "ohlcv" / f"{symU}_{tf}.csv",
            root / symU_clean / "ohlcv" / f"{symU_clean}_{tf}.csv",
        ]
        msg = (
            "CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\\n"
            f"  symbol={symbol} timeframe={timeframe}\\n"
            f"  data_dir={root}\\n"
            "  è©¦ã—ãŸå ´æ‰€ã®ä¾‹:\\n    - " + "\\n    - ".join(str(p) for p in tried)
        )
        raise FileNotFoundError(msg)

    # æ˜ç¤ºä¸€è‡´ï¼ˆ{symbol}_{tf}.csv / cleanç‰ˆï¼‰ãŒã‚ã‚Œã°æœ€å„ªå…ˆ
    exact = [
        p
        for p in uniq
        if p.name.lower()
        in {
            f"{symL}_{tf.lower()}.csv",
            f"{symL_clean}_{tf.lower()}.csv",
        }
    ]
    if exact:
        return exact[0]

    # ãã‚Œä»¥å¤–ã¯æœ€çµ‚æ›´æ–°ãŒæ–°ã—ã„ã‚‚ã®
    return max(uniq, key=lambda p: p.stat().st_mtime)


# ------------------------------------------------------------
# ç‰¹å¾´é‡ç”Ÿæˆ
# ------------------------------------------------------------
def rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = np.clip(delta, 0, None)
    down = -np.clip(delta, None, 0)
    ma_up = up.rolling(period, min_periods=period).mean()
    ma_down = down.rolling(period, min_periods=period).mean()
    rs = ma_up / (ma_down + 1e-12)
    return 100 - (100 / (1 + rs))


def atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    prev_close = close.shift(1)
    tr = pd.concat(
        [
            (high - low),
            (high - prev_close).abs(),
            (low - prev_close).abs(),
        ],
        axis=1,
    ).max(axis=1)
    return tr.rolling(period, min_periods=period).mean()


def build_features(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    å…¥åŠ›: time, open, high, low, close, tick_volume ãªã©ã®OHLCVã‚’æƒ³å®š
    å‡ºåŠ›: ç‰¹å¾´é‡ DataFrameï¼ˆæ¬ æé™¤å»æ¸ˆã¿ï¼‰
    """
    df = df_raw.copy()

    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆã“ã“ã§æ­¢ã¾ã‚‹å ´åˆã¯CSVä¿®æ­£ãŒå¿…è¦ï¼‰
    need = {"time", "open", "high", "low", "close"}
    miss = need - set(df.columns)
    if miss:
        safe_log(f"[WFO][error] CSV missing columns: {sorted(miss)}")
        return pd.DataFrame()

    # æ™‚åˆ»æ•´å‚™
    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time", kind="stable").drop_duplicates(subset=["time"])

    n = len(df)

    # --- ãƒŸãƒ‹ç‰¹å¾´é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆè¡Œæ•°ãŒå°‘ãªã„ã¨ãã®æ•‘æ¸ˆï¼‰ ---
    # 60è¡Œæœªæº€ãªã‚‰ã€ãƒ­ãƒ¼ãƒ«ç³»ã¯ä½¿ã‚ãšã«æœ€ä½é™ã®ç‰¹å¾´é‡ã ã‘ã§è¿”ã™
    if n < 60:
        safe_log(
            f"[WFO][warn] tiny dataset detected ({n} rows). Using mini feature set."
        )
        rng = (df["high"] - df["low"]).replace(0, np.nan)
        mini = pd.DataFrame(
            {
                "time": df["time"],
                "open": df["open"],
                "high": df["high"],
                "low": df["low"],
                "close": df["close"],
                # æœ€ä½é™ï¼š1æœ¬ãƒªã‚¿ãƒ¼ãƒ³ã€ãƒ¬ãƒ³ã‚¸å†…ä½ç½®
                "ret1": df["close"].pct_change().fillna(0.0),
                "pos_in_range": ((df["close"] - df["low"]) / rng).fillna(0.5),
            }
        )
        # æ•°å­¦çš„ã«ãŠã‹ã—ã„å€¤ã‚’é™¤å»
        mini = mini.replace([np.inf, -np.inf], np.nan).dropna()
        return mini

    # --- é€šå¸¸ã®ãƒ•ãƒ«ç‰¹å¾´é‡ãƒ¢ãƒ¼ãƒ‰ ---
    # åŸºæœ¬ã®æˆ»ã‚Šã¨ãƒœãƒ©
    df["ret1"] = df["close"].pct_change()
    df["ret3"] = df["close"].pct_change(3)
    df["ret5"] = df["close"].pct_change(5)
    df["vol20"] = df["close"].pct_change().rolling(20, min_periods=10).std()

    # ç§»å‹•å¹³å‡ãƒ»ãƒãƒ³ãƒ‰ï¼ˆmin_periodsã§æ¶ˆæ»…ã‚’æŠ‘åˆ¶ï¼‰
    for w in (5, 10, 20, 50):
        df[f"sma{w}"] = df["close"].rolling(w, min_periods=max(2, w // 2)).mean()
        df[f"ema{w}"] = df["close"].ewm(span=w, adjust=False).mean()
    df["bb_mid"] = df["close"].rolling(20, min_periods=10).mean()
    df["bb_std"] = df["close"].rolling(20, min_periods=10).std()
    df["bb_p"] = (df["close"] - df["bb_mid"]) / (df["bb_std"] + 1e-12)

    # RSI / ATR
    def _rsi(series: pd.Series, period: int = 14) -> pd.Series:
        delta = series.diff()
        up = np.clip(delta, 0, None)
        down = -np.clip(delta, None, 0)
        ma_up = up.rolling(period, min_periods=period // 2).mean()
        ma_down = down.rolling(period, min_periods=period // 2).mean()
        rs = ma_up / (ma_down + 1e-12)
        return 100 - (100 / (1 + rs))

    def _atr(df_: pd.DataFrame, period: int = 14) -> pd.Series:
        high, low, close = df_["high"], df_["low"], df_["close"]
        prev_close = close.shift(1)
        tr = pd.concat(
            [(high - low), (high - prev_close).abs(), (low - prev_close).abs()],
            axis=1,
        ).max(axis=1)
        return tr.rolling(period, min_periods=period // 2).mean()

    df["rsi14"] = _rsi(df["close"], 14)
    df["atr14"] = _atr(df, 14)

    # ãƒ’ã‚²æ¯”ç‡ï¼ˆãƒ¬ãƒ³ã‚¸ã®ã©ã“ã§å¼•ã‘ãŸã‹ï¼‰
    rng = (df["high"] - df["low"]).replace(0, np.nan)
    df["pos_in_range"] = (df["close"] - df["low"]) / rng

    # å‡ºæ¥é«˜ä»£ç†ï¼ˆã‚ã‚Œã°ï¼‰
    if "tick_volume" in df.columns:
        df["vol_sma20"] = df["tick_volume"].rolling(20, min_periods=10).mean()
        df["vol_chg"] = df["tick_volume"].pct_change()

    feature_cols = [
        "ret1",
        "ret3",
        "ret5",
        "vol20",
        "sma5",
        "sma10",
        "sma20",
        "sma50",
        "ema5",
        "ema10",
        "ema20",
        "ema50",
        "bb_p",
        "rsi14",
        "atr14",
        "pos_in_range",
        "vol_sma20",
        "vol_chg",
    ]
    feature_cols = [c for c in feature_cols if c in df.columns]

    keep_cols = ["time", "open", "high", "low", "close"] + feature_cols
    df = df[keep_cols].copy()

    # å…ˆé ­ã®NaNã‚’ä¸€æ‹¬ã§ãƒˆãƒªãƒ ï¼ˆæœ€å¤§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦50ã«åˆã‚ã›ã‚‹ï¼‰
    trim = 50
    if len(df) > trim:
        df = df.iloc[trim:].copy()

    # ãã‚Œã§ã‚‚æ®‹ã‚‹NaN/infã¯é™¤å»
    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    return df


def make_label(df: pd.DataFrame, horizon: int = 10, pips: float = 0.0) -> pd.Series:
    """
    horizon å¾Œã®æ–¹å‘ãƒ©ãƒ™ãƒ«:
      close_{t+h} - close_t > 0 ãªã‚‰ 1, ãã‚Œä»¥å¤– 0
    pips ã‚’ä¸ãˆãŸå ´åˆã¯é–¾å€¤ã¨ã—ã¦ä½¿ã†ï¼ˆpipsã¯ä¾¡æ ¼å·® 0.01=1pips ç›¸å½“ã®å£åº§ã‚‚ã‚ã‚‹ã®ã§æ³¨æ„ï¼‰
    """
    future = df["close"].shift(-horizon)
    diff = future - df["close"]
    if pips and pips > 0:
        y = (diff > pips).astype(int)
    else:
        y = (diff > 0).astype(int)
    return y


# ------------------------------------------------------------
# WFO ã‚¹ã‚­ãƒ¼ãƒ 
# ------------------------------------------------------------
@dataclass
class WFOMetrics:
    fold: int
    train_start: str
    train_end: str
    test_start: str
    test_end: str
    n_train: int
    n_test: int
    auc: float
    logloss: float
    f1_at_thr: float
    thr: float


def iter_wfo_slices(df: pd.DataFrame, train_bars: int, test_bars: int, step_bars: int):
    """
    walk-forward: å›ºå®šé•·å­¦ç¿’â†’å›ºå®šé•·ãƒ†ã‚¹ãƒˆâ†’stepã§å‰é€²
    """
    n = len(df)
    start = 0
    fold = 0
    while True:
        train_start = start
        train_end = train_start + train_bars
        test_end = train_end + test_bars
        if test_end > n:
            break

        yield fold, slice(train_start, train_end), slice(train_end, test_end)
        fold += 1
        start += step_bars


# ------------------------------------------------------------
# ã—ãã„å€¤æœ€é©åŒ–
# ------------------------------------------------------------
def pick_threshold(y_true: np.ndarray, prob: np.ndarray) -> tuple[float, float]:
    """
    PR æ›²ç·šã‹ã‚‰ F1 æœ€å¤§ç‚¹ã‚’æ¡ç”¨ã€‚é–¾å€¤ã‚’è¿”ã™ã€‚
    """
    precision, recall, thresholds = precision_recall_curve(y_true, prob)
    # thresholds ã®é•·ã•ã¯ len(precision)-1
    f1 = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-12)
    idx = int(np.nanargmax(f1))
    best_thr = float(np.clip(thresholds[idx], 0.05, 0.95))
    return best_thr, float(f1[idx])


# ------------------------------------------------------------
# å­¦ç¿’ï¼ˆLightGBMï¼‰
# ------------------------------------------------------------
def train_lgbm(X: pd.DataFrame, y: pd.Series) -> lgbm.LGBMClassifier:
    params = dict(
        objective="binary",
        boosting_type="gbdt",
        n_estimators=400,
        learning_rate=0.05,
        num_leaves=63,
        max_depth=-1,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_alpha=0.0,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=1,  # VPS 2GB æƒ³å®šã§æ§ãˆã‚
        verbose=-1,
    )
    model = lgbm.LGBMClassifier(**params)
    # DataFrame ã®ã¾ã¾æ¸¡ã™ï¼ˆåˆ—é †ãƒ»åå‰ã‚’ç¶­æŒï¼‰
    model.fit(X, y)
    return model


# ------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³
# ------------------------------------------------------------
def main():
    # å¼•æ•°ãªã—ã§ã‚‚å®‰å…¨ã«å‹•ãã‚ˆã†ã«ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨èª¬æ˜ã‚’è¿½åŠ 
    ap = argparse.ArgumentParser(
        description="LightGBM walk-forward retrain",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    ap.add_argument(
        "--symbol",
        default="USDJPY-",
        help="ä¾‹: USDJPY- ï¼ˆæœªæŒ‡å®šæ™‚ã¯å®‰å…¨ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰",
    )
    ap.add_argument(
        "--timeframe",
        default="M5",
        help="ä¾‹: M5, M15, H1",
    )
    ap.add_argument("--horizon", type=int, default=10, help="äºˆæ¸¬å…ˆ (bars)")
    ap.add_argument(
        "--train_bars",
        type=int,
        default=90_000,
        help="å­¦ç¿’ãƒãƒ¼æ•°ï¼ˆä¾‹: 90kâ‰ˆæ•°ãƒ¶æœˆ~å¹´ï¼‰",
    )
    ap.add_argument(
        "--test_bars",
        type=int,
        default=7_000,
        help="ãƒ†ã‚¹ãƒˆãƒãƒ¼æ•°ï¼ˆä¾‹: 1é€±é–“åˆ†ãã‚‰ã„ï¼‰",
    )
    ap.add_argument(
        "--step_bars",
        type=int,
        default=7_000,
        help="å‰é€²å¹…ï¼ˆé€šå¸¸ test_bars ã¨åŒã˜ï¼‰",
    )
    ap.add_argument(
        "--model_name",
        default="LightGBM_clf",
        help="ä¿å­˜åã®ãƒ™ãƒ¼ã‚¹",
    )
    ap.add_argument(
        "--data-dir",
        type=str,
        default=None,
        help="CSVã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæœªæŒ‡å®šãªã‚‰ FXBOT_DATA / PROJECT_ROOT/data / ./data ã®é †ã§æ¢ç´¢ï¼‰",
    )
    # å±é™ºæ“ä½œåˆ¶å¾¡ãƒ•ãƒ©ã‚°
    ap.add_argument(
        "--apply",
        action="store_true",
        help="æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã¨ã—ãã„å€¤ã‚’ active_model.json ã«åæ˜ ã™ã‚‹",
    )
    ap.add_argument(
        "--dry-run",
        action="store_true",
        help="ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ã¿è¡Œã„ã€active_model.json ãªã©ã¯ä¸€åˆ‡æ›´æ–°ã—ãªã„",
    )
    args = ap.parse_args()

    safe_log(
        f"[WFO] start walkforward retrain | symbol={args.symbol} tf={args.timeframe}"
    )

    # CSV æ¢ç´¢ & èª­ã¿è¾¼ã¿
    csv_path = find_csv(args.symbol, args.timeframe, data_dir=args.data_dir)
    print(f"[retrain] using CSV: {csv_path}")
    safe_log(f"[WFO] load csv: {csv_path}")
    df_raw = pd.read_csv(csv_path)

    # æœ€ä½é™ã®åˆ—ãƒã‚§ãƒƒã‚¯
    need_cols = {"time", "open", "high", "low", "close"}
    missing = need_cols - set(df_raw.columns)
    if missing:
        raise ValueError(f"CSV ã«å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")

    # ç‰¹å¾´é‡
    feats = build_features(df_raw)
    if feats.empty:
        safe_log("[WFO] feature building aborted (not enough rows).")
        sys.exit(1)

    # ãƒ©ãƒ™ãƒ«
    y = make_label(feats, args.horizon)
    feats = feats.iloc[: -args.horizon, :].reset_index(drop=True)
    y = y.iloc[: -args.horizon].reset_index(drop=True)

    # ç‰¹å¾´é‡è¡Œæ•°ãƒã‚§ãƒƒã‚¯
    if feats.shape[0] == 0:
        safe_log(
            "[WFO][error] no rows after feature engineering + horizon alignment. "
            "Likely because rows <= horizon. Provide a longer CSV or reduce --horizon."
        )
        sys.exit(1)

    # èª¬æ˜å¤‰æ•°
    drop_cols = ["time", "open", "high", "low", "close"]
    X = feats.drop(columns=[c for c in drop_cols if c in feats.columns])

    # å¿µã®ãŸã‚ã®æ¬ æé™¤å»
    mask = ~X.isna().any(axis=1)
    X, y = X[mask], y[mask]
    X = X.astype(np.float32)

    n_total = len(X)
    if n_total < (args.train_bars + args.test_bars + 1):
        # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯ 80/20 ã®å˜ç´”ã‚¹ãƒ—ãƒªãƒƒãƒˆã§å­¦ç¿’â†’ä¿å­˜ã®ã¿
        safe_log("[WFO] dataset is small; using simple 80/20 split instead of WFO.")
        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, shuffle=False)

        clf = train_lgbm(Xtr, ytr)
        prob = clf.predict_proba(Xte)[:, 1]  # DataFrameã®ã¾ã¾æ¸¡ã—ã¦ã„ã‚‹
        auc = float(roc_auc_score(yte, prob))
        ll = float(log_loss(yte, np.clip(prob, 1e-6, 1 - 1e-6)))

        thr, f1 = pick_threshold(yte.values, prob)
        safe_log(
            f"[WFO] simple-split auc={auc:.4f} logloss={ll:.4f} thr={thr:.3f} f1={f1:.3f}"
        )

        # å…¨ãƒ‡ãƒ¼ã‚¿å†å­¦ç¿’â†’ä¿å­˜
        final_clf = train_lgbm(X, y)
        model_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = MODELS_DIR / f"{args.model_name}_{model_ts}.pkl"
        dump(final_clf, model_path)
        meta = {
            "model_name": args.model_name,
            "version": model_ts,
            "features": list(X.columns),
            "horizon": args.horizon,
            "metrics": {"auc": auc, "logloss": ll, "thr": thr, "f1": f1},
            "source_csv": str(csv_path.name),
        }
        meta_path = MODELS_DIR / f"{args.model_name}_{model_ts}.meta.json"
        meta_path.write_text(
            json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
        )

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ˆ--apply ã®ã¨ãã ã‘ï¼‰
        safe_log(f"[WFO] wrote: {model_path.name}, {meta_path.name}")
        if args.dry_run:
            safe_log(
                "[WFO] DRY-RUN ã®ãŸã‚ active_model.json ã¯æ›´æ–°ã—ã¾ã›ã‚“ã€‚"
            )
        elif not args.apply:
            safe_log(
                "[WFO] --apply ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ active_model.json ã¯æ›´æ–°ã—ã¾ã›ã‚“ã€‚"
            )
        else:
            active = {
                "model_file": str(model_path.name),
                "meta_file": str(meta_path.name),
                "best_threshold": thr,
                "updated_at": jst_now_str(),
            }
            (MODELS_DIR / "active_model.json").write_text(
                json.dumps(active, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            safe_log(
                f"[WFO] active_model.json updated (best_threshold={thr:.3f})"
            )
        return

    # --- WFO ---
    safe_log(
        f"[WFO] bars: total={n_total} train={args.train_bars} "
        f"test={args.test_bars} step={args.step_bars}"
    )

    metrics: list[WFOMetrics] = []
    prob_oof = np.full(n_total, np.nan, dtype=np.float64)
    thr_list: list[float] = []

    for fold, s_tr, s_te in iter_wfo_slices(
        X, args.train_bars, args.test_bars, args.step_bars
    ):
        Xtr, ytr = X.iloc[s_tr], y.iloc[s_tr]
        Xte, yte = X.iloc[s_te], y.iloc[s_te]

        # å­¦ç¿’
        clf = train_lgbm(Xtr, ytr)

        # äºˆæ¸¬ï¼ˆDataFrameã®ã¾ã¾ï¼‰
        proba = clf.predict_proba(Xte)[:, 1]

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        try:
            auc = float(roc_auc_score(yte, proba))
        except ValueError:
            auc = float("nan")

        ll = float(log_loss(yte, np.clip(proba, 1e-6, 1 - 1e-6)))
        thr, f1 = pick_threshold(yte.values, proba)

        # OOF ã¸
        prob_oof[s_te] = proba
        thr_list.append(thr)

        # æœŸé–“æƒ…å ±
        t_idx = feats.iloc[s_tr, :]["time"]
        tr_start = str(t_idx.iloc[0]) if len(t_idx) else ""
        tr_end = str(t_idx.iloc[-1]) if len(t_idx) else ""
        t_idx2 = feats.iloc[s_te, :]["time"]
        te_start = str(t_idx2.iloc[0]) if len(t_idx2) else ""
        te_end = str(t_idx2.iloc[-1]) if len(t_idx2) else ""

        m = WFOMetrics(
            fold=fold,
            train_start=tr_start,
            train_end=tr_end,
            test_start=te_start,
            test_end=te_end,
            n_train=len(Xtr),
            n_test=len(Xte),
            auc=auc,
            logloss=ll,
            f1_at_thr=f1,
            thr=thr,
        )
        metrics.append(m)
        safe_log(
            f"[WFO][fold {fold}] auc={auc:.4f} logloss={ll:.4f} "
            f"thr={thr:.3f} f1={f1:.3f} n={len(Xtr)}/{len(Xte)}"
        )

    # WFO å…¨ä½“ã¾ã¨ã‚
    valid_idx = ~np.isnan(prob_oof)
    if valid_idx.sum() == 0:
        safe_log("[WFO] no valid test predictions; abort.")
        sys.exit(1)

    y_oof = y.values[valid_idx]
    p_oof = prob_oof[valid_idx]
    auc_oof = float(roc_auc_score(y_oof, p_oof))
    ll_oof = float(log_loss(y_oof, np.clip(p_oof, 1e-6, 1 - 1e-6)))
    thr_oof, f1_oof = pick_threshold(y_oof, p_oof)

    # å°‘ã—å¼•ãæ°—å‘³ã«ï¼ˆéé©åˆ/ã‚ºãƒ¬å¯¾ç­–ã§ 0.95 ã‚’æ›ã‘ã‚‹ï¼‰
    best_thr = float(np.clip(thr_oof * 0.95, 0.05, 0.95))

    safe_log(
        f"[WFO][OOF] auc={auc_oof:.4f} logloss={ll_oof:.4f} "
        f"thr*={best_thr:.3f} (raw={thr_oof:.3f}) f1={f1_oof:.3f}"
    )

    # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«
    final_clf = train_lgbm(X, y)
    model_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = MODELS_DIR / f"{args.model_name}_{model_ts}.pkl"
    dump(final_clf, model_path)

    meta = {
        "model_name": args.model_name,
        "version": model_ts,
        "features": list(X.columns),
        "horizon": args.horizon,
        "oof_metrics": {
            "auc": auc_oof,
            "logloss": ll_oof,
            "thr_oof": thr_oof,
            "f1_oof": f1_oof,
            "thr_final": best_thr,
        },
        "folds": [asdict(m) for m in metrics],
        "source_csv": str(csv_path.name),
        "bars": {
            "total": n_total,
            "train": args.train_bars,
            "test": args.test_bars,
            "step": args.step_bars,
        },
    }
    meta_path = MODELS_DIR / f"{args.model_name}_{model_ts}.meta.json"
    meta_path.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
    )

    safe_log(f"[WFO] wrote: {model_path.name}, {meta_path.name}")

    # active_model.json æ›´æ–°ï¼ˆGUI/å®Ÿé‹ç”¨ãŒèª­ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰: --apply ã®ã¨ãã ã‘
    if args.dry_run:
        safe_log(
            "[WFO] DRY-RUN ã®ãŸã‚ active_model.json ã¯æ›´æ–°ã—ã¾ã›ã‚“ã€‚"
        )
    elif not args.apply:
        safe_log(
            "[WFO] --apply ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ active_model.json ã¯æ›´æ–°ã—ã¾ã›ã‚“ã€‚"
        )
    else:
        active = {
            "model_file": str(model_path.name),
            "meta_file": str(meta_path.name),
            "best_threshold": best_thr,
            "updated_at": jst_now_str(),
        }
        (MODELS_DIR / "active_model.json").write_text(
            json.dumps(active, ensure_ascii=False, indent=2), encoding="utf-8"
        )
        safe_log(
            f"[WFO] active_model.json updated (best_threshold={best_thr:.3f})"
        )
    safe_log("[WFO] done.")


if __name__ == "__main__":
    main()



=== file: scripts/walkforward_train.py ===

# scripts/walkforward_train.py
from __future__ import annotations
import argparse, os, json, time, shutil, hashlib, random
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Tuple
import numpy as np
import pandas as pd

# ä¾å­˜ãŒãªã‘ã‚Œã°ãƒ­ã‚¸å›å¸°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
try:
    import lightgbm as lgb
    HAVE_LGB = True
except Exception:
    from sklearn.linear_model import LogisticRegression
    HAVE_LGB = False
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.calibration import CalibratedClassifierCV
import pickle

MODELS_DIR = "models"
DATA_DIR = "data"

def _now_iso() -> str:
    return datetime.now(timezone.utc).astimezone().isoformat(timespec="seconds")

def _sha256(path: str) -> str:
    import hashlib
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for ch in iter(lambda: f.read(8192), b""):
            h.update(ch)
    return h.hexdigest()

def load_dataset(csv_glob: str) -> pd.DataFrame:
    import glob
    files = sorted(glob.glob(os.path.join(DATA_DIR, csv_glob)))
    if not files:
        raise FileNotFoundError(f"No dataset CSVs under data/ matched: {csv_glob}")
    dfs = []
    for f in files:
        df = pd.read_csv(f)
        dfs.append(df)
    df = pd.concat(dfs, axis=0, ignore_index=True)
    # æœŸå¾…ã‚«ãƒ©ãƒ : time, open, high, low, close, label (0/1 for BUY=1), ...features...
    if "label" not in df.columns:
        raise ValueError("dataset must contain 'label' column (0/1)")
    return df

def split_walkforward(df: pd.DataFrame, weeks_train: int, weeks_valid: int, steps: int) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
    """
    ç›´è¿‘ã‹ã‚‰é¡ã‚‹WFã€‚é€±å˜ä½ã§ train/valid ã‚’åˆ‡ã£ã¦ steps å›ã€‚
    """
    if "time" in df.columns:
        dt = pd.to_datetime(df["time"])
    else:
        # ç–‘ä¼¼æ™‚ç³»åˆ—
        base = datetime(2020,1,1)
        dt = pd.Series([base + timedelta(minutes=i) for i in range(len(df))])
    df = df.copy()
    df["__dt__"] = dt

    spans = []
    end = df["__dt__"].max()
    for k in range(steps):
        valid_end = end - timedelta(weeks=k*weeks_valid)
        valid_start = valid_end - timedelta(weeks=weeks_valid)
        train_end = valid_start
        train_start = train_end - timedelta(weeks=weeks_train)

        tr = df[(df["__dt__"]>=train_start) & (df["__dt__"]<train_end)]
        va = df[(df["__dt__"]>=valid_start) & (df["__dt__"]<valid_end)]
        if len(tr) < 100 or len(va) < 100:
            continue
        spans.append((tr, va))
    spans.reverse()  # å¤ã„â†’æ–°ã—ã„
    return spans[-1:]  # ç›´è¿‘ã®1ã‚¹ãƒ†ãƒƒãƒ—ã ã‘ã§ååˆ†ï¼ˆé«˜é€Ÿï¼‰

def _train_one(tr: pd.DataFrame, va: pd.DataFrame, features: List[str]) -> Tuple[Any, Any, Dict[str, float]]:
    Xtr, ytr = tr[features].values, tr["label"].values
    Xva, yva = va[features].values, va["label"].values

    if HAVE_LGB:
        clf = lgb.LGBMClassifier(
            n_estimators=300, learning_rate=0.05, max_depth=-1,
            subsample=0.8, colsample_bytree=0.8, random_state=42
        )
    else:
        clf = LogisticRegression(max_iter=200)

    clf.fit(Xtr, ytr)
    pva = clf.predict_proba(Xva)[:,1]
    auc = roc_auc_score(yva, pva)
    acc = accuracy_score(yva, (pva>=0.5).astype(int))

    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚¿ï¼ˆisotonicå„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯sigmoidï¼‰
    try:
        cal = CalibratedClassifierCV(base_estimator=clf, method="isotonic", cv=5)
        cal.fit(Xtr, ytr)
        cal_name = "isotonic"
    except Exception:
        cal = CalibratedClassifierCV(base_estimator=clf, method="sigmoid", cv=5)
        cal.fit(Xtr, ytr)
        cal_name = "platt"

    return clf, cal, {"auc": float(auc), "acc": float(acc), "calibrator": cal_name}

def _features_from(df: pd.DataFrame) -> List[str]:
    # æœ€ä½é™ï¼šOHLCã‚„æ´¾ç”ŸãŒå…¥ã£ã¦ã„ã‚‹å‰æã€‚label/time/éæ•°å€¤ã¯é™¤å¤–ã€‚
    feats = [c for c in df.columns if c not in ("time","label") and pd.api.types.is_numeric_dtype(df[c])]
    if not feats:
        raise ValueError("no numeric features found.")
    return feats

def save_bundle(tag: str, clf: Any, cal: Any, features: List[str], classes: Dict[str, int], metrics: Dict[str, Any]) -> str:
    out = os.path.join(MODELS_DIR, tag)
    os.makedirs(out, exist_ok=True)

    with open(os.path.join(out, "LightGBM_clf.pkl"), "wb") as f:
        pickle.dump(clf, f)
    with open(os.path.join(out, "features.json"), "w", encoding="utf-8") as f:
        json.dump(features, f, ensure_ascii=False)
    with open(os.path.join(out, "classes.json"), "w", encoding="utf-8") as f:
        json.dump(classes, f, ensure_ascii=False)

    if metrics.get("calibrator") == "isotonic":
        with open(os.path.join(out, "calib_isotonic.pkl"), "wb") as f:
            pickle.dump(cal, f)
    else:
        with open(os.path.join(out, "calib_platt.pkl"), "wb") as f:
            pickle.dump(cal, f)

    # manifest
    sums = {}
    for name in ("LightGBM_clf.pkl","features.json","classes.json","calib_isotonic.pkl","calib_platt.pkl"):
        p = os.path.join(out, name)
        if os.path.exists(p):
            sums[name] = _sha256(p)

    mani = {
        "tag": tag,
        "ts": _now_iso(),
        "metrics": metrics,
        "sha256": sums,
        "features_hash": hashlib.sha256(json.dumps(features).encode()).hexdigest(),
    }
    with open(os.path.join(out, "manifest.json"), "w", encoding="utf-8") as f:
        json.dump(mani, f, ensure_ascii=False, indent=2)

    # READY ã¯æœ€å¾Œã«
    with open(os.path.join(out, "READY"), "w") as f:
        f.write("ok\n")
    return out

def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", default="*.csv", help="data/ ä»¥ä¸‹ã§èª­ã‚€CSVã®glob")
    ap.add_argument("--weeks-train", type=int, default=156, help="å­¦ç¿’æœŸé–“ï¼ˆé€±ï¼‰=3å¹´")
    ap.add_argument("--weeks-valid", type=int, default=12,  help="æ¤œè¨¼æœŸé–“ï¼ˆé€±ï¼‰=3ãƒ¶æœˆ")
    ap.add_argument("--min-auc", type=float, default=0.55,  help="å·®ã—æ›¿ãˆã®æœ€ä½AUC")
    ap.add_argument("--tag", default=None, help="å‡ºåŠ›ã‚¿ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ—¥æ™‚ï¼‰")
    args = ap.parse_args()

    df = load_dataset(args.csv)
    feats = _features_from(df)
    spans = split_walkforward(df, args.weeks_train, args.weeks_valid, steps=3)
    if not spans:
        raise SystemExit("not enough data for walk-forward.")

    tr, va = spans[-1]
    clf, cal, m = _train_one(tr, va, feats)

    print(f"[METRICS] AUC={m['auc']:.4f} ACC={m['acc']:.4f} CAL={m['calibrator']}")

    if m["auc"] < args.min_auc:
        print(f"[SKIP] AUC {m['auc']:.4f} < min_auc {args.min_auc}")
        return

    tag = args.tag or datetime.now().strftime("lgb_%Y%m%d_%H%M%S")
    bundle = save_bundle(tag, clf, cal, feats, {"BUY":1, "SELL":0}, m)
    print(f"[READY] {bundle}")

if __name__ == "__main__":
    main()



=== file: scripts/weekly_retrain.py ===

#!/usr/bin/env python
"""
scripts/weekly_retrain.py

é€±æ¬¡è‡ªå‹•å†å­¦ç¿’ã‚¸ãƒ§ãƒ–ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

    ãƒ‡ãƒ¼ã‚¿å–å¾—
    -> ç‰¹å¾´é‡ä½œæˆ
    -> LightGBM å­¦ç¿’
    -> Walk-Forward æ¤œè¨¼
    -> ã—ãã„å€¤æœ€é©åŒ–
    -> ãƒ¢ãƒ‡ãƒ«ä¿å­˜ & ç½²å (active_model.json æ›´æ–°)

å‰æ:
- ãƒ«ãƒ¼ãƒˆç›´ä¸‹ (fxbot/) ã‹ã‚‰å®Ÿè¡Œã™ã‚‹ã“ã¨
- è¨­å®š: configs/config.yaml ã‚‚ã—ãã¯ --config ã§æŒ‡å®š
- ä¾¡æ ¼CSV: data/USDJPY/ohlcv/USDJPY_M5.csv ã®ã‚ˆã†ãªæ§‹é€ 
"""

from __future__ import annotations

import argparse
import hashlib
import json
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path

import lightgbm as lgb
import numpy as np
import numpy.typing as npt
import pandas as pd
import yaml
from joblib import dump
from loguru import logger

# ---- å®šæ•° (Ruff ã® magic number å¯¾ç­–ã‚‚å…¼ã­ã‚‹) -----------------------------

MIN_WFO_SPLITS: int = 2
DEFAULT_CLASS_THRESHOLD: float = 0.5

JST = UTC  # å¾Œã§å¿…è¦ãªã‚‰ Asia/Tokyo ã«å¤‰æ›´ã—ã¦ã‚‚OK


# ------------------------
# è¨­å®šèª­ã¿è¾¼ã¿ã¾ã‚ã‚Š
# ------------------------


@dataclass
class PathsConfig:
    data_dir: Path
    models_dir: Path
    logs_dir: Path


@dataclass
class RetrainConfig:
    symbol: str
    timeframe: str
    label_horizon: int = 10  # ä½•ãƒãƒ¼å…ˆã‚’ãƒ©ãƒ™ãƒ«ã«ã™ã‚‹ã‹
    min_pips: float = 1.0  # ã‚¯ãƒ©ã‚¹åˆ†ã‘ã«ä½¿ã†æœ€å°pips
    n_splits: int = 4  # Walk-Forward ã®åˆ†å‰²æ•°
    threshold_grid: list[float] | None = None  # None ã‚’è¨±å®¹

    def __post_init__(self) -> None:
        if self.threshold_grid is None:
            # DEFAULT_CLASS_THRESHOLD ã‚’ä¸­å¿ƒã«ã€å°‘ã—å‰å¾Œã‚’è¦‹ã‚‹
            self.threshold_grid = [
                DEFAULT_CLASS_THRESHOLD - 0.05,
                DEFAULT_CLASS_THRESHOLD,
                DEFAULT_CLASS_THRESHOLD + 0.05,
                DEFAULT_CLASS_THRESHOLD + 0.10,
                DEFAULT_CLASS_THRESHOLD + 0.15,
            ]


@dataclass
class WeeklyRetrainConfig:
    paths: PathsConfig
    retrain: RetrainConfig


def load_config(config_path: Path) -> WeeklyRetrainConfig:
    """YAML è¨­å®šã®ãƒ­ãƒ¼ãƒ‰ã€‚"""

    if not config_path.exists():
        raise FileNotFoundError(f"config.yaml ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")

    with config_path.open("r", encoding="utf-8") as f:
        raw = yaml.safe_load(f)

    paths_raw = raw.get("paths", {}) or {}
    runtime_raw = raw.get("runtime", {}) or {}
    ai_raw = raw.get("ai", {}) or {}
    retrain_raw = ai_raw.get("retrain", {}) or {}

    data_dir = Path(paths_raw.get("data_dir", "./data")).expanduser()
    models_dir = Path(paths_raw.get("models_dir", "./models")).expanduser()
    logs_dir = Path(paths_raw.get("logs_dir", "./logs")).expanduser()

    symbol = runtime_raw.get("symbol", "USDJPY")
    timeframe = runtime_raw.get("timeframe_exec", "M5")

    label_horizon = int(retrain_raw.get("label_horizon_bars", 10))
    min_pips = float(retrain_raw.get("min_pips", 1.0))
    n_splits = int(retrain_raw.get("wfo_n_splits", 4))

    thr_raw = retrain_raw.get("threshold_grid")
    if thr_raw is None:
        threshold_grid: list[float] | None = None
    else:
        threshold_grid = [float(x) for x in thr_raw]

    cfg = WeeklyRetrainConfig(
        paths=PathsConfig(
            data_dir=data_dir,
            models_dir=models_dir,
            logs_dir=logs_dir,
        ),
        retrain=RetrainConfig(
            symbol=symbol,
            timeframe=timeframe,
            label_horizon=label_horizon,
            min_pips=min_pips,
            n_splits=n_splits,
            threshold_grid=threshold_grid,
        ),
    )
    return cfg


# ------------------------
# ãƒ‡ãƒ¼ã‚¿ & ç‰¹å¾´é‡
# ------------------------


def load_price_data(csv_path: Path) -> pd.DataFrame:
    """MT5 ã‹ã‚‰æ›¸ãå‡ºã—ãŸä¾¡æ ¼CSVã‚’èª­ã¿è¾¼ã‚€ã€‚"""

    if not csv_path.exists():
        raise FileNotFoundError(f"ä¾¡æ ¼CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")

    df = pd.read_csv(csv_path)
    if "time" not in df.columns:
        raise ValueError("CSV ã« 'time' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time").reset_index(drop=True)

    # åˆ—åã®ã‚†ã‚‰ãã«å¯¾å¿œ
    vol_col: str | None = None
    for cand in ("tick_volume", "volume", "vol"):
        if cand in df.columns:
            vol_col = cand
            break
    if vol_col is None:
        df["volume"] = 0.0
        vol_col = "volume"

    for col in ("open", "high", "low", "close"):
        if col not in df.columns:
            raise ValueError(f"CSV ã« '{col}' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    return df[["time", "open", "high", "low", "close", vol_col]].rename(
        columns={vol_col: "volume"}
    )


def compute_rsi(close: pd.Series, period: int = 14) -> pd.Series:
    diff = close.diff()
    gain = diff.clip(lower=0)
    loss = -diff.clip(upper=0)
    avg_gain = gain.ewm(alpha=1 / period, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1 / period, adjust=False).mean()
    rs = avg_gain / (avg_loss + 1e-9)
    rsi = 100 - (100 / (1 + rs))
    return rsi


def compute_atr(
    high: pd.Series,
    low: pd.Series,
    close: pd.Series,
    period: int = 14,
) -> pd.Series:
    prev_close = close.shift(1)
    tr1 = high - low
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = tr.ewm(alpha=1 / period, adjust=False).mean()
    return atr


def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã€‚
    å¾Œã§ core/feature_pipeline.py ã«å·®ã—æ›¿ãˆã¦ã‚‚OKã€‚
    """

    out = pd.DataFrame(index=df.index)

    out["ret_1"] = df["close"].pct_change()
    out["ret_5"] = df["close"].pct_change(5)
    out["ema_5"] = df["close"].ewm(span=5, adjust=False).mean()
    out["ema_20"] = df["close"].ewm(span=20, adjust=False).mean()
    out["ema_ratio"] = out["ema_5"] / (out["ema_20"] + 1e-9)

    out["rsi_14"] = compute_rsi(df["close"], period=14)
    out["atr_14"] = compute_atr(df["high"], df["low"], df["close"], period=14)

    out["range"] = (df["high"] - df["low"]) / (df["close"].shift(1) + 1e-9)
    out["vol_chg"] = df["volume"].pct_change().fillna(0.0)

    out = out.replace([np.inf, -np.inf], np.nan)
    out = out.dropna()
    return out


def build_labels(
    df: pd.DataFrame,
    horizon: int = 10,
    min_pips: float = 1.0,
) -> pd.Series:
    """
    horizon è¶³å¾Œã®æ–¹å‘ãƒ©ãƒ™ãƒ«ã‚’ä½œã‚‹ã€‚
    - USDJPY å‰æã§ 1pips = 0.01 ã¨ã—ã¦è¨ˆç®—ã€‚
    - ä¸Šæ˜‡(min_pipsè¶…) = 1, ä¸‹é™(min_pipsè¶…) = 0
      ãã‚Œä»¥å¤–ï¼ˆå¤‰åŒ–ãŒå°ã•ã„ï¼‰ã¯ NaN ã«ã—ã¦é™¤å¤–ã€‚
    """

    future = df["close"].shift(-horizon)
    delta = future - df["close"]
    pips = delta * 100.0  # USDJPY å‰æ
    y = pd.Series(index=df.index, dtype="float32")
    y[pips >= min_pips] = 1.0
    y[pips <= -min_pips] = 0.0
    return y


def align_features_and_labels(
    feats: pd.DataFrame,
    labels: pd.Series,
) -> tuple[pd.DataFrame, pd.Series]:
    df = feats.join(labels.rename("y"), how="left")
    df = df.dropna()
    y = df.pop("y").astype(int)
    X = df
    return X, y


# ------------------------
# Walk-Forward æ¤œè¨¼ & å­¦ç¿’
# ------------------------


@dataclass
class FoldResult:
    fold: int
    train_start: str
    train_end: str
    val_start: str
    val_end: str
    logloss: float
    accuracy: float
    n_train: int
    n_val: int


@dataclass
class WFOResult:
    folds: list[FoldResult]
    mean_logloss: float
    mean_accuracy: float


def iter_walkforward_indices(
    n_samples: int,
    n_splits: int,
) -> list[tuple[np.ndarray, np.ndarray]]:
    """
    éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãª walk-forwardã€‚
    - ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã« time ã§ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å‰æ
    - n_splits+1 å€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã€å‰æ–¹ç´¯ç©ã‚’ trainã€æ¬¡ãƒ–ãƒ­ãƒƒã‚¯ã‚’ val ã«ã™ã‚‹
    """

    if n_splits < MIN_WFO_SPLITS:
        raise ValueError("n_splits ã¯æœ€ä½ 2 ä»¥ä¸Šã‚’æ¨å¥¨ã—ã¾ã™ã€‚")

    block = n_samples // (n_splits + 1)
    indices = np.arange(n_samples, dtype=int)

    splits: list[tuple[np.ndarray, np.ndarray]] = []
    for k in range(n_splits):
        train_end = block * (k + 1)
        val_end = block * (k + 2)
        if val_end <= train_end:
            break
        train_idx = indices[:train_end]
        val_idx = indices[train_end:val_end]
        splits.append((train_idx, val_idx))
    return splits


def train_lightgbm_wfo(
    X: pd.DataFrame,
    y: pd.Series,
    cfg: RetrainConfig,
) -> Tuple[WFOResult, List[lgb.Booster], npt.NDArray[np.float64]]:
    params: dict[str, object] = {
        "objective": "binary",
        "metric": ["binary_logloss"],
        "learning_rate": 0.05,
        "num_leaves": 31,
        "max_depth": -1,
        "min_data_in_leaf": 50,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "verbosity": -1,
        "force_col_wise": True,
    }

    n = len(X)
    splits = iter_walkforward_indices(n, cfg.n_splits)

    oof_pred: npt.NDArray[np.float_] = np.full(
        shape=n,
        fill_value=np.nan,
        dtype="float32",
    )
    boosters: list[lgb.Booster] = []
    fold_results: list[FoldResult] = []

    for fold_idx, (tr_idx, va_idx) in enumerate(splits):
        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]
        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]

        train_data = lgb.Dataset(X_tr, label=y_tr)
        valid_data = lgb.Dataset(X_va, label=y_va)

        logger.info(
            f"[WFO] fold={fold_idx} train={len(X_tr)} val={len(X_va)} "
            f"from={tr_idx[0]} to={va_idx[-1]}"
        )

        booster = lgb.train(
            params,
            train_data,
            num_boost_round=500,
            valid_sets=[valid_data],
            valid_names=["valid"],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
            ],
        )

        boosters.append(booster)

        y_proba: npt.NDArray[np.float_] = booster.predict(
            X_va,
            num_iteration=booster.best_iteration,
        )
        oof_pred[va_idx] = y_proba.astype("float32")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        eps = 1e-15
        y_clipped: npt.NDArray[np.float_] = np.clip(
            y_proba,
            eps,
            1 - eps,
        )
        logloss = float(
            -np.mean(y_va * np.log(y_clipped) + (1 - y_va) * np.log(1 - y_clipped))
        )

        preds_label = (y_proba >= DEFAULT_CLASS_THRESHOLD).astype(int)
        acc = float(((y_va == preds_label).sum()) / len(y_va))

        fold_results.append(
            FoldResult(
                fold=fold_idx,
                train_start=str(tr_idx[0]),
                train_end=str(tr_idx[-1]),
                val_start=str(va_idx[0]),
                val_end=str(va_idx[-1]),
                logloss=logloss,
                accuracy=acc,
                n_train=int(len(X_tr)),
                n_val=int(len(X_va)),
            )
        )

        logger.info(f"[WFO] fold={fold_idx} logloss={logloss:.5f} acc={acc:.4f}")

    valid_mask = ~np.isnan(oof_pred)
    mean_logloss = float("nan")
    mean_accuracy = float("nan")
    if valid_mask.sum() > 0:
        y_valid_arr: npt.NDArray[np.int_] = y[valid_mask].to_numpy()
        p_valid: npt.NDArray[np.float_] = oof_pred[valid_mask]

        eps = 1e-15
        p_clip = np.clip(p_valid, eps, 1 - eps)
        mean_logloss = float(
            -np.mean(
                y_valid_arr * np.log(p_clip) + (1 - y_valid_arr) * np.log(1 - p_clip)
            )
        )
        preds_valid = (p_valid >= DEFAULT_CLASS_THRESHOLD).astype(int)
        mean_accuracy = float((y_valid_arr == preds_valid).sum() / len(y_valid_arr))

    wfo_result = WFOResult(
        folds=fold_results,
        mean_logloss=mean_logloss,
        mean_accuracy=mean_accuracy,
    )

    return wfo_result, boosters, oof_pred


# ------------------------
# ã—ãã„å€¤æœ€é©åŒ–
# ------------------------


def optimize_threshold(
    y: pd.Series,
    oof_pred: npt.NDArray[np.float_],
    threshold_grid: list[float],
) -> dict[str, float]:
    """
    éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªã€Œ1ãƒˆãƒ¬ãƒ¼ãƒ‰ +1 / -1ã€ã®ç–‘ä¼¼æç›Šã§æœ€é©ãªã—ãã„å€¤ã‚’æ±ºã‚ã‚‹ã€‚
    """

    valid_mask = ~np.isnan(oof_pred)
    y_valid_arr: npt.NDArray[np.int_] = y[valid_mask].to_numpy()
    p_valid: npt.NDArray[np.float_] = oof_pred[valid_mask]

    best_thr = DEFAULT_CLASS_THRESHOLD
    best_score = -1e9
    results: list[tuple[float, float, float]] = []

    for thr in threshold_grid:
        trade_mask = p_valid >= thr
        if trade_mask.sum() == 0:
            continue

        y_tr = y_valid_arr[trade_mask]
        pnl = np.where(y_tr == 1, 1.0, -1.0)
        equity = pnl.cumsum()
        total = float(equity[-1])
        winrate = float((pnl > 0).sum() / len(pnl))
        results.append((thr, total, winrate))

        if total > best_score:
            best_score = total
            best_thr = thr

    logger.info(
        "[THR] grid_results="
        + ", ".join(
            f"thr={thr:.3f} total={total:.1f} win={win:.3f}"
            for thr, total, win in results
        )
    )
    logger.info(f"[THR] best_thr={best_thr:.3f} equity={best_score:.1f}")

    return {
        "best_threshold": float(best_thr),
        "best_equity": float(best_score),
    }


def save_wfo_report_and_equity(
    cfg: WeeklyRetrainConfig,
    df_prices: pd.DataFrame,
    X: pd.DataFrame,
    y: pd.Series,
    oof_pred: npt.NDArray[np.float_],
    wfo_result: WFOResult,
    thr_info: dict[str, float],
) -> str:
    """
    Walk-Forward ã®çµæœã‚µãƒãƒª (report_*.json) ã¨
    æ“¬ä¼¼çš„ãª train/test ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã‚«ãƒ¼ãƒ– (equity_train_*.csv / equity_test_*.csv)
    ã‚’ logs/retrain/ ä»¥ä¸‹ã«å‡ºåŠ›ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤ã¯ run_id (ãƒ•ã‚¡ã‚¤ãƒ«åã® *_run_id éƒ¨åˆ†)ã€‚
    """

    # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    base_dir = cfg.paths.logs_dir / "retrain"
    base_dir.mkdir(parents=True, exist_ok=True)

    # ä¸€æ„ãªIDã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰ä½œã‚‹
    ts = datetime.now(tz=UTC)
    run_id = str(int(ts.timestamp()))

    # ---- ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ç”¨ã®ä¸‹ã”ã—ã‚‰ãˆ ---------------------------------
    # X ã¨ y ã¯ df_prices ã®ä¸€éƒ¨ãªã®ã§ã€ãã® time ã‚’åˆã‚ã›ã‚‹
    # ï¼ˆalign_features_and_labels ã®ã‚ã¨ã® X.index ã¯ df_prices.index ã®ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
    df_all = pd.DataFrame(
        {
            "time": df_prices.loc[X.index, "time"].to_numpy(),
            "y": y.to_numpy(),
            "proba": oof_pred,
        }
    ).reset_index(drop=True)

    # NaN ã¯ã€Œãƒˆãƒ¬ãƒ¼ãƒ‰ã—ãªã„ã€ã¨ã¿ãªã™
    best_thr = float(thr_info.get("best_threshold", DEFAULT_CLASS_THRESHOLD))

    def make_equity_curve(df: pd.DataFrame) -> tuple[pd.DataFrame, dict[str, float]]:
        """
        proba >= best_thr ã®ã¨ãã ã‘ã€Œ+1 / -1ã€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã¨ã—ã¦
        ç–‘ä¼¼ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã‚’ä½œã‚‹ã€‚
        equity: ç´¯ç©æç›Šï¼ˆåˆæœŸ 0ï¼‰
        signal: +1 (å‹ã¡ãƒˆãƒ¬ãƒ¼ãƒ‰), -1 (è² ã‘ãƒˆãƒ¬ãƒ¼ãƒ‰), 0 (ãƒãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰)
        """
        equity_list: list[float] = []
        signal_list: list[int] = []
        pnl_list: list[float] = []

        equity = 0.0
        for _, row in df.iterrows():
            p = float(row["proba"])
            sig = 0
            if not np.isnan(p) and p >= best_thr:
                pnl = 1.0 if int(row["y"]) == 1 else -1.0
                equity += pnl
                pnl_list.append(pnl)
                sig = 1 if pnl > 0 else -1

            equity_list.append(equity)
            signal_list.append(sig)

        out = pd.DataFrame(
            {
                "time": df["time"].to_numpy(),
                "equity": equity_list,
                "signal": signal_list,
            }
        )

        n_trades = len(pnl_list)
        total = float(sum(pnl_list)) if pnl_list else 0.0
        wins = float(sum(p > 0 for p in pnl_list)) if pnl_list else 0.0
        gross_profit = float(sum(p for p in pnl_list if p > 0.0))
        gross_loss = float(-sum(p for p in pnl_list if p < 0.0))
        pf = gross_profit / gross_loss if gross_loss > 0 else float("inf")
        winrate = wins / n_trades if n_trades > 0 else 0.0

        stats = {
            "n_trades": float(n_trades),
            "total_pnl": total,
            "win_rate": winrate,
            "gross_profit": gross_profit,
            "gross_loss": gross_loss,
            "profit_factor": pf,
        }
        return out, stats

    # ã–ã£ãã‚Š 70% ã‚’ trainã€æ®‹ã‚Š 30% ã‚’ test ã¨ã—ã¦åˆ†å‰²
    n_all = len(df_all)
    split = int(n_all * 0.7)
    df_train = df_all.iloc[:split].copy()
    df_test = df_all.iloc[split:].copy()

    eq_train_df, stats_train = make_equity_curve(df_train)
    eq_test_df, stats_test = make_equity_curve(df_test)

    # ---- CSV å‡ºåŠ› ----------------------------------------------------
    equity_train_path = base_dir / f"equity_train_{run_id}.csv"
    equity_test_path = base_dir / f"equity_test_{run_id}.csv"

    eq_train_df.to_csv(equity_train_path, index=False)
    eq_test_df.to_csv(equity_test_path, index=False)

    logger.info(f"[WFO] equity_train saved: {equity_train_path}")
    logger.info(f"[WFO] equity_test  saved: {equity_test_path}")

    # ---- JSON ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› -------------------------------------------
    report = {
        "run_id": run_id,
        "created_at_utc": ts.isoformat(),
        "symbol": cfg.retrain.symbol,
        "timeframe": cfg.retrain.timeframe,
        "label_horizon_bars": cfg.retrain.label_horizon,
        "min_pips": cfg.retrain.min_pips,
        "n_samples": int(len(X)),
        "wfo": {
            "mean_logloss": wfo_result.mean_logloss,
            "mean_accuracy": wfo_result.mean_accuracy,
            "folds": [asdict(f) for f in wfo_result.folds],
        },
        "threshold": thr_info,
        "equity_train_stats": stats_train,
        "equity_test_stats": stats_test,
        "data_range": {
            "from": df_prices["time"].min().isoformat(),
            "to": df_prices["time"].max().isoformat(),
        },
    }

    report_path = base_dir / f"report_{run_id}.json"
    with report_path.open("w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    logger.info(f"[WFO] report saved: {report_path}")

    return run_id


# ------------------------
# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ & ç½²å
# ------------------------


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def save_model_and_meta(  # noqa: PLR0913  (å¼•æ•°å¤šã‚ã§ã‚‚ã“ã“ã¯OKã¨ã™ã‚‹)
    booster: lgb.Booster,
    cfg: WeeklyRetrainConfig,
    wfo_result: WFOResult,
    threshold_info: dict[str, float],
    feature_cols: list[str],
    data_info: dict[str, str],
) -> Path:
    cfg.paths.models_dir.mkdir(parents=True, exist_ok=True)

    ts = datetime.now(tz=UTC)
    ts_str = ts.strftime("%Y%m%d_%H%M%S")
    version = ts.timestamp()

    model_name = f"LightGBM_clf_{ts_str}.pkl"
    model_path = cfg.paths.models_dir / model_name

    dump(booster, model_path)

    sha = sha256_file(model_path)

    meta = {
        "model_name": "LightGBM_clf",
        "file": model_name,
        "created_at_utc": ts.isoformat(),
        "version": version,
        "symbol": cfg.retrain.symbol,
        "timeframe": cfg.retrain.timeframe,
        "label_horizon_bars": cfg.retrain.label_horizon,
        "min_pips": cfg.retrain.min_pips,
        "features": list(feature_cols),
        "wfo": {
            "mean_logloss": wfo_result.mean_logloss,
            "mean_accuracy": wfo_result.mean_accuracy,
            "folds": [asdict(f) for f in wfo_result.folds],
        },
        "threshold": threshold_info,
        "data": data_info,
        "sha256": sha,
    }

    meta_path = cfg.paths.models_dir / f"{model_name}.meta.json"
    with meta_path.open("w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    logger.info(f"[SAVE] model={model_path} sha256={sha}")
    logger.info(f"[SAVE] meta={meta_path}")

    active = {
        "model_name": "LightGBM_clf",
        "file": model_name,
        "meta_file": meta_path.name,
        "version": version,
        "best_threshold": threshold_info.get("best_threshold"),
    }
    active_path = cfg.paths.models_dir / "active_model.json"
    with active_path.open("w", encoding="utf-8") as f:
        json.dump(active, f, ensure_ascii=False, indent=2)

    logger.info(f"[SAVE] active_model={active_path}")
    return model_path


# ------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ------------------------


def run_weekly_retrain(cfg: WeeklyRetrainConfig, dry_run: bool = False) -> None:
    paths = cfg.paths
    rt = cfg.retrain

    paths.logs_dir.mkdir(parents=True, exist_ok=True)
    log_file = (
        paths.logs_dir / f"weekly_retrain_{datetime.now().strftime('%Y%m%d')}.log"
    )
    logger.add(log_file, encoding="utf-8")

    logger.info(
        f"[CFG] symbol={rt.symbol} tf={rt.timeframe} label_horizon={rt.label_horizon}"
    )

    # config ã® symbol ãŒ "USDJPY-" ã§ã‚‚ã€
    # å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ data/USDJPY/ohlcv/USDJPY_M5.csv ã‚’èª­ã‚€
    symbol_dir = rt.symbol.replace("-", "")
    symbol_file = rt.symbol.replace("-", "")

    csv_path = (
        paths.data_dir / symbol_dir / "ohlcv" / f"{symbol_file}_{rt.timeframe}.csv"
    )

    logger.info(f"[STEP] load_price_data csv={csv_path}")
    df_prices = load_price_data(csv_path)
    logger.info(
        f"[STEP] loaded rows={len(df_prices)} "
        f"from={df_prices['time'].min()} to={df_prices['time'].max()}"
    )

    logger.info("[STEP] build_features")
    feats = build_features(df_prices)
    logger.info(f"[STEP] features shape={feats.shape}")

    logger.info("[STEP] build_labels")
    labels = build_labels(
        df_prices,
        horizon=rt.label_horizon,
        min_pips=rt.min_pips,
    )

    logger.info("[STEP] align_features_and_labels")
    X, y = align_features_and_labels(feats, labels)
    logger.info(
        f"[DATA] X={X.shape} y_pos={int((y == 1).sum())} y_neg={int((y == 0).sum())}"
    )

    if len(X) < 1000:
        logger.warning(
            "[WARN] å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™(1000è¡Œæœªæº€)ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚"
        )
        return

    logger.info("[STEP] train_lightgbm_wfo")
    wfo_result, boosters, oof_pred = train_lightgbm_wfo(X, y, rt)
    logger.info(
        f"[WFO] mean_logloss={wfo_result.mean_logloss:.5f} "
        f"mean_acc={wfo_result.mean_accuracy:.4f}"
    )

    logger.info("[STEP] optimize_threshold")
    thr_info = optimize_threshold(y, oof_pred, rt.threshold_grid or [])

    logger.info("[STEP] save_wfo_report_and_equity")
    run_id = save_wfo_report_and_equity(
        cfg=cfg,
        df_prices=df_prices,
        X=X,
        y=y,
        oof_pred=oof_pred,
        wfo_result=wfo_result,
        thr_info=thr_info,
    )
    logger.info(f"[WFO] artifacts saved with run_id={run_id}")

    if dry_run:
        logger.info(
            "[DRYRUN] dry-run æŒ‡å®šã®ãŸã‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜/ç½²åã¯è¡Œã„ã¾ã›ã‚“ã€‚ã“ã“ã§çµ‚äº†ã—ã¾ã™ã€‚"
        )
        return

    logger.info("[STEP] train final model on all data")
    params: dict[str, object] = {
        "objective": "binary",
        "metric": ["binary_logloss"],
        "learning_rate": 0.05,
        "num_leaves": 31,
        "max_depth": -1,
        "min_data_in_leaf": 50,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "verbosity": -1,
        "force_col_wise": True,
    }
    train_all = lgb.Dataset(X, label=y)
    best_iters = [b.best_iteration or 200 for b in boosters]
    num_boost_round = int(np.median(best_iters))
    booster_all = lgb.train(
        params,
        train_all,
        num_boost_round=num_boost_round,
    )

    logger.info("[STEP] save_model_and_meta")
    data_info = {
        "csv_path": str(csv_path),
        "from": df_prices["time"].min().isoformat(),
        "to": df_prices["time"].max().isoformat(),
        "n_rows_raw": int(len(df_prices)),
        "n_rows_train": int(len(X)),
    }
    model_path = save_model_and_meta(
        booster=booster_all,
        cfg=cfg,
        wfo_result=wfo_result,
        threshold_info=thr_info,
        feature_cols=list(X.columns),
        data_info=data_info,
    )

    logger.info(f"[DONE] weekly retrain completed. model={model_path}")


def main() -> None:
    parser = argparse.ArgumentParser(description="é€±æ¬¡è‡ªå‹•å†å­¦ç¿’ (weekly_retrain)")
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ (default: configs/config.yaml)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="å­¦ç¿’ã ã‘è¡Œã„ã€ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚„ active_model æ›´æ–°ã¯è¡Œã‚ãªã„",
    )
    args = parser.parse_args()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€™è£œ: configs/config.yaml
    default_config = Path("configs/config.yaml")
    config_path = Path(args.config) if args.config else default_config

    cfg = load_config(config_path)
    run_weekly_retrain(cfg, dry_run=args.dry_run)


if __name__ == "__main__":
    main()



=== file: scripts/weekly_wf.ps1 ===

# scripts/weekly_wf.ps1
param(
  [double]$MinAuc = 0.55
)

$ErrorActionPreference = "Stop"
Set-Location (Split-Path -Parent $PSCommandPath)

# 1) Compile
python -m compileall app core scripts | Out-Null

# 2) Train (WF)
python -m scripts.walkforward_train --csv *.csv --weeks-train 156 --weeks-valid 12 --min-auc $MinAuc

# 3) Swap to live (if new READY exists, it will be the latest)
python -m scripts.swap_model

# 4) Smoke (strict) to ensure it runs
powershell -ExecutionPolicy Bypass -File scripts\verify_smoke.ps1 -Mode strict -Ticks 300 -DtMs 20 -AtrPct 0.00080

Write-Host "[WEEKLY] done"



=== file: setup_weekly_job.ps1 ===

param(
  [string]$TaskName   = "FXBot_WeeklyRetrain",
  [string]$PythonExe  = "$env:USERPROFILE\AppData\Local\Programs\Python\Python313\python.exe",
  [string]$ProjectDir = "$env:USERPROFILE\OneDrive\fxbot",
  [ValidateSet("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")]
  [string]$DayOfWeek  = "Saturday",
  [string]$StartTime  = "03:05",
  [switch]$RunHighest = $true
)

$ErrorActionPreference = "Stop"

$Py = Resolve-Path -LiteralPath $PythonExe
$ScriptPath = Join-Path $ProjectDir "scripts\weekly_retrain.py"
if (!(Test-Path -LiteralPath $ScriptPath)) { throw "not found: $ScriptPath" }

$h,$m = $StartTime.Split(":")
$at = [DateTime]::Today.AddHours([int]$h).AddMinutes([int]$m)
$trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek $DayOfWeek -At $at

$action  = New-ScheduledTaskAction -Execute $Py -Argument "`"$ScriptPath`""

$principal = New-ScheduledTaskPrincipal -UserId $env:UserName -RunLevel ($(if ($RunHighest) {"Highest"} else {"Limited"}))

try { Unregister-ScheduledTask -TaskName $TaskName -Confirm:$false -ErrorAction SilentlyContinue } catch {}

Register-ScheduledTask -TaskName $TaskName -Action $action -Trigger $trigger -Principal $principal `
  -Description "Weekly retrain job for fxbot (LightGBM/XGB WFO + promote)"

Write-Host "`n[OK] Task '$TaskName' registered for $DayOfWeek $StartTime (JST)."
Write-Host "Python: $Py"
Write-Host "Script: $ScriptPath"



=== file: temp_equity_check.py ===

import pandas as pd, pathlib, numpy as np
p = pathlib.Path(r"D:\macht\OneDrive\fxbot\logs\backtest\USDJPY\M5\equity_curve.csv")
df = pd.read_csv(p)
print("[equity] cols:", df.columns.tolist())
sig = df.get("signal", pd.Series(0, index=df.index)).astype('int', errors='ignore')
chg = sig.ne(sig.shift(1)).fillna(sig.iloc[0] != 0)
print("signal unique:", sorted(pd.unique(sig)))
print("nonzero count:", int((sig != 0).sum()), "/", len(sig))
print("change points:", int(chg.sum()))
print("first rows:\n", df.head(3))



=== file: tests/test_sanity.py ===

# tests/test_sanity.py
def test_sanity() -> None:
    assert 1 + 1 == 2



=== file: tmp_view.py ===

from pathlib import Path
lines=Path('app/core/mt5_client.py').read_text('utf-8', errors='replace').splitlines()
for i in range(120, 190):
    print(f'{i+1:03}: {lines[i]}')



=== file: To Doãƒªã‚¹ãƒˆ.txt ===

æœ€é«˜ã€‚ã“ã“ã¾ã§å…·ä½“ãªã‚‰ã€ã‚ã¨ã¯â€œå®Ÿè£…é †åºâ€ã¨â€œè½ã¨ã—ç©´å›é¿â€ãŒéµã§ã™ã€‚ã¾ãšã¯ä»•æ§˜ã«å°‘ã—ã ã‘è¿½è¨˜ã—ã¦ã‹ã‚‰ã€ã‚¼ãƒ­â†’é‹ç”¨ã¾ã§ã®ä½œæ¥­ãƒªã‚¹ãƒˆï¼ˆDoD=å®Œäº†æ¡ä»¶ã¤ãï¼‰ã‚’ä¸€æ°—ã«ä¸¦ã¹ã¾ã™ã€‚

# è¿½è¨˜ã—ã¦ãŠãã¨å®‰å…¨ã«ãªã‚‹ãƒ«ãƒ¼ãƒ«ï¼ˆæœ€å°é™ã®æŠœã‘æ¼ã‚Œï¼‰

* **ãƒ—ãƒ­ã‚»ã‚¹è¨­è¨ˆ**ï¼šGUI(ãƒ¡ã‚¤ãƒ³)ï¼å¸‚å ´ãƒ‡ãƒ¼ã‚¿è³¼èª­ï¼ˆéåŒæœŸï¼‰ï¼å£²è²·å®Ÿè¡Œï¼ˆé€æ¬¡ï¼‰ï¼å­¦ç¿’ï¼ˆåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ï¼‰ã®**åˆ†é›¢**ã€‚å­¦ç¿’ãƒ»æœ€é©åŒ–ã¯å¿…ãšåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã«ã—ã¦GUIã‚’å›ºã‚ãªã„ã€‚
* **ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•**ï¼š

  1. â€œæœ¬å£åº§â€ãƒœã‚¿ãƒ³ã¯**äºŒæ®µéšç¢ºèª**ï¼‹`max_lot_cap`åˆ¶é™å¿…é ˆã€‚
  2. **æ—¥æ¬¡DDã‚¹ãƒˆãƒƒãƒ—**ãƒ»**é€£æ•—ã‚¹ãƒˆãƒƒãƒ—**ãƒ»**ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸æ€¥å¢—**ã§ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã€‚
  3. **max_positions=1**ã‚’ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼å´ã®ç´„å®šå¿œç­”å¾…ã¡ãƒ­ãƒƒã‚¯ã§å³å®ˆï¼ˆéåŒæœŸç«¶åˆå¯¾ç­–ï¼‰ã€‚
* **ãƒ­ãƒƒãƒˆç®—å‡ºã®åŸºæº–**ï¼š`lot = floor((equity/100000)*1)/100` ã§ã¯ãªã**æ˜ç¤º**ï¼šã€Œæœ‰åŠ¹è¨¼æ‹ é‡‘ 10,000 å††ã§ 0.01 lotã€20,000 å††ã§ 0.02 lotâ€¦ã€ï¼ `lot = max(min_lot, round_down(equity/1_000_000, 2))`ï¼ˆUSDJPY/æ¨™æº–å£åº§æƒ³å®šã€‚ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼ã®æœ€å°ãƒ­ãƒƒãƒˆãƒ»åˆ»ã¿ã«åˆã‚ã›ã¦æ­£è¦åŒ–ï¼‰ã€‚
* **æ™‚åˆ»è»¸**ï¼šå…¨ãƒ­ã‚°ãƒ»é›†è¨ˆã¯**JST**åŸºæº–ã€ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼ã‚µãƒ¼ãƒæ™‚åˆ»ã¨ã®å·®ã¯çµ±ä¸€ãƒ©ãƒƒãƒ‘ã§å¸åã€‚
* **ãƒ‡ãƒ¼ã‚¿å“è³ª**ï¼šMT5å±¥æ­´ã®**ã‚®ãƒ£ãƒƒãƒ—æ¤œçŸ¥**ãƒ»**é‡è¤‡ãƒãƒ¼é™¤å»**ãƒ»**TZè£œæ­£**ã‚’å‰å‡¦ç†ã§å¼·åˆ¶ã€‚æ¬ æ >0.5% ãªã‚‰å­¦ç¿’ä¸­æ­¢ã€‚
* **ãƒ¢ãƒ‡ãƒ«ç®¡ç†**ï¼š`models/`ä¸‹ã«**ç½²åï¼ˆSHA256ï¼‰**ã¨**ãƒ¡ã‚¿æƒ…å ±ï¼ˆç‰¹å¾´é‡å®šç¾©ã®ãƒãƒƒã‚·ãƒ¥ï¼‰**ã‚’ä¿å­˜ã€‚ç‰¹å¾´é‡ã‚ºãƒ¬æ¤œçŸ¥ã§**å¼·åˆ¶ãƒ–ãƒ­ãƒƒã‚¯**ã€‚
* **ãƒ­ã‚°åŸºæº–**ï¼šå…¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã«**Decision Trace**ï¼ˆå…¥åŠ›â†’å„ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬â†’ãƒ¡ã‚¿åˆ¤å®šâ†’æœ€çµ‚æ³¨æ–‡ï¼‰ã‚’JSONã§ä¿å­˜ã€‚å†ç¾æ€§ã‚’ç¢ºä¿ã€‚
* **Sakura VPS(2GB)**ï¼šLSTMã¯å°å‹åŒ–ï¼ˆéš ã‚Œæ¬¡å…ƒâ‰¤64ã€å±¤â‰¤2ï¼‰ï¼‹ãƒãƒƒãƒå­¦ç¿’ã¯é€±æœ«ã®ã¿ã€‚å¹³æ™‚ã¯LightGBM/XGBoostã®æ¨è«–ä¸­å¿ƒã€‚

---

# å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆã–ã£ãã‚Šï¼‰

* **UIå±¤ï¼ˆPyQt6 + PyQtGraph + matplotlibï¼‰**
  Qtã‚¹ãƒ¬ãƒƒãƒ‰ï¼GUIå°‚ç”¨ã€‚ZMQ/Queueã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®çŠ¶æ…‹ã‚’è³¼èª­ã€‚
* **ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆPython 3.13ï¼‰**

  * å¸‚å ´ãƒ‡ãƒ¼ã‚¿è³¼èª­ï¼šMT5 API â†’ å†…éƒ¨Pub/Subãƒã‚¹ã¸
  * ã‚·ã‚°ãƒŠãƒ«æ¨è«–ï¼šå‰å‡¦ç†â†’ç‰¹å¾´é‡â†’å„ãƒ¢ãƒ‡ãƒ«â†’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
  * åŸ·è¡Œï¼šæ³¨æ–‡ãƒ©ãƒƒãƒ‘ï¼ˆãƒªãƒˆãƒ©ã‚¤ã€ä¾¡æ ¼æ”¹å–„ã€ç´„å®šã‚¤ãƒ™ãƒ³ãƒˆï¼‰
  * ãƒ­ã‚¬ãƒ¼ï¼šæ§‹é€ åŒ–JSON + ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
* **å­¦ç¿’/ãƒãƒƒãƒ**ï¼ˆé€±æœ«ï¼‰ï¼šãƒ‡ãƒ¼ã‚¿æŠ½å‡ºâ†’ç‰¹å¾´é‡â†’å­¦ç¿’â†’WFOâ†’æœ€é©åŒ–â†’ãƒ¢ãƒ‡ãƒ«ç½²åâ†’é…å¸ƒ
* **è¨­å®š**ï¼š`config.yaml`ï¼ˆãƒ‡ãƒ¢ï¼æœ¬ç•ªåˆ‡æ›¿ã€ãƒªã‚¹ã‚¯ã€æˆ¦ç•¥ã€å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ï¼‹`.env`ï¼ˆå£åº§èªè¨¼ï¼‰

---

# ä½œæ¥­ãƒªã‚¹ãƒˆï¼ˆãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ãƒ»å®Œäº†æ¡ä»¶ã¤ãï¼‰

## ãƒ•ã‚§ãƒ¼ã‚º0ï¼šç’°å¢ƒ/éª¨æ ¼ï¼ˆMVPã®åœŸå°ï¼‰

1. **Python 3.13 + MT5 + PyQt6 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**

   * ä¾å­˜ï¼š`MetaTrader5`, `PyQt6`, `pyqtgraph`, `matplotlib`, `lightgbm`, `xgboost`, `numpy`, `pandas`, `scikit-learn`, `joblib`, `shap`, `pydantic`, `pyyaml`, `loguru`
   * **DoD**ï¼š`python -c "import MetaTrader5; print('ok')"`ãŒé€šã‚‹ã€‚MT5æ¥ç¶šãƒ†ã‚¹ãƒˆOKï¼ˆãƒ‡ãƒ¢å£åº§ï¼‰ã€‚

2. **ãƒªãƒã‚¸ãƒˆãƒªé››å½¢ & ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ**

   ```
   fxai/
     app/                 # GUI
     core/                # å–å¼•/æ¨è«–/åŸ·è¡Œ
     data/                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ç”Ÿæˆç‰¹å¾´é‡
     models/              # pkl/onnx + meta.json
     logs/                # æ§‹é€ åŒ–ãƒ­ã‚°
     batch/               # å­¦ç¿’ãƒ»WFOãƒ»æœ€é©åŒ–
     tests/               # pytest
     config.yaml
     .env.example
     README.md
   ```

   * **DoD**ï¼š`pytest -q`ãŒæœ€ä½é™ãƒ‘ã‚¹ã€`pre-commit`ã§black/isort/ruffé€šéã€‚

3. **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é››å½¢ï¼ˆãƒ‡ãƒ¢/æœ¬ç•ªåˆ‡æ›¿ï¼‰**

   ```yaml
   account:
     mode: demo           # demo | live
     server: "OANDA-XYZ"
   risk:
     max_positions: 1
     daily_loss_stop: 0.02
     dd_stop: 0.10
     slip_stop_pips: 2.0
     lot:
       min: 0.01
       step: 0.01
       formula: "scale_10000yen_per_0_01lot"
       max_cap: 0.50
   strategy:
     symbol: "USDJPY"
     timeframes: ["M15","H1"]   # Phase1
     entry_threshold: 0.60
     tp_sl:
       mode: "fixed"            # fixed | atr | ai
       tp_pips: 15
       sl_pips: 10
   features:
     ema: [10,20,50]
     rsi: [14]
     atr: [14]
     adx: [14]
     bbands: [20,2.0]
     lookback: 1000
   training:
     val_ratio: 0.2
     cv_folds: 5
     enable_lstm: false
   logging:
     level: "INFO"
     jst: true
   ```

   * **DoD**ï¼š`pydantic`ç­‰ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒé€šã‚‹ã€‚

## ãƒ•ã‚§ãƒ¼ã‚º1ï¼šGUIã®éª¨æ ¼ + å£åº§/ãƒã‚¸ã‚·ãƒ§ãƒ³å¯è¦–åŒ–

4. **Dashboardã‚¿ãƒ–ï¼ˆå£åº§ï¼‹ãƒã‚¸ã‚·ãƒ§ãƒ³ä¸€è¦§ï¼‰**

   * MT5å£åº§æƒ…å ±ã‚’5ç§’æ›´æ–°ã€ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆBuy=é’/Sell=èµ¤ï¼‰
   * **DoD**ï¼šæ®‹é«˜/Equity/å«ã¿æç›Š/ãƒã‚¸æ•°/ãƒ­ãƒƒãƒˆåˆè¨ˆãŒæ­£ç¢ºã€è‰²åˆ†ã‘OKã€NPEãªã—ã€‚
5. **Controlã‚¿ãƒ–ï¼ˆStart/Stop/å­¦ç¿’/ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒœã‚¿ãƒ³ã€ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ï¼‰**

   * å–å¼•ON/OFFãƒˆã‚°ãƒ«ã€ãƒ­ãƒƒãƒˆï¼é–¾å€¤ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã€SL/TPå…¥åŠ›
   * **DoD**ï¼šUIæ“ä½œâ†’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®šã«å³åæ˜ ã€‚èª¤æ“ä½œã‚¬ãƒ¼ãƒ‰è¡¨ç¤ºã€‚

## ãƒ•ã‚§ãƒ¼ã‚º2ï¼šãƒ‡ãƒ¼ã‚¿å–å¾—â†’ç‰¹å¾´é‡â†’æ¨è«–ï¼ˆM15+H1ã€å›ºå®šTP/SLï¼‰

6. **å±¥æ­´å–å¾—ãƒ»å‰å‡¦ç†**

   * MT5ã‹ã‚‰M15/H1ã®OHLCVã‚’DLã€ã‚®ãƒ£ãƒƒãƒ—ãƒ»é‡è¤‡å‡¦ç†ã€JSTæ•´åˆ—
   * **DoD**ï¼šæ¬ æç‡<0.5%ã€ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã§æ™‚ç³»åˆ—ä¸€è²«æ€§OKã€‚
7. **ç‰¹å¾´é‡ç”Ÿæˆï¼ˆå…±é€šï¼‰**

   * EMA/RSI/ATR/ADX/BBã€å¤‰åŒ–ç‡ã€å‡ºæ¥é«˜ã€ãƒ’ã‚²æ¯”ç‡ãªã©
   * **DoD**ï¼š`features.parquet`ç”Ÿæˆã€åˆ—ãƒ¡ã‚¿ï¼ˆå®šç¾©ãƒãƒƒã‚·ãƒ¥ï¼‰ä¿å­˜ã€‚
8. **ãƒ©ãƒ™ãƒªãƒ³ã‚°**

   * ä¾‹ï¼š`y_dir`ï¼ˆ10ãƒãƒ¼å¾Œã®ä¸Šæ˜‡/ä¸‹é™/æ¨ªã°ã„ï¼‰ï¼‹`y_pips`ï¼ˆå›å¸°ï¼‰
   * **DoD**ï¼šã‚¯ãƒ©ã‚¹åˆ†å¸ƒãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªãƒ¼ã‚¯æ¤œæŸ»OKã€‚
9. **å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆLGB/XGBï¼‰å­¦ç¿’ & æ¨è«–API**

   * ã¾ãš**åˆ†é¡(LGB, XGB)**ã¨**å›å¸°(LGB)**ã€‚LSTMã¯ç„¡åŠ¹ã§é€²ã‚€ã€‚
   * **DoD**ï¼š`predict_proba/predict_pips`ãŒjsonableã§è¿”ã‚‹ã€æ¨è«–<5ms/ã‚µãƒ³ãƒ—ãƒ«ï¼ˆVPSï¼‰ã€‚
10. **ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ or LGBï¼‰**

* å„ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›â†’æœ€çµ‚Buy/Sell/Skip
* **DoD**ï¼šæ¤œè¨¼AUC/LoglossãŒå˜ä½“ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ï¼ˆæœ€ä½ã‚ãšã‹ã§ã‚‚ï¼‰ã€‚

## ãƒ•ã‚§ãƒ¼ã‚º3ï¼šåŸ·è¡Œãƒ¬ã‚¤ãƒ¤ & ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼

11. **ç™ºæ³¨ãƒ©ãƒƒãƒ‘ï¼ˆæˆè¡Œ/SL/TP/ç´„å®šã‚¤ãƒ™ãƒ³ãƒˆï¼‰**

* ä¾¡æ ¼ã‚ºãƒ¬å†è©¦è¡Œã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè€æ€§
* **DoD**ï¼šã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ã§ãƒŸãƒ‹ç™ºæ³¨â†’ç´„å®šâ†’ã‚¯ãƒ­ãƒ¼ã‚ºã¾ã§è‡ªå‹•å®Œäº†ã€‚

12. **max_positions=1ã®å¼·åˆ¶**

* **ç´„å®šå¾…ã¡ãƒ­ãƒƒã‚¯**ï¼ˆin-flightæ³¨æ–‡ãŒã‚ã‚‹é–“ã¯æ–°è¦ç¦æ­¢ï¼‰
* **DoD**ï¼šã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã§åŒæ™‚ä¿¡å·ã§ã‚‚2ä»¶ç›®ã¯ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã€‚

13. **ãƒ­ãƒƒãƒˆè¨ˆç®—ï¼ˆæœ‰åŠ¹è¨¼æ‹ é‡‘ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰**

* `lot = max(min_lot, round_to_step(equity/1_000_000, step))`ã€`<= max_cap`
* **DoD**ï¼š10,000å††â†’0.01 lotã€20,000å††â†’0.02 lotâ€¦ã®ç¢ºèªãƒ†ã‚¹ãƒˆã€‚

14. **ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼**

* æ—¥æ¬¡DD/ç·DD/ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸æ€¥å¢—/é€£æ•— N å›
* **DoD**ï¼šé–¾å€¤åˆ°é”ã§è‡ªå‹•åœæ­¢ã€UIã«èµ¤ãƒãƒŠãƒ¼è¡¨ç¤ºã€æ‰‹å‹•è§£é™¤ã®ã¿å†é–‹ã€‚

## ãƒ•ã‚§ãƒ¼ã‚º4ï¼šAIã‚¿ãƒ–/Chartã‚¿ãƒ–ã®â€œè¦‹ãˆã‚‹åŒ–â€

15. **AIã‚¿ãƒ–**

* å‹ç‡/PF/DD/é€£å‹é€£æ•—/ç·å–å¼•ã€ãƒ¢ãƒ‡ãƒ«åã€æœ€æ–°å­¦ç¿’æ—¥æ™‚ã€ç›´è¿‘ç¢ºç‡ãƒãƒ¼ã€FIãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆLGB/XGBï¼‰ã€SHAPä¸Šä½3
* **DoD**ï¼šæœ€æ–°ã®æ¨è«–ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã€SHAPã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ã§å³æ™‚æç”»ã€‚

16. **Chartã‚¿ãƒ–**

* ã‚¨ã‚¯ã‚¤ãƒ†ã‚£æ›²ç·šã€DDæ¨ç§»ã€æ™‚é–“å¸¯Ã—æ—¥ä»˜ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã€AIç¢ºç‡ã®æ™‚ç³»åˆ—
* **DoD**ï¼š1æ—¥10,000ç‚¹è¶…ã§ã‚‚æ»‘ã‚‰ã‹ï¼ˆpyqtgraphä½¿ç”¨ï¼‰ã€CPUå æœ‰<40%ã€‚

## ãƒ•ã‚§ãƒ¼ã‚º5ï¼šå‡ºå£ã®é€²åŒ–ï¼ˆATRâ†’AIæœ€é©åŒ–ï¼‰

17. **å›ºå®šTP/SL â†’ ATRé€£å‹•**

* å¼·ãƒˆãƒ¬ãƒ³ãƒ‰æ™‚TPæ‹¡å¼µã€ãƒ¬ãƒ³ã‚¸æ™‚çŸ­ç¸®
* **DoD**ï¼šé€±æ¬¡ABãƒ†ã‚¹ãƒˆã§åŸºæº–ã‚ˆã‚ŠPFâ†‘ã‹DDâ†“ã®ã„ãšã‚Œã‹ã‚’é”æˆã€‚

18. **AIå‡ºå£ï¼ˆå›å¸°pred_pipsï¼‹å‹¢ã„ã‚¹ã‚³ã‚¢ï¼‰**

* ç›®æ¨™TP/SLã¨ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹ç‚¹ã‚’ãƒ¡ã‚¿æœ€é©åŒ–
* **DoD**ï¼šWFOã§åŸºæº–æ¯” ã‚·ãƒ£ãƒ¼ãƒ—â†‘ã€ã¾ãŸã¯æœ€å¤§DDâ†“ã‚’ç¢ºèªã€‚

## ãƒ•ã‚§ãƒ¼ã‚º6ï¼šé‹ç”¨è©•ä¾¡ãƒ»ãƒ‰ãƒªãƒ•ãƒˆãƒ»é€±æœ«ã‚¸ãƒ§ãƒ–

19. **KPI/SLOã‚¦ã‚©ãƒƒãƒãƒ£ãƒ¼**

* ç§»å‹•çª“ã®PF/å‹ç‡/æœŸå¾…å€¤/ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ç­‰
* **DoD**ï¼šSLOå‰²ã‚Œã§è‡ªå‹•ã€Œç¸®å°é‹è»¢ã€ã¸ï¼ˆãƒ­ãƒƒãƒˆ1/2ã€ã¾ãŸã¯åœæ­¢ï¼‰ã€‚

20. **ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥**

* PSI/KL/KSï¼‹æ€§èƒ½ï¼ˆBrier/Loglossï¼‰
* **DoD**ï¼šè­¦å‘Šâ†’ç·Šæ€¥å†å­¦ç¿’ã‚¸ãƒ§ãƒ–èµ·å‹•ï¼ˆåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ï¼‰ã€‚

21. **é€±æœ«ãƒãƒƒãƒï¼ˆå­¦ç¿’ãƒ»WFOãƒ»æœ€é©åŒ–ï¼‰**

* åœŸæ›œæœJSTï¼šå­¦ç¿’â†’WFOâ†’ãƒ™ã‚¤ã‚ºæœ€é©â†’ãƒ¢ãƒ‡ãƒ«ç½²åâ†’å±•é–‹
* **DoD**ï¼šè‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆï¼ˆHTML/PDFï¼‰ç”Ÿæˆâ†’Logã‚¿ãƒ–é€šçŸ¥ã€‚

## ãƒ•ã‚§ãƒ¼ã‚º7ï¼šM5ã®è¿½åŠ â†’ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°å®Œæˆ

22. **M5ç‰¹å¾´ã®è¿½åŠ ï¼ˆã‚¿ã‚¤ãƒŸãƒ³ã‚°è£œå®Œï¼‰**

* çŸ­æœŸEMAå‚¾ã/ãƒ’ã‚²æ¯”/ç›´è¿‘ATR ç­‰
* **DoD**ï¼šã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®å¹³å‡ä¸åˆ©ç´„å®šè·é›¢ï¼ˆslippageæ–¹å‘ï¼‰æ”¹å–„ã€ã¾ãŸã¯å¹³å‡ä¿æŒæ™‚é–“çŸ­ç¸®ã€‚

23. **M15+M5+H1ã®ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°**

* 3ç³»ã®å‡ºåŠ›ã‚’ãƒ¡ã‚¿ã§çµ±åˆ
* **DoD**ï¼šWFOã§Phase2æ¯”ã®ç·åˆæ”¹å–„ï¼ˆPF or DD or Sharpeï¼‰ã€‚

---

# ä¸»è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨è²¬å‹™ï¼ˆå®Ÿè£…æ™‚ã®åœ°å›³ï¼‰

* `core/mt5_client.py`ï¼šæ¥ç¶šã€ç›¸å ´è³¼èª­ã€ç™ºæ³¨ã€ç´„å®šã‚¤ãƒ™ãƒ³ãƒˆ
* `core/position_guard.py`ï¼šmax_positionsã€ç´„å®šå¾…ã¡ãƒ­ãƒƒã‚¯ã€é‡è¤‡é˜²æ­¢
* `core/risk_manager.py`ï¼šæ—¥æ¬¡DDãƒ»ç·DDãƒ»é€£æ•—ãƒ»ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ç›£è¦–
* `core/feature_pipeline.py`ï¼šå±¥æ­´DLâ†’å‰å‡¦ç†â†’ç‰¹å¾´é‡ä¿å­˜/ãƒ­ãƒ¼ãƒ‰
* `core/models/`ï¼š`lgb_clf.py` `xgb_clf.py` `lgb_reg.py` `lstm.py` `meta_model.py`
* `core/inference.py`ï¼šå„ãƒ¢ãƒ‡ãƒ«æ¨è«–â†’ãƒ¡ã‚¿åˆ¤å®šâ†’ã‚·ã‚°ãƒŠãƒ«å‡ºåŠ›
* `core/execution.py`ï¼šæ³¨æ–‡ç”Ÿæˆã€SL/TPè¨­å®šã€ãƒªãƒˆãƒ©ã‚¤
* `core/logger.py`ï¼šæ§‹é€ åŒ–ãƒ­ã‚°ã€Decision Trace
* `app/gui.py`ï¼šPyQt6å„ã‚¿ãƒ–ã€ã‚¹ãƒ¬ãƒƒãƒ‰å®‰å…¨ãªè³¼èª­
* `batch/train.py`ï¼šå­¦ç¿’ã€ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã€ãƒ¡ã‚¿JSONå‡ºåŠ›ï¼ˆç‰¹å¾´é‡ãƒãƒƒã‚·ãƒ¥å«ã‚€ï¼‰
* `batch/walkforward.py`ï¼šæœŸé–“ã‚¹ã‚­ãƒ¼ãƒ ã€é›†è¨ˆã€DoDåˆ¤å®š
* `batch/optimize.py`ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆé–¾å€¤/ATRä¿‚æ•°/ãƒˆãƒ¬ãƒ¼ãƒ«ä¿‚æ•°ï¼‰
* `tests/`ï¼šãƒ¦ãƒ‹ãƒƒãƒˆï¼‹çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆç–‘ä¼¼MT5ã‚¹ã‚¿ãƒ–ï¼‰

---

# Logã‚¿ãƒ–ã®ã‚¤ãƒ™ãƒ³ãƒˆè¦ç´„ï¼ˆä¾‹ï¼‰

```json
{
  "ts_jst":"2025-10-30T13:05:12",
  "type":"trade_open",
  "symbol":"USDJPY",
  "signal":{"p_buy":0.67,"p_sell":0.31,"meta":"BUY"},
  "features_hash":"a1b2c3...",
  "models":{"lgb_clf":0.64,"xgb_clf":0.62,"lgb_reg_pred_pips":8.3},
  "order":{"req_lot":0.03,"lot":0.03,"sl_pips":10,"tp_pips":15},
  "guard":{"inflight":false,"max_positions":1},
  "slippage":0.2
}
```

---

# ãƒ ãƒ¼ãƒ‰è¡¨ç¤ºï¼ˆAIã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼‰

* ãƒ«ãƒ¼ãƒ«ï¼šç›´è¿‘Nåˆ†ã®**ATRå¤‰åŒ–ç‡**ãƒ»**æˆè¡Œæ‹’å¦ç‡**ãƒ»**ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦(ADX)**ã§3çŠ¶æ…‹ã‚’åˆ¤å®š
  ä¾‹ï¼šã€Œå¸‚å ´ã¯é™ç©ã€ã€Œãƒœãƒ©æ€¥ä¸Šæ˜‡æ³¨æ„ã€ã€Œãƒˆãƒ¬ãƒ³ãƒ‰åè»¢è­¦æˆ’ä¸­ã€
* DoDï¼šãƒ ãƒ¼ãƒ‰çŠ¶æ…‹ãŒé–¾å€¤ã§ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹åˆ¶å¾¡ï¼ˆãƒ”ã‚³ãƒ”ã‚³åˆ‡æ›¿é˜²æ­¢ï¼‰

---

# ãƒ†ã‚¹ãƒˆè¨ˆç”»ï¼ˆè¦ç‚¹ï¼‰

* **æ©Ÿèƒ½**ï¼šmax_positionsç«¶åˆã€ãƒ­ãƒƒãƒˆè¨ˆç®—ã€DDã‚¹ãƒˆãƒƒãƒ—ã€ç™ºæ³¨ãƒªãƒˆãƒ©ã‚¤ã€ãƒ‡ãƒ¼ã‚¿ã‚®ãƒ£ãƒƒãƒ—å‡¦ç†
* **æ€§èƒ½**ï¼šæ¨è«–QPSã€UIæç”»FPSã€I/Oå¾…ã¡
* **å›å¸°**ï¼šç‰¹å¾´é‡ãƒãƒƒã‚·ãƒ¥ãŒå¤‰ã‚ã£ãŸã‚‰æ—§ãƒ¢ãƒ‡ãƒ«ã‚’æ‹’å¦
* **ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ**ï¼šWFOã§`PF, Sharpe, MaxDD, Win%`ã®å››å¤©ç‹ã‚’æœ€ä½å‡ºåŠ›

---

# çœãƒ¡ãƒ¢ãƒªï¼†é«˜é€ŸåŒ–ãƒ¡ãƒ¢ï¼ˆ2GBæƒ³å®šï¼‰

* pandasã¯`float32`/`int32`ã«ç¸®å°ã€ç‰¹å¾´é‡ã¯**å­¦ç¿’ç”¨ã¨æ¨è«–ç”¨ã§åˆ—æœ€å°åŒ–**
* LightGBMã¯`num_leaves`ã¨`feature_fraction`ã§è»½é‡åŒ–
* SHAPã¯**èª¬æ˜ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚µãƒ–ã‚»ãƒƒãƒˆ**ï¼ˆä¸Šä½æœ€è¿‘2000ä»¶ãªã©ï¼‰
* LSTMã¯**é€±æœ«ã®ã¿**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€å¹³æ™‚ã¯å‡çµæ¨è«– or ç„¡åŠ¹

---

# åˆå›ã‚¹ãƒ—ãƒªãƒ³ãƒˆï¼ˆ1é€±é–“ç›®ã®ç€åœ°ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰

1. ãƒ•ã‚§ãƒ¼ã‚º0ã€œ1ã‚’å®Œäº†ï¼ˆGUIéª¨æ ¼ï¼‹å£åº§/ãƒã‚¸è¡¨ç¤ºï¼‹åˆ¶å¾¡ç³»ï¼‰
2. ãƒ•ã‚§ãƒ¼ã‚º2ã®å‰å‡¦ç†ã€œå˜ä½“LGBåˆ†é¡ã®æ¨è«–ã¾ã§é€šã—
3. **ãƒ‡ãƒ¢å£åº§**ã§1æ—¥ãƒšãƒ¼ãƒ‘ãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰é‹ç”¨â†’ãƒ­ã‚°æ¤œè¨¼

---

# æ¬¡ã«åºƒã’ã‚‹ãªã‚‰

* æœ¬ç•ªåˆ‡æ›¿ã®**äºŒé‡ç¢ºèª**
* ã€Œå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚¤ãƒ‹ãƒ³ã‚°ã€ã‹ã‚‰**è‡ªå‹•ãƒ•ã‚£ãƒ«ã‚¿å€™è£œ**ç”Ÿæˆ
* é€±æœ«ã‚¸ãƒ§ãƒ–ã®**HTMLãƒ¬ãƒãƒ¼ãƒˆ**ï¼ˆãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ»KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼‰

ã“ã®ãƒªã‚¹ãƒˆã§å®Ÿè£…ã™ã‚Œã°ã€ã¾ãšã¯**M15+H1Ã—å›ºå®šTP/SL**ã®å®‰å®šç¨¼åƒâ†’å¾ã€…ã«**ATR/AIå‡ºå£**â†’**M5çµ±åˆã®ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°**ã¨ã„ã†ã€å£Šã‚Œã«ãã„é€²åŒ–ã‚³ãƒ¼ã‚¹ã«ãªã‚Šã¾ã™ã€‚æ¬¡ã¯ã€Œãƒ•ã‚§ãƒ¼ã‚º0ã€œ1ã®ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆã²ãªå½¢ã‚³ãƒ¼ãƒ‰ï¼`config.yaml`ï¼GUIãƒ¯ã‚¤ãƒ¤ãƒ¼ï¼‰ã€ã‚’ç§ã‹ã‚‰ç”¨æ„ã—ã¦ã€ãã®ã¾ã¾VS Codeã§èµ°ã‚‹ã¨ã“ã‚ã¾ã§ä¸€æ°—ã«ã„ãã¾ã—ã‚‡ã†ã€‚



=== file: To Doãƒªã‚¹ãƒˆ2.txt ===

ã¾ãšç¾çŠ¶ã‚’ã–ã£ãã‚Šæ•´ç†ã—ã¦ã€ãã®ã†ãˆã§ã€Œã“ã‚Œã‹ã‚‰ã‚„ã‚‹ä½œæ¥­1ã€œã€ã‚’ä¸¦ã¹ç›´ã—ã¾ã™ã€‚
ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã¯**ä¸€è¦§ã®ä½œã‚Šç›´ã—ã ã‘**ã‚„ã£ã¦ã€å®Ÿéš›ã®å®Ÿè£…ã¯æ¬¡ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰1å€‹ãšã¤ã‚„ã£ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

---

## 1. ã„ã¾åˆ†ã‹ã£ã¦ã„ã‚‹ã€Œå®Œäº†æ¸ˆã¿ã€ãŸã¡

æ‰‹å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨å‰ã®ä¼šè©±ã‚’çªãåˆã‚ã›ã‚‹ã¨ã€å°‘ãªãã¨ã‚‚ã“ã“ã¾ã§ã¯çµ‚ã‚ã£ã¦ã„ã¾ã™ã€‚

### 1-1. ç’°å¢ƒãƒ»éª¨æ ¼ã¾ã‚ã‚Š

* Python 3.13 / ä»®æƒ³ç’°å¢ƒ `.venv` / VS Code / PowerShell 7 ã¾ã‚ã‚Šã®æ•´å‚™

  * `ä»®æƒ³ç’°å¢ƒ.txt` ã« `.venv\Scripts\Activate.ps1` ãŒãƒ¡ãƒ¢æ¸ˆã¿ã€‚
* ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆãƒ»é–‹ç™ºãƒ„ãƒ¼ãƒ«

  * `pyproject.toml`ï¼ˆruffè¨­å®šï¼‰ã€`pytest.ini`ã€`requirements.txt` ãªã©ãŒæƒã£ã¦ã„ã‚‹ã€‚
  * `tests/test_sanity.py` ã§æœ€ä½é™ã® pytest ãŒå‹•ä½œã™ã‚‹çŠ¶æ…‹ã€‚

### 1-2. AIã¾ã‚ã‚Šãƒ»ãƒãƒƒãƒ

* LightGBM ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ¢ãƒ‡ãƒ« + ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆPlatt / Isotonicï¼‰æ§‹æˆ

  * `tools/train_lightgbm.py` ã¨ `models/LightGBM_clf.pkl` ä¸€å¼ã‚ã‚Šã€‚
* ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

  * `tools/train_scaler.py` ã§ M15 CSV ã‹ã‚‰ç‰¹å¾´é‡ã‚’ä½œã‚Šã€StandardScaler ã‚’ä¿å­˜ã™ã‚‹ã¨ã“ã‚ã¾ã§å®Ÿè£…æ¸ˆã¿ã€‚
* Walkforward / é€±æ¬¡å†å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

  * `scripts/walkforward_train.py` / `scripts/walkforward_retrain.py` / `scripts/weekly_wf.ps1` ãŒå­˜åœ¨ã—ã€ãƒ•ã‚§ãƒ¼ã‚ºAã€œHã®æ¤œè¨¼çµæœã‚‚ãƒ•ã‚¡ã‚¤ãƒ«åŒ–æ¸ˆã¿ã€‚

### 1-3. å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ãƒ»ãƒªã‚¹ã‚¯åˆ¶å¾¡

* ATRã‚²ãƒ¼ãƒˆã®ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ã€ATRãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã€side_bias æ–°ãƒ­ã‚¸ãƒƒã‚¯

  * `ä½œæ¥­6ï½8.txt` / `ä½œæ¥­9.txt` / `ä½œæ¥­10.txt` ãŒã™ã¹ã¦ã€Œä½œæ¥­X çµæœã€ã¨ãªã£ã¦ãŠã‚Šã€
    ATRãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ãƒ»ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ãƒ»entryåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®æ–°å®Ÿè£…ã¨æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚ 
* ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ & decision ãƒ­ã‚°

  * `scripts/cb_smoke.py` ã‚„ `logs/decisions_*.jsonl` ã‚’å‰æã«ã—ãŸãƒ•ã‚§ãƒ¼ã‚ºAã€œHã®è¨ºæ–­ãŒ `ãƒ•ã‚§ãƒ¼ã‚ºA~Hã¾ã§ã®çµæœ.txt` ã«ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã€‚

### 1-4. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ»å¯è¦–åŒ–

* ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨ã‚¨ã‚¯ã‚¤ãƒ†ã‚£æ›²ç·šå‡ºåŠ›

  * `tools/backtest_run.py` ã¨ `tools/backtest_equity_curve.py` ãŒã‚ã‚Šã€`logs/backtest/â€¦/equity_curve.csv` ã‚’å‡ºã™ã¨ã“ã‚ã¾ã§ã¯å‹•ä½œç¢ºèªæ¸ˆã¿ã€‚
* GUI å´ã®ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã‚°ãƒ©ãƒ•è¡¨ç¤º

  * ã‚¨ã‚¯ã‚¤ãƒ†ã‚£æ›²ç·šã®è¡¨ç¤ºãƒ»ã‚ºãƒ¼ãƒ ãƒ»æœŸé–“ç§»å‹•ãªã©ã¯ä¸€é€šã‚Šå®Ÿè£…ï¼†ä½•åº¦ã‹æ”¹è‰¯æ¸ˆã¿ï¼ˆãƒãƒ¼ã‚«ãƒ¼ã¯ã€Œå‡ºã¦ãªã„ã‘ã©ã‚‚ã†ã„ã„ã‚„ã€ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¤æ–­ã§ä¸€æ—¦å®Œäº†æ‰±ã„ï¼‰ã€‚

### 1-5. é€±æ¬¡ã‚¸ãƒ§ãƒ–ãƒ»ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©

* é€±æ¬¡å†å­¦ç¿’ã®è‡ªå‹•å®Ÿè¡Œ

  * `scripts/register_weekly_task.ps1` / `scripts/setup_weekly_job.ps1` ãŒã‚ã‚Šã€
    `ã‚„ã‚‹ã¹ãã“ã¨.txt` ã«æ›¸ã‹ã‚Œã¦ã„ãŸã€Œregister_weekly_task.ps1 ã®å®Ÿè¡Œã€ã‚‚æ¸ˆã‚“ã§ã„ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œé€±æ¬¡å†å­¦ç¿’ã®ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ç™»éŒ²ã¯å®Œäº†ã€ã¨ç™ºè¨€ï¼‰ã€‚

---

## 2. ã€Œã¾ã ã“ã‚Œã‹ã‚‰ã€ã®ä»£è¡¨çš„ãªä»•æ§˜

å…ƒã® To Do ã¨ä»•æ§˜ã‚’è¦‹ã‚‹é™ã‚Šã€**ã¾ã æ‰‹ã‚’ã¤ã‘ã¦ã„ãªã„ or é€”ä¸­ã§æ­¢ã‚ã¦ã„ã‚‹ã£ã½ã„ã‚‚ã®**ã¯ã“ã®è¾ºã§ã™ã€‚ 

* ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¿ãƒ–ã§

  * æœŸé–“æŒ‡å®šï¼ˆFrom/Toï¼‰ï¼‹ã€ŒALLã€ãƒœã‚¿ãƒ³
  * å˜ç´”ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ vs Walkforward ç”£å‡ºçµæœã®åˆ‡æ›¿ãƒ»é‡ã­æç”»
* è¤‡æ•°PCï¼ˆé–‹ç™º2å°ï¼‹é‹ç”¨1å°ï¼‰ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹å®Œå…¨æ•´ç†

  * MT5 ãƒ’ã‚¹ãƒˆãƒªã®æ‰±ã„ã‚’å«ã‚ã€ã€Œã©ã®PCã§ã‚‚åŒã˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å‹•ãã€çŠ¶æ…‹ã¾ã§è©°ã‚ã‚‹
* AIã‚¿ãƒ–ã®ä»•ä¸Šã’

  * æœˆé–“/é€±é–“æç›Šã‚°ãƒ©ãƒ•ã€ãƒ¢ãƒ‡ãƒ«åã€æ¤œè¨¼æŒ‡æ¨™ã€é‡è¦ç‰¹å¾´é‡ã€SHAPä¸Šä½3ã€å­¦ç¿’æ—¥æ™‚ã€ä¿¡é ¼åº¦ãƒ¡ãƒ¼ã‚¿ãƒ¼ãªã©
* ãƒ ãƒ¼ãƒ‰è¡¨ç¤ºï¼ˆã€Œé™ç©ã€ã€Œãƒœãƒ©æ€¥ä¸Šæ˜‡ã€ã€Œåè»¢è­¦æˆ’ä¸­ã€ï¼‹æ ¹æ‹ å¹ãå‡ºã—ï¼‰
* drift ç›£è¦–ï¼ˆPSI / Loglossï¼‰ã¨è­¦å‘Šè¡¨ç¤º
* M5/H1/M15 ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚° & å‡ºå£æˆ¦ç•¥ã®é«˜åº¦åŒ–
* ã€Œãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®çµæœã‚’è¨ºæ–­ã™ã‚‹AIã€ã¿ãŸã„ãªå¾Œä»˜ã‘ã‚¢ãƒŠãƒªã‚¹ãƒˆ

ã“ã®ä¸­ã‹ã‚‰ã€**ä»Šã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã¨æœ€è¿‘ã®ä¼šè©±**ã«æ²¿ã£ã¦ã€
ã€Œå„ªå…ˆåº¦é«˜ã‚ã§ã€1ã‚¹ãƒ¬ãƒƒãƒ‰1ä½œæ¥­ã§å›ã›ã‚‹ã‚µã‚¤ã‚ºã€ã®ã‚‚ã®ã‚’ä¸¦ã¹ç›´ã—ã¾ã™ã€‚

---

## 3. ã“ã‚Œã‹ã‚‰ã®ä½œæ¥­ãƒªã‚¹ãƒˆ v2ï¼ˆä½œæ¥­1ã€œ7ï¼‰

ã“ã“ã‹ã‚‰ãŒæ–°ã—ã„ ToDo æœ¬ä½“ã§ã™ã€‚
**ã“ã®ç•ªå·ã¯ã€Œã“ã‚Œã‹ã‚‰ã‚„ã‚‹åˆ†ã€ã®é€šã—ç•ªå·**ã¨æ€ã£ã¦ãã ã•ã„ï¼ˆæ˜”ã®ä½œæ¥­6ã€œ10ã¨ã¯åˆ¥ã‚«ã‚¦ãƒ³ã‚¿ï¼‰ã€‚

### ä½œæ¥­1ï¼šãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ & MT5 å±¥æ­´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ€çµ‚æ•´ç†

**ç›®çš„**

* ã©ã®PCã§ã‚‚åŒã˜ã‚³ãƒãƒ³ãƒ‰ã§

  * MT5 å±¥æ­´ â†’ CSV ç”Ÿæˆ
  * `data/` é…ä¸‹ã«ä¿å­˜
    ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã€`fxbot_path` / ç’°å¢ƒå¤‰æ•°ã®ãƒ«ãƒ¼ãƒ«ã‚’ç¢ºå®šã•ã›ã‚‹ã€‚

**ã–ã£ãã‚Šä¸­èº«**

* `fxbot_path.py` ã®ä»•æ§˜ã‚’ç¢ºèªã—ã€ã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã€ã€Œdataãƒ«ãƒ¼ãƒˆã€ã®æ±ºã‚æ–¹ã‚’æœ€çµ‚æ±ºå®š
* `scripts/export_mt5_history.py` / `scripts/make_csv_from_mt5.py` / `scripts/mt5_export_csv.py` ã®ã©ã‚Œã‚’æ­£ã¨ã™ã‚‹ã‹æ±ºã‚ã¦ä¸€æœ¬åŒ–
* 3å°ã®PCã§åŒã˜ã‚³ãƒãƒ³ãƒ‰ä¾‹ã§å‹•ãã“ã¨ã‚’ç¢ºèªã—ã€ãã®ä½¿ã„æ–¹ã‚’ `README.md` ã«è¿½è¨˜

**å®Œäº†åˆ¤å®šï¼ˆDoDï¼‰**

* 3å°ã™ã¹ã¦ã§ã€ä¾‹ãˆã°ã“ã‚“ãªã‚³ãƒãƒ³ãƒ‰ãŒé€šã‚‹ï¼š

  ```powershell
  # ä¾‹
  .\.venv\Scripts\python.exe scripts\export_mt5_history.py --symbol USDJPY --timeframe M5
  ```

* ç”Ÿæˆã•ã‚ŒãŸ CSV ã®ãƒ‘ã‚¹ã¨ã€GUI / å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå‚ç…§ã™ã‚‹ãƒ‘ã‚¹ãŒä¸€è‡´ã—ã¦ã„ã‚‹

* `README.md` ã«ã€Œãƒ‡ãƒ¼ã‚¿æ›´æ–°æ‰‹é †ã€ãŒ1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦ã¾ã¨ã¾ã£ã¦ã„ã‚‹

---

### ä½œæ¥­2ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ CLI ã«æœŸé–“æŒ‡å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 

**ç›®çš„**

* `tools/backtest_run.py` ã« `--start-date` / `--end-date` ãªã©ã®å¼•æ•°ã‚’è¿½åŠ ã—ã¦ã€
  ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå¯¾è±¡æœŸé–“ã‚’çµã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

**ã–ã£ãã‚Šä¸­èº«**

* `tools/backtest_run.py` ã« `argparse` ã‚’è¿½åŠ 
* æœŸé–“æŒ‡å®šãŒã‚ã‚Œã°ã€ãã®ç¯„å›²ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã—ã¦ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
* å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆä¾‹ï¼š`logs/backtest/â€¦`ï¼‰ã®å‘½åã«æœŸé–“æƒ…å ±ã‚’å«ã‚ã‚‹

**å®Œäº†åˆ¤å®š**

* ä¾‹ï¼š

  ```powershell
  .\.venv\Scripts\python.exe tools\backtest_run.py --symbol USDJPY --tf M5 `
      --start-date 2024-07-01 --end-date 2024-12-31
  ```

  ã§ã€æŒ‡å®šæœŸé–“ã ã‘ã‚’å¯¾è±¡ã« `equity_curve.csv` ãŒç”Ÿæˆã•ã‚Œã‚‹
* æŒ‡å®šãªã—ã§å®Ÿè¡Œã—ãŸå ´åˆã¯ã€Œå…¨æœŸé–“ã€ã§å¾“æ¥é€šã‚Šå‹•ã

---

### ä½œæ¥­3ï¼šGUI ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¿ãƒ–ã®æœŸé–“æŒ‡å®šï¼†ã€ŒALLã€ãƒœã‚¿ãƒ³é€£å‹•

**ç›®çš„**

* GUI ã® Backtest ã‚¿ãƒ–ã‹ã‚‰

  * From/To æ—¥ä»˜ãƒ”ãƒƒã‚«ãƒ¼
  * ã€ŒALLï¼ˆã‚ºãƒ¼ãƒ ãƒªã‚»ãƒƒãƒˆï¼‰ã€ãƒœã‚¿ãƒ³
    ã‚’æ“ä½œ â†’ `backtest_run.py` ã‚’å‘¼ã³å‡ºã—ã€ã‚°ãƒ©ãƒ•ã‚’æ›´æ–°ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

**ã–ã£ãã‚Šä¸­èº«**

* `app/gui/` å†…ã® Backtest ã‚¿ãƒ–å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãŠãã‚‰ã `backtest_tab.py` ã‹ `history_tab.py`ï¼‰ã‚’ç‰¹å®š
* ä½œæ¥­2ã§è¿½åŠ ã—ãŸ CLI ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åˆã‚ã›ã¦ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å‘¼ã³å‡ºã—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£
* ã€ŒALLã€ãƒœã‚¿ãƒ³ã§

  * æ—¥ä»˜ãƒ”ãƒƒã‚«ãƒ¼ã‚’å…¨æœŸé–“ã«ãƒªã‚»ãƒƒãƒˆ
  * ã‚°ãƒ©ãƒ•ã®ã‚ºãƒ¼ãƒ ã‚‚å…¨æœŸé–“ã«æˆ»ã™

**å®Œäº†åˆ¤å®š**

* GUI ã§

  * ä»»æ„ã®æœŸé–“ã‚’æŒ‡å®šã—ã¦ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ â†’ ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã‚°ãƒ©ãƒ•ãŒãã®æœŸé–“ã ã‘ã«å¤‰ã‚ã‚‹
  * ã€ŒALLã€ã‚¯ãƒªãƒƒã‚¯ã§ãƒ•ãƒ«æœŸé–“è¡¨ç¤ºã«æˆ»ã‚‹
* ä¾‹å¤–ãŒå‡ºãšã€ãƒ­ã‚°ã«ã‚‚æ˜ã‚‰ã‹ãªã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã„

---

### ä½œæ¥­4ï¼šBacktest vs Walkforward çµæœã®åˆ‡æ›¿ãƒ»é‡ã­æç”»

**ç›®çš„**

* Backtest ã‚¿ãƒ–ã§

  * ç´ ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ
  * Walkforward æ¤œè¨¼ï¼ˆWFOï¼‰ã‹ã‚‰ã®ã‚µãƒãƒª
    ã‚’åˆ‡ã‚Šæ›¿ãˆãŸã‚Šã€é‡ã­ã¦è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

**ã–ã£ãã‚Šä¸­èº«**

* Walkforward ã®çµæœï¼ˆPF, MaxDD, Sharpe, Win% ãªã©ï¼‰ãŒã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºã¦ã„ã‚‹ã‹ã‚’ç¢ºèª

  * ä¾‹ï¼š`logs/retrain/report_*.json` ã‚„ `ãƒ•ã‚§ãƒ¼ã‚ºA~Hã¾ã§ã®çµæœ.txt` ã®å½¢å¼ã‚’æ•´ç†
* Backtest ã‚¿ãƒ–ã«ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‚„ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 

  * ã€Œãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®ã¿ã€
  * ã€ŒWalkforwardã‚µãƒãƒªã®ã¿ã€
  * ã€Œä¸¡æ–¹ã€
* åŒã˜ã‚°ãƒ©ãƒ•ä¸Šã«

  * ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®ã‚¨ã‚¯ã‚¤ãƒ†ã‚£
  * Walkforward ã§ã®ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ or KPI
    ã‚’æç”»

**å®Œäº†åˆ¤å®š**

* GUIã§ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆãŸã¨ãã€ã‚°ãƒ©ãƒ•ã®ã‚·ãƒªãƒ¼ã‚ºãŒåˆ‡ã‚Šæ›¿ã‚ã‚‹
* å°‘ãªãã¨ã‚‚ï¼‘ã¤ã® WFO çµæœã«ã¤ã„ã¦ã€æ•°å­—ãŒæ‰‹è¨ˆç®—ã¨åˆã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

---

### ä½œæ¥­5ï¼šAIã‚¿ãƒ–ã® KPI / FI / SHAP ã®ã€Œé‹ç”¨ç‰ˆã€ä»•ä¸Šã’

**ç›®çš„**

* ä»•æ§˜ã«æ›¸ã„ã¦ã‚ã‚‹ AIã‚¿ãƒ–ã®æƒ…å ±ã‚’ã€
  ã€Œæœ€ä½é™ã ãŒå®Ÿé‹ç”¨ã«ååˆ†ã€ãªã¨ã“ã‚ã¾ã§åŸ‹ã‚ã‚‹ã€‚

**ã–ã£ãã‚Šä¸­èº«**

* `tools/dump_feature_importance.py` ã‚„ `models/LightGBM_clf.features.json` ã‚’ä½¿ã£ã¦ FI ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ•´ç†
* ç›´è¿‘ Nãƒˆãƒ¬ãƒ¼ãƒ‰ã®é›†è¨ˆã‹ã‚‰

  * å‹ç‡ / PF / æœ€å¤§DD / é€£å‹ãƒ»é€£æ•—
    ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’ã©ã“ã‹ï¼ˆservices or toolsï¼‰ã«å®šç¾©
* GUI AIã‚¿ãƒ–ã§

  * ãƒ¢ãƒ‡ãƒ«å
  * æ¤œè¨¼æŒ‡æ¨™ï¼ˆLogloss, AUC ãªã©ï¼‰
  * é‹ç”¨æŒ‡æ¨™ï¼ˆæœ€è¿‘ã®å‹ç‡ãªã©ï¼‰
  * ä¸Šä½ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
    ã‚’è¡¨ç¤º

**å®Œäº†åˆ¤å®š**

* GUIã® AIã‚¿ãƒ–ã‚’é–‹ãã¨ã€ç›´è¿‘ã® `logs/decisions_*.jsonl` ã‹ã‚‰é›†è¨ˆã—ãŸæ•°å­—ãŒè¡¨ç¤ºã•ã‚Œã‚‹
* ãƒ¢ãƒ‡ãƒ«ã‚’å…¥ã‚Œæ›¿ãˆãŸå¾Œã«å†èµ·å‹•ã™ã‚‹ã¨ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«åï¼æŒ‡æ¨™ã«æ›´æ–°ã•ã‚Œã‚‹

---

### ä½œæ¥­6ï¼šãƒ‡ãƒ¢å£åº§å‘ã‘ MT5 ç™ºæ³¨ãƒ©ãƒƒãƒ‘ãƒ¼ã®æœ€åˆã®ä¸€æ­©

**ç›®çš„**

* ã“ã‚Œã¾ã§ã® `execution_stub` ãƒ™ãƒ¼ã‚¹ã® dryrun ã‹ã‚‰ä¸€æ­©é€²ã‚ã¦ã€
  **MT5 ãƒ‡ãƒ¢å£åº§ã«å¯¾ã—ã¦å®Ÿéš›ã«ç™ºæ³¨ã—ã¦ã¿ã‚‹ãŸã‚ã®æœ€å°ãƒ©ãƒƒãƒ‘ãƒ¼**ã‚’ä½œã‚‹ã€‚

**ã–ã£ãã‚Šä¸­èº«**

* `core/mt5_client.py`ï¼ˆã¾ã ç„¡ã‘ã‚Œã°æ–°è¦ï¼‰ã‚’ä½œã‚Šã€

  * æ¥ç¶šãƒ»ãƒ­ã‚°ã‚¤ãƒ³
  * æˆè¡Œç™ºæ³¨ï¼ˆBuy/Sellï¼‰
  * æ±ºæ¸ˆï¼ˆCloseï¼‰
    ã‚’ãƒ©ãƒƒãƒ—
* `app/services/execution_stub.py` ã‚’åˆ†é›¢ãƒ»æ•´ç†ã—ã¦ã€

  * ãƒ­ã‚¸ãƒƒã‚¯éƒ¨åˆ†ï¼ˆãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ãƒ»ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ãƒ»ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¬ãƒ¼ãƒ‰ï¼‰ã¯å…±é€š
  * å®Ÿéš›ã®ç™ºæ³¨éƒ¨åˆ†ã ã‘ã‚’æ–°ã—ã„ mt5_client ã«å§”è­²
* `scripts/selftest_order_flow.py` ã§ã€
  ã€Œå°ãƒ­ãƒƒãƒˆã§1å›ç™ºæ³¨ â†’ ã™ãæ±ºæ¸ˆã€ãŒé€šã‚‹ã“ã¨ã‚’ç¢ºèª

**å®Œäº†åˆ¤å®š**

* ãƒ‡ãƒ¢å£åº§ã§ 0.01 ãƒ­ãƒƒãƒˆç¨‹åº¦ã®è©¦é¨“ãƒˆãƒ¬ãƒ¼ãƒ‰ãŒ

  * ã‚¨ãƒ©ãƒ¼ãªãç™ºæ³¨
  * ç´„å®š
  * ã‚¯ãƒ­ãƒ¼ã‚º
    ã§ãã‚‹
* æ±ºæ¸ˆçµæœãŒ MT5 ã®å±¥æ­´ã‚¿ãƒ–ã¨ `logs/decisions_*.jsonl` ã®ä¸¡æ–¹ã§ç¢ºèªã§ãã‚‹

---

### ä½œæ¥­7ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼ˆv0ï¼‰ã‚’ä½œã£ã¦éŠã¶

**ç›®çš„**

* ã€Œãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚’ã©ã†è§£é‡ˆã™ã‚‹ã‹ï¼Ÿã€ã‚’åŠè‡ªå‹•åŒ–ã™ã‚‹ãŸã‚ã®
  **è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãªã„ã—ç°¡æ˜“AIï¼‰ã€ã®æœ€åˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³**ã‚’ä½œã‚‹ã€‚

**ã–ã£ãã‚Šä¸­èº«**

* `logs/backtest/â€¦/equity_curve.csv` ã¨ `logs/decisions_*.jsonl` ã‚’èª­ã¿è¾¼ã‚€ Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ–°è¦ä½œæˆ
* ã–ã£ãã‚Šãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã‚ˆã„ã®ã§ã€

  * ãƒˆãƒ¬ãƒ³ãƒ‰ç›¸å ´ã§ã®æˆç¸¾ vs ãƒ¬ãƒ³ã‚¸ç›¸å ´ã§ã®æˆç¸¾
  * æ™‚é–“å¸¯ã”ã¨ã®å‹ç‡
  * å¤§ããªDDç™ºç”Ÿå‰å¾Œã®ç‰¹å¾´
    ãªã©ã‚’é›†è¨ˆ
* çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‹ç°¡æ˜“HTMLã¨ã—ã¦å‡ºåŠ›

**å®Œäº†åˆ¤å®š**

* 1ã‚³ãƒãƒ³ãƒ‰ã§ã€Œãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆã€ãŒç”Ÿæˆã•ã‚Œã‚‹
* ãƒ¬ãƒãƒ¼ãƒˆã‚’è¦‹ã¦ã€Œã“ã®æœŸé–“ã¯ATRãŒä½ã™ãã¦ENTRYã»ã¼ãªã—ã€ãªã©ã€
  äººé–“ãŒç´å¾—ã§ãã‚‹ã‚³ãƒ¡ãƒ³ãƒˆãŒå‡ºã¦ã„ã‚‹

---

## 4. ã“ã®ã‚ã¨ã®é€²ã‚æ–¹ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰é‹ç”¨ãƒ«ãƒ¼ãƒ«ï¼‰

ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã¯ **ã€Œä½œæ¥­ãƒªã‚¹ãƒˆv2ã€ã®è¦‹ç›´ã—**ã¾ã§ã‚„ã‚Šã¾ã—ãŸã€‚

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ï¼š

1. æ¬¡ã®æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œã‚‹
2. ãã®æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§
   ä¾‹ï¼‰

   > ä½œæ¥­1ã‹ã‚‰å§‹ã‚ãŸã„ã€‚
   > ä½œæ¥­1ã®æ‰‹é †ã¨ã€å¿…è¦ãªä¿®æ­£ã‚³ãƒ¼ãƒ‰ã‚’è©³ã—ãæ•™ãˆã¦ã€‚

ã¿ãŸã„ã«æ›¸ã„ã¦ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚

ãã†ã—ãŸã‚‰ã€**ãã®ä½œæ¥­å°‚ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰**ã¨ã—ã¦

* ã‚„ã‚‹ã“ã¨ã®ç´°åˆ†åŒ–
* ã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã©ã®è¡Œã‚’ã©ã†ç›´ã™ã‹
* Codex ã«ãã®ã¾ã¾è²¼ã‚Œã‚‹ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
* ä½œæ¥­å¾Œã®ç¢ºèªæ‰‹é †ï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ»GUIæ“ä½œï¼‰

ã‚’ã¾ã¨ã‚ã¦å‡ºã—ã¾ã™ã€‚

ä½œæ¥­1ãŒå®Œäº†ã—ãŸã‚‰ã€ãã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã€Œå®Œäº†ã€ã¨æ›¸ã„ã¦é–‰ã˜ã¦ã€
ã¾ãŸæ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã€Œä½œæ¥­2ã‚’ã‚„ã‚ŠãŸã„ã€ã¨å®£è¨€ã—ã¦é€²ã‚ã‚‹ã€ã¨ã„ã†ã‚µã‚¤ã‚¯ãƒ«ã«ã—ã¾ã—ã‚‡ã†ã€‚
ãã†ã—ã¦ãŠãã¨ã€ã“ã®å·¨å¤§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚‚ãƒ¬ãƒ™ãƒ«åˆ¶RPGã¿ãŸã„ã«ä¸€æ­©ãšã¤æ”»ç•¥ã§ãã¾ã™ã€‚



=== file: To Doãƒªã‚¹ãƒˆv4.txt ===

ToDoãƒªã‚¹ãƒˆ v4ï¼ˆå†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©å‰æï¼‰
ã‚°ãƒ«ãƒ¼ãƒ—Aï¼š3ï¼…KPIå‘ã‘ã®ã‚³ã‚¢æ©Ÿèƒ½ï¼ˆæ—¢å­˜A1ã€œA7ã®æ•´ç†ï¼‰

ã“ã“ã¯å¤§ããå¤‰ãˆãšã€æ—¢å­˜ã®A1ã€œA7ã‚’ãã®ã¾ã¾é€²ã‚ã‚‹æ–¹é‡ï¼š

A1ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‹ã‚‰ monthly_returns.csv ã‚’å¿…ãšå‡ºã™ã€‚

A2ï¼šåŸºæº–ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ï¼ˆWFOï¼‹æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã§å¦¥å½“ãªè¨­å®šï¼‰ã‚’1ã¤æ±ºã‚ã‚‹ã€‚

A3ï¼štarget_monthly_return / target_monthly_vol / max_monthly_dd ã‚’ä½¿ã†ãƒ­ãƒƒãƒˆãƒ»SLOãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…ã€‚

A4ï¼šAIã‚¿ãƒ–ã‚’ã€Œæœˆæ¬¡3ï¼…ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã€ã«ä»•ä¸Šã’ã‚‹ã€‚

A5ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AI v0ï¼ˆæ™‚é–“å¸¯åˆ¥ãƒ»DDåŸå› åˆ†æï¼‰ã€‚

A6ï¼šMT5ç™ºæ³¨ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‹1ãƒ¶æœˆãƒ‡ãƒ¢é‹ç”¨ãƒ­ã‚°ã€‚

A7ï¼šæœ¬ç•ªç§»è¡Œãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆdocs/production_checklist.mdï¼‰ã€‚

ã‚°ãƒ«ãƒ¼ãƒ—Bï¼šæ—¥æœ¬èªUIãƒ»ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¶å¾¡ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°

B1ï¼šæ—¥æœ¬èªUIåŒ–ãƒ»æ–‡å­—åŒ–ã‘æ¤œæŸ»

GUIå…¨ã‚¿ãƒ–ã®æ—¥æœ¬èªåŒ–ã€‚

ãƒ­ã‚°å‡ºåŠ›ãƒ»ã‚¨ãƒ©ãƒ¼æ–‡è¨€ã®è¦‹ç›´ã—ã€‚

æ–‡å­—ã‚³ãƒ¼ãƒ‰UTF-8çµ±ä¸€ã¨å®Ÿéš›ã®ãƒ­ã‚°ãƒ»ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ç¢ºèªã€‚

B2ï¼šã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã¨æ©Ÿèƒ½ãƒ­ãƒƒã‚¯

Edition ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆä¾‹ï¼šEditionGuardï¼‰ã‚’ä½œæˆã€‚

edition=free/basic/pro/expert/master ã«å¿œã˜ã¦ï¼š

æ¥ç¶šå¯èƒ½å£åº§ï¼ˆãƒ‡ãƒ¢ã®ã¿ã‹ã©ã†ã‹ï¼‰

ãƒ­ãƒƒãƒˆä¸Šé™

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®šã‚¿ãƒ–ã®æ©Ÿèƒ½ON/OFF

KPI/AIã‚¿ãƒ–ã®é …ç›®è¡¨ç¤º
ã‚’åˆ¶å¾¡ã€‚

B3ï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ v0ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒƒã‚¯ï¼‰

RankingService ã‚’å®Ÿè£…ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«JSONã‚’ã€Œæ“¬ä¼¼ã‚µãƒ¼ãƒã€ã¨ã—ã¦èª­ã¿æ›¸ãã€‚

ã€Œãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ã‚¿ãƒ–ã‚’è¿½åŠ ã—ã¦ã€ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸€è¦§ãƒ»è‡ªåˆ†ã®æˆç¸¾è¡¨ç¤ºã€‚

Freeé–²è¦§ã®ã¿ã€Basicä»¥ä¸Šã§é€ä¿¡ãƒœã‚¿ãƒ³ONãŒå‹•ãã“ã¨ã‚’ç¢ºèªã€‚

ã‚°ãƒ«ãƒ¼ãƒ—Cï¼šå†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¨é€±æ¬¡å†å­¦ç¿’

C1ï¼šJobScheduler åŸºç¤ã‚¯ãƒ©ã‚¹å®Ÿè£…

ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§å˜ç´”ãªã€Œæ™‚åˆ»ãƒã‚§ãƒƒã‚¯â†’ã‚¸ãƒ§ãƒ–å®Ÿè¡Œã€ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ã‚’ services ã«å®Ÿè£…ã€‚

ã‚¸ãƒ§ãƒ–å®šç¾©ã¯ä¸€æ—¦ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆé€±1å›ã®å†å­¦ç¿’ï¼‰ã§å‹•ãã“ã¨ã‚’ç¢ºèªã€‚

C2ï¼šè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é€£æºï¼ˆconfig.scheduler.yamlï¼‰

config.scheduler.yamlï¼ˆã¾ãŸã¯ config.yaml ã®ä¸€éƒ¨ï¼‰ã«ã‚¸ãƒ§ãƒ–å®šç¾©ã‚’è¨˜è¿°ã€‚

Editionã”ã¨ã® enabled / ç·¨é›†å¯å¦ã‚’åæ˜ ã€‚

C3ï¼šGUIã‹ã‚‰ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç·¨é›†UI

è¨­å®šã‚¿ãƒ–ã«ã€Œé€±æ¬¡å†å­¦ç¿’ï¼ˆå†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼‰ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã€‚

Basicï¼š

ã‚¹ã‚¤ãƒƒãƒï¼ˆON/OFFï¼‰ã®ã¿ã€‚

Proä»¥ä¸Šï¼š

æ›œæ—¥ãƒ»æ™‚åˆ»ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†ã€‚

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ç¾åœ¨çŠ¶æ…‹ï¼ˆæ¬¡å›äºˆå®šæ™‚åˆ»ï¼‰ã‚’è¡¨ç¤ºã€‚

C4ï¼šé€±æ¬¡å†å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç¢ºèª

å†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ONã«ã—ã¦ã€ãƒ†ã‚¹ãƒˆç”¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ•°åˆ†å¾Œï¼‰ã§ scripts.walkforward_retrain.py ãŒèµ·å‹•ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚

æœ¬ç•ªç”¨ã¨ã—ã¦åœŸæ›œæ·±å¤œå®Ÿè¡Œãªã©ã«åˆ‡ã‚Šæ›¿ãˆã€ãƒ­ã‚°ãŒæœŸå¾…ã©ãŠã‚Šã«æ®‹ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚

C5ï¼šæ—§Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ•´ç†

weekly_wf.ps1 ãªã©ã®ã€ŒWindowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©å‰æã‚¹ã‚¯ãƒªãƒ—ãƒˆã€ã¯ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–åŒ–ï¼š

scripts/legacy/ ç­‰ã«ç§»å‹•ã—ã€ã€Œé–‹ç™ºæ™‚ã®å‚è€ƒç”¨ã€ã¨æ˜è¨˜ã€‚

ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé¡ã‹ã‚‰ã‚‚ Task Scheduler ä¾å­˜ã®è¨˜è¿°ã‚’å‰Šé™¤ã¾ãŸã¯ã€Œéæ¨å¥¨ã€ã¨æ˜è¨˜ã€‚


=== file: To Doãƒªã‚¹ãƒˆv5.txt ===

ãƒŸãƒãƒ“ã‚­ ToDoãƒªã‚¹ãƒˆ v4ï¼ˆæ­£å¼ç‰ˆï¼‰

ï¼ˆä»•æ§˜æ›¸ v4 ã«å®Œå…¨æº–æ‹ ï¼‰


ãƒŸãƒãƒ“ã‚­ v4 ã® ToDo ã¯ A/B/C/D ã®4ãƒ–ãƒ­ãƒƒã‚¯ ã«æ•´ç†ã•ã‚Œã¾ã™ã€‚
é †ç•ªã¯ â€œå®‰å…¨é‹ç”¨ â†’ UI â†’ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© â†’ ãƒ©ãƒ³ã‚­ãƒ³ã‚°â€ ã®æµã‚Œã«æœ€é©åŒ–ã€‚

Aï¼š3ï¼…KPIã«å¿…è¦ãªAIãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆåŸºç›¤ï¼ˆã‚³ã‚¢ï¼‰

ãƒŸãƒãƒ“ã‚­ã® â€œæ€§èƒ½ãã®ã‚‚ã®â€ ã‚’è¦å®šã™ã‚‹åŸºç¤ãƒ‘ãƒ¼ãƒˆã€‚
æ—¢å­˜ã® A1ã€œA7 ã‚’ v4 å‘ã‘ã«èª¿æ•´ã—ã¦å†æ§‹æˆã—ãŸã‚‚ã®ã€‚

A1ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã«å¿…ãš monthly_returns.csv ã‚’å‡ºåŠ›

å…¨ã¦ã®ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼/å…¨æœŸé–“ã§æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼

Expert ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ã«ãªã‚‹

A2ï¼šåŸºæº–ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ï¼ˆWFO + ATR ä»•æ§˜ï¼‰ã‚’1ã¤ç¢ºå®š

ã€ŒãƒŸãƒãƒ“ã‚­æ¨™æº–è¨­å®šã€ï¼Pro/Expertã®ãƒ‡ãƒ¢å‹•ä½œã§åˆ©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«

AIã‚¿ãƒ–ã®KPIè¨ˆç®—ã®æ¨™æº–åŒ–ã«ã‚‚ä½¿ã†

A3ï¼šæœˆæ¬¡ç›®æ¨™ï¼ˆ3ï¼…ï¼‰ã‚’åˆ©ç”¨ã™ã‚‹ãƒ­ãƒƒãƒˆï¼†ãƒªã‚¹ã‚¯ç®¡ç†ãƒ­ã‚¸ãƒƒã‚¯

target_monthly_return

max_monthly_dd

monthly_vol_target

ã“ã‚Œã‚‰ 3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ­ãƒƒãƒˆå…¬å¼ã‚’è‡ªå‹•èª¿æ•´

A4ï¼šAIã‚¿ãƒ–ã® â€œæœˆæ¬¡3ï¼…ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰â€ ã‚’å®Œæˆ

KPIï¼šä»Šæœˆæç›Šï¼… / PF / DD / é€£å‹/é€£æ•—

ãƒ¢ãƒ‡ãƒ«æƒ…å ±ï¼šAUC / Logloss / ãƒãƒ¼ã‚¸ãƒ§ãƒ³

æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³æŠ˜ã‚Œç·šï¼ˆExpertã§æ‹¡å¼µï¼‰

Proï¼šFI20 / SHAP3

Expertï¼šFIå…¨ä»¶ / SHAP20

A5ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼ˆæ™‚é–“å¸¯ Ã— ç›¸å ´ã‚¿ã‚¤ãƒ—ï¼‰v0

Proï¼šç°¡æ˜“è¨ºæ–­ï¼ˆæ™‚é–“å¸¯/ç›¸å ´ï¼‰

Expertï¼šé€£æ•—åŒºé–“ãƒ»DDç›´å‰ç‰¹å¾´ãƒ»ç•°å¸¸ç‚¹æŠ½å‡ºãƒ»å‹ç‡é«˜ã„ç›¸å ´ç‰¹å¾´ãªã©

A6ï¼šMT5ç™ºæ³¨ãƒ©ãƒƒãƒ‘ãƒ¼ã®å®Œæˆ ï¼‹ 1ãƒ¶æœˆãƒ‡ãƒ¢é‹ç”¨ãƒ­ã‚°

MT5Clientã®å®‰å®šåŒ–

ç™ºæ³¨ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒ¢ï¼‰ã‚’1ãƒ¶æœˆé‹ç”¨ã—ã¦æŒ™å‹•ç¢ºèª

A7ï¼šæœ¬ç•ªé‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆdocs/production_checklist.mdï¼‰

æœ¬å£åº§ç§»è¡Œå‰ã®ç¢ºèªç‚¹ã‚’ã¾ã¨ã‚ã‚‹

Bï¼šæ—¥æœ¬èªUIåŒ– & ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¶å¾¡ï¼ˆv4å¯¾å¿œï¼‰

ä»•æ§˜æ›¸ v4 ã®ä¸­æ ¸ã§ã‚ã‚‹ â€œFree/Basic/Pro/Expert/Masterâ€ ã‚’æ©Ÿèƒ½çš„ã«å®Ÿè£…ã™ã‚‹é ˜åŸŸã€‚

B1ï¼šæ—¥æœ¬èªUIãƒ•ãƒ«å¯¾å¿œ

å…¨ã‚¿ãƒ–ã®æ—¥æœ¬èªåŒ–ï¼ˆè¨­å®š/AI/ãƒ©ãƒ³ã‚­ãƒ³ã‚°/ãƒ­ã‚°/ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼‰

æ–‡å­—åŒ–ã‘ä¿®æ­£ï¼ˆUTF-8 BOMãªã—ï¼‰

ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚æ—¥æœ¬èªä¸­å¿ƒã«çµ±ä¸€

B2ï¼šEditionGuardï¼ˆã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¶å¾¡ã‚¯ãƒ©ã‚¹ï¼‰

å„ edition ãŒä»¥ä¸‹ã‚’åˆ¶å¾¡ï¼š

æ¥ç¶šå¯èƒ½å£åº§ï¼ˆãƒ‡ãƒ¢ã®ã¿ / æœ¬å£åº§OKï¼‰

ãƒ­ãƒƒãƒˆä¸Šé™

AIã‚¿ãƒ–è¡¨ç¤ºï¼ˆKPIã®ã¿ / FI / SHAPï¼‰

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ç·¨é›†å¯å¦

ãƒ©ãƒ³ã‚­ãƒ³ã‚°é€ä¿¡å¯å¦

ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šï¼ˆç°¡æ˜“ / è¤‡åˆï¼‰

B3ï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ v0ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒƒã‚¯ã‚µãƒ¼ãƒï¼‰

RankingService ã®å®Ÿè£…

local JSON ã§ mock ã‚µãƒ¼ãƒã¨ã—ã¦å‹•ä½œ

ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«

Freeï¼šé–²è¦§ã®ã¿

Basicï¼šé€ä¿¡å¯

Proï¼šProå°‚ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°

Expertï¼šè¤‡åˆã‚¹ã‚³ã‚¢ï¼ˆReturnÃ—DDÃ—PFï¼‰è¡¨ç¤º

B4ï¼šãƒŸãƒãƒ“ã‚­ç”¨ UI ã®æœ€çµ‚èª¿æ•´

ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ãƒ–

AIã‚¿ãƒ–ã®ã‚¿ãƒ–åŒ–ï¼ˆKPI / FI / SHAP / è¨ºæ–­ï¼‰

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®šç”»é¢

Cï¼šå†…è”µ JobSchedulerï¼ˆã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ä¸è¦åŒ–ï¼‰

ãƒŸãƒãƒ“ã‚­ v4 æœ€å¤§ã®å¤‰æ›´ç‚¹ã€‚
Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ä½¿ã‚ãšãƒŸãƒãƒ“ã‚­å†…ã§å®Œçµã€‚

C1ï¼šJobSchedulerï¼ˆå¸¸é§ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰åŸºç¤ã‚¯ãƒ©ã‚¹

30ã€œ60ç§’é–“éš”ã§æ™‚åˆ»ãƒã‚§ãƒƒã‚¯

ã‚¸ãƒ§ãƒ–ã® last_run_at ã‚’æŒã¡äºŒé‡å®Ÿè¡Œé˜²æ­¢

å®Ÿè¡Œå±¥æ­´ã‚’ logs/scheduler.log ã«è¨˜éŒ²

C2ï¼šconfig.scheduler.yaml ã‚’æ­£å¼æ¡ç”¨

edition_min

weekday/hour/minute

command

Basic ã§ã¯ ON/OFF ã ã‘ç·¨é›†å¯

C3ï¼šGUIã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ç·¨é›†UI

Freeï¼šè¡¨ç¤ºã®ã¿
Basicï¼šON/OFFã®ã¿
Proï¼š1ã¤ã‚¸ãƒ§ãƒ–ç·¨é›†å¯èƒ½
Expertï¼šè¤‡æ•°ã‚¸ãƒ§ãƒ–ã€é€£é–ï¼ˆå†å­¦ç¿’â†’è¨ºæ–­â†’æ•´ç†ï¼‰

C4ï¼šé€±æ¬¡å†å­¦ç¿’ã®å®Ÿåƒãƒ†ã‚¹ãƒˆ

2åˆ†å¾Œãªã©ã®ãƒ†ã‚¹ãƒˆæ™‚åˆ»ã‚’è¨­å®šã—ã€
â†’ python -m scripts.walkforward_retrain ãŒå®Ÿéš›ã«èµ·å‹•ã™ã‚‹ã“ã¨ã‚’ç¢ºèª

C5ï¼šæ—§Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å»ƒæ­¢

scripts/legacy ã«ç§»å‹•ã— â€œéæ¨å¥¨â€ ã¨æ˜è¨˜

Dï¼šã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ»ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š

ãƒŸãƒãƒ“ã‚­ v4 è¿½åŠ åˆ†ã€‚é‹ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ·±ã„è¨­å®šã€‚

D1ï¼šProãƒ•ã‚£ãƒ«ã‚¿

æ™‚é–“å¸¯ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä½¿ãˆã‚‹ï¼‰

ATRã‚ªãƒ³/ã‚ªãƒ•

ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿ UI

D2ï¼šExpertãƒ•ã‚£ãƒ«ã‚¿

è¤‡åˆãƒ•ã‚£ãƒ«ã‚¿ï¼ˆATRÃ—ãƒœãƒ©Ã—æ™‚é–“å¸¯Ã—ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰

é€£æ•—å›é¿ãƒ¢ãƒ¼ãƒ‰

è‡ªå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ‡æ›¿

æˆ¦ç•¥ã®ãƒœãƒ©å¸¯æœ€é©åŒ–

D3ï¼šãƒ•ã‚£ãƒ«ã‚¿ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆåæ˜ 

backtests/{profile}/analyze.json ã«å„ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ä¿å­˜

Eï¼šè²©å£²æº–å‚™ï¼ˆUIãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¡¨ç¤ºï¼‰

ã“ã‚Œã¯ãƒŸãƒãƒ“ã‚­ã¨ã—ã¦å¿…è¦ã«ãªã‚‹ â€œè£½å“åŒ–â€ é ˜åŸŸã€‚

E1ï¼šãƒ©ã‚¤ã‚»ãƒ³ã‚¹èªè¨¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰

edition ã®åˆ‡æ›¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«/ã‚­ãƒ¼/APIã§è¡Œã†

E2ï¼šã‚¢ãƒ—ãƒªã® About ç”»é¢

ãƒŸãƒãƒ“ã‚­ã®ãƒ­ã‚´

ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·

edition è¡¨ç¤º

æ›´æ–°ãƒ­ã‚°ãƒªãƒ³ã‚¯

E3ï¼šã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©æº–å‚™ï¼ˆæœ€çµ‚æ®µéšï¼‰

Windows ç”¨ã®é…å¸ƒå½¢å¼ï¼ˆPyInstaller ã¾ãŸã¯ Briefcaseï¼‰

ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã®å‡çµ

ğŸ”¥ å…¨ä½“ã®é€²è¡Œé †ï¼ˆæ¨å¥¨ï¼‰

Aï¼šæœˆæ¬¡3ï¼…ã®åŸºç¤ï¼ˆAI/ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ/MT5ï¼‰

Bï¼šæ—¥æœ¬èªUI ï¼‹ ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¶å¾¡

Cï¼šå†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©

Dï¼šExpertå‘ã‘é«˜åº¦è¨­å®š

Eï¼šè²©å£²æº–å‚™ï¼ˆãƒ©ã‚¤ã‚»ãƒ³ã‚¹/ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ï¼‰


=== file: ToDoãƒªã‚¹ãƒˆ Masterç‰ˆ.txt ===

ToDoãƒªã‚¹ãƒˆ Masterç‰ˆï¼ˆå…¨éƒ¨å…¥ã‚Šï¼‹ä¸‹ä½ç‰ˆã‚’ä½œã‚Šã‚„ã™ã„æ§‹é€ ï¼‰

Masterç‰ˆã¯ã€ŒExpert ã®å…¨éƒ¨ï¼‹é–‹ç™ºè€…ç”¨ãƒ„ãƒ¼ãƒ«ï¼‹åˆ¶é™ãªã—ã€ã§ã™ã€‚
ã“ã“ã‚’åŸºæº–ã«ã—ã¦ã€å¾Œã‹ã‚‰ EditionGuard ã§ãƒ­ãƒƒã‚¯ã‚’ã‹ã‘ã¦ Freeã€œExpert ã«è½ã¨ã—ã¦ã„ãã¾ã™ã€‚

M-Aï¼š3ï¼…KPIã¨ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆåŸºç›¤ï¼ˆMasterã§ã‚‚å…±é€šã®â€œå¿ƒè‡“éƒ¨â€ï¼‰

M-A1ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ monthly_returns.csv ã®æ¨™æº–åŒ–

ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœãŒ
backtests/{profile}/monthly_returns.csv ã‚’å¿…ãšå‡ºã™ã‚ˆã†ã«çµ±ä¸€ã€‚

CSVã®ã‚«ãƒ©ãƒ ä»•æ§˜ã‚’å›ºå®šï¼ˆå¹´æœˆã€ãƒªã‚¿ãƒ¼ãƒ³ï¼…ã€DDï¼…ãªã©ï¼‰ã€‚

å°†æ¥ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»AIã‚¿ãƒ–ãƒ»è¨ºæ–­AIãŒã“ã®å½¢å¼ã‚’å‰æã«å‹•ãã‚ˆã†ã«ã™ã‚‹ã€‚
å®Œäº†

M-A2ï¼šåŸºæº–ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ï¼ˆãƒŸãƒãƒ“ã‚­æ¨™æº–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã®ç¢ºå®š

WFOï¼‹ATRãƒ™ãƒ¼ã‚¹ã§ã€Œã¨ã‚Šã‚ãˆãš3ï¼…ã‚’ç‹™ãˆã‚‹ã€è¨­å®šã‚’1ã¤æ±ºã‚ã‚‹ã€‚

ãã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Master/Expert ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ã™ã‚‹ã€‚
å®Œäº†

M-A3ï¼štarget_monthly_return / max_monthly_dd ã‚’ä½¿ã†ãƒ­ãƒƒãƒˆè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯

config.yaml ç­‰ã«

target_monthly_returnï¼ˆä¾‹: 0.03ï¼‰

max_monthly_ddï¼ˆä¾‹: -0.20ï¼‰

ã‚’æŒãŸã›ã€ã“ã‚Œã‚’å…ƒã« lot ã‚µã‚¤ã‚ºã‚„ SLOï¼ˆæåˆ‡ã‚Šãƒ¬ãƒ™ãƒ«ï¼‰ã®ç›®å®‰ã‚’å‡ºã™ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å®Ÿè£…ã€‚

å¾Œã®ä¸‹ä½ç‰ˆã§ã¯ã“ã®ãƒ­ãƒƒãƒˆè¨ˆç®—ã®ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç·¨é›†ã€ã‚’åˆ¶é™ã™ã‚‹ã ã‘ã«ã™ã‚‹æƒ³å®šã€‚

M-A4ï¼šAIã‚¿ãƒ– â€œæœˆæ¬¡3ï¼…ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰â€ ã®å®Œæˆï¼»ä¸€éƒ¨å®Œäº†ï¼½

ã™ã§ã«ï¼š

KPIè¡¨ç¤ºã€FIã€SHAPã‚¿ãƒ–ã€SHAP Top3 ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã¯å®Ÿè£…æ¸ˆã¿ã€‚

Masterã§è¿½åŠ ã™ã‚‹ã“ã¨ï¼š

æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã®ã‚°ãƒ©ãƒ•ï¼ˆæŠ˜ã‚Œç·šã¾ãŸã¯æ£’ï¼‰

ä»Šæœˆã®ãƒªã‚¿ãƒ¼ãƒ³ vs ç›®æ¨™3ï¼…ã®è¡¨ç¤º

ãƒ¢ãƒ‡ãƒ«æŒ‡æ¨™ï¼ˆLogloss/AUC/ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰è¡¨ç¤º

Expert/Masterç”¨ã«ã€ŒFIå…¨ä»¶ã€ã€ŒSHAP Top20ã€ã‚’è¡¨ç¤ºå¯èƒ½ã«ã™ã‚‹ã“ã¨
ï¼ˆPro ã®åˆ¶é™ã¯å¾Œã§ EditionGuard å´ã§ã‹ã‘ã‚‹å‰æï¼‰

M-A5ï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼ˆMaster/Expertç‰ˆãƒ•ãƒ«ï¼‰

v0ï¼ˆæ™‚é–“å¸¯ Ã— ç›¸å ´ã‚¿ã‚¤ãƒ—ï¼‰ã®è¨ºæ–­ã¯ ToDo v4 A5 ãƒ™ãƒ¼ã‚¹ã€‚

Master ã§ã¯ã•ã‚‰ã«ï¼š

DDç›´å‰åŒºé–“ã®ç‰¹å¾´æŠ½å‡º

é€£å‹åŒºé–“ã®ç‰¹å¾´

å‹ç‡ãŒé«˜ã„ç›¸å ´æ¡ä»¶ã®æŠ½å‡º

æç›Šæ›²ç·šã®ç•°å¸¸ç‚¹æ¤œå‡º

ç°¡æ˜“ã®ã€Œæ¥é€±ã®æ¡ä»¶ã‚·ãƒŠãƒªã‚ªã€å‡ºåŠ›

å®Ÿè£…å…ˆï¼šapp.services.diagnosis_service çš„ãªã‚µãƒ¼ãƒ“ã‚¹å±¤ã‚’æƒ³å®š

M-A6ï¼šMT5ç™ºæ³¨ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‹è‡ªå·±è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼»APIã¯ã»ã¼å®Œäº†ãƒ»é‹ç”¨ãƒ•ã‚§ãƒ¼ã‚ºã¸ï¼½

MT5Client: initialize / login / order_send / close_position / get_positions ç³»ã¯æ—¢ã«å®Ÿè£…ãƒ»è‡ªå·±ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã€‚

Masterå´ã§ã¯ï¼š

è‡ªå·±è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆselftestï¼‰ã®ãƒ­ã‚°å½¢å¼ã‚’æœ¬ç•ªæƒ³å®šã«æƒãˆã‚‹

æ±ºå®šãƒ­ã‚°ï¼ˆlogs/decisions_*.jsonlï¼‰ã¨ã®é€£æºï¼ˆã„ã¤ãƒ»ã©ã®æˆ¦ç•¥ã§ãƒ»ã©ã‚Œãã‚‰ã„ã®æœŸå¾…å€¤ã§ç™ºæ³¨ã—ãŸã‹ï¼‰

M-A7ï¼šæœ¬ç•ªç§»è¡Œãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆdocs/production_checklist.mdï¼‰

Masteré–‹ç™ºè€…ç”¨ã«ã€

å›ç·šãƒ»VPS

MT5ã®è¨­å®šï¼ˆè‡ªå‹•å£²è²·è¨±å¯ã€è¨¼æ‹ é‡‘ãƒã‚§ãƒƒã‚¯ï¼‰

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®æŒ™å‹•ç¢ºèª

ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª

ãªã©ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã‚’æ–‡æ›¸åŒ–ã€‚

M-Bï¼šãƒ•ã‚£ãƒ«ã‚¿ï¼†ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼åˆ¶å¾¡ï¼ˆExpert/Masterã®ã€Œå‹ç‡ç›´çµéƒ¨åˆ†ã€ï¼‰

M-B1ï¼šProç”¨ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿ã¨Expertç”¨è¤‡åˆãƒ•ã‚£ãƒ«ã‚¿ã®å…±é€šåŸºç›¤ã‚¯ãƒ©ã‚¹

1ã¤ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆä¾‹ï¼šStrategyFilterEngineï¼‰ã§

æ™‚é–“å¸¯

ATR

ãƒœãƒ©ï¼ˆÏƒï¼‰

ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦

ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«è¨­è¨ˆã€‚

Pro/Expert/Freeç­‰ã®å·®ã¯ã€Œã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šå¯èƒ½ã«ã™ã‚‹ã‹ã€ã§å¾Œã‹ã‚‰åˆ¶é™ã€‚

M-B2ï¼šé€£æ•—å›é¿ãƒ¢ãƒ¼ãƒ‰

æœ€è¿‘ã®Nãƒˆãƒ¬ãƒ¼ãƒ‰ã‹ã‚‰é€£æ•—æ•°ãƒ»æå¤±é¡ã‚’è¦‹ã¦ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ä¸€æ™‚åœæ­¢ã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦

æœ€å¤§é€£æ•—æ•°

å†é–‹ã™ã‚‹ãŸã‚ã®æ¡ä»¶ï¼ˆæ™‚é–“ãƒ»å‹ã¡ãƒˆãƒ¬ãƒ¼ãƒ‰æ•°ï¼‰

ã‚’æŒãŸã›ã‚‹ã€‚

ä¸‹ä½ç‰ˆã§ã¯ã€Œè¨­å®šUIã‚’éš ã™ãƒ»å›ºå®šå€¤ã«ã™ã‚‹ã€æ–¹å‘ã§åˆ¶é™å¯èƒ½ãªã‚ˆã†ã«ã™ã‚‹ã€‚

M-B3ï¼šå‹ç‡é«˜ã„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•åˆ‡æ›¿

monthly_returns / ç›´è¿‘Xãƒˆãƒ¬ãƒ¼ãƒ‰ãªã©ã‹ã‚‰ã€Œä»Šä¸€ç•ªèª¿å­ã®è‰¯ã„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€ã‚’é¸ã³ã€

Master/Expertã§ã¯è‡ªå‹•åˆ‡æ›¿ON/OFFå¯èƒ½

ä¸‹ä½ç‰ˆã§ã¯æ‰‹å‹•ã®ã¿ã€ãªã©ã§åˆ¶å¾¡ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãŠãã€‚

M-B4ï¼šãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å¸¯ã”ã¨ã®æˆ¦ç•¥æœ€é©åŒ–

ãƒœãƒ©æŒ‡æ¨™ï¼ˆATRã‚„æ¨™æº–åå·®ï¼‰ã§

ä½ãƒœãƒ©

ä¸­ãƒœãƒ©

é«˜ãƒœãƒ©

ã®3ãƒ¬ã‚¸ãƒ¼ãƒ ã«åˆ†ã‘ã€ãã‚Œãã‚Œã§ä½¿ã†ãƒ•ã‚£ãƒ«ã‚¿ or ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ‡æ›¿ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

M-Cï¼šJobSchedulerï¼ˆMasterç‰ˆãƒ•ãƒ«æ©Ÿèƒ½ï¼‰

ä»•æ§˜æ›¸ v4 ã® JobScheduler è¦æ±‚ã‚’ ãƒ•ãƒ«ã§å®Ÿè£… ã—ã¦ãŠãã€
ä¸‹ä½ç‰ˆã¯ã€Œã‚¸ãƒ§ãƒ–æ•°ãƒ»ç¨®é¡ãƒ»ç·¨é›†å¯å¦ã€ã§çµã‚‹æ–¹é‡ã€‚

M-C1ï¼šJobScheduler åŸºç¤ã‚¯ãƒ©ã‚¹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰

æ—¢å­˜ ToDo v4 C1 ã¨åŒã˜ã ãŒã€æœ€åˆã‹ã‚‰è¤‡æ•°ã‚¸ãƒ§ãƒ–ãƒ»é€£é–ã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆã«ã™ã‚‹ã€‚

jobs ã¯ config.scheduler.yaml + ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç·¨é›†ã‚’æƒ³å®šã€‚

last_run_at ã‚’å†…éƒ¨ç®¡ç†ã—ã€é‡è¤‡èµ·å‹•é˜²æ­¢ã€‚

M-C2ï¼šã‚¸ãƒ§ãƒ–å‹ã®å®šç¾©ï¼ˆMasterç‰ˆï¼‰

å†å­¦ç¿’ï¼‹Walk-Forward

ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

ãƒ­ã‚°åœ§ç¸®ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

å°†æ¥æ‹¡å¼µç”¨ã«ã€Œä»»æ„ã‚³ãƒãƒ³ãƒ‰ã€ã‚‚è¨±å¯ã€‚

ä¸‹ä½ç‰ˆã§ã¯ã€Œä½¿ãˆã‚‹ job_type ã‚’åˆ¶é™ã™ã‚‹ã€ã ã‘ã§æ¸ˆã‚€ã‚ˆã†ã«ã™ã‚‹ã€‚

M-C3ï¼šconfig.scheduler.yaml ã®æ¡ç”¨

edition_min ãªã©ã‚‚å«ã‚ãŸå½¢å¼ã§è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºå®šã€‚

Masterã§ã¯GUIã‹ã‚‰ã™ã¹ã¦ç·¨é›†å¯ã€‚

ä¸‹ä½ç‰ˆã§ã¯ EditionGuard ãŒç·¨é›†å¯å¦ã‚’æ±ºã‚ã‚‹å½¢ã«ã™ã‚‹ã€‚

M-C4ï¼šGUI ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®šã‚¿ãƒ–ï¼ˆMasterãƒ¢ãƒ¼ãƒ‰ï¼‰

å…¨ã‚¸ãƒ§ãƒ–ã®ä¸€è¦§ã€æ¬¡å›å®Ÿè¡Œäºˆå®šæ™‚åˆ»ã€å‰å›å®Ÿè¡Œçµæœã€‚

ã‚¸ãƒ§ãƒ–è¿½åŠ ãƒ»ç·¨é›†ãƒ»å‰Šé™¤ãƒ»é€£é–è¨­å®šã€‚

å®Ÿè¡Œå±¥æ­´ãƒ“ãƒ¥ãƒ¼ï¼ˆlogs/scheduler.log ã®ç°¡æ˜“ãƒ“ãƒ¥ãƒ¼ï¼‰ã€‚

M-C5ï¼šæ—§Windows ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ç³»ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ•´ç†ï¼»ã‚ã¨ã§ã‚„ã‚‹ãƒãƒ¼ã‚¯ã§ã‚‚OKï¼½

scripts/legacy/ ã«ç§»å‹•ã—ã¦ã€Œé–‹ç™ºæ™‚ã®å‚è€ƒç”¨ã€ã¨æ˜è¨˜ã€‚

M-Dï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ï¼‹Masterå°‚ç”¨è¨­å®š

M-D1ï¼šRankingServiceï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ï¼‰ã®æœ¬å®Ÿè£…

ToDo v4 B3 ã®ã€Œãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒƒã‚¯ã€ã‚’ç™ºå±•ã•ã›ã€

ãƒ­ãƒ¼ã‚«ãƒ«JSONã‚’ã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€

HTTP APIï¼ˆã‚µãƒ¼ãƒå®Ÿè£…å‰æï¼‰ã®ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚‚ç”¨æ„ã€‚

M-D2ï¼šMasterå°‚ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®šUI

ã‚µãƒ¼ãƒURL

APIã‚­ãƒ¼

ãƒ†ã‚¹ãƒˆé€ä¿¡ãƒœã‚¿ãƒ³

ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å†å–å¾—ï¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ãªã©

M-D3ï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ãƒ–ï¼ˆMasterè¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼‰

ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥ã®ã‚¿ãƒ– or ã‚³ãƒ³ãƒœï¼ˆBasic/Pro/Expertï¼‰ã€‚

è‡ªåˆ†ã®æˆç¸¾ï¼‹éå»å±¥æ­´ï¼‹ä¸Šä½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‚

ãƒã‚¹ã‚¿ãƒ¼ã§ã¯ã€Œãƒ¢ãƒƒã‚¯ã¨æœ¬ç•ªã‚µãƒ¼ãƒã®åˆ‡æ›¿ãƒœã‚¿ãƒ³ã€ã‚‚ç”¨æ„ã€‚

M-Eï¼šæ—¥æœ¬èªUIãƒ»ãƒ­ã‚°ãƒ»é–‹ç™ºè€…å‘ã‘ãƒ„ãƒ¼ãƒ«

M-E1ï¼šæ—¥æœ¬èªUIå®Œå…¨ãƒã‚§ãƒƒã‚¯ ï¼»ä¸€éƒ¨å®Œäº†ãƒ»è¦ç·ä»•ä¸Šã’ï¼½

å…¨ã‚¿ãƒ–ãƒ»å…¨ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«çµ±ä¸€ã€‚

ç”¨èªï¼ˆæœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã€åˆ©ç›Šç‡ã€PF ç­‰ï¼‰ã‚’ä»•æ§˜æ›¸ v4 ã«æƒãˆã‚‹ã€‚

M-E2ï¼šãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ•´ç†

æ—¥æœ¬èªï¼‹ç°¡æ˜“è‹±èªä½µè¨˜ã€UTF-8çµ±ä¸€ã€‚

Masterå‘ã‘ã«ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚„å‡ºåŠ›å…ˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ï¼‰ã‚‚è¨­å®šå¯èƒ½ã«ã€‚

M-E3ï¼šMasterå°‚ç”¨ã€Œé–‹ç™ºãƒ„ãƒ¼ãƒ«ã€ã‚¿ãƒ–ï¼ˆä»»æ„ï¼‰

ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆä¸€æ‹¬å®Ÿè¡Œãƒœã‚¿ãƒ³

ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

SHAPã‚„è¨ºæ–­AIã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

ã“ã“ã¯å®Œå…¨ã«é–‹ç™ºè€…å‘ã‘ãªã®ã§ UI ã¯ç°¡æ˜“ã§ã‚‚OKã€‚



=== file: ToDoãƒªã‚¹ãƒˆ ä¸‹ä½ç‰ˆ.txt ===

ToDoãƒªã‚¹ãƒˆ ä¸‹ä½ç‰ˆï¼ˆFree / Basic / Pro / Expert ã‚’ Master ã‹ã‚‰å‰Šã‚‹ä½œæ¥­ï¼‰

ã“ã“ã‹ã‚‰ã¯ã€ŒMaster ãŒå…¨éƒ¨å‹•ãå‰æã€ã§ã€EditionGuard ã¨æ©Ÿèƒ½åˆ¶é™ã‚’ã‹ã‘ã¦ã„ããŸã‚ã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã§ã™ã€‚

L-Aï¼šEditionGuard å®Ÿè£…

L-A1ï¼šEditionGuard ã‚¯ãƒ©ã‚¹

edition: strï¼ˆ"free/basic/pro/expert/master"ï¼‰ã‚’å—ã‘å–ã‚Šã€

å£åº§ç¨®åˆ¥ï¼ˆdemo only? real ok?ï¼‰

ãƒ­ãƒƒãƒˆä¸Šé™

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æœ€å¤§ã‚¸ãƒ§ãƒ–æ•°

è¨ºæ–­AIãƒ¬ãƒ™ãƒ«ï¼ˆãªã— / v0 / fullï¼‰

ãƒ©ãƒ³ã‚­ãƒ³ã‚°é€ä¿¡å¯å¦

ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šãƒ¬ãƒ™ãƒ«ï¼ˆãªã— / ç°¡æ˜“ / è¤‡åˆï¼‰

ãªã©ã‚’è¿”ã™ã€‚

L-A2ï¼šè¨­å®šãƒ•ã‚¡ã‚¤ãƒ« or ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±ã¨ã®é€£æº

config.edition.yaml ã‚„ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯APIã‚’æƒ³å®šã€‚

ã¨ã‚Šã‚ãˆãšã¯ãƒ­ãƒ¼ã‚«ãƒ«è¨­å®šã§ edition ã‚’åˆ‡æ›¿ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã€å¾Œã‹ã‚‰ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚µãƒ¼ãƒã«ç¹‹ã’ã‚‹å‰æã€‚

L-Bï¼šã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥æ©Ÿèƒ½ãƒ­ãƒƒã‚¯ï¼ˆã‚µãƒ¼ãƒ“ã‚¹å±¤ï¼‰

L-B1ï¼šMT5æ¥ç¶šåˆ¶é™

Freeï¼šãƒ‡ãƒ¢å£åº§ã®ã¿

Basicä»¥ä¸Šï¼šæœ¬å£åº§ã‚‚è¨±å¯

å®Ÿè£…ï¼šMT5ClientåˆæœŸåŒ–å‰ã« EditionGuard ã«å•ã„åˆã‚ã›ã‚‹ã€‚

L-B2ï¼šAIã‚¿ãƒ–ã®åˆ¶é™

Freeï¼šKPIã®ã¿ã€FI/SHAP/è¨ºæ–­ã¯éè¡¨ç¤ºã€‚

Basicï¼šKPIï¼‹æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ç°¡æ˜“è¡¨ç¤ºã®ã¿ã€‚

Proï¼šFI Top20ã€SHAP Top3ã€è¨ºæ–­AI v0ã€‚

Expertï¼šFIå…¨ä»¶ã€SHAP Top20ã€è¨ºæ–­AIãƒ•ãƒ«ã€‚

L-B3ï¼šãƒ•ã‚£ãƒ«ã‚¿æ©Ÿèƒ½ã®åˆ¶é™

Free/Basicï¼šãƒ•ã‚£ãƒ«ã‚¿UIéè¡¨ç¤º or æœ€ä½é™ã€‚

Proï¼šæ™‚é–“å¸¯ï¼‹ATRã‚ªãƒ³ã‚ªãƒ•ã®ã¿ã€‚

Expertï¼šM-Bç³»ã®å…¨éƒ¨ã€‚

L-Cï¼šã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥åˆ¶é™

L-C1ï¼šJobScheduler Ã— EditionGuard é€£æº

Freeï¼šè¡¨ç¤ºã®ã¿ï¼å†…éƒ¨çš„ã«ã¯ scheduler.enabled=False å›ºå®šã€‚

Basicï¼šé€±1å†å­¦ç¿’ã®ã¿ã€ON/OFFã®ã¿ã€‚

Proï¼š1ã‚¸ãƒ§ãƒ–ã®ã¿ã€job_type ã¯ã€Œå†å­¦ç¿’ã€ã‹ã€Œè¨ºæ–­ã€ã®ã©ã¡ã‚‰ã‹ã€‚

Expertï¼šMasterã¨åŒç­‰ã®è¤‡æ•°ã‚¸ãƒ§ãƒ–ãƒ»é€£é–ã€‚

L-C2ï¼šã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©UIåˆ¶å¾¡

ãƒœã‚¿ãƒ³ disable / hidden ã®åˆ‡ã‚Šæ›¿ãˆã€‚

ç·¨é›†ã§ããªã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ãƒ­ãƒƒã‚¯ã‚¢ã‚¤ã‚³ãƒ³ï¼‹ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ã§èª¬æ˜ã€‚

L-Dï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ã®ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥åˆ¶é™

L-D1ï¼šé€ä¿¡å¯å¦ï¼‹ã‚¹ã‚³ã‚¢å†…å®¹ã®åˆ¶é™

Freeï¼šé–²è¦§ã®ã¿ï¼ˆé€ä¿¡ãƒœã‚¿ãƒ³éè¡¨ç¤ºï¼‰ã€‚

Basicï¼šBasicãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¸é€ä¿¡å¯ã€‚è¡¨ç¤ºå†…å®¹ã¯ãƒªã‚¿ãƒ¼ãƒ³ä¸­å¿ƒã€‚

Proï¼šProå°‚ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€è¡¨ç¤ºã¯ãƒªã‚¿ãƒ¼ãƒ³ï¼…ã®ã¿ã€‚

Expertï¼šè¤‡åˆã‚¹ã‚³ã‚¢ï¼ˆãƒªã‚¿ãƒ¼ãƒ³Ã—DDÃ—PFï¼‰ã€è‡ªå‹•é€ä¿¡ ON/OFFã€‚

L-D2ï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ãƒ–UIèª¿æ•´

Edition ã”ã¨ã«ãƒ©ãƒ™ãƒ«ã‚„èª¬æ˜æ–‡ã‚’å¤‰ãˆã‚‹ï¼ˆã€Œã‚ãªãŸã®ãƒ—ãƒ©ãƒ³ã§ã¯â—‹â—‹ã¾ã§ä½¿ãˆã¾ã™ã€ãªã©ï¼‰ã€‚

L-Eï¼šæ—¥æœ¬èªUIãƒ»ã‚³ãƒ”ãƒ¼ã®èª¿æ•´ï¼ˆâ€œå•†å“ã¨ã—ã¦ã®é¡”â€ éƒ¨åˆ†ï¼‰

L-E1ï¼šã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥ã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆ

å„ã‚¿ãƒ–ã«ã€Œã“ã®ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã§ã¯ã“ã“ã¾ã§ä½¿ãˆã¾ã™ã€ã®ä¸€è¨€ãƒ˜ãƒ«ãƒ—ã€‚

ç‰¹ã« Proâ†’Expert ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰å‹•æ©Ÿã‚’å¼·èª¿ã™ã‚‹æ–‡è¨€ã‚’ç”¨æ„ã€‚

L-E2ï¼šAbout ç”»é¢ã« edition è¡¨ç¤º

ã€ŒãƒŸãƒãƒ“ã‚­ Freeã€ã€ŒãƒŸãƒãƒ“ã‚­ Expertã€ãªã©ã‚’æ˜ç¤ºã€‚

ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰æ¡ˆå†…ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ï¼ˆå°†æ¥ã®è²©å£²ã‚µã‚¤ãƒˆURLï¼‰ã€‚

L-Fï¼šã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³åˆ¥ãƒ†ã‚¹ãƒˆãƒ»ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

L-F1ï¼šFreeç‰ˆãƒã‚§ãƒƒã‚¯

ãƒ‡ãƒ¢ã®ã¿æ¥ç¶š

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©OFFå›ºå®š

ãƒ©ãƒ³ã‚­ãƒ³ã‚°é€ä¿¡ä¸å¯

AIã‚¿ãƒ–ãŒKPIã®ã¿

L-F2ï¼šBasicç‰ˆãƒã‚§ãƒƒã‚¯

æœ¬å£åº§æ¥ç¶šå¯

é€±1å†å­¦ç¿’ON/OFFã®ã¿

Basicãƒ©ãƒ³ã‚­ãƒ³ã‚°é€ä¿¡OK

AIã‚¿ãƒ–ç°¡æ˜“è¡¨ç¤º

L-F3ï¼šProç‰ˆãƒã‚§ãƒƒã‚¯

è¤‡æ•°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å‹•ä½œ

FI Top20 / SHAP3

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©1ã‚¸ãƒ§ãƒ–åˆ¶é™

Proãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ï¼‰

L-F4ï¼šExpertç‰ˆãƒã‚§ãƒƒã‚¯

è¤‡åˆãƒ•ã‚£ãƒ«ã‚¿

é€£æ•—å›é¿ï¼‹è‡ªå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ‡æ›¿

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¤‡æ•°ã‚¸ãƒ§ãƒ–ï¼‹é€£é–

Expertè¤‡åˆã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°


=== file: tools/__init__.py ===




=== file: tools/backtest_equity_curve.py ===

# tools/backtest_equity_curve.py
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))  # â†ã“ã‚Œã‚’è¿½åŠ 

import argparse, json, glob, os
from typing import Any
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from core.ai.loader import load_lgb_clf
from core.ai.features import build_features

def _load_active_meta() -> dict[str, Any]:
    p = "models/active_model.json"
    j = json.load(open(p, encoding="utf-8"))
    best_t = float(j.get("best_threshold", 0.2))
    lookahead = int(j.get("selected_lookahead", 15))
    return {"best_threshold": best_t, "lookahead": lookahead}

def _load_dataset(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    # å¿…è¦åˆ—: time/open/high/low/close/volume ã‚’æƒ³å®š
    if not set(["open","high","low","close","volume"]).issubset(df.columns):
        raise ValueError("CSVã«OHLCVåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
    df = build_features(df)  # å­¦ç¿’æ™‚ã¨åŒã˜æ‹¡å¼µ20åˆ—
    df = df.dropna().reset_index(drop=True)
    return df

def _ensure_feature_order(df: pd.DataFrame, model: Any) -> tuple[pd.DataFrame, list[str]]:
    # ãƒ©ãƒƒãƒ‘ã¯ model.expected_features ã‚’æŒãŸã›ã¦ã‚ã‚‹æƒ³å®šï¼ˆç„¡ã‘ã‚Œã°æ¨å®šï¼‰
    feat = getattr(model, "expected_features", None)
    if feat is None:
        # ãƒ¢ãƒ‡ãƒ«å´ã«ç„¡ã„å ´åˆã¯ã€å­¦ç¿’ã§ä½¿ã£ã¦ã„ãã†ãª20åˆ—ã‚’æ‹¾ã†ï¼ˆå¿œæ€¥ï¼‰
        candidates = ["open","high","low","close","volume",
                      "ret_1","ret_3","ret_5","ret_10",
                      "ret_std_10","ret_std_20",
                      "tr","atr_14","rsi_14","adx_14","bbp_20",
                      "upper_wick_ratio","lower_wick_ratio","body_ratio","vol_zscore_20"]
        feat = [c for c in candidates if c in df.columns]
    X = df[feat].copy()
    return X, feat

def run_backtest(csv_path: str, out_csv: str, init_equity: float = 100_000.0, show: bool = True) -> None:
    meta = _load_active_meta()
    best_t = float(meta.get("best_threshold", 0.2))
    L = int(meta.get("lookahead", 15))
    model = load_lgb_clf("models/LightGBM_clf.pkl")  # calibä»˜ãã§ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹
    df_raw = pd.read_csv(csv_path)
    df = _load_dataset(csv_path)
    X, feat = _ensure_feature_order(df, model)

    # (B) build_features â†’ dropna å¾Œã«è¡Œæ•°ãƒã‚§ãƒƒã‚¯
    # çª“ã‚’ä½¿ã†æŒ‡æ¨™ï¼ˆRSI14/ATR14/ret_std_20 ç­‰ï¼‰ã§å†’é ­ãŒNaNã«ãªã‚‹ãŸã‚ã€
    # çŸ­ã™ãã‚‹CSVã ã¨0è¡Œã«ãªã£ã¦ã—ã¾ã†ã€‚è¶³ã‚Šãªã„å ´åˆã¯æ—©æœŸçµ‚äº†ã—ã¦æ¡ˆå†…ã€‚
    if X.shape[0] == 0 or X.shape[0] < 40:
        print("[bt][error] too few rows after feature engineering (need â‰ˆ40+ rows). "
              "Your CSV is too short; provide more bars (100+ recommended).")
        return

    # äºˆæ¸¬
    # (A) åˆ—åä»˜ã DataFrame ã®ã¾ã¾æ¸¡ã™ï¼ˆå­¦ç¿’æ™‚ã®åˆ—åã‚’ç¶­æŒï¼‰
    proba = model.predict_proba(X)  # shape (n,2) ã‚’æƒ³å®šï¼ˆ[neg, pos]ï¼‰
    # ã“ã“ã§ã¯ [p_sell, p_buy] ã§ã¯ãªã [p0,p1]=[neg,pos] ã‚’ buy=pos ã¨ã¿ãªã™
    if proba.shape[1] == 2:
        p_buy = proba[:,1]
        p_sell = proba[:,0]
    else:
        # 2ã‚¯ãƒ©ã‚¹ã§ãªã„å ´åˆã®ä¿é™º
        p_buy = proba.ravel()
        p_sell = 1.0 - p_buy

    closes = df["close"].to_numpy()
    n = len(df)

    equity = init_equity
    equity_curve = []
    pos = 0        # 0:ãƒãƒ¼ãƒã‚¸, +1:è²·ã„, -1:å£²ã‚Š
    entry_idx = -1
    entry_price = np.nan

    for i in range(n):
        # ã‚¨ã‚°ã‚¸ãƒƒãƒˆåˆ¤å®š
        if pos != 0 and (i - entry_idx) >= L:
            exit_price = closes[i]
            ret = (exit_price/entry_price - 1.0) * (1 if pos>0 else -1)
            equity *= (1.0 + ret)
            pos = 0
            entry_idx = -1
            entry_price = np.nan

        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼åˆ¤å®šï¼ˆãƒãƒ¼ãƒã‚¸æ™‚ã®ã¿ï¼‰
        if pos == 0:
            if (p_buy[i] >= best_t) and (p_buy[i] > p_sell[i]):
                pos = +1
                entry_idx = i
                entry_price = closes[i]
            elif (p_sell[i] >= best_t) and (p_sell[i] > p_buy[i]):
                pos = -1
                entry_idx = i
                entry_price = closes[i]

        equity_curve.append(equity)

    out = pd.DataFrame({
        "time": df_raw.loc[df.index, "time"] if "time" in df_raw.columns else np.arange(n),
        "close": closes,
        "p_buy": p_buy,
        "p_sell": p_sell,
        "equity": equity_curve
    })
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    out.to_csv(out_csv, index=False, encoding="utf-8")
    print(f"[bt] wrote equity: {out_csv}  (final={equity_curve[-1]:.2f}, ret={(equity_curve[-1]/init_equity-1)*100:.2f}%)")

    if show:
        plt.figure(figsize=(9,4))
        plt.plot(out["equity"])
        plt.title(f"Equity Curve (start={init_equity:.0f} JPY, L={L}, thr={best_t})")
        plt.xlabel("bars")
        plt.ylabel("equity (JPY)")
        plt.tight_layout()
        plt.show()

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", default=None, help="ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå¯¾è±¡CSVã€‚æœªæŒ‡å®šãªã‚‰ data/*.csv ã®æœ€æ–°ã‚’ä½¿ç”¨")
    ap.add_argument("--out", default="logs/backtest/equity_curve.csv")
    ap.add_argument("--capital", type=float, default=100_000.0)
    ap.add_argument("--no-show", action="store_true")
    args = ap.parse_args()

    csv = args.csv or sorted(glob.glob("data/*.csv"))[-1]
    run_backtest(csv, args.out, init_equity=args.capital, show=not args.no_show)



=== file: tools/backtest_run.py ===

# tools/backtest_run.py
from __future__ import annotations

import argparse
import json
import sys
from dataclasses import dataclass
from pathlib import Path

import numpy as np

# --- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ sys.path ã«è¿½åŠ ã—ã¦ã‹ã‚‰ app.* ã‚’ import ã™ã‚‹ ---
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from app.strategies.ai_strategy import (
    build_features,
    load_active_model,
    predict_signals,
    trades_from_signals,
)

LOG_DIR = PROJECT_ROOT / "logs" / "backtest"

# === equity utils ===

# === monthly_returns.csv ã‚’ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
import pandas as pd


def compute_monthly_returns(equity_csv_path: str, out_path: str):
    df = pd.read_csv(equity_csv_path)

    if "timestamp" not in df.columns and "time" in df.columns:
        df = df.rename(columns={"time": "timestamp"})

    # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯ï¼štimestamp, equity
    if not {"timestamp", "equity"}.issubset(df.columns):
        raise ValueError("equity_curve.csv ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ (timestamp, equity)")

    # timestamp â†’ datetime å¤‰æ›
    df["timestamp"] = pd.to_datetime(df["timestamp"])

    # æœˆæœ«ã® equity ã‚’é›†è¨ˆ
    df["year"] = df["timestamp"].dt.year
    df["month"] = df["timestamp"].dt.month

    # æœˆåˆå€¤ã‚’å–å¾—
    first = df.groupby(["year", "month"])["equity"].first()
    last = df.groupby(["year", "month"])["equity"].last()

    # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
    monthly_return = (last - first) / first * 100

    # åŒæ™‚ã«æœˆæ¬¡DDã‚‚è¨ˆç®—ã™ã‚‹ï¼ˆpeak-to-troughï¼‰
    def calc_dd(sub):
        peak = sub["equity"].cummax()
        dd = (sub["equity"] - peak) / peak * 100
        return dd.min()

    dd_rows = []
    for (y, m), sub in df.groupby(["year", "month"]):
        dd_rows.append({
            "year": y,
            "month": m,
            "dd_pct": calc_dd(sub)
        })
    dd_df = pd.DataFrame(dd_rows).set_index(["year", "month"])

    out = pd.DataFrame({
        "return_pct": monthly_return,
    }).join(dd_df)

    out = out.reset_index().sort_values(["year", "month"])
    out.to_csv(out_path, index=False)

    print(f"[bt] write monthly {out_path}")


@dataclass
class Trade:
    entry_time: pd.Timestamp
    entry_price: float
    exit_time: pd.Timestamp
    exit_price: float
    direction: int  # +1 long, -1 short
    profit_jpy: float


def equity_from_bnh(df: pd.DataFrame, capital: float) -> pd.Series:
    """
    Buy&Holdï¼ˆç¾ç‰©1å€ï¼‰ç›¸å½“ã®æŒ‡æ•°ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã€‚close/close0ã§ã‚¹ã‚±ãƒ¼ãƒ«ã€‚
    """
    close = df["close"].astype(float)
    idx = close / close.iloc[0]
    return capital * idx


def trades_from_signal_series(
    df: pd.DataFrame,
    sig: pd.Series,
    lot: float = 0.1,
    contract_size: int = 100_000,
) -> list[Trade]:
    """
    signalï¼ˆ1/-1/0ï¼‰ã‹ã‚‰ãƒ•ãƒªãƒƒãƒ—æ–¹å¼ã§ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ—ã‚’ä½œã‚‹ã€‚
    - signal ãŒ 1â†’ãƒ­ãƒ³ã‚°ä¿æœ‰ã€-1â†’ã‚·ãƒ§ãƒ¼ãƒˆä¿æœ‰ã€0â†’ãƒãƒ¼ãƒã‚¸
    - signal ãŒå¤‰ã‚ã£ãŸæ™‚ç‚¹ã§å‰ãƒã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ã‚ºâ†’æ–°ãƒã‚¸ã‚’å»ºã¦ã‚‹
    - JPYãƒšã‚¢ã‚’æƒ³å®šï¼šæç›Š[JPY] = (exit - entry) * direction * lot * contract_size
    """
    sig = sig.astype(int).reindex(df.index).fillna(0)
    close = df["close"].astype(float)
    times = pd.to_datetime(df["time"])

    cur_dir = 0
    cur_price: float | None = None
    cur_time: pd.Timestamp | None = None
    out: list[Trade] = []

    for t, px, s in zip(times, close, sig):
        s = int(s)
        if cur_dir == 0:
            if s in (1, -1):
                cur_dir = s
                cur_price = px
                cur_time = t
        else:
            if s == cur_dir:
                continue
            # æ–¹å‘ãŒå¤‰ã‚ã£ãŸ/0ã«ãªã£ãŸ â†’ ã‚¯ãƒ­ãƒ¼ã‚º
            profit = (px - cur_price) * cur_dir * lot * contract_size  # type: ignore[arg-type]
            out.append(Trade(cur_time, cur_price, t, px, cur_dir, profit))  # type: ignore[arg-type]
            cur_dir = 0
            cur_price = None
            cur_time = None
            # æ–°è¦ã«å»ºã¦ç›´ã™ï¼ˆ0ã§ãªã‘ã‚Œã°ï¼‰
            if s in (1, -1):
                cur_dir = s
                cur_price = px
                cur_time = t

    # çµ‚ç«¯ã§ã‚ªãƒ¼ãƒ—ãƒ³ä¸­ãªã‚‰ã€æœ€çµ‚å€¤ã§ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¦ã—ã¾ã†
    if cur_dir != 0 and cur_price is not None and cur_time is not None:
        px = close.iloc[-1]
        t = times.iloc[-1]
        profit = (px - cur_price) * cur_dir * lot * contract_size
        out.append(Trade(cur_time, cur_price, t, px, cur_dir, profit))

    return out


def equity_from_trades(
    df: pd.DataFrame, trades: list[Trade], capital: float
) -> pd.Series:
    """
    ãƒˆãƒ¬ãƒ¼ãƒ‰é…åˆ—ã‹ã‚‰ã‚¨ã‚¯ã‚¤ãƒ†ã‚£æ›²ç·šã‚’ä½œã‚‹ï¼ˆé€æ¬¡åŠ ç®—ï¼‰ã€‚
    """
    eq = pd.Series(capital, index=pd.to_datetime(df["time"]))
    cum = capital
    i = 0
    for tr in trades:
        # æˆç«‹æ™‚åˆ»ã§æç›Šã‚’åæ˜ 
        while i < len(eq.index) and eq.index[i] <= tr.exit_time:
            if eq.index[i] == tr.exit_time:
                cum += tr.profit_jpy
            eq.iloc[i] = cum
            i += 1
    # ä»¥é™ã‚‚ãƒ©ã‚¹ãƒˆå€¤ã§åŸ‹ã‚ã‚‹
    while i < len(eq.index):
        eq.iloc[i] = cum
        i += 1
    return eq


def equity_from_trade_df(
    df_ohlcv: pd.DataFrame, trades_df: pd.DataFrame, capital: float
) -> pd.Series:
    """
    trades_df å½¢å¼ï¼ˆDataFrameï¼‰ã‹ã‚‰å…¨ãƒãƒ¼ã«å±•é–‹ã—ãŸã‚¨ã‚¯ã‚¤ãƒ†ã‚£æ›²ç·šã‚’ä½œã‚‹ã€‚
    å¿…é ˆã‚«ãƒ©ãƒ : exit_time, pnl
    ä»»æ„: entry_time, entry_price, exit_price, directionï¼ˆç„¡ãã¦ã‚‚OKï¼‰
    """
    if trades_df is None or trades_df.empty:
        # å–å¼•ãªã—ãªã‚‰ãƒ•ãƒ©ãƒƒãƒˆ
        idx = pd.to_datetime(df_ohlcv["time"])
        return pd.Series(capital, index=idx)

    td = trades_df.copy()

    # æ™‚åˆ»ã‚«ãƒ©ãƒ ã‚’æ™‚ç³»åˆ—åŒ–ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘ï¼‰
    for col in ("entry_time", "exit_time"):
        if col in td.columns:
            td[col] = pd.to_datetime(td[col], errors="coerce")

    if "exit_time" not in td.columns:
        raise ValueError("trades_df ã« exit_time åˆ—ãŒå¿…è¦ã§ã™ã€‚")

    # pnl ã¯æ•°å€¤åŒ–
    td["pnl"] = pd.to_numeric(td.get("pnl", 0.0), errors="coerce").fillna(0.0)

    # exit_time ã§ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã—ã¦ã€åŒæ™‚æ±ºæ¸ˆãŒã‚ã‚Œã°åˆç®—
    pnl_by_exit = td.groupby("exit_time")["pnl"].sum().sort_index()

    # å…¨ãƒãƒ¼ã¸å±•é–‹ï¼šexit_time ã®ãƒãƒ¼ã§ã®ã¿æç›Šã‚’åŠ ç®—ã€ä»¥é™ã¯å‰å€¤ã§FFill
    idx = pd.to_datetime(df_ohlcv["time"])
    eq = pd.Series(capital, index=idx)
    cum = capital
    i = 0
    exit_times = pnl_by_exit.index.to_list()
    k = 0

    while i < len(idx):
        t = idx[i]
        # ã“ã®ãƒãƒ¼ã® exit_time ã«æ±ºæ¸ˆãŒã‚ã‚Œã°ã™ã¹ã¦åŠ ç®—
        while k < len(exit_times) and exit_times[k] <= t:
            cum += float(pnl_by_exit.iloc[k])
            k += 1
        eq.iloc[i] = cum
        i += 1

    return eq


def to_equity(close: pd.Series, capital: float = 100000.0) -> pd.DataFrame:
    close = close.astype(float)
    ret = close.pct_change().fillna(0.0)
    eq = (1.0 + ret).cumprod() * capital
    return pd.DataFrame({"time": close.index, "equity": eq.values})


def _max_consecutive(x: pd.Series, val: int) -> int:
    # æœ€å¤§é€£ç¶šã‚«ã‚¦ãƒ³ãƒˆï¼ˆval=1ã‚’æ•°ãˆã‚‹ï¼‰
    c = 0
    m = 0
    for v in x:
        if v == val:
            c += 1
            m = max(m, c)
        else:
            c = 0
    return m


def _dd_duration_max(eq: pd.Series) -> int:
    """ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³æœŸé–“ã®æœ€å¤§æ—¥æ•°ã‚’ç®—å‡ºã€‚æ™‚ç³»åˆ—ãŒintãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚"""
    peak = -np.inf
    last_peak_time: pd.Timestamp | None = None
    max_days = 0
    for t, v in eq.items():
        # t ãŒ datetime ã§ãªã„å ´åˆã¯é£›ã°ã™
        if not hasattr(t, "to_pydatetime") and not hasattr(t, "year"):
            continue
        if v > peak:
            peak = v
            last_peak_time = pd.Timestamp(t)
        elif last_peak_time is not None:
            d = (pd.Timestamp(t) - last_peak_time).days
            max_days = max(max_days, d)
    return int(max_days)


def metrics_from_equity(eq: pd.Series) -> dict:
    ret = eq.pct_change().fillna(0.0)
    total = eq.iloc[-1] / eq.iloc[0] - 1.0
    dd = (eq / eq.cummax() - 1.0).min()
    sharpe = (ret.mean() / (ret.std() + 1e-12)) * np.sqrt(
        252 * 24 * 12
    )  # M5ç›¸å½“ã®ä¾¿å®œã‚¹ã‚±ãƒ¼ãƒ«
    return {
        "start_equity": float(eq.iloc[0]),
        "end_equity": float(eq.iloc[-1]),
        "total_return": float(total),
        "max_drawdown": float(dd),
        "sharpe_like": float(sharpe),
        "bars": int(len(eq)),
        "max_dd_days": _dd_duration_max(eq),
    }


def monthly_returns_from_equity(
    eq_df: pd.DataFrame,
    trades_df: pd.DataFrame | None = None,
) -> pd.DataFrame:
    """
    ã‚¨ã‚¯ã‚¤ãƒ†ã‚£æ›²ç·šï¼ˆeq_dfï¼‰ã¨ãƒˆãƒ¬ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆtrades_dfï¼‰ã‹ã‚‰ã€
    æœˆæ¬¡ã®ãƒªã‚¿ãƒ¼ãƒ³ãƒ»DDãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‰çµ±è¨ˆã‚’ã¾ã¨ã‚ãŸ DataFrame ã‚’è¿”ã™ã€‚

    è¿”ã‚Šå€¤ã‚«ãƒ©ãƒ :
        year, month, return_pct, dd_pct, trades, win_rate, pf
    """
    if eq_df is None or eq_df.empty:
        return pd.DataFrame(
            columns=[
                "year",
                "month",
                "return_pct",
                "dd_pct",
                "trades",
                "win_rate",
                "pf",
            ]
        )

    df = eq_df.copy()
    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time")
    df = df.set_index("time")

    # --- æœˆæ¬¡ã®ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆï¼…ï¼‰ ---
    monthly_last = df["equity"].resample("ME").last()
    monthly_first = df["equity"].resample("ME").first()

    # 0å‰²ã‚Šé˜²æ­¢
    ret_raw = (monthly_last / monthly_first).replace([np.inf, -np.inf], np.nan) - 1.0
    return_pct = (ret_raw * 100.0).rename("return_pct")  # ï¼…ã«å¤‰æ›

    # --- æœˆæ¬¡ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ï¼ˆï¼…ï¼‰ ---
    def _month_dd(equity: pd.Series) -> float:
        if equity.empty:
            return 0.0
        dd = (equity / equity.cummax() - 1.0).min()
        return float(dd * 100.0)  # ï¼…ã«å¤‰æ›ï¼ˆ-5.4 ãªã‚‰ -5.4%ï¼‰

    dd_pct = df["equity"].resample("ME").apply(_month_dd).rename("dd_pct")

    # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ DataFrameï¼ˆyear/month ä½œæˆï¼‰
    out = pd.concat([return_pct, dd_pct], axis=1).dropna(how="all")
    if out.empty:
        return pd.DataFrame(
            columns=[
                "year",
                "month",
                "return_pct",
                "dd_pct",
                "trades",
                "win_rate",
                "pf",
            ]
        )

    out.index = pd.to_datetime(out.index)
    out["year"] = out.index.year
    out["month"] = out.index.month

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‰çµ±è¨ˆã¯ 0 ã§åˆæœŸåŒ–ï¼‰
    out["trades"] = 0
    out["win_rate"] = 0.0
    out["pf"] = 0.0

    # --- ãƒˆãƒ¬ãƒ¼ãƒ‰çµ±è¨ˆï¼ˆæœˆæ¬¡ï¼‰ ---
    if trades_df is not None and not trades_df.empty and "exit_time" in trades_df.columns:
        td = trades_df.copy()
        td["exit_time"] = pd.to_datetime(td["exit_time"])
        td = td.dropna(subset=["exit_time"])

        if "pnl" in td.columns:
            td["pnl"] = pd.to_numeric(td["pnl"], errors="coerce").fillna(0.0)

            rows = []
            for (y, m), g in td.groupby([td["exit_time"].dt.year, td["exit_time"].dt.month]):
                pnl = g["pnl"].astype(float)
                n_tr = int(len(pnl))
                if n_tr == 0:
                    continue
                win_mask = pnl > 0
                win_rate = float(win_mask.mean() * 100.0)

                sum_win = float(pnl[win_mask].sum())
                sum_loss_abs = float((-pnl[~win_mask]).clip(lower=0).sum())

                if sum_loss_abs > 0:
                    pf = float(sum_win / sum_loss_abs)
                elif sum_win > 0:
                    pf = float("inf")
                else:
                    pf = 0.0

                rows.append(
                    {
                        "year": int(y),
                        "month": int(m),
                        "trades": n_tr,
                        "win_rate": win_rate,
                        "pf": pf,
                    }
                )

            if rows:
                stats = pd.DataFrame(rows)
                out = out.merge(stats, on=["year", "month"], how="left", suffixes=("", "_t"))

                # æ¬ æã‚’åˆæœŸå€¤ã§åŸ‹ã‚ã‚‹
                out["trades"] = out["trades_t"].fillna(out["trades"]).astype(int)
                out["win_rate"] = out["win_rate_t"].fillna(out["win_rate"])
                out["pf"] = out["pf_t"].fillna(out["pf"])

                # ä¸€æ™‚åˆ—ã‚’å‰Šé™¤
                out = out.drop(columns=[c for c in out.columns if c.endswith("_t")])

    # ã‚«ãƒ©ãƒ é †ã‚’æœ€çµ‚ä»•æ§˜ã«æƒãˆã‚‹
    out = out[["year", "month", "return_pct", "dd_pct", "trades", "win_rate", "pf"]]
    out = out.reset_index(drop=True)
    return out


def trades_from_buyhold(df: pd.DataFrame, capital: float) -> pd.DataFrame:
    # ãƒ†ãƒ³ãƒ—ãƒ¬ï¼šé–‹å§‹â†’çµ‚äº†ã®å˜ä¸€ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼ˆå°†æ¥ã¯æˆ¦ç•¥ã§è¤‡æ•°ãƒˆãƒ¬ãƒ¼ãƒ‰ã«å·®ã—æ›¿ãˆï¼‰
    if df.empty:
        return pd.DataFrame(
            columns=[
                "entry_time",
                "exit_time",
                "pnl",
                "holding_bars",
                "holding_days",
                "win",
            ]
        )
    entry = df["time"].iloc[0]
    exit_ = df["time"].iloc[-1]
    close = df["close"].astype(float)
    ret = (close.iloc[-1] / close.iloc[0]) - 1.0
    pnl = capital * ret
    holding_bars = len(df)
    holding_days = (pd.Timestamp(exit_) - pd.Timestamp(entry)).days
    win = int(pnl > 0)
    return pd.DataFrame(
        [
            {
                "entry_time": entry,
                "exit_time": exit_,
                "pnl": float(pnl),
                "holding_bars": int(holding_bars),
                "holding_days": int(holding_days),
                "win": win,
            }
        ]
    )


def trade_metrics(trades: pd.DataFrame) -> dict:
    if trades.empty:
        return {
            "trades": 0,
            "win_rate": 0.0,
            "avg_pnl": 0.0,
            "profit_factor": 0.0,
            "avg_holding_bars": 0.0,
            "avg_holding_days": 0.0,
            "max_consec_win": 0,
            "max_consec_loss": 0,
        }
    wins = trades["pnl"] > 0
    sum_win = trades.loc[wins, "pnl"].sum()
    sum_loss_abs = (-trades.loc[~wins, "pnl"]).clip(lower=0).sum()
    pf = (
        float(sum_win / sum_loss_abs)
        if sum_loss_abs > 0
        else float("inf") if sum_win > 0 else 0.0
    )

    seq = wins.astype(int)
    consec_win = _max_consecutive(seq, 1)
    consec_loss = _max_consecutive(1 - seq, 1)

    return {
        "trades": int(len(trades)),
        "win_rate": float(wins.mean()) if len(trades) else 0.0,
        "avg_pnl": float(trades["pnl"].mean()) if len(trades) else 0.0,
        "profit_factor": pf,
        "avg_holding_bars": (
            float(trades["holding_bars"].mean()) if len(trades) else 0.0
        ),
        "avg_holding_days": (
            float(trades["holding_days"].mean()) if len(trades) else 0.0
        ),
        "max_consec_win": int(consec_win),
        "max_consec_loss": int(consec_loss),
    }


def slice_period(
    df: pd.DataFrame, start: str | None = None, end: str | None = None
) -> pd.DataFrame:
    """
    æŒ‡å®šæœŸé–“ã§ DataFrame ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹ã€‚
    start / end ã®ã©ã¡ã‚‰ã‹ã€ã¾ãŸã¯ä¸¡æ–¹ãŒ None ã®å ´åˆã¯ã€ãã®æ¡ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
    ä¸¡æ–¹ None ã®å ´åˆã¯å…¨æœŸé–“ã‚’è¿”ã™ã€‚
    """
    if start is None and end is None:
        return df.reset_index(drop=True)

    m = pd.Series(True, index=df.index)
    if start is not None:
        m &= df["time"] >= pd.Timestamp(start)
    if end is not None:
        m &= df["time"] <= pd.Timestamp(end)
    return df.loc[m].reset_index(drop=True)


def run_backtest(
    data_csv: Path,
    start: str | None,
    end: str | None,
    capital: float,
    out_dir: Path,
) -> Path:
    print("[bt] start", flush=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    print(f"[bt] read_csv {data_csv}", flush=True)
    df = pd.read_csv(data_csv, parse_dates=["time"])

    tag_start = start or "ALL"
    tag_end = end or "ALL"
    print(f"[bt] slice {tag_start} .. {tag_end}", flush=True)
    df = slice_period(df, start, end)
    if df.empty:
        raise RuntimeError("No data in the requested period.")
    close = df["close"]
    close.index = df["time"]

    print("[bt] equity compute", flush=True)
    eq_df = to_equity(close, capital)
    eq_df["signal"] = 0  # Buy&Holdãªã®ã§ã‚·ã‚°ãƒŠãƒ«ç„¡ã—
    eq_csv = out_dir / "equity_curve.csv"
    print(f"[bt] write equity {eq_csv}", flush=True)
    eq_df.to_csv(eq_csv, index=False)
    monthly_path = out_dir / "monthly_returns.csv"
    compute_monthly_returns(eq_csv, monthly_path)

    # ä»®ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼ˆBuy&Holdï¼‰
    trades = trades_from_buyhold(df, capital)
    trades.to_csv(out_dir / "trades.csv", index=False)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    base = metrics_from_equity(eq_df["equity"])
    tmet = trade_metrics(trades)
    base.update(tmet)
    (out_dir / "metrics.json").write_text(
        json.dumps(base, ensure_ascii=False, indent=2)
    )

    print("[bt] done", flush=True)
    return eq_csv


def run_wfo(
    data_csv: Path,
    start: str | None,
    end: str | None,
    capital: float,
    out_dir: Path,
    train_ratio: float = 0.7,
) -> Path:
    print("[wfo] start", flush=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    print(f"[wfo] read_csv {data_csv}", flush=True)
    df = pd.read_csv(data_csv, parse_dates=["time"])

    tag_start = start or "ALL"
    tag_end = end or "ALL"
    print(f"[wfo] slice {tag_start} .. {tag_end}", flush=True)
    df = slice_period(df, start, end)
    if df.empty:
        raise RuntimeError("No data in the requested period.")
    n = len(df)
    n_tr = max(10, int(n * train_ratio))
    df_tr = df.iloc[:n_tr].reset_index(drop=True)
    df_ts = df.iloc[n_tr:].reset_index(drop=True)

    def _one(d: pd.DataFrame, name: str) -> dict:
        print(f"[wfo] equity compute {name}", flush=True)
        feat = build_features(d, params={})

        # --- å¿…é ˆåˆ—ã®è£œå®Œ ---
        if "time" not in feat.columns:
            feat["time"] = pd.to_datetime(d["time"]).reset_index(drop=True)
        if "close" not in feat.columns:
            feat["close"] = pd.Series(d["close"].astype(float)).reset_index(drop=True)

        try:
            kind, payload, threshold, params = load_active_model()
            print(f"[wfo] using model: {payload} threshold={threshold}", flush=True)

            # äºˆæ¸¬ â†’ ã‚·ã‚°ãƒŠãƒ«
            feat["signal"] = predict_signals(kind, payload, feat, threshold, params)
            signal_series = feat["signal"].astype(int).reset_index(drop=True)

            # ãƒ­ã‚°ã§â€œå‡ºã¦ã‚‹ã‹â€ãƒã‚§ãƒƒã‚¯
            nz = int((signal_series != 0).sum())
            print(f"[wfo] signals nonzero={nz} / {len(signal_series)}", flush=True)
            (out_dir / f"signals_{name}.csv").write_text(
                pd.DataFrame({"time": feat["time"], "signal": signal_series}).to_csv(
                    index=False
                )
            )

            # ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆ
            trades = trades_from_signals(feat, capital, params)

            # ã‚¨ã‚¯ã‚¤ãƒ†ã‚£å±•é–‹ï¼ˆDataFrame ã§ã‚‚ list[Trade] ã§ã‚‚OKã«ã™ã‚‹ï¼‰
            if isinstance(trades, pd.DataFrame):
                eq_series = equity_from_trade_df(feat, trades, capital)
            else:
                eq_series = equity_from_trades(feat, trades, capital)

            eq_df = pd.DataFrame(
                {
                    "time": eq_series.index,
                    "equity": eq_series.values,
                    "signal": signal_series.values,
                }
            )
        except Exception as e:
            print(f"[wfo] AI model not used ({e}) -> fallback to buy&hold", flush=True)
            close = d["close"]
            close.index = d["time"]
            eq_df = to_equity(close, capital)
            eq_df["signal"] = 0
            trades = trades_from_buyhold(d, capital)

        p = out_dir / f"equity_{name}.csv"
        print(f"[wfo] write {p}", flush=True)
        eq_df.to_csv(p, index=False)
        trades.to_csv(out_dir / f"trades_{name}.csv", index=False)

        # æœˆæ¬¡æç›Šï¼‹ãƒˆãƒ¬ãƒ¼ãƒ‰çµ±è¨ˆ
        mr = monthly_returns_from_equity(eq_df, trades_df=trades)
        mr.to_csv(out_dir / f"monthly_returns_{name}.csv", index=False)

        m = metrics_from_equity(eq_df["equity"])
        m.update(trade_metrics(trades))
        return m

    m_tr = _one(df_tr, "train")
    m_ts = _one(df_ts, "test")

    # å¯è¦–åŒ–ç”¨ã« test ã‚’ãƒ¡ã‚¤ãƒ³ã¸ã‚³ãƒ”ãƒ¼
    (out_dir / "equity_curve.csv").write_text((out_dir / "equity_test.csv").read_text())
    (out_dir / "metrics_wfo.json").write_text(
        json.dumps({"train": m_tr, "test": m_ts}, ensure_ascii=False, indent=2)
    )

    print("[wfo] done", flush=True)
    return out_dir / "equity_curve.csv"


def _normalize_dates_from_args(
    args: argparse.Namespace,
    parser: argparse.ArgumentParser,
) -> tuple[str | None, str | None]:
    """
    --start-date / --end-date ã‚’å„ªå…ˆã—ã¤ã¤ã€
    æ—§ --start / --end ã‚‚äº’æ›ç”¨ã¨ã—ã¦ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚

    è¿”ã‚Šå€¤ã¯ YYYY-MM-DD å½¢å¼ã®æ–‡å­—åˆ—ã‹ Noneã€‚
    """
    raw_start = getattr(args, "start_date", None) or getattr(args, "start", None)
    raw_end = getattr(args, "end_date", None) or getattr(args, "end", None)

    def _norm(x: str | None) -> str | None:
        if x is None or x == "":
            return None
        try:
            ts = pd.to_datetime(x)
        except Exception:
            parser.error(f"invalid date format: {x!r} (expected YYYY-MM-DD)")
        return ts.strftime("%Y-%m-%d")

    start_str = _norm(raw_start)
    end_str = _norm(raw_end)

    if start_str is not None and end_str is not None:
        if pd.Timestamp(start_str) > pd.Timestamp(end_str):
            parser.error("start date must be <= end date")

    return start_str, end_str


def _build_period_tag(start: str | None, end: str | None) -> str:
    """
    ãƒ­ã‚°ç”¨ã®æœŸé–“ã‚¿ã‚°ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    ä¾‹:
      start=2024-07-01, end=2024-12-31 â†’ '2024-07-01_to_2024-12-31'
      start=None,       end=2024-12-31 â†’ 'ALL_to_2024-12-31'
      start=2024-07-01, end=None       â†’ '2024-07-01_to_ALL'
      start=None,       end=None       â†’ 'ALL_to_ALL'
    """
    s = start or "ALL"
    e = end or "ALL"
    return f"{s}_to_{e}"


def _mirror_latest_run(period_dir: Path, base_dir: Path) -> None:
    """
    æœŸé–“ä»˜ããƒ•ã‚©ãƒ«ãƒ€ã«å‡ºåŠ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    (logs/backtest/{symbol}/{timeframe}) ã«ã‚‚ã‚³ãƒ”ãƒ¼ã—ã¦ã€
    GUI ã‚„ä»–ãƒ„ãƒ¼ãƒ«å‘ã‘ã®ã€Œæœ€æ–°çµæœã€ã¨ã—ã¦è¦‹ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
    """
    base_dir.mkdir(parents=True, exist_ok=True)
    if not period_dir.exists():
        return
    for f in period_dir.glob("*"):
        if f.is_file():
            target = base_dir / f.name
            target.write_bytes(f.read_bytes())


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    # æ—§ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆäº’æ›ç”¨ï¼‰
    ap.add_argument("--start", help="(legacy) é–‹å§‹æ—¥ä»˜ YYYY-MM-DD", required=False)
    ap.add_argument("--end", help="(legacy) çµ‚äº†æ—¥ä»˜ YYYY-MM-DD", required=False)
    # æ–°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ¨å¥¨ï¼‰
    ap.add_argument("--start-date", help="é–‹å§‹æ—¥ä»˜ YYYY-MM-DD", required=False)
    ap.add_argument("--end-date", help="çµ‚äº†æ—¥ä»˜ YYYY-MM-DD", required=False)

    ap.add_argument("--capital", type=float, default=100000.0)
    ap.add_argument("--mode", choices=["bt", "wfo"], default="bt")
    ap.add_argument("--symbol", default="USDJPY")
    ap.add_argument("--timeframe", default="M5")
    ap.add_argument("--layout", choices=["flat", "per-symbol"], default="per-symbol")
    ap.add_argument("--train-ratio", type=float, default=0.7)
    args = ap.parse_args()

    csv = Path(args.csv).resolve()
    base_dir = LOG_DIR / args.symbol / args.timeframe
    base_dir.mkdir(parents=True, exist_ok=True)

    # æ—¥ä»˜å¼•æ•°ã®æ­£è¦åŒ–ï¼ˆYYYY-MM-DD or Noneï¼‰
    start_str, end_str = _normalize_dates_from_args(args, ap)
    period_tag = _build_period_tag(start_str, end_str)
    period_dir = base_dir / f"backtest_{period_tag}"
    period_dir.mkdir(parents=True, exist_ok=True)

    print(f"[main] mode={args.mode} csv={csv}", flush=True)
    print(f"[main] period={period_tag}", flush=True)
    if args.mode == "bt":
        p = run_backtest(csv, start_str, end_str, args.capital, period_dir)
    else:
        p = run_wfo(
            csv,
            start_str,
            end_str,
            args.capital,
            period_dir,
            train_ratio=args.train_ratio,
        )

    # æœŸé–“ä»˜ããƒ•ã‚©ãƒ«ãƒ€ã®å†…å®¹ã‚’ã€Œæœ€æ–°çµæœã€ã¨ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ãƒŸãƒ©ãƒ¼
    _mirror_latest_run(period_dir, base_dir)

    print(str(p), flush=True)


if __name__ == "__main__":
    main()



=== file: tools/dump_feature_importance.py ===

# tools/dump_feature_importance.py
import csv
import glob
import json
import os
from datetime import datetime
from typing import Any, Iterable, Tuple

import joblib


def _load_latest_report() -> Tuple[str | None, dict[str, Any] | None]:
    rps = sorted(glob.glob(os.path.join("logs", "retrain", "report_*.json")))
    if not rps:
        return None, None
    rp = rps[-1]
    with open(rp, encoding="utf-8") as f:
        j = json.load(f)
    return rp, j


def _load_features_from_report(j: dict[str, Any]) -> list[str]:
    feats = j.get("features") or []
    return list(feats)


def _load_model(pkl_path: str) -> Any:
    return joblib.load(pkl_path)


def _write_feat_csv(model: Any, feat_cols: list[str], out_csv: str) -> str:
    try:
        booster = getattr(model, "booster_", None)
        if booster is None:
            split_importance = getattr(model, "feature_importances_", None)
            gain_importance = None
            names = feat_cols
        else:
            names = booster.feature_name()
            split_importance = booster.feature_importance(importance_type="split")
            gain_importance = booster.feature_importance(importance_type="gain")

        def _all_column_style(xs: Iterable[str]) -> bool:
            if not xs:
                return False
            return all(str(x).startswith("Column_") for x in xs)

        if not names or len(names) != len(feat_cols) or _all_column_style(names):
            names = feat_cols[:]

        rows: list[dict[str, float | str]] = []
        for i, name in enumerate(names):
            s = (
                float(split_importance[i])
                if (split_importance is not None and i < len(split_importance))
                else 0.0
            )
            g = (
                float(gain_importance[i])
                if (gain_importance is not None and i < len(gain_importance))
                else 0.0
            )
            rows.append({"feature": name, "gain": g, "split": s})
        rows.sort(key=lambda r: (r["gain"], r["split"]), reverse=True)

        os.makedirs(os.path.dirname(out_csv), exist_ok=True)
        with open(out_csv, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["feature", "gain", "split"])
            w.writeheader()
            for r in rows:
                w.writerow(r)
        return out_csv
    except Exception as e:
        print(f"[dump] failed: {e}")
        return ""


def main() -> None:
    # ãƒ¢ãƒ‡ãƒ«ã¯ active_model.json or æ—¢å®šã® models/LightGBM_clf.pkl ã‚’ä½¿ã†
    active_meta = os.path.join("models", "active_model.json")
    if os.path.exists(active_meta):
        try:
            j = json.load(open(active_meta, encoding="utf-8"))
            model_path = (
                j.get("target_path")
                or j.get("source_path")
                or os.path.join("models", "LightGBM_clf.pkl")
            )
        except Exception:
            model_path = os.path.join("models", "LightGBM_clf.pkl")
    else:
        model_path = os.path.join("models", "LightGBM_clf.pkl")

    rp, rep = _load_latest_report()
    if rep is None:
        print("[dump] no reports found. specify features manually.")
        return
    feats = _load_features_from_report(rep)
    if not feats:
        print("[dump] features not found in report.")
        return

    model = _load_model(model_path)
    tag = "manual"
    if rep is not None:
        lh = (rep.get("lookahead") or {}).get("selected")
        tag = f"lk{lh}" if lh is not None else "manual"
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_csv = os.path.join("logs", "retrain", f"feat_importance_{tag}_{ts}.csv")
    out = _write_feat_csv(model, feats, out_csv)
    if out:
        print("[dump] wrote:", out)
        print("[dump] top5 preview:")
        with open(out, encoding="utf-8") as f:
            for i, line in enumerate(f):
                print(line.rstrip())
                if i >= 5:
                    break


if __name__ == "__main__":
    main()



=== file: tools/export_tree_clean.ps1 ===

# tools/export_tree_clean.ps1
param(
  [string]$Root = ".",
  [string]$OutFile = "project_tree.txt",
  [switch]$IncludeDirs = $true
)

$ErrorActionPreference = "Stop"
$Utf8NoBom = New-Object System.Text.UTF8Encoding($false)

# å®Œå…¨ã«é™¤å¤–ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼ˆå¿…è¦ãªã‚‰è¿½åŠ ï¼‰
$ExcludeNames = @(".git", ".venv", ".vscode", "__pycache__", ".mypy_cache", ".pytest_cache", ".ruff_cache", "site-packages", "node_modules", "dist", "build")

function PathHasExcludedSegment {
  param([string]$FullPath)
  $norm = ($FullPath -replace '\\','/').TrimEnd('/')
  # ãƒ‘ã‚¹ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²ã—ã¦ **å®Œå…¨ä¸€è‡´** ã§åˆ¤å®š
  $segs = $norm -split '/'
  foreach ($seg in $segs) {
    foreach ($ex in $ExcludeNames) {
      if ($seg -eq $ex) { return $true }
    }
  }
  return $false
}

Write-Host "ğŸŒ³ Exporting clean tree from: $Root"
Write-Host "ğŸ§¹ Excluding: $($ExcludeNames -join ', ')"

$lines = New-Object System.Collections.Generic.List[string]

# ã¾ãšãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªâ†’æ¬¡ã«ãƒ•ã‚¡ã‚¤ãƒ«ã€ã®é †ã§åˆ—æŒ™ï¼ˆé †åºãŒå®‰å®šï¼‰
Get-ChildItem -LiteralPath $Root -Recurse -Force -Directory -ErrorAction SilentlyContinue |
  Where-Object { -not (PathHasExcludedSegment $_.FullName) } |
  ForEach-Object {
    $rel = Resolve-Path -LiteralPath $_.FullName -Relative
    $rel = ($rel -replace '^[.][\\/]', '') -replace '\\','/'
    if ($IncludeDirs -and $rel) { $lines.Add($rel) }
  }

Get-ChildItem -LiteralPath $Root -Recurse -Force -File -ErrorAction SilentlyContinue |
  Where-Object { -not (PathHasExcludedSegment $_.FullName) } |
  ForEach-Object {
    $rel = Resolve-Path -LiteralPath $_.FullName -Relative
    $rel = ($rel -replace '^[.][\\/]', '') -replace '\\','/'
    if ($rel) { $lines.Add($rel) }
  }

$lines = $lines | Sort-Object
[System.IO.File]::WriteAllLines($OutFile, $lines, $Utf8NoBom)
Write-Host "âœ… Wrote $OutFile (UTF-8, no BOM). Count=$($lines.Count)"



=== file: tools/inspect_report.py ===

ï»¿import json, glob

rp = sorted(glob.glob("logs/retrain/report_*.json"))[-1]
with open(rp, encoding="utf-8") as f:
    j = json.load(f)

lk = j.get("lookahead", {})
print("report:", rp)
print("status:", j.get("status"))
print("selected_lookahead:", lk.get("selected"))
print("promote_thresholds:", j.get("promote_thresholds"))
print("metrics_test:", j.get("metrics_test"))
print("calibration:", j.get("calibration"))
print("candidates (lookahead -> f1@best, auc@cal):")
for c in lk.get("candidates", []):
    m = c["metrics_test"]
    print(f"  L={c['lookahead']}: f1@best={m.get('f1@best')}, auc@cal={m.get('auc@cal', m.get('auc'))}")



=== file: tools/list_wfo_reports.py ===

"""
tools/list_wfo_reports.py

Walkforward / å†å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ (logs/retrain/report_*.json) ã®ä¸€è¦§ã‚’è¡¨ç¤ºã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€‚

ç›®çš„:
- WFO ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œå ´æ‰€ã€ã¨ã€Œæœ€ä½é™ã®ä¸­èº«ã€ã‚’ç¢ºèªã™ã‚‹
- GUI ã‹ã‚‰å‚ç…§ã™ã‚‹ã¨ãã®å‰æã‚’æƒãˆã‚‹
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

try:
    import fxbot_path
except ImportError:
    fxbot_path = None


@dataclass
class WFOReportSummary:
    path: Path
    id: str
    created_at: datetime | None
    symbol: str | None
    timeframe: str | None
    label_horizon: int | None
    pf: float | None
    max_dd: float | None
    sharpe: float | None
    win_rate: float | None

    @classmethod
    def from_json(cls, path: Path, data: dict[str, Any]) -> WFOReportSummary:
        # 1) ID
        rid = str(data.get("id") or path.stem.replace("report_", ""))

        # 2) created_at
        created_raw = data.get("created_at")
        created_at: datetime | None = None
        if isinstance(created_raw, str):
            for fmt in ("%Y-%m-%dT%H:%M:%S", "%Y-%m-%d %H:%M:%S"):
                try:
                    created_at = datetime.strptime(created_raw, fmt)
                    break
                except ValueError:
                    continue

        # 3) top-level keys
        symbol = data.get("symbol")
        timeframe = data.get("timeframe")
        label_horizon = data.get("label_horizon")

        # 4) metrics
        metrics = data.get("metrics") or {}
        pf = metrics.get("pf")
        max_dd = metrics.get("max_dd")
        sharpe = metrics.get("sharpe")
        win_rate = metrics.get("win_rate")

        return cls(
            path=path,
            id=rid,
            created_at=created_at,
            symbol=symbol,
            timeframe=timeframe,
            label_horizon=label_horizon,
            pf=pf,
            max_dd=max_dd,
            sharpe=sharpe,
            win_rate=win_rate,
        )


def get_project_root() -> Path:
    """fxbot_path ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã„ã€ãªã‘ã‚Œã°ã‚«ãƒ¬ãƒ³ãƒˆã‹ã‚‰æ¨æ¸¬ã€‚"""
    if fxbot_path is not None and hasattr(fxbot_path, "get_project_root"):
        return Path(fxbot_path.get_project_root())
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®è¦ªã®è¦ªã‚’ãƒ«ãƒ¼ãƒˆã¨ã¿ãªã™
    return Path(__file__).resolve().parents[1]


def find_wfo_reports(root: Path | None = None) -> list[WFOReportSummary]:
    if root is None:
        root = get_project_root()

    logs_retrain = root / "logs" / "retrain"
    if not logs_retrain.exists():
        print(f"[WARN] logs/retrain/ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {logs_retrain}")
        return []

    summaries: list[WFOReportSummary] = []
    for path in sorted(logs_retrain.glob("report_*.json")):
        try:
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            summary = WFOReportSummary.from_json(path, data)
            summaries.append(summary)
        except Exception as e:  # noqa: BLE001
            print(f"[ERROR] {path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    return summaries


def print_table(reports: list[WFOReportSummary]) -> None:
    if not reports:
        print("[INFO] WFO ãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ãƒ˜ãƒƒãƒ€
    header = [
        "idx",
        "id",
        "created_at",
        "symbol",
        "tf",
        "horizon",
        "PF",
        "MaxDD",
        "WinRate",
        "path",
    ]
    print("\t".join(header))

    for idx, r in enumerate(reports, start=1):
        created_str = (
            r.created_at.strftime("%Y-%m-%d %H:%M:%S") if r.created_at else "-"
        )
        row = [
            str(idx),
            r.id,
            created_str,
            r.symbol or "-",
            r.timeframe or "-",
            str(r.label_horizon) if r.label_horizon is not None else "-",
            f"{r.pf:.3f}" if isinstance(r.pf, (int, float)) else "-",
            f"{r.max_dd:.3f}" if isinstance(r.max_dd, (int, float)) else "-",
            f"{r.win_rate:.3f}" if isinstance(r.win_rate, (int, float)) else "-",
            str(r.path.relative_to(get_project_root())),
        ]
        print("\t".join(row))


def main() -> None:
    root = get_project_root()
    print(f"[INFO] project_root = {root}")
    reports = find_wfo_reports(root)
    print_table(reports)


if __name__ == "__main__":
    main()



=== file: tools/train_lightgbm.py ===

# tools/train_lightgbm.py
from __future__ import annotations
from pathlib import Path
import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
from app.strategies.ai_strategy import build_features_recipe

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_ROOT / "data" / "USDJPY" / "ohlcv" / "USDJPY_M15.csv"
MODEL_DIR = PROJECT_ROOT / "models"
MODEL_DIR.mkdir(exist_ok=True, parents=True)

# =============================
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
# =============================
LOOKAHEAD = 5  # 5æœ¬å…ˆã®çµ‚å€¤ã¨æ¯”è¼ƒã—ã¦ä¸Šæ˜‡ã—ã¦ã„ã‚‹ã‹ã‚’åˆ†é¡
THRESH_PCT = 0.001  # 0.1%ä»¥ä¸Šä¸Šæ˜‡ã‚’1ã¨ã¿ãªã™

# =============================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & ç‰¹å¾´é‡ç”Ÿæˆ
# =============================
print(f"[train] load {DATA_PATH}")
df = pd.read_csv(DATA_PATH)
df["time"] = pd.to_datetime(df["time"])

# build_features_recipe() ã¯ ai_strategy.py ã«ã‚ã‚‹æ—¢å­˜é–¢æ•°
feat = build_features_recipe(df, "ohlcv_tech_v1")

# ç›®çš„å¤‰æ•°ï¼š5ãƒãƒ¼å¾Œã«0.1%ä»¥ä¸Šä¸Šæ˜‡ã—ã¦ã„ã‚‹ã‹
feat["target"] = (feat["close"].shift(-LOOKAHEAD) / feat["close"] - 1.0 > THRESH_PCT).astype(int)
feat = feat.dropna().reset_index(drop=True)

X = feat.drop(columns=["time", "target"])
y = feat["target"]

print(f"[train] samples={len(X)} features={X.shape[1]} pos_rate={y.mean():.3f}")

# =============================
# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
# =============================
params = dict(
    objective="binary",
    metric="binary_logloss",
    learning_rate=0.05,
    num_leaves=31,
    n_estimators=200,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
)

model = lgb.LGBMClassifier(**params)
model.fit(X, y)

# =============================
# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆpkl ã¨ Booster ã®äºŒåˆ€æµï¼‰
# =============================
MODEL_PATH = MODEL_DIR / "LightGBM_clf.pkl"
BOOSTER_PATH = MODEL_DIR / "LightGBM_clf.txt"

# pklï¼ˆäº’æ›æ€§é‡è¦–ï¼‰
joblib.dump(model, MODEL_PATH, compress=0, protocol=4)
print(f"[train] saved model (pkl) -> {MODEL_PATH}")

# booster ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
try:
    booster = model.booster_
    booster.save_model(str(BOOSTER_PATH))
    print(f"[train] saved model (booster txt) -> {BOOSTER_PATH}")
except Exception as e:
    print(f"[train] WARN: booster save failed: {e}")



=== file: tools/train_scaler.py ===

# tools/train_scaler.py
from pathlib import Path
import json
import pandas as pd
from sklearn.preprocessing import StandardScaler
from app.strategies.ai_strategy import build_features_recipe

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_ROOT / "data" / "USDJPY" / "ohlcv" / "USDJPY_M15.csv"
INFO_PATH = PROJECT_ROOT / "models" / "LightGBM_info.json"
OUT_DIR = PROJECT_ROOT / "models" / "scalers"
OUT_DIR.mkdir(parents=True, exist_ok=True)

print(f"[train_scaler] load {DATA_PATH}")
df = pd.read_csv(DATA_PATH)
feat = build_features_recipe(df, "ohlcv_tech_v1")

# å­¦ç¿’æ™‚ã®åˆ—é †ã‚’å–ã‚Šå‡ºã™
info = json.loads(INFO_PATH.read_text(encoding="utf-8"))
cols = info["features"]
missing = [c for c in cols if c not in feat.columns]
if missing:
    raise RuntimeError(f"[train_scaler] missing columns from build_features: {missing}")

X = feat.loc[:, cols].dropna()

scaler = StandardScaler()
scaler.fit(X.values)

import joblib
joblib.dump(scaler, OUT_DIR / "std_v1.pkl")
print(f"[train_scaler] saved: {OUT_DIR / 'std_v1.pkl'} (shape={X.shape})")



=== file: ã‚„ã‚‹ã¹ãã“ã¨.txt ===

scripts/register_weekly_task.ps1
ã®å®Ÿè¡Œ
powershell -ExecutionPolicy Bypass -File scripts\register_weekly_task.ps1 -ProjectDir "C:\fxbot" -StartTime "03:05" -DayOfWeek Sunday


=== file: ãƒ•ã‚§ãƒ¼ã‚ºA~Hã¾ã§ã®çµæœ.txt ===

ãƒ•ã‚§ãƒ¼ã‚ºBçµæœ
å¤‰æ›´å†…å®¹

app/services/trade_service.py (line 49) ã§æ—§ãƒ­ã‚¸ãƒƒã‚¯ã‚’å»ƒã—ã€decide_entry_from_probs ã‚’å”¯ä¸€ã®åˆ¤å®šçª“å£ã¨ã—ã¦ãƒ©ãƒƒãƒ—ã™ã‚‹ã‚ˆã† decide_entry ã‚’ç°¡ç´ åŒ–ã€‚ç”Ÿã®è¨­å®šå€¤ã‹ã‚‰ prob_threshold / entry_min_edge / side_bias ã‚’å–ã‚Šå‡ºã—ã¦ ENTRY/SKIP åˆ¤å®šã¨ side ã‚’è¿”ã™æ–°ãƒ­ã‚¸ãƒƒã‚¯ã«å·®ã—æ›¿ãˆã¾ã—ãŸ (app/services/trade_service.py (line 54))ã€‚
app/services/execution_stub.py (line 213) ã§ AI å‡ºåŠ›å‡¦ç†ã‚’çµ±åˆãƒ­ã‚¸ãƒƒã‚¯ã¸ç§»è¡Œã€‚ãƒ•ã‚£ãƒ«ã‚¿é€šéå¾Œã« decide_entry_from_probs ã‚’å‘¼ã³å‡ºã—ã€SKIP æ™‚ã¯ç†ç”±ã‚’ä¿æŒã—ãŸã¾ã¾å³æ™‚ returnã€‚ENTRY æ™‚ã¯ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¬ãƒ¼ãƒ‰ã‚’å†ç¢ºèªã—ã€è¨±å¯ã•ã‚Œã‚Œã° signal ã‚’çµ„ã¿ç«‹ã¦ã€å¿…è¦ãªã‚‰ _build_exit_planï¼_build_decision_exit_plan ã‚’åˆ©ç”¨ã—ã¦ exit_plan ã‚’ä»˜ä¸ã—ã€ãƒ­ã‚°ãƒ»æˆ»ã‚Šå€¤ã¨ã‚‚ã« dec æƒ…å ±ä»˜ãã§è¿”ã™ã‚ˆã†æ›´æ–°ã€‚

PS D:\macht\OneDrive\fxbot> python -m compileall app core scripts
Listing 'app'...
Listing 'app\\core'...
Listing 'app\\data'...
Listing 'app\\gui'...
Listing 'app\\models'...
Listing 'app\\services'...
Listing 'core'...
Listing 'core\\ai'...
Listing 'core\\utils'...
Listing 'scripts'...


ãƒ•ã‚§ãƒ¼ã‚ºCçµæœ
configs/config.yaml (line 25) ã§ side_bias ã‚’ "auto" ã«æ˜ç¤ºã—ã¦çµ±ä¸€ã—ã€min_edge: 0.0 # ã¾ãŸã¯ entry_min_edge: 0.0 ã‚’è¿½åŠ ã—ã¦ã‚¨ãƒƒã‚¸æ¡ä»¶ã‚’é–‹æ”¾ã—ã¾ã—ãŸã€‚

ãƒ•ã‚§ãƒ¼ã‚ºDçµæœ
scripts/dryrun_smoke.py å®Ÿè¡Œã§ decision: 'ENTRY'ï¼ˆSELLå´ï¼‰ã‚’ç¢ºèªæ¸ˆã¿ã€‚è©²å½“ãƒ­ã‚°ã¯ logs/decisions/decisions_USDJPY-.jsonl ã«è¿½è¨˜ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãƒ­ã‚°æ¤œè¨¼
Get-Content ... | Select-String '"decision": "ENTRY"' ã«ã¦ ENTRY è¡Œã‚’æ¤œå‡ºã€‚ç›´è¿‘ã§ã¯ SELL ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãŒè¨˜éŒ²ã€‚
"meta": "BUY" å‡ºç¾æ•°: 20ã€"meta": "SELL" å‡ºç¾æ•°: 20ã€‚
è£œè¶³å¯¾å¿œ
app/services/trade_service.py (line 47) ã§è¨­å®šèª­ã¿è¾¼ã¿ã‚’éƒ½åº¦è¡Œã†ã‚ˆã†ã«ã—ã€core.config ã¸ã®ä¾å­˜ã‚’è§£æ¶ˆã€‚
scripts/dryrun_smoke.py (line 23) ã¨  (line 67) ä»˜è¿‘ã§ trade_state.update(...) ã‚’è¿½åŠ ã—ã€ã‚¹ãƒ¢ãƒ¼ã‚¯å®Ÿè¡Œæ™‚ã«ã‚‚å–å¼•è¨±å¯ãƒ»é–¾å€¤ãŒåæ˜ ã•ã‚Œã‚‹ã‚ˆã†èª¿æ•´ã€‚

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 17:31:06.113 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 17:31:06.243 | INFO     | app.services.execution_stub:_emit:183 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'ENTRY', 'reason': None, 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': '1a32d184aa294d6463e0c7ac60fb4d195a9485084626b92dcd20d0b97c4fec63', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SELL'}, 'filters': {'spread': 0.6, 'spread_limit': 1.5, 'adx': 31.46339, 'min_adx': 15.0, 'adx_disabled': True, 'atr_pct': 0.00048845, 'min_atr_pct': 0.0004, 'prob_threshold': 0.5, 'side_bias': 'auto', 'blocked': None}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"decision": "ENTRY"'

{"ts_jst": "2025-10-31T17:28:23+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.1, "spread_limit": 1. 
5, "adx": 32.31987, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00049188, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": "pos_guard"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SELL", "threshold": 0.5, "decision": {"act 
ion": "BLOCKED", "reason": "pos_guard", "ai_meta": "SELL", "dec 
": {"decision": "ENTRY", "meta": "SELL", "side": "SELL", "thres 
hold": 0.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + 
 calib[isotonic]", "version": "1761815421.0", "features_hash":  
"6acc2276334fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cb 
e0", "p_buy": 0.49677207571944404, "p_sell": 0.503227924280556, 
 "p_skip": 0.49677207571944404, "meta": "SELL"}, "cb": {"trippe 
d": false, "reason": null, "consecutive_losses": 0, "threshold_ 
losses": 5, "reset_marker": null}, "features_hash": "6acc227633 
4fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cbe0", "model 
": "LightGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 
 0.10000000000047748, "spread_limit_pips": 1.5, "min_adx": 15.0 
, "disable_adx_gate": true, "min_atr_pct": 0.0004, "prob_thresh 
old": 0.5, "side_bias": "auto", "max_positions": 1, "ai_thresho 
ld": 0.5}}
{"ts_jst": "2025-10-31T17:30:06+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 31.42594, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00046944, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": null}, "probs": {"buy": 0.496 
772, "sell": 0.503228, "skip": 0.496772}, "calibrator": "isoton 
ic", "meta": "SELL", "threshold": 0.5, "decision": {"action": " 
ENTRY", "reason": null, "ai_meta": "SELL", "signal": {"side": " 
SELL", "prob": 0.503227924280556, "meta": "SELL"}, "dec": {"dec 
ision": "ENTRY", "meta": "SELL", "side": "SELL", "threshold": 0 
.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + calib[i 
sotonic]", "version": "1761815421.0", "features_hash": "02928ba 
944155c7912993a4f55f6eac9025b1d4941b7321330edacf7de11f167", "p_ 
buy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_skip 
": 0.49677207571944404, "meta": "SELL"}, "cb": {"tripped": fals 
e, "reason": null, "consecutive_losses": 0, "threshold_losses": 
 5, "reset_marker": null}, "features_hash": "02928ba944155c7912 
993a4f55f6eac9025b1d4941b7321330edacf7de11f167", "model": "Ligh 
tGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 0.60000 
00000000227, "spread_limit_pips": 1.5, "min_adx": 15.0, "disabl 
e_adx_gate": true, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "max_positions": 1, "ai_threshold": 0.5} 
}
{"ts_jst": "2025-10-31T17:31:06+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 31.46339, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00048845, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": null}, "probs": {"buy": 0.496 
772, "sell": 0.503228, "skip": 0.496772}, "calibrator": "isoton 
ic", "meta": "SELL", "threshold": 0.5, "decision": {"action": " 
ENTRY", "reason": null, "ai_meta": "SELL", "signal": {"side": " 
SELL", "prob": 0.503227924280556, "meta": "SELL"}, "dec": {"dec 
ision": "ENTRY", "meta": "SELL", "side": "SELL", "threshold": 0 
.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + calib[i 
sotonic]", "version": "1761815421.0", "features_hash": "1a32d18 
4aa294d6463e0c7ac60fb4d195a9485084626b92dcd20d0b97c4fec63", "p_ 
buy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_skip 
": 0.49677207571944404, "meta": "SELL"}, "cb": {"tripped": fals 
e, "reason": null, "consecutive_losses": 0, "threshold_losses": 
 5, "reset_marker": null}, "features_hash": "1a32d184aa294d6463 
e0c7ac60fb4d195a9485084626b92dcd20d0b97c4fec63", "model": "Ligh 
tGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 0.60000 
00000000227, "spread_limit_pips": 1.5, "min_adx": 15.0, "disabl 
e_adx_gate": true, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "max_positions": 1, "ai_threshold": 0.5} 
}

PS D:\macht\OneDrive\fxbot> (Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"meta": "BUY"').Count       
20
PS D:\macht\OneDrive\fxbot> (Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"meta": "SELL"').Count      
21


ãƒ•ã‚§ãƒ¼ã‚ºEçµæœ
configs/config.yaml (line 26) ã§ prob_threshold ã‚’ 0.62ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã¯è‹±èªã«ç½®æ›ï¼‰ã¸ã€configs/config.yaml (line 30) ã§ min_edge ã‚’ 0.02 ã«è¨­å®šã—ã€configs/config.yaml (lines 53-54) ã§ ADXã‚²ãƒ¼ãƒˆã‚’æœ‰åŠ¹ (adx_disable: false)ãƒ»ATRãƒ•ã‚£ãƒ«ã‚¿ã‚’ 0.0006 ã«å¼•ãä¸Šã’ã¾ã—ãŸã€‚
.venv\Scripts\python.exe -m scripts.dryrun_smoke å®Ÿè¡Œã¯ WARNING ãƒ­ã‚°ã¨ãªã‚Šã€ãƒ•ã‚£ãƒ«ã‚¿åˆ¤å®šã§ ATR ä¸‹é™ (atr_low) ã«ã‚ˆã‚Š ENTRY ã¯ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸï¼ˆAIãƒ¡ã‚¿ã¯ SKIPï¼‰ã€‚
ãƒ­ã‚°æ¤œè¨¼ï¼šSelect-String '"blocked": "atr_low"' ã¯éå»åˆ†ã¨ä»Šå›åˆ†ã‚’æ¤œå‡ºã€"blocked": "adx_low" ã¯ãƒ’ãƒƒãƒˆãªã—ã§ã—ãŸã€‚

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 17:35:26.183 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 17:35:26.289 | WARNING  | app.services.execution_stub:_emit:179 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': '285310bf40fa5081802422eded472a7c034186013956c3e95a54a401b5d84c52', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SKIP'}, 'filters': {'spread': 0.6, 'spread_limit': 1.5, 'adx': 30.66809, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00047144, 'min_atr_pct': 0.0006, 'prob_threshold': 0.62, 'side_bias': 'auto', 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}  

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"blocked": "atr_low"'        

{"ts_jst": "2025-10-31T16:05:51+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 33.68549, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00048653, "min_atr_pct": 0.02, "prob_threshold": 0.55, 
 "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy": 0 
.502041, "sell": 0.497959, "skip": 0.497959}, "calibrator": "is 
otonic", "meta": "SKIP", "threshold": 0.55, "decision": {"actio 
n": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "Ligh 
tGBM_clf + calib[isotonic]", "version": "1761815421.0", "featur 
es_hash": "e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e 
6a96912863e1", "p_buy": 0.5020408163265306, "p_sell": 0.4979591 
8367346936, "p_skip": 0.49795918367346936, "meta": "SKIP"}, "cb 
": {"tripped": false, "reason": null, "consecutive_losses": 0,  
"threshold_losses": 5, "reset_marker": null}, "features_hash":  
"e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e6a96912863 
e1", "model": "LightGBM_clf + calib[isotonic]", "runtime": {"sp 
read_pips": 0.49999999999954525, "spread_limit_pips": 1.5, "min 
_adx": 15.0, "disable_adx_gate": true, "min_atr_pct": 0.02, "pr 
ob_threshold": 0.55, "side_bias": "auto", "max_positions": 1, " 
ai_threshold": 0.55}}
{"ts_jst": "2025-10-31T17:34:14+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.2, "spread_limit": 1. 
5, "adx": 31.46339, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00049362, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "303e2ece8a2138840f6be93a86da70447f176216708018e2d 
4c5ce3db35a5e0e", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "303e2ece8a2138840f6be93a86da70447f176216708018e2d4c5ce3db35a 
5e0e", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.20000000000095497, "spread_limit_pips": 1.5, "m 
in_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006 
, "prob_threshold": 0.62, "side_bias": "auto", "max_positions": 
 1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:35:26+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 30.66809, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00047144, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "285310bf40fa5081802422eded472a7c034186013956c3e95 
a54a401b5d84c52", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "285310bf40fa5081802422eded472a7c034186013956c3e95a54a401b5d8 
4c52", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.6000000000000227, "spread_limit_pips": 1.5, "mi 
n_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006, 
 "prob_threshold": 0.62, "side_bias": "auto", "max_positions":  
1, "ai_threshold": 0.62}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"blocked": "adx_low"' 

ãƒ•ã‚§ãƒ¼ã‚ºGçµæœ
å¯¾å¿œå†…å®¹

configs/config.yaml (line 26) ã§ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã—ãã„å€¤ã‚’æœ¬ç•ªå¯„ã‚Šã«æˆ»ã—ã¤ã¤ã€exits ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå›ºå®š/ATR/ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã®å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
core/indicators.py ã‚’æ–°è¦ä½œæˆã—ã€true_range ã¨ã‚·ãƒ³ãƒ—ãƒ«ãª ATR è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ã‚’æä¾›ã€‚
app/services/trade_service.py (line 8) ã« _atr ã‚’å–ã‚Šè¾¼ã¿ã€build_exit_planï¼ˆåŒ:86ï¼‰ã§ exits è¨­å®šã‹ã‚‰å›ºå®šãƒ»ATRãƒ»none ã®è¨ˆç”»ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†å®Ÿè£…ã€‚
app/services/execution_stub.py (line 64) ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã« exit_plan ã‚’å«ã‚ã€:254 ä»¥é™ã§ trade_service.build_exit_plan ã‚’ç”¨ã„ãŸã‚·ã‚°ãƒŠãƒ«çµ„ã¿ç«‹ã¦ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’è¿½åŠ ã€‚
ç¢ºèª

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Select-String -Path logs\decisions\decisions_*.jsonl -Pattern '"exit_plan"' ã§ JSONL ã« exit_plan ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã€‚

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"exit_plan": {'
>> 

{"ts_jst": "2025-10-31T17:41:35+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.4, "spread_limit": 1.
5, "adx": 29.75924, "min_adx": 15.0, "adx_disabled": false, "at
r_pct": 0.00046644, "min_atr_pct": 0.0006, "prob_threshold": 0.
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy"
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator": 
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea
tures_hash": "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b46601
3696a29fe67c604", "p_buy": 0.49677207571944404, "p_sell": 0.503
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, "
cb": {"tripped": false, "reason": null, "consecutive_losses": 0
, "threshold_losses": 5, "reset_marker": null}, "features_hash"
: "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b466013696a29fe67
c604", "model": "LightGBM_clf + calib[isotonic]", "exit_plan":  
{"mode": "none"}, "runtime": {"spread_pips": 0.4000000000019099 
4, "spread_limit_pips": 1.5, "min_adx": 15.0, "disable_adx_gate 
": false, "min_atr_pct": 0.0006, "prob_threshold": 0.62, "side_ 
bias": "auto", "max_positions": 1, "ai_threshold": 0.62}}  


ãƒ•ã‚§ãƒ¼ã‚ºH
é›†è¨ˆçµæœ

DECISIONS: Counter({'BLOCKED': 18, 'BUY': 17, 'SKIP': 15, 'SELL': 12, 'ENTRY': 2})
REASONS: Counter({'ai_threshold': 29, 'ai_skip': 15, 'consecutive_losses(5)>=threshold(5)': 9, 'atr_low': 5, 'test_trip': 3, 'pos_guard': 1})
ENTRY ã¯ decisions_USDJPY.jsonl å†…ã§ 2 ä»¶ã€‚decisions_USDJPY-.jsonl å´ã¯ 0 ä»¶ã§ã—ãŸã€‚

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ENTRY ã‚’å¢—ã‚„ã™ãŸã‚ã«ã¯ä¸€æ™‚çš„ã« entry.prob_threshold ã‚’ 0.49 ã¸ä¸‹ã’ã‚‹ã‹ min_edge: 0.0 ã«æˆ»ã—ã¦æŒ™å‹•ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚





=== file: ãƒŸãƒãƒ“ã‚­ FX è‡ªå‹•å£²è²·ã‚·ã‚¹ãƒ†ãƒ ï¼šä»•æ§˜æ›¸ v4ï¼ˆæ­£å¼ç‰ˆï¼‰.txt ===

ãƒŸãƒãƒ“ã‚­ FX è‡ªå‹•å£²è²·ã‚·ã‚¹ãƒ†ãƒ ï¼šä»•æ§˜æ›¸ v4ï¼ˆæ­£å¼ç‰ˆï¼‰

2025 / Windowsç‰ˆï¼ˆPyQt6 GUIï¼‰

0. ç›®çš„ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
â–  ç›®çš„

MT5ï¼ˆMetaTrader5ï¼‰ã‚’ç”¨ã„ã€
ã€Œæœˆé–“ï¼‹3ï¼…å‰å¾Œã®ãƒªã‚¿ãƒ¼ãƒ³ã‚’ç›®æŒ‡ã™ã€AIè‡ªå‹•å£²è²·ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

ãƒ»æœˆé–“3%ã¯ç¾©å‹™ã§ã¯ãªã KPIï¼ˆç›®æ¨™å€¤ï¼‰
ãƒ»æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã¯ -15ã€œ-20ï¼…ä»¥å†… ã‚’å®‰å…¨åœã¨ã™ã‚‹
ãƒ»Walk-Forward å†å­¦ç¿’ã€ç‰¹å¾´é‡å¯è¦–åŒ–ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãªã©ã®åˆ†æåŸºç›¤ã‚’å†…è”µã™ã‚‹

ãƒŸãƒãƒ“ã‚­ã¯
è£é‡ â†’ åŠè‡ªå‹• â†’ è‡ªå‹•
ã¸ç§»è¡Œã—ãŸã„å€‹äººãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼å‘ã‘ã®ã€Œé‹ç”¨æ”¯æ´ã‚½ãƒ•ãƒˆã€ã€‚

â–  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼

ãƒ»Windows (10/11)
ãƒ»æ—¥æœ¬èªç’°å¢ƒ
ãƒ»MT5ãƒ¦ãƒ¼ã‚¶ãƒ¼
ãƒ»åˆå¿ƒè€…ã€œä¸Šç´šè€…ã¾ã§
ãƒ»AIåˆ†æï¼‹è‡ªå‹•å£²è²·ã«èˆˆå‘³ãŒã‚ã‚‹äºº

1. å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

3ãƒ¬ã‚¤ãƒ¤æ§‹é€ ã§è¨­è¨ˆã™ã‚‹ã€‚

1) GUI å±¤ï¼ˆapp.guiï¼‰

ãƒ»PyQt6 ãƒ™ãƒ¼ã‚¹
ãƒ»ãƒ¡ã‚¤ãƒ³ç”»é¢ã€AIã‚¿ãƒ–ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ãƒ–ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®šã€ãƒ­ã‚°ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚’å«ã‚€

2) ã‚µãƒ¼ãƒ“ã‚¹å±¤ï¼ˆapp.servicesï¼‰

ãƒ»AIã‚µãƒ¼ãƒ“ã‚¹ï¼ˆAISvcï¼‰
ãƒ»KPIè¨ˆç®—
ãƒ»Feature Importance / SHAP
ãƒ»JobSchedulerï¼ˆå†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼‰
ãƒ»RankingService
ãƒ»ãƒ­ã‚°å‡ºåŠ›

3) ã‚³ã‚¢å±¤ï¼ˆapp.coreï¼‰

ãƒ»MT5ãƒ©ãƒƒãƒ‘ãƒ¼
ãƒ»æˆ¦ç•¥ãƒ­ã‚¸ãƒƒã‚¯
ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³
ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€
ãƒ»ãƒ¢ãƒ‡ãƒ«ç®¡ç†

2. å®Ÿè¡Œå½¢æ…‹
â— GUIã‚¢ãƒ—ãƒªã¨ã—ã¦èµ·å‹•ï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼‰
â— å°†æ¥çš„ã«ã€Œã‚¿ã‚¹ã‚¯ãƒˆãƒ¬ã‚¤å¸¸é§ãƒ¢ãƒ¼ãƒ‰ã€ã¸æ‹¡å¼µå¯èƒ½
3. ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆå…±é€šãƒ•ã‚©ãƒ«ãƒ€ï¼‰
config/                 è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«  
models/                 å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«  
logs/                   ãƒ­ã‚°ã€æ±ºå®šãƒ­ã‚°  
backtests/{profile}/    ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ  

4. ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æ§‹æˆï¼ˆv4ï¼‰

v3ã®æ§‹é€ ã«åŸºã¥ãã¤ã¤ã€Proâ†’Expertã®å·®ã‚’å¼·åŒ–ã—ã¦ Expert ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¯”ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†æœ€é©åŒ–ã€‚

â–  å…±é€šãƒ«ãƒ¼ãƒ«

ãƒ»å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¯å…¨ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³å…±é€š
ãƒ»æ©Ÿèƒ½å·®ã¯ edition: free/basic/pro/expert/master ãƒ•ãƒ©ã‚°ã§åˆ¶å¾¡
ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹èªè¨¼ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§æ±ºå®š

â—† å„ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³è©³ç´°ï¼ˆv4ï¼‰
ğŸ”¥ Freeï¼ˆç„¡æ–™ç‰ˆï¼‰
ç›®çš„

ãŠè©¦ã—ç”¨ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å…¥ã‚Šå£ã«èª˜å°ã™ã‚‹ã€‚

æ©Ÿèƒ½

ãƒ»ãƒ‡ãƒ¢å£åº§ã®ã¿
ãƒ»è‡ªå‹•å£²è²·ï¼šå¯èƒ½ï¼ˆãƒ­ãƒƒãƒˆåˆ¶é™ï¼‰
ãƒ»AIã‚¿ãƒ–ï¼šKPIã®ã¿ï¼ˆå‹ç‡ã€ä»Šæœˆæç›Šï¼…ã€PF ç­‰ï¼‰
ãƒ»Feature Importance / SHAPï¼šéè¡¨ç¤º
ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼šé–²è¦§ã®ã¿ï¼ˆé€ä¿¡ä¸å¯ï¼‰
ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼šç”»é¢è¡¨ç¤ºã®ã¿ï¼ˆè¨­å®šä¸å¯ï¼OFFå›ºå®šï¼‰

ğŸ”¥ Basicï¼ˆæœ‰æ–™ãƒ©ã‚¤ãƒˆç‰ˆï¼‰
ç”¨é€”

åˆå¿ƒè€…å‘ã‘ã®æœ¬å£åº§é‹ç”¨ã€‚

æœˆé¡æƒ³å®šï¼š2,480å††å‰å¾Œ
æ©Ÿèƒ½

ãƒ»æœ¬å£åº§å¯¾å¿œ
ãƒ»å˜ä¸€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°é€ä¿¡ï¼ˆBasicãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
ãƒ»AIã‚¿ãƒ–ï¼š
ã€€- KPI
ã€€- æœˆé–“ãƒªã‚¿ãƒ¼ãƒ³ç°¡æ˜“è¡¨ç¤º
ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼š
ã€€- é€±1å†å­¦ç¿’ï¼ˆæ›œæ—¥ãƒ»æ™‚åˆ»ã¯å›ºå®šï¼‰
ã€€- ON/OFFã®ã¿å¯èƒ½
ãƒ»FI/SHAPï¼šè¡¨ç¤ºãªã—

ğŸ”¥ Proï¼ˆä¸­ç´šè€…ç‰ˆï¼šä¾¿åˆ©ã ãŒâ€œã‚ã¨å°‘ã—è¶³ã‚Šãªã„â€è¨­è¨ˆï¼‰
æœˆé¡æƒ³å®šï¼š6,480å††å‰å¾Œ
ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

SHAP / åˆ†æãŒä½¿ãˆã‚‹ãŒã€ã€Œå‹ç‡ã«ç›´çµã™ã‚‹æ·±ã„è¨­å®šã€ãŒä¸è¶³ã™ã‚‹ã‚ˆã†ã«æ„å›³çš„ã«è¨­è¨ˆ

æ©Ÿèƒ½

ãƒ»è¤‡æ•°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œ
ãƒ»Feature Importanceï¼šTop20
ãƒ»SHAPï¼šTop3 ã¾ã§
ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼š
ã€€- åŒæ™‚ã« 1 ã‚¸ãƒ§ãƒ–ã®ã¿
ã€€- å†å­¦ç¿’ or è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ ã®ã©ã¡ã‚‰ã‹
ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼š
ã€€- Proå°‚ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
ã€€- ãƒªã‚¿ãƒ¼ãƒ³ï¼…ã®ã¿
ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ï¼š
ã€€- æ™‚é–“å¸¯ãƒ•ã‚£ãƒ«ã‚¿
ã€€- ATRã®å˜ç´”ã—ãã„å€¤ï¼ˆã‚ªãƒ³ã‚ªãƒ•ï¼‰
ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼š
ã€€- æ™‚é–“å¸¯ Ã— ç›¸å ´ã‚¿ã‚¤ãƒ—ã®åŸºæœ¬åˆ†æã®ã¿

â†’ ååˆ†ã«ä½¿ãˆã‚‹ãŒ â€œExpertãŒæ¬²ã—ããªã‚‹â€ ä½ç½®ä»˜ã‘

ğŸ”¥ Expertï¼ˆæœ€ä¸Šä½ç‰ˆï¼šå‹ç‡ã«ç›´çµã™ã‚‹å…¨æ©Ÿèƒ½ã‚’è£…å‚™ï¼‰
æœˆé¡æƒ³å®šï¼š8,980å††å‰å¾Œ

ï¼ˆProã¨ã®å·®ã‚’2,500å††ä»¥å†…ã«ã™ã‚‹ã®ãŒå¿ƒç†çš„ã«é‡è¦ï¼‰

ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

â€œå‹ç‡å‘ä¸Šã«é–¢ã‚ã‚‹è¨­å®šã¯ã™ã¹ã¦Expertã«é›†ç´„â€
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæœ€ã‚‚é¸ã¶éšå±¤ã‚’æ„å›³çš„ã«ä½œã‚‹ã€‚

æ©Ÿèƒ½

ãƒ»è¤‡åˆãƒ•ã‚£ãƒ«ã‚¿ï¼ˆATR Ã— ãƒœãƒ© Ã— æ™‚é–“å¸¯ Ã— ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼‰
ãƒ»é€£æ•—å›é¿ãƒ¢ãƒ¼ãƒ‰
ãƒ»å‹ç‡é«˜ã„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
ãƒ»Feature Importanceï¼šå…¨ä»¶
ãƒ»SHAPï¼šTop20ï¼ˆã¾ãŸã¯å…¨ä½“ï¼‰
ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼ˆå®Œå…¨ç‰ˆï¼‰ï¼š
ã€€- DDç›´å‰ã®ç‰¹å¾´
- é€£å‹åŒºé–“ç‰¹å¾´
- å‹ç‡ä¸Šæ˜‡ã®ç›¸å ´æ¡ä»¶æŠ½å‡º
- æç›Šæ›²ç·šã®ç•°å¸¸ç‚¹ã®æ¤œå‡º
ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼š
- è¤‡æ•°ã‚¸ãƒ§ãƒ–ï¼ˆå†å­¦ç¿’ï¼‹è¨ºæ–­ï¼‹ãƒ­ã‚°æ•´ç†ãªã©ï¼‰
- è‡ªå‹•ã‚¸ãƒ§ãƒ–é€£é–
- æœˆæ¬¡è‡ªå‹•é‹ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬
ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼š
- Expertå°‚ç”¨è¤‡åˆã‚¹ã‚³ã‚¢ï¼ˆãƒªã‚¿ãƒ¼ãƒ³Ã—DDÃ—PFï¼‰
- éå»å±¥æ­´ã®å®Œå…¨åŒæœŸ
- è‡ªå‹•é€ä¿¡ ON/OFF å¯èƒ½

â†’ å®Ÿé‹ç”¨è€…ã®ã»ã¼å…¨å“¡ãŒé¸ã¶éšå±¤ï¼ˆç‹™ã„é€šã‚Š Expert 5å‰²ã«å¯„ã‚‹ï¼‰

ğŸ”¥ Masterï¼ˆé–‹ç™ºè€…ç‰ˆï¼‰

ä¸€èˆ¬è²©å£²ãªã—ã€‚
å…¨æ©Ÿèƒ½è§£æ”¾ã€‚
ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚µãƒ¼ãƒè¨­å®šã€APIã‚­ãƒ¼ã€æ¤œè¨¼ç”¨ãƒ„ãƒ¼ãƒ«ã™ã¹ã¦ä½¿ç”¨å¯èƒ½ã€‚

5. JobSchedulerï¼ˆå†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼‰
â–  åŸºæœ¬ä»•æ§˜

ãƒ»Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ä½¿ç”¨ã—ãªã„
ãƒ»ã‚¢ãƒ—ãƒªå†…ã§
ã€€- æ™‚åˆ»ç®¡ç†
ã€€- ã‚¸ãƒ§ãƒ–ç®¡ç†
ã€€- å®Ÿè¡Œå±¥æ­´
ã€€ã‚’å®Œçµã•ã›ã‚‹

â–  v4ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³å·®åˆ†
Free

ãƒ»è¡¨ç¤ºã®ã¿ï¼æ©Ÿèƒ½ã¯ç„¡åŠ¹

Basic

ãƒ»é€±1å†å­¦ç¿’ï¼ˆå›ºå®šæ™‚åˆ»ï¼‰
ãƒ»ON/OFFã®ã¿

Pro

ãƒ»ã‚¸ãƒ§ãƒ–ã‚’1ã¤ã ã‘ç™»éŒ²å¯èƒ½
ãƒ»å†å­¦ç¿’ OR è¨ºæ–­AI ã®ã©ã¡ã‚‰ã‹ã®ã¿é¸æŠå¯
ãƒ»è©³ç´°ãƒ­ã‚°ãªã—ï¼ˆç°¡æ˜“ãƒ­ã‚°ï¼‰

Expert

ãƒ»è¤‡æ•°ã‚¸ãƒ§ãƒ–
ãƒ»ã‚¸ãƒ§ãƒ–é€£é–ï¼ˆä¾‹ï¼šå†å­¦ç¿’â†’è¨ºæ–­â†’æœ€é©ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ‡æ›¿ï¼‰
ãƒ»è©³ç´°ãƒ­ã‚°
ãƒ»æ¯é€±ï¼æ¯æ—¥ï¼æœˆæœ«å‡¦ç†ãŒå¯èƒ½
ãƒ»ãƒ­ã‚°åœ§ç¸®ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚‚ã‚¸ãƒ§ãƒ–åŒ–ã§ãã‚‹

6. AIã‚¿ãƒ–ï¼ˆKPI / Feature Importance / SHAP / è¨ºæ–­ï¼‰
â–  Pro

ãƒ»KPI
ãƒ»FI Top20
ãƒ»SHAP Top3
ãƒ»åŸºæœ¬è¨ºæ–­ï¼ˆæ™‚é–“å¸¯Ã—ç›¸å ´ã‚¿ã‚¤ãƒ—ï¼‰

â–  Expert

ãƒ»KPI
ãƒ»FI å…¨ä»¶
ãƒ»SHAP Top20
ãƒ»é«˜åº¦è¨ºæ–­
ã€€- DDç›´å‰ç‰¹æ€§
- é€£å‹åŒºé–“
- å‹ç‡é«˜ã„ç›¸å ´ç‰¹å¾´
- æç›Šæ›²ç·šã®ç•°å¸¸ç‚¹
- æ¥é€±ã®ç°¡æ˜“äºˆæ¸¬ï¼ˆExperté™å®šï¼‰

7. ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
â–  Pro

ãƒ»æ™‚é–“å¸¯ãƒ•ã‚£ãƒ«ã‚¿
ãƒ»ATR ã‚ªãƒ³ã‚ªãƒ•
ãƒ»ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿ã®ã¿

â–  Expert

ãƒ»è¤‡åˆãƒ•ã‚£ãƒ«ã‚¿ï¼ˆATR Ã— ãƒœãƒ© Ã— æ™‚é–“å¸¯ Ã— ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
ãƒ»é€£æ•—å›é¿
ãƒ»å‹ç‡é«˜ã„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•åˆ‡æ›¿
ãƒ»ãƒœãƒ©å¸¯ã”ã¨ã®æˆ¦ç•¥æœ€é©åŒ–

8. ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä»»æ„æ©Ÿèƒ½ï¼‰
â–  Basic

ãƒ»Basicãƒ©ãƒ³ã‚­ãƒ³ã‚°é€ä¿¡
ãƒ»é–²è¦§å¯

â–  Pro

ãƒ»Proãƒ©ãƒ³ã‚­ãƒ³ã‚°
ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ï¼…ã®ã¿
ãƒ»æ‰‹å‹•é€ä¿¡

â–  Expert

ãƒ»Expertè¤‡åˆã‚¹ã‚³ã‚¢
ãƒ»éå»å±¥æ­´ãŒã‚µãƒ¼ãƒã«ä¿å­˜
ãƒ»è‡ªå‹•é€ä¿¡ON/OFF
ãƒ»è¤‡åˆã‚¹ã‚³ã‚¢ï¼ˆãƒªã‚¿ãƒ¼ãƒ³Ã—DDÃ—PFï¼‰

9. æ—¥æœ¬èªä»•æ§˜

ãƒ»UIãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã™ã¹ã¦æ—¥æœ¬èª
ãƒ»ãƒ­ã‚°ã¯æ—¥æœ¬èªï¼‹ç°¡æ˜“è‹±èªä½µè¨˜
ãƒ»æ–‡å­—ã‚³ãƒ¼ãƒ‰ UTF-8ï¼ˆBOMãªã—ï¼‰

10. ä»˜éŒ²ï¼šæ¨å¥¨ä¾¡æ ¼ãƒ¢ãƒ‡ãƒ«ï¼ˆv4ï¼‰
Basicï¼š2,480å††/æœˆ  
Proï¼š6,480å††/æœˆ  
Expertï¼š8,980å††/æœˆ  
åˆæœŸè²»ç”¨ï¼ˆè²·ã„åˆ‡ã‚Šï¼‰ï¼š9,800ã€œ29,800å††  


â€»Proã¨Expertã®å·®ã‚’å°ã•ãã™ã‚‹ã“ã¨ã§
ã€€Expertæ¯”ç‡ã‚’ç´„50%ã«èª˜å°ã§ãã‚‹ã€‚


=== file: ä»•æ§˜æ›¸.txt ===

ä»•æ§˜ã®çµ±åˆã¾ã¨ã‚ï¼ˆï¼‹å®Ÿæˆ¦ã§åŠ¹ãè¿½è¨˜ï¼‰
ã‚ãªãŸã®è¦ä»¶ã¯ã€Python 3.13 ä¸»ä½“ã§ MT5 ã‚’æ“ä½œã—ã€PyQt6 GUIã§é‹ç”¨ãƒ»å­¦ç¿’ãƒ»å¯è¦–åŒ–ã¾ã§å›ã™â€œç·åˆãƒœãƒƒãƒˆâ€ã€‚ä¸‹è¨˜ã«å…¨ä½“åƒâ†’GUIâ†’AIâ†’å£²è²·åˆ¶å¾¡â†’é‹ç”¨ãƒ»ç›£è¦–â†’é–‹ç™ºä½“åˆ¶ã®é †ã§æ•´ç†ã—ã€è¶³ã‚Šãªã„ã‘ã©åŠ¹ãè¿½è¨˜ã‚’åŸ‹ã‚è¾¼ã¿ã¾ã—ãŸã€‚
________________________________________
0) å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆå½¹å‰²åˆ†é›¢ãŒå‘½ï¼‰
â€¢	UI å±¤ï¼ˆPyQt6 + PyQtGraph + matplotlibï¼‰
è¡¨ç¤ºãƒ»æ“ä½œã ã‘ã€‚å­¦ç¿’ã‚„ç™ºæ³¨ã¯Qt Worker/ThreadçµŒç”±ã§éåŒæœŸå®Ÿè¡Œã€‚
â€¢	ã‚¢ãƒ—ãƒªå±¤ï¼ˆController/Serviceï¼‰
o	TradeServiceï¼šç™ºæ³¨ãƒ»ãƒã‚¸ç®¡ç†ï¼ˆMT5 Python APIï¼‰
o	AISvcï¼šæ¨è«–ï¼ˆLightGBM/XGB/LSTM ã‚¹ã‚¿ãƒƒã‚¯ â†’ ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼‰
o	BacktestSvc / WFOSvcï¼šãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ»ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
o	RiskSvcï¼šæ—¥æ¬¡DD/é€£æ•—/ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ç›£è¦–ãƒ»ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼
â€¢	ãƒ‡ãƒ¼ã‚¿å±¤
o	DataStoreï¼šOHLCVãƒ»ç‰¹å¾´é‡ãƒ»å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã€Feature Store é¢¨ã«ç®¡ç†
o	ModelRegistryï¼šãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåå‰ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»æ¤œè¨¼æŒ‡æ¨™ï¼‰
o	LogStoreï¼šå–å¼•ãƒ»æ¨è«–ãƒ»ã‚¨ãƒ©ãƒ¼ãƒ»ç´„å®šãƒ»KPI ãƒ­ã‚°ï¼ˆCSV/Parquetï¼‰
â€¢	è¨­å®šå±¤
o	config.yamlï¼šå£åº§åˆ‡æ›¿ã€ã—ãã„å€¤ã€ãƒ­ãƒƒãƒˆã€ãƒªã‚¹ã‚¯ã€å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ã‚¿
o	Secrets ã¯ .envï¼ˆAPI ã‚­ãƒ¼é¡ã¯çµ¶å¯¾ã«ã‚³ãƒ¼ãƒ‰ç›´æ›¸ãã—ãªã„ï¼‰
è¿½è¨˜ãƒã‚¤ãƒ³ãƒˆï¼šå½¹å‰²åˆ†é›¢ã¨éåŒæœŸåŒ–ï¼ˆUI ãŒå›ºã¾ã‚‰ãªã„ï¼‰ã‚’åˆæ‰‹ã§æ±ºã‚ã‚‹ã€‚ã“ã‚ŒãŒâ€œå¾Œã‹ã‚‰ã®åœ°ç„â€ã‚’é˜²ãæœ€å¤§ã®ä¸€æ‰‹ã€‚
________________________________________
1) GUI æ§‹æˆï¼ˆã‚ãªãŸã®è¦æ±‚ï¼‹é‹ç”¨ã§ä¾¿åˆ©ãªè¿½è¨˜ï¼‰
Dashboard
â€¢	æ®‹é«˜ / æœ‰åŠ¹è¨¼æ‹ é‡‘ / å«ã¿æç›Š / ä½¿ç”¨è¨¼æ‹ é‡‘ / ä½™å‰°è¨¼æ‹ é‡‘ / ãƒã‚¸æ•° / ãƒ­ãƒƒãƒˆåˆè¨ˆ
â€¢	ä¿æœ‰ä¸€è¦§ï¼ˆBuy=é’ / Sell=èµ¤ã€SL/TPã€å»ºå€¤ã€ç¾åœ¨å€¤ã€å«ã¿æç›Šã€çµŒéæ™‚é–“ï¼‰
â€¢	è¿½è¨˜ï¼šã€Œé€šä¿¡çŠ¶æ…‹ã€ï¼ˆMT5 æ¥ç¶š/å†æ¥ç¶šå›æ•°/ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· msï¼‰ã¨**ã€Œã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰/å¹³å‡ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ã€**ã‚’ä¸Šæ®µã«å¸¸æ™‚è¡¨ç¤ºã€‚
AIã‚¿ãƒ–
â€¢	å‹ç‡ã€å¹³å‡æç›Šã€PFã€å¹³å‡ä¿æœ‰æ™‚é–“ã€æœ€å¤§DDã€é€£å‹/é€£æ•—ã€ç·å–å¼•æ•°
â€¢	æœˆé–“/é€±é–“æç›Šã‚°ãƒ©ãƒ•ã€ç›´è¿‘äºˆæ¸¬ç¢ºç‡ï¼ˆBuy/Sell ç¢ºä¿¡åº¦ï¼‰
â€¢	ãƒ¢ãƒ‡ãƒ«å/æ¤œè¨¼æŒ‡æ¨™/é‡è¦ç‰¹å¾´é‡/SHAP ä¸Šä½3 / æœ€æ–°å­¦ç¿’æ—¥æ™‚
â€¢	ä¿¡é ¼åº¦ãƒ¡ãƒ¼ã‚¿ãƒ¼ï¼ˆValidation â†’ æœ€è¿‘ã®å®Ÿé‹ç”¨ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã‚‚è¡¨ç¤ºï¼‰
â€¢	è¿½è¨˜ï¼šãƒ‡ãƒ¼ã‚¿/æ€§èƒ½ãƒ‰ãƒªãƒ•ãƒˆè­¦å‘Šç¯ï¼ˆPSI/Logloss è­¦æˆ’ã§é»„ã€å±é™ºã§èµ¤ï¼‰
Chartã‚¿ãƒ–
â€¢	ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ã€ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã€æ™‚é–“å¸¯Ã—æ—¥ä»˜ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
â€¢	AI å‡ºåŠ›ç¢ºç‡ã®æ™‚ç³»åˆ—ã¨æŒ‡æ¨™ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼ˆSMA/RSI/ATRï¼‰
â€¢	è¿½è¨˜ï¼šã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ»å–å¼•å‰å¾Œã®ãƒœãƒ©åˆ†å¸ƒ
Controlã‚¿ãƒ–
â€¢	å–å¼• ON/OFFã€ãƒ­ãƒƒãƒˆã€ç¢ºä¿¡åº¦ã—ãã„å€¤ã€å›ºå®š SL/TPã€å†å­¦ç¿’ã€ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
â€¢	ãƒªã‚¹ã‚¯ãƒ‘ãƒ©ãƒ¡ã‚¿è¡¨ç¤ºï¼ˆMAX_STAKE, DD_STOP, TREND_THRESHOLDâ€¦ï¼‰
â€¢	è¿½è¨˜ï¼šã€Œç·Šæ€¥åœæ­¢ï¼ˆå…¨æ±ºæ¸ˆï¼‹åœæ­¢ï¼‰ã€ã€**ã€Œä¿å®ˆãƒ¢ãƒ‡ãƒ«ã«åˆ‡æ›¿ã€**ãƒœã‚¿ãƒ³
Logã‚¿ãƒ–
â€¢	å–å¼•å±¥æ­´ãƒ»ç™ºæ³¨/ç´„å®šãƒ­ã‚°ãƒ»ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆDD/é€£æ•—/æ»‘ã‚Š/æ¥ç¶šæ–­ï¼‰
ãƒ ãƒ¼ãƒ‰è¡¨ç¤º
â€¢	ã€Œé™ç©ã€ã€Œãƒœãƒ©æ€¥ä¸Šæ˜‡ã€ã€Œåè»¢è­¦æˆ’ä¸­ã€ãªã©
â€¢	è¿½è¨˜ï¼šæ ¹æ‹ ã®ãƒŸãƒ‹å¹ãå‡ºã—ï¼ˆä¾‹ï¼šH1 ADX>25 ï¼‹ ATRâ†‘ï¼‰
________________________________________
2) AI æˆ¦ç•¥ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ï¼ˆã‚ãªãŸã®ä¸‰æ®µæ§‹ãˆï¼‹é‹ç”¨æœ€é©åŒ–ï¼‰
ãƒ•ã‚§ãƒ¼ã‚º1ï¼šM15ï¼‹H1ï¼ˆåœ°åˆã„ãƒ¢ãƒ‡ãƒ«ï¼‰
â€¢	ã¾ãšã¯å›ºå®š TP=15, SL=10ã§æ–¹å‘å­¦ç¿’ã®å®‰å®šæ€§ã‚’ä½œã‚‹ã€‚
â€¢	LightGBM/XGBoost/LSTM ã‚’åˆ¥ã€…ã«å­¦ç¿’â†’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§çµ±åˆã€‚
â€¢	è¿½è¨˜ï¼šç›®çš„é–¢æ•°ã¯ Logloss + å®Ÿé‹ç”¨ã§ã®æœŸå¾…å€¤ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰ãƒ¢ãƒ‹ã‚¿ã€‚
ãƒ•ã‚§ãƒ¼ã‚º2ï¼šM5 è¿½åŠ ï¼ˆã‚¿ã‚¤ãƒŸãƒ³ã‚°è£œå®Œï¼‰
â€¢	M5 ã®â€œç¬é–“æƒ…å ±â€ï¼ˆçŸ­æœŸ EMA å‚¾ãã€ãƒ’ã‚²æ¯”ç‡ã€ç›´è¿‘ ATRã€ãƒœãƒ©åŠ é€Ÿï¼‰ã‚’è¿½åŠ ã€‚
â€¢	è¿½è¨˜ï¼š**ç›´è¿‘ N ãƒˆãƒ¬ãƒ¼ãƒ‰ã®äºˆæ¸¬è¼ƒæ­£ï¼ˆPlatt/Isotonicï¼‰**ã‚’é€±æ¬¡ã§æ›´æ–°ã€‚
ãƒ•ã‚§ãƒ¼ã‚º3ï¼šã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°çµ±åˆï¼ˆM15ï¼‹M5ï¼‹H1ï¼‰
â€¢	å„ãƒ¢ãƒ‡ãƒ«ã®ç¢ºç‡/å›å¸° pips äºˆæ¸¬â†’LightGBM or ãƒ­ã‚¸å›å¸°ã§æœ€çµ‚åˆ¤å®šã€‚
â€¢	å‡ºå£ï¼šLightGBM å›å¸° pred_pipsï¼‹LSTMã€Œå‹¢ã„ã€â†’TP/SL/ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°è‡ªå‹•èª¿æ•´ã€‚
ãƒ‰ãƒªãƒ•ãƒˆ&å¥å…¨æ€§
â€¢	ç‰¹å¾´é‡ãƒ‰ãƒªãƒ•ãƒˆï¼ˆPSI, KSï¼‰ã€æ€§èƒ½ãƒ‰ãƒªãƒ•ãƒˆï¼ˆç§»å‹•çª“ Logloss/Brrierï¼‰
â€¢	ã—ãã„å€¤æœ€é©åŒ–ï¼šæ¯é€±æœ«ã« ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆp_buy é–¾å€¤ / ATR ä¿‚æ•° / ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹BPï¼‰
â€¢	ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼šè¤‡æ•°ã®å‡ºå£ãƒãƒªã‚·ãƒ¼ã‚’å°ãƒ­ãƒƒãƒˆã§ä¸¦èµ°ã—ã€è‰¯ã„æ–¹ã«é‡ã¿ä»˜ã‘ã€‚
________________________________________
3) å£²è²·åˆ¶å¾¡ï¼ˆç´„å®šã®â€œå …ç‰¢åŒ–â€ãŒè‚ï¼‰
â€¢	ãƒ­ãƒƒãƒˆè¨ˆç®—ï¼ˆã‚ãªãŸã®ãƒ«ãƒ¼ãƒ«ï¼‰ï¼šæœ‰åŠ¹è¨¼æ‹ é‡‘ 1 ä¸‡å††ã‚ãŸã‚Š 0.01 ãƒ­ãƒƒãƒˆ
ä¾‹ï¼š20,000 å†† â†’ 0.02 lotï¼ˆmin_lot, lot_stepã‚’è€ƒæ…®ã€ä¸Šé™ max_lotï¼‰
â€¢	æœ€å¤§ä¿æœ‰ãƒã‚¸ã‚·ãƒ§ãƒ³æ•°ï¼šmax_positions å³å®ˆï¼ˆéŠ˜æŸ„å˜ä½ / å…¨ä½“å˜ä½ã®ä¸¡æ–¹ï¼‰
o	ç™ºæ³¨å‰ã«å£åº§ã¨éŠ˜æŸ„ã®æœªæ±ºæ¸ˆä»¶æ•°/ãƒã‚±ãƒƒãƒˆã‚’å†å–å¾—ï¼ˆäºŒé‡ç™ºæ³¨é˜²æ­¢ï¼‰
o	ãƒŸãƒ¥ãƒ¼ãƒ†ãƒƒã‚¯ã‚¹ã§åŒæ™‚ç™ºæ³¨ã®ç«¶åˆã‚’é®æ–­
o	â€œç´„å®šç¢ºèªã®å†è©¦è¡Œâ€ï¼ˆä¸€æ™‚ã‚¨ãƒ©ãƒ¼/ãƒªã‚¯ã‚ªãƒ¼ãƒˆ/æ¥ç¶šæºã‚Œï¼‰
â€¢	ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼
o	æ—¥æ¬¡ DD% è¶…é / é€£æ•— N è¶… / ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸æ€¥å¢— â†’ å–å¼•åœæ­¢ï¼‹ã‚¢ãƒ©ãƒ¼ãƒˆ
o	å†é–‹æ¡ä»¶ã‚’æ˜è¨˜ï¼ˆä¾‹ï¼š24h ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‹ç§»å‹•çª“ PF>1.05 ãªã©ï¼‰
è¿½è¨˜ï¼šå®Ÿé‹ç”¨ã§åŠ¹ã3ç‚¹
1.	ç™ºæ³¨ãƒ‡ãƒ‡ãƒ¥ãƒ¼ãƒ—ï¼ˆåŒä¸€ç§’ã®åŒä¸€ã‚·ã‚°ãƒŠãƒ«ã« UUID ã‚’ä»˜ä¸ã—ã¦1å›ã®ã¿ï¼‰
2.	ãƒã‚¸éå¤šã‚¬ãƒ¼ãƒ‰ï¼ˆâ€œUI ã¨åˆ¥ã«â€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ç›£è¦–ãŒ max_positions ã‚’è¶…ãˆãŸã‚‰å¼·åˆ¶ã‚¯ãƒ­ãƒ¼ã‚º or ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰
3.	ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰/ãƒœãƒ©éå¤§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆé–¾å€¤ä»¥ä¸Šã§æ–°è¦ã‚¨ãƒ³ãƒˆãƒªãƒ¼ç¦æ­¢ï¼‰
________________________________________
4) é‹ç”¨ãƒ»ç›£è¦–ãƒ»è‡ªå‹•å†å­¦ç¿’ï¼ˆVPS 2GB å‰æã®ç¾å®Ÿè§£ï¼‰
â€¢	ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼ˆé€±æœ«éå–å¼•æ™‚é–“ã«ï¼‰ï¼š 
1.	ãƒ‡ãƒ¼ã‚¿åŒæœŸ â†’ 2) ç‰¹å¾´é‡æ›´æ–° â†’ 3) å­¦ç¿’ â†’ 4) WFO â†’ 5) ã—ãã„å€¤æœ€é©åŒ– â†’ 6) æ¤œè¨¼ã«é€šã£ãŸãƒ¢ãƒ‡ãƒ«ã ã‘ Promote
â€¢	è»½é‡åŒ–ï¼ˆ2GB å¯¾ç­–ï¼‰ 
o	LightGBM/XGB ã¯ num_leaves/ max_depth/ n_estimators ã‚’æ§ãˆç›®ã€n_jobs=1
o	LSTM ã¯å°å‹ï¼ˆå±¤æµ…ã‚ãƒ»ãƒ¦ãƒ‹ãƒƒãƒˆå°‘ãªã‚ï¼‰ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·çŸ­ã‚
o	å­¦ç¿’ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§æ®µéšçš„ã€‚GUI ã¯æ¨è«–ã®ã¿å¸¸æ™‚ã€‚
â€¢	å†æ¥ç¶š/ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ 
o	MT5 æ¥ç¶š watchdogï¼ˆPingã€initialize å†è©¦è¡Œã€ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼åˆ‡æ›¿ï¼‰
o	ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯/æ›¸è¾¼æ¨©é™/OneDrive ç«¶åˆã®æ¤œçŸ¥
â€¢	ãƒ­ã‚°é‹ç”¨ 
o	å–å¼•ãƒ»æ¨è«–ãƒ»ç™ºæ³¨ãƒ»ç´„å®šãƒ»ã‚¨ãƒ©ãƒ¼ãƒ»KPI ã‚’åˆ†é›¢ãƒ­ã‚°
o	ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚µã‚¤ã‚º/æ—¥æ¬¡ï¼‰ï¼‹CSVâ†’Parquet å¤‰æ›ã§é«˜é€Ÿé›†è¨ˆ
________________________________________
5) ãƒªã‚¹ã‚¯ãƒ»KPI ã®SLOï¼ˆä¸‹é™ç·šã‚’æœ€åˆã«å¼•ãï¼‰
â€¢	SLO ä¸€ä¾‹
o	ç›´è¿‘ 100 å–å¼•ã§ PF < 1.05 ã‹ã¤ å‹ç‡ < 52% â†’ ç¸®å°é‹è»¢ï¼ˆãƒ­ãƒƒãƒˆ 1/2ï¼‰
o	1 é€±é–“ DD > 5% â†’ å–å¼•åœæ­¢ï¼‹ã€ŒåŸå› åˆ†æã‚¸ãƒ§ãƒ–ã€è‡ªå‹•å®Ÿè¡Œ
o	å¹³å‡ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ > ç›®æ¨™ã® 2 å€ â†’ æ–°è¦åœæ­¢ï¼ˆæ±ºæ¸ˆã®ã¿è¨±å¯ï¼‰
â€¢	KPI å®šæœŸãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
å‹ç‡ / PF / æœŸå¾…å€¤ / æœ€å¤§DD / ç´„å®šæ‹’å¦ç‡ / æ‰‹æ•°æ–™æ¯”ç‡ / æ™‚é–“å¸¯åˆ¥æˆç¸¾ / é€šè²¨åˆ¥æˆç¸¾ / ãƒ‰ãƒªãƒ•ãƒˆã‚¹ã‚³ã‚¢
________________________________________
6) å£åº§åˆ‡æ›¿ãƒ»ç’°å¢ƒå·®åˆ†ãƒ»ç§˜å¯†ç®¡ç†
â€¢	æœ¬ç•ª / ãƒ‡ãƒ¢ã¯ config.yaml ã® account.profile ã§å³åˆ‡æ›¿ï¼ˆæ¥ç¶šå…ˆ/å£åº§ID/ã‚µãƒ¼ãƒãƒ¼ï¼‰
â€¢	ç’°å¢ƒè‡ªå‹•èªè­˜ï¼ˆPC A / B / VPSï¼‰ï¼šHOSTNAME ã§ãƒ‘ã‚¹ã¨ MT5 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«åã‚’åˆ‡æ›¿
â€¢	.env ã«ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±/ãƒ‘ã‚¹/ãƒˆãƒ¼ã‚¯ãƒ³ã€Git ã«ã¯çµ¶å¯¾ã«ä¸Šã’ãªã„
________________________________________
7) å‚è€ƒãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆï¼ˆæœ€åˆã‹ã‚‰â€œè‚²ã¤â€é…ç½®ï¼‰
fxbot/
  app/
    gui/                # PyQt6 ç”»é¢ï¼ˆTabs/Widgets/Workersï¼‰
    services/           # TradeSvc, AISvc, RiskSvc, BacktestSvc, WFOSvc
    data/               # DataStore, Feature pipelines
    models/             # ModelRegistry, loaders, metrics
    core/               # utils, config loader, logging, schedulers
  configs/
    config.yaml         # å£åº§/ãƒªã‚¹ã‚¯/å­¦ç¿’/æœ€é©åŒ–/GUI ã—ãã„å€¤
    env.example         # å¿…è¦ãª .env ã‚­ãƒ¼ä¸€è¦§
  notebooks/            # å®Ÿé¨“ç”¨ï¼ˆVPSå¤–ã§ã‚‚OKï¼‰
  logs/                 # trade/*.csv, errors/*.log, metrics/*.csv
  models_store/         # *.pkl, *.onnx + metadata.json
  scripts/              # train.py, make_features.py, backtest.py, wfo.py
  tests/                # å˜ä½“/çµ±åˆãƒ†ã‚¹ãƒˆ

________________________________________
8) config.yaml é››å½¢ï¼ˆè¦ç‚¹ãŒå…¨éƒ¨å…¥ã‚‹ç‰ˆï¼‰
account:
  profile: "demo"            # demo | live
  server:
    demo:  "OANDA-Demo"
    live:  "OANDA-Live"
  login_env_key: "MT5_LOGIN"
  password_env_key: "MT5_PASSWORD"

runtime:
  timezone: "Asia/Tokyo"
  symbol: "USDJPY"
  timeframe_exec: "M5"
  max_positions: 1
  spread_limit: 0.015   # 1.5pips
  slip_limit: 0.015     # 1.5pips

lot:
  base_equity_per_0p01: 10000   # 1ä¸‡å††ã§0.01lot
  min_lot: 0.01
  max_lot: 1.00
  lot_step: 0.01

risk:
  daily_loss_stop: 0.05   # 5%
  dd_stop: 0.10           # 10%
  max_consecutive_losses: 5
  emergency_close_all: true

entry:
  threshold_buy: 0.60
  threshold_sell: 0.60
  holdout_minutes_after_news: 15

exit:
  mode: "fixed"           # fixed | atr | ai
  tp_pips: 15
  sl_pips: 10
  atr_period: 14
  atr_k_tp: 1.5
  atr_k_sl: 1.0
  trail_start_pips: 8
  trail_step_pips: 3

ai:
  stacking: true
  models:
    - name: "lgbm_cls"
    - name: "xgb_cls"
    - name: "lstm_seq"
  meta_model: "lgbm_meta"
  features:
    base: ["ema_5","ema_20","rsi_14","atr_14","adx_14","bbp","vol_chg","wick_ratio"]
    m15_h1: true
    m5_boost: true
  labels:
    type: "direction_10bars"   # or "pips_regression_10bars"
  retrain:
    schedule_cron: "sat 03:00"
    wfo: { enabled: true, window_days: 90, step_days: 7 }
    bayes_opt: { enabled: true, iters: 50 }

monitor:
  kpi_window_trades: 100
  slo:
    pf_min: 1.05
    winrate_min: 0.52
    weekly_dd_max: 0.05

paths:
  data_dir: "./data"
  logs_dir: "./logs"
  models_dir: "./models_store"

________________________________________
9) å£²è²·ãƒ•ãƒ­ãƒ¼ï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã®è¦ï¼‰
on_tick():
  if circuit_breaker.tripped(): return
  if spread > spread_limit: return
  if positions.count(symbol) >= max_positions: return

  p_buy, p_sell, pred_pips = ai.predict(features_now)

  signal = decide(p_buy, p_sell, thresholds)
  if not signal: return

  lot = calc_lot(equity, base_equity_per_0p01, min_lot, max_lot, lot_step)

  sl, tp = exit_policy(signal, pred_pips, atr, config.exit)

  ticket = trade_service.place_order_once(symbol, signal, lot, sl, tp)
  log_trade(signal, lot, sl, tp, p_buy, p_sell, pred_pips, ticket)

å …ç‰¢åŒ–ãƒ•ãƒƒã‚¯ï¼šplace_order_once() å†…ã§
â€¢	ãƒŸãƒ¥ãƒ¼ãƒ†ãƒƒã‚¯ã‚¹ / ç›´å‰é‡è¤‡ãƒã‚§ãƒƒã‚¯ / ç´„å®šç¢ºèª / ä¾‹å¤–â†’å†è©¦è¡Œï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
â€¢	ãã‚Œã§ã‚‚å¤±æ•— â†’ GUI ã«ã‚¢ãƒ©ãƒ¼ãƒˆï¼‹ãƒ­ã‚°
________________________________________
10) ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ & WFOï¼ˆç¾å®Ÿçš„ãªæ¤œè¨¼ï¼‰
â€¢	æœŸé–“ï¼šç›´è¿‘ 2â€“3 å¹´ã‚’åŸºæœ¬ã€ãƒ­ãƒ¼ãƒ«ãƒ³ã‚° WFOï¼ˆä¾‹ï¼š90 æ—¥å­¦ç¿’â†’7 æ—¥å‰é€²ï¼‰
â€¢	æŒ‡æ¨™ï¼šå®Ÿé‹ç”¨ KPI å„ªå…ˆï¼ˆPFã€æœŸå¾…å€¤ã€æœ€å¤§DDã€ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸è¾¼ã¿ï¼‰
â€¢	æ¯”è¼ƒï¼šå›ºå®šTP/SL vs ATRé€£å‹• vs AIå‡ºå£ã€A/B ãƒ©ãƒ³ã§é€±æ¬¡æ›´æ–°
________________________________________
11) é–‹ç™ºãƒ»é‹ç”¨ã®å°æŠ€ï¼ˆåŠ¹ãã‚„ã¤ï¼‰
â€¢	å†ç¾æ€§ï¼šä¹±æ•°ç¨®å›ºå®šã€ãƒ¢ãƒ‡ãƒ«ã¨å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒŸãƒƒãƒˆIDã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
â€¢	ãƒ¢ãƒ‡ãƒ«å‘½åè¦å‰‡ï¼šLightGBM_YYYYMMDD_HHMM_foldK.pkl
â€¢	ãƒ†ã‚¹ãƒˆï¼šãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆãƒ­ãƒƒãƒˆè¨ˆç®—/ã—ãã„å€¤åˆ¤å®šï¼‰ï¼‹ç–‘ä¼¼ç´„å®šãƒ†ã‚¹ãƒˆ
â€¢	éšœå®³è€æ€§ï¼šãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ï¼ˆWindows ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© or NSSMï¼‰ã€ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã«è‡ªå‹•å†èµ·å‹•
â€¢	å¯æ¬æ€§ï¼šä¾å­˜ã¯ requirements.txt å›ºå®šã€VPS ã¯ä»®æƒ³ç’°å¢ƒé‹ç”¨
________________________________________
12) æœ€åˆã®å®Ÿè£…æ‰‹é †ï¼ˆ1ï½2 æ—¥ã§â€œå‹•ãèŠ¯â€ã¾ã§ï¼‰
1.	ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã¨ config.yaml ã‚’ä½œã‚‹
2.	MT5 æ¥ç¶šï¼‹ãƒ†ã‚£ãƒƒã‚¯è³¼èª­ï¼‹å£åº§æƒ…å ±ã®å–å¾—ï¼ˆGUI Dashboard ã«ç”Ÿè¡¨ç¤ºï¼‰
3.	å›ºå®šãƒ«ãƒ¼ãƒ«ã®å£²è²·ï¼ˆTP=15/SL=10ã€ã—ãã„å€¤ã¯ãƒ€ãƒŸãƒ¼ 0.6ï¼‰ã§ç´„å®šï½ãƒ­ã‚°ã¾ã§ä¸€æ°—é€šã—
4.	AI æ¨è«–ï¼ˆéå»å­¦ç¿’æ¸ˆã¿ã®è»½é‡ LightGBM ä¸€ã¤ï¼‰ã‚’æŒ¿å…¥
5.	ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚¬ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
6.	å­¦ç¿’ãƒ»WFOãƒ»ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¯é€±æœ«ãƒãƒƒãƒã¨ã—ã¦æœ€å¾Œã«æ¥ç¶š
________________________________________
ä»˜ã‘åŠ ãˆãŸä¸»ãªãƒã‚¤ãƒ³ãƒˆï¼ˆè¦ç´„ï¼‰
â€¢	éåŒæœŸï¼†å½¹å‰²åˆ†é›¢ã®æ˜æ–‡åŒ–ï¼ˆGUI ãŒå›ºã¾ã‚‰ãªã„ï¼‰
â€¢	ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰/ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸åˆ¶å¾¡
â€¢	ç™ºæ³¨ã®ãƒ‡ãƒ‡ãƒ¥ãƒ¼ãƒ— / ç«¶åˆé˜²æ­¢ / å†è©¦è¡Œã§â€œäºŒé‡å»ºã¦ãƒ»å–ã‚Šã“ã¼ã—â€ã‚’æ’²æ»…
â€¢	ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥ãƒ»ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå‡ºå£ã§é‹ç”¨ä¸­ã®é©å¿œæ€§ã‚’ç¢ºä¿
â€¢	VPS 2GB å‘ã‘ã®è»½é‡åŒ–æ–¹é‡ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ»ä¸¦åˆ—ãƒ»å­¦ç¿’ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼‰
â€¢	ãƒ¢ãƒ‡ãƒ«/ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã¨å†ç¾æ€§ã®ç¢ºä¿
________________________________________
ã“ã®è¨­è¨ˆã«è½ã¨ã—è¾¼ã‚ã°ã€**â€œå‹•ãæœ€å°æ§‹æˆâ€â†’â€œè³¢ãè‚²ã¤æ§‹æˆâ€**ã¸æ»‘ã‚‰ã‹ã«æ‹¡å¼µã§ãã¾ã™ã€‚æ¬¡ã¯ã€é››å½¢ã‚³ãƒ¼ãƒ‰ï¼ˆGUI éª¨æ ¼ï¼‹TradeService ã‚¹ã‚¿ãƒ–ï¼‹å›ºå®šTP/SLã®ç™ºæ³¨ï¼‰ã‚’ä¸€æ°—ã«ç½®ã„ã¦ã€GUI ä¸Šã§Balance/Equity ã¨ç™ºæ³¨ãƒœã‚¿ãƒ³ãŒç”Ÿãã¦ã‚‹çŠ¶æ…‹ã¾ã§ä½œã‚Šè¾¼ã¿ã¾ã—ã‚‡ã†ã€‚




=== file: ä»•æ§˜æ›¸v3.txt ===

ï¼ˆå†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼‹æ—¥æœ¬èªç‰ˆï¼‹ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æ§‹æˆï¼‰
0) ç›®çš„ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

ç›®çš„ï¼š

MT5 + PyQt6 + AIãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBM ä»–ï¼‰ã‚’åˆ©ç”¨ã—ã€

æœˆé–“3ï¼…å‰å¾Œã®ãƒªã‚¿ãƒ¼ãƒ³ã‚’ã€Œç›®æŒ‡ã™ã€ è‡ªå‹•FXå£²è²·ãƒœãƒƒãƒˆã‚’æ§‹ç¯‰ãƒ»é‹ç”¨ã™ã‚‹ã€‚

3ï¼…/æœˆã¯é”æˆç¾©å‹™ã§ã¯ãªã KPIï¼ˆç›®æ¨™æŒ‡æ¨™ï¼‰ ã¨ã—ã¦æ‰±ã†ã€‚

åŒæ™‚ã«ã€æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ï¼ˆè³‡ç”£ãƒ”ãƒ¼ã‚¯æ¯”ï¼‰ -15ã€œ-20ï¼…ä»¥å†… ã‚’å®‰å…¨ãƒ©ã‚¤ãƒ³ã¨ã™ã‚‹ã€‚

ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼š

Windowsãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆæ—¥æœ¬èªç’°å¢ƒã®å€‹äººãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ï¼‰

MT5ã‚’ä½¿ã£ãŸè£é‡ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã€åŠè‡ªå‹•ã€œè‡ªå‹•ã«ç§»è¡Œã—ãŸã„å±¤

EAã¯è§¦ã£ãŸã“ã¨ãŒã‚ã‚‹ãŒã€ŒAIï¼‹çµ±è¨ˆï¼‹å¯è¦–åŒ–ã€ã§é‹ç”¨ã‚’æ”¹å–„ã—ãŸã„äºº

éæ©Ÿèƒ½çš„è¦ä»¶ï¼š

Windows 10 / 11 (64bit)

æ—¥æœ¬èªUIï¼ˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ»ãƒ­ã‚°ï¼‰

Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ä¾å­˜ã—ãªã„
â†’ ã™ã¹ã¦ã®å®šæœŸå®Ÿè¡Œã¯ å†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© ã§è¡Œã†ã€‚

1) å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

3ãƒ¬ã‚¤ãƒ¤æ§‹æˆï¼š

GUIå±¤ï¼ˆapp.guiï¼‰ â€¦ PyQt6ãƒ™ãƒ¼ã‚¹ã®ç”»é¢

ã‚µãƒ¼ãƒ“ã‚¹å±¤ï¼ˆapp.servicesï¼‰ â€¦ AIã‚µãƒ¼ãƒ“ã‚¹ / KPIè¨ˆç®— / ãƒ­ã‚° / ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© / ãƒ©ãƒ³ã‚­ãƒ³ã‚° etc.

ã‚³ã‚¢å±¤ï¼ˆapp.coreï¼‰ â€¦ MT5ãƒ©ãƒƒãƒ‘ãƒ¼ã€æˆ¦ç•¥ãƒ­ã‚¸ãƒƒã‚¯ã€ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³

å®Ÿè¡Œå½¢æ…‹ï¼š

é€šå¸¸ã¯ GUIã‚¢ãƒ—ãƒªã¨ã—ã¦èµ·å‹•ã€‚

å°†æ¥çš„ã«ã€Œå¸¸é§ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¹ã‚¯ãƒˆãƒ¬ã‚¤å¸¸é§ã®ã¿ï¼‰ã€ã«ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã€ã‚µãƒ¼ãƒ“ã‚¹å±¤ã‚’GUIã«å¼·ãä¾å­˜ã•ã›ãªã„è¨­è¨ˆã«ã™ã‚‹ã€‚

è¨­å®šãƒ»ãƒ‡ãƒ¼ã‚¿ï¼š

å…±é€šè¨­å®šï¼šconfig.yaml + config.local.yaml

ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼šmodels/

ãƒ­ã‚°ï¼šlogs/

æ±ºå®šãƒ­ã‚°ï¼šlogs/decisions_*.jsonl

ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœï¼šbacktests/{profile}/...

2) ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æ§‹æˆï¼ˆFree / Basic / Pro / Expert / Masterï¼‰
å…±é€šä»•æ§˜

ã™ã¹ã¦ã®ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã§ åŒä¸€ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ« ã‚’ä½¿ã†ã€‚

æ©Ÿèƒ½å·®ã¯ã€Œã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ï¼ˆedition: free/basic/pro/expert/masterï¼‰ã€ã§åˆ¶å¾¡ã™ã‚‹ã€‚

Edition æƒ…å ±ã¯

ãƒ©ã‚¤ã‚»ãƒ³ã‚¹èªè¨¼

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

ã¾ãŸã¯ãƒ“ãƒ«ãƒ‰æ™‚ã«åŸ‹ã‚è¾¼ã¿
ã®ã„ãšã‚Œã‹ã§æ±ºã¾ã‚‹ã€‚

fxbot Freeï¼ˆç„¡ç†ã®ãŠè©¦ã—ç‰ˆï¼‰

æ¥ç¶šå¯èƒ½å£åº§ï¼šãƒ‡ãƒ¢å£åº§ã®ã¿

è‡ªå‹•å£²è²·ï¼šOKï¼ˆãŸã ã—ãƒ­ãƒƒãƒˆä¸Šé™å°ï¼‰

ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼šOKï¼ˆæœŸé–“åˆ¶é™ãªã—ï¼‰

å†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼š

ç”»é¢ä¸Šã«ã¯è¡¨ç¤ºã™ã‚‹ãŒã€å¸¸ã«ã€ŒOFFå›ºå®šã€ï¼ˆè¨­å®šä¸å¯ï¼‰

AIã‚¿ãƒ–ï¼š

KPIã®åŸºæœ¬é …ç›®ã®ã¿è¡¨ç¤ºï¼ˆä»Šæœˆæç›Šï¼…ã€å‹ç‡ã€PFãªã©ï¼‰

Feature Importance / SHAP ã¯éè¡¨ç¤º

ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼š

ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°é–²è¦§ã®ã¿ï¼ˆè‡ªåˆ†ã®æˆç¸¾é€ä¿¡ã¯ä¸å¯ï¼‰

fxbot Basic

æ¥ç¶šå¯èƒ½å£åº§ï¼šãƒ‡ãƒ¢ï¼‹æœ¬å£åº§

è‡ªå‹•å£²è²·ï¼š

å˜ä¸€ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã§ç¨¼åƒ

å†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼š

é€±1å› å†å­¦ç¿’ï¼†WFO ã‚’ON/OFFã§åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ï¼ˆæ›œæ—¥ãƒ»æ™‚åˆ»ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ã¿å¤‰æ›´ä¸å¯ï¼‰

AIã‚¿ãƒ–ï¼š

KPI + æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ç°¡æ˜“è¡¨ç¤º

ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼š

Basicã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°é–²è¦§

ã€Œä»Šæœˆã®æˆç¸¾ã‚’é€ä¿¡ã€ãƒœã‚¿ãƒ³æœ‰åŠ¹

fxbot Pro

Basicã®ã™ã¹ã¦ï¼‹

ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ï¼š

è¤‡æ•°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸ ç­‰ï¼‰ã®åˆ‡ã‚Šæ›¿ãˆ

AIã‚¿ãƒ–ï¼š

Feature Importance

SHAPä¸Šä½ç‰¹å¾´ï¼ˆTop Nï¼‰

å†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼š

è¤‡æ•°ã‚¸ãƒ§ãƒ–ï¼ˆå†å­¦ç¿’ï¼è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆï¼‰ãŒç™»éŒ²å¯èƒ½

æ›œæ—¥ãƒ»æ™‚åˆ»ã®ç·¨é›†å¯èƒ½

ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼š

v0ï¼ˆæ™‚é–“å¸¯åˆ¥ãƒ»ç›¸å ´æ¡ä»¶åˆ¥é›†è¨ˆï¼‰ã®ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›

ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼š

Proå°‚ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°

è‡ªåˆ†ã®éå»ãƒ©ãƒ³ã‚­ãƒ³ã‚°å±¥æ­´ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ï¼‰ã‚‚è¡¨ç¤º

fxbot Expert

Proã®ã™ã¹ã¦ï¼‹

é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿è¨­å®šï¼ˆATRãƒ»æ™‚é–“å¸¯ãƒ»ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å¸¯ãªã©ï¼‰ã‚’GUIã‹ã‚‰ç·¨é›†å¯

ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼š

ä»»æ„Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå†…éƒ¨å®šç¾©ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–ï¼‰ã®çµ„åˆã›ã§å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚’ä½œæˆå¯èƒ½

ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­AIï¼š

è¿½åŠ ã®åˆ‡ã‚Šå£ï¼ˆä¾‹ï¼šé€£æ•—ä¸­ã®ã¿æŠ½å‡ºã€DDç›´å‰ã®ç‰¹å¾´ãªã©ï¼‰

ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼š

Expertå°‚ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ã¨DDã‚’è€ƒæ…®ã—ãŸè¤‡åˆã‚¹ã‚³ã‚¢ãªã©ï¼‰

fxbot Masterï¼ˆé–‹ç™ºè€…ãƒ»è‡ªåˆ†å°‚ç”¨ï¼‰

ä¸€èˆ¬è²©å£²ãªã—ã€‚

ã™ã¹ã¦ã®æ©Ÿèƒ½ã‚’åˆ¶é™ãªã—ã§åˆ©ç”¨å¯èƒ½ã€‚

è¿½åŠ ï¼š

ãƒ©ãƒ³ã‚­ãƒ³ã‚°é€ä¿¡å…ˆURLã€APIã‚­ãƒ¼ã€ãƒ†ã‚¹ãƒˆé€ä¿¡ãªã©ã®é–‹ç™ºç”¨ãƒ„ãƒ¼ãƒ«

å†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã«ä»»æ„ã‚¸ãƒ§ãƒ–ã‚’ç™»éŒ²å¯èƒ½ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆä¸€æ‹¬å®Ÿè¡Œã€ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãªã©ï¼‰

3) å†…è”µã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ä»•æ§˜ï¼ˆJobSchedulerï¼‰
3-1. åŸºæœ¬æ–¹é‡

Windowsã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯ä½¿ã‚ãªã„ã€‚

ã‚¢ãƒ—ãƒªå†…éƒ¨ã« JobSchedulerï¼ˆå¸¸é§ã‚µãƒ¼ãƒ“ã‚¹ï¼‰ã‚’æŒã¡ã€

æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ã«ãªã£ãŸã‚‰

æŒ‡å®šã•ã‚ŒãŸã€Œã‚¸ãƒ§ãƒ–ã€ã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¾ãŸã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã€‚

GUIãŒèµ·å‹•ã—ã¦ã„ã‚‹é–“ã®ã¿å‹•ãã®ã‹ã€ãƒˆãƒ¬ã‚¤å¸¸é§ã«ã™ã‚‹ã®ã‹ã¯æ®µéšçš„ã«æ‹¡å¼µï¼š

v1ï¼šGUIèµ·å‹•ä¸­ã®ã¿å‹•ä½œï¼ˆé€±æœ«ã¯PCï¼‹ã‚¢ãƒ—ãƒªèµ·å‹•ãŒå‰æï¼‰

v2ï¼šã‚¿ã‚¹ã‚¯ãƒˆãƒ¬ã‚¤å¸¸é§ãƒ¢ãƒ¼ãƒ‰ã§ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã¦ã‚‚ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒå‹•ãç¶šã‘ã‚‹ï¼ˆExpert / Masterå‘ã‘ï¼‰

3-2. ã‚¸ãƒ§ãƒ–ã®ç¨®é¡ï¼ˆv1ï¼‰

å†å­¦ç¿’ï¼‹Walk-Forward æ¤œè¨¼ï¼š

python -m scripts.walkforward_retrain --profile ...

ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼š

python -m scripts.diagnose_backtest --profile ...

å°†æ¥ï¼š

ãƒ­ã‚°åœ§ç¸®ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãªã©

3-3. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¡¨ç¾

config.scheduler.yaml ã¾ãŸã¯ config.yaml ã® scheduler ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¨˜è¿°ï¼š

scheduler:
  enabled: true
  jobs:
    - id: weekly_retrain
      enabled: true
      edition_min: basic
      weekday: 5        # 0=æœˆ, 6=æ—¥
      hour: 3
      minute: 0
      command: "python -m scripts.walkforward_retrain --profile default"
    - id: weekly_report
      enabled: true
      edition_min: pro
      weekday: 6
      hour: 8
      minute: 0
      command: "python -m scripts.diagnose_backtest --profile default"


Proä»¥ä¸Šã§ã¯ GUI ã‹ã‚‰ç·¨é›†å¯èƒ½ã€‚

Basic ã§ã¯ weekly_retrain ON/OFF ã ã‘å¯èƒ½ï¼ˆæ›œæ—¥ãƒ»æ™‚åˆ»ã¯å›ºå®šï¼‰ã€‚

3-4. JobScheduler ã®å‹•ä½œ

ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã« config ã‚’èª­ã¿è¾¼ã‚“ã§ JobScheduler ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã€‚

ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§

30ç§’ã€œ1åˆ†ã”ã¨ã«ç¾åœ¨æ™‚åˆ»ã‚’ãƒã‚§ãƒƒã‚¯

å®Ÿè¡Œæ¡ä»¶ã‚’æº€ãŸã™ã‚¸ãƒ§ãƒ–ã ã‘ã‚’ä¸€åº¦å®Ÿè¡Œ

å®Ÿè¡Œå±¥æ­´ï¼ˆæˆåŠŸ/å¤±æ•—ã€é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»ï¼‰ã‚’ logs/scheduler.log ã«æ®‹ã™ã€‚

åŒä¸€ã‚¸ãƒ§ãƒ–ã¯åŒã˜æ—¥ä»˜ãƒ»åŒã˜ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§äºŒé‡èµ·å‹•ã•ã‚Œãªã„ã‚ˆã† last_run_at ã‚’ç®¡ç†ã€‚

4) ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½ä»•æ§˜ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³åˆ¥ï¼‰
4-1. ç›®çš„

ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã”ã¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œæœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å…±æœ‰ã—ã€ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å¾—ã‚‹ã€‚

ä»•æ§˜ã¨ã—ã¦ã¯ â€œä»»æ„å‚åŠ ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½â€ ã«ã¨ã©ã‚ã‚‹ï¼ˆåˆ©ç”¨ã—ãªãã¦ã‚‚æœ¬ä½“æ©Ÿèƒ½ã¯ãƒ•ãƒ«å‹•ä½œï¼‰ã€‚

4-2. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´

ã‚µãƒ¼ãƒ“ã‚¹å±¤ã« RankingService ã‚’å®Ÿè£…ï¼š

æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã®é›†è¨ˆï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ»å®Ÿé‹ç”¨ã®ä¸¡æ–¹ï¼‰

ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã€å¹´æœˆã€ãƒªã‚¿ãƒ¼ãƒ³ï¼…ã€ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ï¼‰ç”Ÿæˆ

é€ä¿¡çµæœã®ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ï¼ˆå†é€åˆ¶å¾¡ï¼‰

GUIï¼š

ã€Œãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ã‚¿ãƒ–ï¼ˆ or AIã‚¿ãƒ–å†…ã®ã‚µãƒ–ã‚¿ãƒ–ï¼‰

ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã§ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³é¸æŠï¼ˆBasic/Pro/Expert åˆ¥ï¼‰

è‡ªåˆ†ã®é †ä½ãƒ»è‡ªåˆ†ã®æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³è¡¨ç¤º

Freeã¯é–²è¦§ã®ã¿ã€Basicä»¥ä¸Šã§é€ä¿¡ãƒœã‚¿ãƒ³æœ‰åŠ¹ã€‚

4-3. ã‚µãƒ¼ãƒå´ï¼ˆã“ã“ã§ã¯è¦æ±‚ã ã‘ï¼‰

/api/ranking/submitï¼š

èªè¨¼æ¸ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æˆç¸¾é€ä¿¡

/api/ranking/top?edition=pro&year=2025&month=11ï¼š

ä¸Šä½Nåã‚’è¿”ã™

ä»Šã®æ®µéšã§ã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒƒã‚¯å®Ÿè£…ã‹ã‚‰å§‹ã‚ã¦ã€ã‚µãƒ¼ãƒãŒæº–å‚™ã§ããŸã‚‰åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚

5) æ—¥æœ¬èªUIãƒ»è¡¨è¨˜ãƒ«ãƒ¼ãƒ«

ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ãƒœã‚¿ãƒ³ï¼ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã¯ã™ã¹ã¦æ—¥æœ¬èªè¡¨è¨˜ã€‚

å˜èªä¾‹ï¼š

ã€Œåˆ©ç›Šç‡ã€,ã€Œæœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã€,ã€Œå‹ç‡ã€,ã€Œãƒ—ãƒ­ãƒ•ã‚£ãƒƒãƒˆãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆPFï¼‰ã€,ã€Œé€±æ¬¡å†å­¦ç¿’ã€,ã€Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šã€ãªã©ã€‚

ãƒ­ã‚°å‡ºåŠ›ã‚‚æ—¥æœ¬èªï¼‹ç°¡å˜ãªè‹±èªä½µè¨˜ï¼ˆä¾‹ï¼šã€ŒMT5 æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ (login error)ã€ï¼‰ã€‚

æ–‡å­—ã‚³ãƒ¼ãƒ‰ã¯ã™ã¹ã¦ UTF-8ï¼ˆBOMãªã—ï¼‰ã«çµ±ä¸€ã™ã‚‹ã€‚



=== file: ä½œæ¥­6ï½8.txt ===

ä½œæ¥­6 çµæœ
configs/config.yaml (line 54) ã« atr_hysteresis ã‚’è¿½åŠ ã—ã€ATRã‚²ãƒ¼ãƒˆç”¨ã®å…¥å ´ãƒ»è§£é™¤é–¾å€¤ã¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯é•·ã‚’è¨­å®šã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚
app/services/execution_stub.py å†’é ­ã§ deque/statistics ã‚’å°å…¥ã—ã€_atr_gate_okï¼ˆä¸­å¤®å€¤ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ä»˜ãï¼‰ã‚’å®Ÿè£…ã€ATRåˆ¤å®šéƒ¨ã‚’ãã®ã‚²ãƒ¼ãƒˆã§ç½®ãæ›ãˆã¾ã—ãŸã€‚ãƒ­ã‚°ã«ã¯ atr_refãƒ»atr_gate_stateãƒ»atr_enable_minãƒ»atr_disable_min ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†èª¿æ•´æ¸ˆã¿ã§ã™ã€‚runtime_payload ã¸ filters æƒ…å ±ã‚’æ¸¡ã—ã€ã‚²ãƒ¼ãƒˆãŒè¨­å®šå€¤ã‚’å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã‚‚ã—ã¦ã„ã¾ã™ã€‚
æ¤œè¨¼

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"'
æœ€æ–°ã‚¹ãƒ¢ãƒ¼ã‚¯ã§ã¯ atr_ref â‰ˆ 0.00042 ã§ atr_gate_state ãŒ closed ã®ã¾ã¾ãªãŸã‚ atr_low ãƒ–ãƒ­ãƒƒã‚¯ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ­ãƒ¼ãƒ«ãƒ³ã‚°ä¸­å¤®å€¤ãŒ enable_min_pct (0.00055) ã‚’è¶…ãˆã‚Œã°ã‚²ãƒ¼ãƒˆãŒé–‹æ”¾ã•ã‚Œã€ENTRY ãƒãƒ£ãƒ³ã‚¹ãŒæˆ»ã‚‹æƒ³å®šã§ã™ã€‚å¿…è¦ã«å¿œã˜ã¦ atr_hysteresis ã®å€¤èª¿æ•´ã‚„è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã§æŒ™å‹•ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

PS D:\macht\OneDrive\fxbot> python -m compileall app core scripts
Listing 'app'...
Listing 'app\\core'...
Listing 'app\\data'...
Listing 'app\\gui'...
Listing 'app\\models'...
Listing 'app\\services'...
Listing 'core'...
Listing 'core\\ai'...
Listing 'core\\utils'...
Listing 'scripts'...

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 17:57:01.763 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 17:57:01.876 | WARNING  | app.services.execution_stub:_emit:246 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': 'f43d5483282d4351a91740f83541dbff9ebca276686ed0016fda4fafec290039', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SKIP'}, 'filters': {'spread': 0.5, 'spread_limit': 1.5, 'adx': 25.93729, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00043559, 'min_atr_pct': 0.0006, 'prob_threshold': 0.62, 'side_bias': 'auto', 'atr_ref': 0.00043559, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"'

{"ts_jst": "2025-10-31T16:05:51+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 33.68549, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00048653, "min_atr_pct": 0.02, "prob_threshold": 0.55, 
 "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy": 0 
.502041, "sell": 0.497959, "skip": 0.497959}, "calibrator": "is 
otonic", "meta": "SKIP", "threshold": 0.55, "decision": {"actio 
n": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "Ligh 
tGBM_clf + calib[isotonic]", "version": "1761815421.0", "featur 
es_hash": "e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e 
6a96912863e1", "p_buy": 0.5020408163265306, "p_sell": 0.4979591 
8367346936, "p_skip": 0.49795918367346936, "meta": "SKIP"}, "cb 
": {"tripped": false, "reason": null, "consecutive_losses": 0,  
"threshold_losses": 5, "reset_marker": null}, "features_hash":  
"e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e6a96912863 
e1", "model": "LightGBM_clf + calib[isotonic]", "runtime": {"sp 
read_pips": 0.49999999999954525, "spread_limit_pips": 1.5, "min 
_adx": 15.0, "disable_adx_gate": true, "min_atr_pct": 0.02, "pr 
ob_threshold": 0.55, "side_bias": "auto", "max_positions": 1, " 
ai_threshold": 0.55}}
{"ts_jst": "2025-10-31T17:34:14+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.2, "spread_limit": 1. 
5, "adx": 31.46339, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00049362, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "303e2ece8a2138840f6be93a86da70447f176216708018e2d 
4c5ce3db35a5e0e", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "303e2ece8a2138840f6be93a86da70447f176216708018e2d4c5ce3db35a 
5e0e", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.20000000000095497, "spread_limit_pips": 1.5, "m 
in_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006 
, "prob_threshold": 0.62, "side_bias": "auto", "max_positions": 
 1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:35:26+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 30.66809, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00047144, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "285310bf40fa5081802422eded472a7c034186013956c3e95 
a54a401b5d84c52", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "285310bf40fa5081802422eded472a7c034186013956c3e95a54a401b5d8 
4c52", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.6000000000000227, "spread_limit_pips": 1.5, "mi 
n_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006, 
 "prob_threshold": 0.62, "side_bias": "auto", "max_positions":  
1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:40:31+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 29.84356, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00046456, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "ef03c983519f3fa491ff38083c2450a80a4e24f1fc8774135 
8d935e056dab66d", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "ef03c983519f3fa491ff38083c2450a80a4e24f1fc87741358d935e056da 
b66d", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.6000000000000227, "spread_limit_pips": 1.5, "mi 
n_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006, 
 "prob_threshold": 0.62, "side_bias": "auto", "max_positions":  
1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:41:35+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.4, "spread_limit": 1. 
5, "adx": 29.75924, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00046644, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b46601 
3696a29fe67c604", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b466013696a29fe67 
c604", "model": "LightGBM_clf + calib[isotonic]", "exit_plan":  
{"mode": "none"}, "runtime": {"spread_pips": 0.4000000000019099 
4, "spread_limit_pips": 1.5, "min_adx": 15.0, "disable_adx_gate 
": false, "min_atr_pct": 0.0006, "prob_threshold": 0.62, "side_ 
bias": "auto", "max_positions": 1, "ai_threshold": 0.62}}       
{"ts_jst": "2025-10-31T17:55:35+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 25.93729, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.0004221, "min_atr_pct": 0.0006, "prob_threshold": 0.6 
2, "side_bias": "auto", "atr_ref": 0.0004221, "atr_gate_state": 
 "closed", "atr_enable_min": 0.00055, "atr_disable_min": 0.0004 
5, "blocked": "atr_low"}, "probs": {"buy": 0.496772, "sell": 0. 
503228, "skip": 0.496772}, "calibrator": "isotonic", "meta": "S 
KIP", "threshold": 0.62, "decision": {"action": "BLOCKED", "rea 
son": "atr_low"}, "ai": {"model_name": "LightGBM_clf + calib[is 
otonic]", "version": "1761815421.0", "features_hash": "6eab60cd 
e26cbccacdf8c2042aa19b8894099c914f9b3586116ee35e63abb842", "p_b 
uy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_skip" 
: 0.49677207571944404, "meta": "SKIP"}, "cb": {"tripped": false 
, "reason": null, "consecutive_losses": 0, "threshold_losses":  
5, "reset_marker": null}, "features_hash": "6eab60cde26cbccacdf 
8c2042aa19b8894099c914f9b3586116ee35e63abb842", "model": "Light 
GBM_clf + calib[isotonic]", "exit_plan": {"mode": "none"}, "run 
time": {"spread_pips": 0.49999999999954525, "spread_limit_pips" 
: 1.5, "min_adx": 15.0, "disable_adx_gate": false, "min_atr_pct 
": 0.0006, "prob_threshold": 0.62, "side_bias": "auto", "max_po 
sitions": 1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:57:01+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 25.93729, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00043559, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "atr_ref": 0.00043559, "atr_gate_state 
": "closed", "atr_enable_min": 0.00055, "atr_disable_min": 0.00 
045, "blocked": "atr_low"}, "probs": {"buy": 0.496772, "sell":  
0.503228, "skip": 0.496772}, "calibrator": "isotonic", "meta":  
"SKIP", "threshold": 0.62, "decision": {"action": "BLOCKED", "r 
eason": "atr_low"}, "ai": {"model_name": "LightGBM_clf + calib[ 
isotonic]", "version": "1761815421.0", "features_hash": "f43d54 
83282d4351a91740f83541dbff9ebca276686ed0016fda4fafec290039", "p 
_buy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_ski 
p": 0.49677207571944404, "meta": "SKIP"}, "cb": {"tripped": fal 
se, "reason": null, "consecutive_losses": 0, "threshold_losses" 
: 5, "reset_marker": null}, "features_hash": "f43d5483282d4351a 
91740f83541dbff9ebca276686ed0016fda4fafec290039", "model": "Lig 
htGBM_clf + calib[isotonic]", "exit_plan": {"mode": "none"}, "r 
untime": {"spread_pips": 0.49999999999954525, "spread_limit_pip 
s": 1.5, "min_adx": 15.0, "disable_adx_gate": false, "min_atr_p 
ct": 0.0006, "prob_threshold": 0.62, "side_bias": "auto", "max_ 
positions": 1, "ai_threshold": 0.62}}


ä½œæ¥­7ã€€çµæœ
å¤‰æ›´ç‚¹

configs/config.yaml (line 8) ã« post_fill_grace_sec: 60 ã‚’è¿½åŠ ã—ã€filters (line 55) ã¸ ATR ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹è¨­å®šã‚’è¿½è¨˜ã€‚
core/utils/clock.py ã‚’æ–°è¨­ã— now_jst() ã‚’æä¾›ã€‚
app/services/trade_service.py (line 96) ã§ mark_filled_now / post_fill_grace_active ã‚’è¿½åŠ ã—ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼å¾Œã®å…è²¬ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç®¡ç†ã€‚
app/services/execution_stub.py (line 17) ã§ deque ãƒ™ãƒ¼ã‚¹ã® ATR ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ _atr_gate_ok ã‚’å°å…¥ã€ãƒ•ã‚£ãƒ«ã‚¿åŸºæº–ã«å…è²¬ã‚„ãƒ­ã‚°ç”¨ãƒ¡ã‚¿æƒ…å ±ï¼ˆpost_fill_grace, atr_ref ç­‰ï¼‰ã‚’è¿½åŠ ã€ENTRY è¨ˆä¸Šæ™‚ã« mark_filled_now() ã‚’å‘¼ã³å‡ºã—ã€‚pos_guard ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯å³åº§ã« position_guard.on_order_rejected_or_canceled() ã§è§£é™¤ã€‚
core/position_guard.py (line 61) ã«æ³¨æ–‡æ‹’å¦/å–æ¶ˆæ™‚ã®ãƒªãƒªãƒ¼ã‚¹ API ã‚’è¿½åŠ ã€‚
æ¤œè¨¼

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "pos_guard"'ï¼ˆéå»ãƒ­ã‚°ãŒ1ä»¶ã®ã¿ã€æ–°è¦å†è©¦è¡Œã§é€£ç¶šãƒ–ãƒ­ãƒƒã‚¯ã¯ç™ºç”Ÿã›ãšï¼‰
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"' ã§ ATR åˆ¤å®šã‚’ç¢ºèªï¼ˆä¸­å¤®å€¤ãŒé–¾å€¤æœªæº€ã®ãŸã‚å¼•ãç¶šããƒ–ãƒ­ãƒƒã‚¯ä¸­ï¼‰
è£œè¶³

ç¾çŠ¶ ATR ã¯ 0.00044 å‰å¾Œã§æ¨ç§»ã—ã¦ãŠã‚Šã€ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ã®æœ‰åŠ¹é–¾å€¤ (0.00055) ã‚’è¶…ãˆã‚‹ã¾ã§ã¯ã‚²ãƒ¼ãƒˆãŒé–‰ã˜ãŸã¾ã¾ã§ã™ã€‚å®Ÿãƒˆãƒ¬ãƒ¼ãƒ‰ä»¥é™ã« mark_filled_now() ãŒå‘¼ã°ã‚Œã‚‹ã¨å…è²¬æœŸé–“ä¸­ã¯ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰/ADX/ATRã®è»½å¾®ãªNGã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ENTRY ç™ºç”Ÿå¾Œã®æŒ™å‹•ã‚’è¦³å¯Ÿã—ã¤ã¤ã€å¿…è¦ãªã‚‰ enable_min_pct ã‚’å†èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 18:04:55.275 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 18:04:55.389 | WARNING  | app.services.execution_stub:_emit:250 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': 'ea09b090b9a024b0608b14034af07615bdb08f5183c4c9485f6d62c1cd4face7', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SKIP'}, 'filters': {'spread': 0.5, 'spread_limit': 1.5, 'adx': 23.78869, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00045428, 'min_atr_pct': 0.0006, 'prob_threshold': 0.62, 'side_bias': 'auto', 'atr_ref': 0.00045428, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'post_fill_grace': False, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "pos_guard"'

{"ts_jst": "2025-10-31T17:28:23+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.1, "spread_limit": 1. 
5, "adx": 32.31987, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00049188, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": "pos_guard"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SELL", "threshold": 0.5, "decision": {"act 
ion": "BLOCKED", "reason": "pos_guard", "ai_meta": "SELL", "dec 
": {"decision": "ENTRY", "meta": "SELL", "side": "SELL", "thres 
hold": 0.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + 
 calib[isotonic]", "version": "1761815421.0", "features_hash":  
"6acc2276334fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cb 
e0", "p_buy": 0.49677207571944404, "p_sell": 0.503227924280556, 
 "p_skip": 0.49677207571944404, "meta": "SELL"}, "cb": {"trippe 
d": false, "reason": null, "consecutive_losses": 0, "threshold_ 
losses": 5, "reset_marker": null}, "features_hash": "6acc227633 
4fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cbe0", "model 
": "LightGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 
 0.10000000000047748, "spread_limit_pips": 1.5, "min_adx": 15.0 
, "disable_adx_gate": true, "min_atr_pct": 0.0004, "prob_thresh 
old": 0.5, "side_bias": "auto", "max_positions": 1, "ai_thresho 
ld": 0.5}}


ä½œæ¥­8ã€€çµæœ
configs/config.yaml (line 8) ã§ prob_threshold ã‚’ 0.56ã€min_edge ã‚’ 0.01 ã«èª¿æ•´ã—ã€filters ã® min_atr_pct ã‚’ 0.0005 ã¸ç·©å’Œã€‚session.allow_hours_jst ã‚’è¿½åŠ ã—ã€JST ã®ç¨¼åƒæ™‚é–“å¸¯ã‚’æ˜ç¤ºã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚
core/utils/clock.py ã‚’æ–°è¦ä½œæˆã—ã¦ now_jst() ã‚’æä¾›ã€app/services/trade_service.py (line 8) ã§ã¯ã“ã‚Œã‚’åˆ©ç”¨ã—ã¦ç´„å®šè¨˜éŒ² (mark_filled_now) ã¨å…è²¬åˆ¤å®š (post_fill_grace_active) ã‚’å®Ÿè£…ã€‚
app/services/execution_stub.py ã« JST æ™‚é–“å¸¯ãƒ•ã‚£ãƒ«ã‚¿ _session_hour_allowed ã¨ã‚¾ãƒ¼ãƒ³æƒ…å ± import ã‚’è¿½åŠ ã€‚ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹æƒ…å ±ã‚„å…è²¬çŠ¶æ…‹ã‚’å«ã‚€ base_filters ã‚’æ‹¡å……ã—ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³å¤–ã§ã¯ session_closed ç†ç”±ã§ SKIP ã‚’è¨˜éŒ²ã€‚ENTRY æ™‚ã«ã¯ trade_service.mark_filled_now() ã‚’å‘¼ã³ã€pos_guard ãƒ–ãƒ­ãƒƒã‚¯ç™ºç”Ÿæ™‚ã« position_guard.on_order_rejected_or_canceled(symbol) ã§å³è§£é™¤ã€‚
core/position_guard.py (line 72) ã«æ‹’å¦/å–æ¶ˆæ™‚ã®ã‚¢ãƒ³ãƒ­ãƒƒã‚¯ API ã‚’è¿½åŠ ã€runtime.post_fill_grace_sec ã‚’ 60 ç§’ã¨ã™ã‚‹è¨­å®šã‚’åæ˜ ã€‚
æ¤œè¨¼

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "session_closed"'ï¼ˆè©²å½“ãªã—ï¼‰
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"'
ã‚µãƒãƒª: logs/decisions\decisions_USDJPY-.jsonl Counter({'BUY': 10, 'SELL': 9, 'SKIP': 8, 'BLOCKED': 3}), logs/decisions\decisions_USDJPY.jsonl Counter({'BLOCKED': 21, 'SKIP': 7, 'BUY': 7, 'SELL': 3, 'ENTRY': 2})
ENTRYã¯ç¾çŠ¶ ATR é–¾å€¤ã§æŠ‘åˆ¶ã•ã‚Œã¦ã„ã¾ã™ãŒã€æ®µéšçš„ã« prob_threshold ã‚’æˆ»ã—ã¤ã¤ç¨¼åƒæ™‚é–“å¸¯ãƒ•ã‚£ãƒ«ã‚¿ã§â€œæ­»ã‚“ã æ™‚é–“â€ã‚’é¿ã‘ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 18:08:53.349 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 18:08:53.476 | WARNING  | app.services.execution_stub:_emit:278 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': 'b97fff3648654fffb2eb94398653d0ad8ae16cee673bc34b06c302ac054783a3', 'p_buy': 0.4785202863961814, 'p_sell': 0.5214797136038186, 'p_skip': 0.4785202863961814, 'meta': 'SKIP'}, 'filters': {'spread': 0.0, 'spread_limit': 1.5, 'adx': 22.83583, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.000447, 'min_atr_pct': 0.0005, 'prob_threshold': 0.56, 'side_bias': 'auto', 'atr_ref': 0.000447, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'post_fill_grace': False, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}} 

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "session_closed"'




=== file: ä½œæ¥­9.txt ===

ä½œæ¥­9ã€€çµæœ
configs/config.yaml (line 26) â†’ ã—ãã„å€¤ã‚’ Step1 è¨­å®š (prob_threshold: 0.56, min_edge: 0.01) ã«å¤‰æ›´ã—ã€exits.atr.trailing ã«ç™ºç«/åˆ»ã¿/BEãƒ­ãƒƒã‚¯/ãƒãƒ¼ãƒ‰ãƒ•ãƒ­ã‚¢/price_source å«ã‚€è©³ç´°ãƒãƒ–ã‚’è¿½åŠ ã€‚
app/services/trade_service.py (line 8) â†’ datetime ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æ‹¡å¼µã—ã€build_exit_plan ã® ATR æˆ»ã‚Šå€¤ã«æ–° trailing ãƒãƒ–ã‚’ãã®ã¾ã¾åæ˜ ã€‚mark_filled_now/post_fill_grace_active ã‚’åˆ©ç”¨ã—ã¤ã¤ Optional[datetime] ã®å‹æ³¨é‡ˆã‚’æ•´å‚™ã€‚
app/services/trailing.pyï¼ˆæ–°è¦ï¼‰â†’ ATR ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ç”¨ã® TrailConfig/TrailState/AtrTrailer ã‚’å®Ÿè£…ã€‚ç™ºç«ãƒ»åˆ»ã¿ãƒ»BEãƒ­ãƒƒã‚¯ãƒ»hard_floorãƒ»only_in_profit ã‚’ä¸€æ‹¬ç®¡ç†ã€‚
app/services/execution_stub.py:18,86,212,352 ä»˜è¿‘ â†’ session ãƒ•ã‚£ãƒ«ã‚¿å°å…¥ã€ATR ã‚²ãƒ¼ãƒˆã®ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹æ‹¡å¼µã€ENTRY æ™‚ã® trail_state ç™»éŒ²/runtime_trail_states ä¿æŒã€ãƒ†ã‚£ãƒƒã‚¯æ¯ã« _update_trailing_state ã§ TRAIL_UPDATE ã‚’ãƒ­ã‚°ã™ã‚‹ä»•çµ„ã¿ã‚’è¿½åŠ ã€‚ä¾¡æ ¼ç®—å‡ºãƒ˜ãƒ«ãƒ‘ (_tick_to_dict, _current_price_for_side) ã‚„ pip/point æ¨å®šã€ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°åˆæœŸåŒ–/æ›´æ–°å‡¦ç†ã¨ trail_state ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‡ºåŠ›ã‚’çµ„ã¿è¾¼ã¿ã€‚
scripts/sim_trailing.pyï¼ˆæ–°è¦ï¼‰â†’ CLI ã§ AtrTrailer ã®æŒ™å‹•ã‚’å¯è¦–åŒ–ã™ã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’ç”¨æ„ã€‚
app/services/execution_stub.py (line 422) ãªã© â†’ runtime_payload ã« tick æƒ…å ±ã‚’æ¸¡ã™ã‚ˆã†å¤‰æ›´ã—ã€scripts.dryrun_smoke ã‹ã‚‰ã‚‚åŒæƒ…å ±ãŒ on_tick ã«å±Šãã‚ˆã†ã«èª¿æ•´ã€‚
ç¢ºèªæ¸ˆã¿
.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.sim_trailing --side BUY --entry 150.000 --atr 0.120 --activate 0.5 --step 0.25 --lockbe 0.3 --floor 5
ï¼ˆdryrun ã¯ä½ATRã®ãŸã‚ atr_low ã§ BLOCKEDã€‚ENTRYç™ºç”Ÿå¾Œã« TRAIL_UPDATE ã‚’ç¢ºèªï¼‰

ãƒ¡ãƒ¢

configs/config.yaml ã‚’ python -c "import yaml, json; cfg=yaml.safe_load(open('configs/config.yaml')); print(json.dumps(cfg['exits']['atr']['trailing'], indent=2))" ãªã©ã§ç¢ºèªã™ã‚‹ã¨ãƒãƒ–ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã®ã‚’ç¢ºèªã§ãã¾ã™ã€‚
ATR ã‚²ãƒ¼ãƒˆãŒé–‹ãç›¸å ´ï¼ˆenable_min_pct â‰ˆ 0.00055 ä»¥ä¸Šï¼‰ã§ ENTRY ãŒå‡ºã‚‹ã¨ã€logs/decisions/ ã« TRAIL_UPDATE ã¨ trail_state ãŒè¨˜éŒ²ã•ã‚Œã¾ã™ã€‚å¿…è¦ã«å¿œã˜ atr_hysteresis.enable_min_pct ã‚’ä¸€æ™‚çš„ã«ç·©å’Œã—ã¦ãã ã•ã„ã€‚
å®Ÿå£²è²·ã¸æŒã¡è¾¼ã‚€éš›ã¯ StopLevel/FreezeLevel ã¨ä¾¡æ ¼ä¸¸ã‚ã‚’å¿…ãšè€ƒæ…®ã—ã€BUY=bid / SELL=ask ãªã©ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼ä»•æ§˜ã«åˆã‚ã›ã¦ _current_price_for_side ã®é¸æŠã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

PS D:\macht\OneDrive\fxbot> python -m scripts.sim_trailing --side BUY --entry 150.000 --atr 0.120 --activate 0.5 --step 0.25 --lockbe 0.3 --floor 5
# side=BUY entry=150.0 atr=0.12
# price, activated, be_locked, layers, current_sl, new_sl       
150.00000, False, False, 0, None, None
150.01200, False, False, 0, None, None
150.02400, False, False, 0, None, None
150.03600, False, False, 0, None, None
150.04800, False, False, 0, None, None
150.06000, True, False, 0, 150.05, 150.05
150.07200, True, True, 0, 150.05, None
150.08400, True, True, 2, 150.05, None
150.09600, True, True, 3, 150.066, 150.066
150.10800, True, True, 3, 150.066, None
150.12000, True, True, 4, 150.09, 150.09
150.13200, True, True, 4, 150.09, None
150.14400, True, True, 4, 150.09, None
150.15600, True, True, 5, 150.126, 150.126
150.16800, True, True, 5, 150.126, None
150.18000, True, True, 6, 150.15, 150.15
150.19200, True, True, 6, 150.15, None
150.20400, True, True, 6, 150.15, None
150.21600, True, True, 7, 150.186, 150.186
150.22800, True, True, 7, 150.186, None
150.24000, True, True, 8, 150.21, 150.21
150.25200, True, True, 8, 150.21, None
150.26400, True, True, 8, 150.21, None
150.27600, True, True, 9, 150.246, 150.246
150.28800, True, True, 9, 150.246, None
150.30000, True, True, 10, 150.27, 150.27
150.31200, True, True, 10, 150.27, None
150.32400, True, True, 10, 150.27, None
150.33600, True, True, 11, 150.306, 150.306
150.34800, True, True, 11, 150.306, None
150.36000, True, True, 12, 150.33, 150.33

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 19:01:13.499 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 19:01:14.273 | WARNING  | app.services.execution_stub:_emit:458 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': '06df81b926ebe26648452e8e57552b98b1161017190bbdc97c388801a32338cc', 'p_buy': 0.4785202863961814, 'p_sell': 0.5214797136038186, 'p_skip': 0.4785202863961814, 'meta': 'SKIP'}, 'filters': {'spread': 0.3, 'spread_limit': 1.5, 'adx': 22.33428, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00039826, 'min_atr_pct': 0.0005, 'prob_threshold': 0.56, 'side_bias': 'auto', 'atr_ref': 0.00039826, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'post_fill_grace': False, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}
PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"TRAIL_UPDATE"|trail_new_sl|trail_state'




=== file: å‚™å¿˜éŒ².txt ===

make_csv_from_mt5.py
python -m scripts.make_csv_from_mt5 --layout per-symbol


=== file: è¨­å®š/ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ.txt ===

# ==== ãƒ•ã‚©ãƒ«ãƒ€ãƒ„ãƒªãƒ¼ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ ====
# å‡ºåŠ›: project_tree.txt
# ãƒ•ã‚©ãƒ«ãƒ€: D:\fxbotï¼ˆå¿…è¦ãªã‚‰æ›¸ãæ›ãˆï¼‰

$root = "D:\fxbot"
$output = "project_tree.txt"

function Write-Tree {
    param(
        [System.IO.DirectoryInfo]$Dir,
        [int]$Level
    )

    $indent = " " * ($Level * 2)
    Add-Content -Path $output -Value ("{0}{1}/" -f $indent, $Dir.Name)

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    foreach ($file in $Dir.GetFiles()) {
        Add-Content -Path $output -Value ("{0}  {1}" -f $indent, $file.Name)
    }

    # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°å‡¦ç†
    foreach ($sub in $Dir.GetDirectories()) {
        Write-Tree -Dir $sub -Level ($Level + 1)
    }
}

# æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å‰Šé™¤
if (Test-Path $output) {
    Remove-Item $output
}

# å®Ÿè¡Œ
Write-Tree -Dir (Get-Item $root) -Level 0

Write-Host "ãƒ•ã‚©ãƒ«ãƒ€ãƒ„ãƒªãƒ¼ã‚’ project_tree.txt ã«ä¿å­˜ã—ã¾ã—ãŸã€‚"


=== file: è¨­å®š/ä»®.txt ===

# PowerShell 7 ç”¨ï¼šruff.toml / mypy.ini / .vscode/settings.json ã‚’ã¾ã¨ã‚ã¦ä½œæˆ
cd D:\fxbot

$code = @'
from pathlib import Path

# ==== å‰æï¼šã“ã“ã§ã®ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒ D:\fxbot ====
root = Path.cwd()
print(f"[INFO] working dir = {root}")

# -------------------------------
# ruff.toml ã®ä¸­èº«
# -------------------------------
ruff_toml = r"""
line-length = 88
target-version = "py311"

# .venv ã‚„çµæœãƒ•ã‚©ãƒ«ãƒ€ã¯ãƒªãƒ³ãƒ†ã‚£ãƒ³ã‚°å¯¾è±¡å¤–
extend-exclude = [
    ".venv",
    "backtests",
    "models",
    "logs",
    "dist",
    "build",
]

[lint]
# ãƒ™ãƒ¼ã‚·ãƒƒã‚¯ãª lint + ãƒã‚°æ¤œå‡º + import æ•´ç† + è¿‘ä»£åŒ– + ã‚·ãƒ³ãƒ—ãƒ«åŒ–
select = [
    "E",  # pycodestyle (PEP8)
    "F",  # pyflakes
    "W",  # ä¸€èˆ¬çš„ãªè­¦å‘Š
    "B",  # flake8-bugbearï¼ˆãƒã‚°ã«ãªã‚Šãã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    "I",  # isortï¼ˆimport ä¸¦ã¹æ›¿ãˆï¼‰
    "UP", # pyupgradeï¼ˆæœ€æ–°æ§‹æ–‡ã¸ï¼‰
    "SIM" # flake8-simplifyï¼ˆæ›¸ãæ–¹ã®ç°¡ç•¥åŒ–ï¼‰
]

# è¡Œé•·ã¯ black ã«ä»»ã›ã‚‹ã®ã§ E501 ã¯ç„¡è¦–
ignore = ["E501"]

[lint.isort]
known-first-party = ["app"]
combine-as-imports = true
"""

# -------------------------------
# mypy.ini ã®ä¸­èº«
# -------------------------------
mypy_ini = r"""
[mypy]
python_version = 3.13
warn_unused_configs = True

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆMetaTrader5 ãªã©ï¼‰ã®å‹å®šç¾©ãŒãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ã—ãªã„
ignore_missing_imports = True

# ãã“ãã“å³ã—ã‚ã ã‘ã©å…¨å£Šã¯ã—ãªã„ãƒãƒ©ãƒ³ã‚¹
no_implicit_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_return_any = True
strict_equality = True

# å°†æ¥çš„ã« app.* ã ã‘ã¯ã¡ã‚‡ã£ã¨å³ã—ã‚ã«ã™ã‚‹
[mypy-app.*]
disallow_untyped_defs = True
check_untyped_defs = True

# scripts ã¯ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒå¤šã„ã®ã§å°‘ã—ç·©ã‚
[mypy-scripts.*]
disallow_untyped_defs = False
check_untyped_defs = False
"""

# -------------------------------
# VSCode ã® .vscode/settings.json
# -------------------------------
settings_json = r"""
{
  // ==== Python ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ï¼ˆ.venvï¼‰ ====
  "python.defaultInterpreterPath": "D:\\fxbot\\.venv\\Scripts\\python.exe",

  // è¡Œæœ«ãƒ»æœ«å°¾æ”¹è¡Œã®æ•´ç†ï¼ˆGit ç”¨ï¼‰
  "files.trimTrailingWhitespace": true,
  "files.insertFinalNewline": true,

  // å…¨ä½“ã¨ã—ã¦ä¿å­˜æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æœ‰åŠ¹åŒ–
  "editor.formatOnSave": true,

  // Python ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã®è¨­å®š
  "[python]": {
    // ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã¯ Black æ‹…å½“
    "editor.defaultFormatter": "ms-python.black-formatter",
    // ä¿å­˜æ™‚ã« Ruff ã® fix / import ä¸¦ã¹æ›¿ãˆã‚’èµ°ã‚‰ã›ã‚‹
    "editor.codeActionsOnSave": {
      "source.fixAll": "explicit",
      "source.organizeImports": "explicit"
    }
  },

  // æ—§ Python æ‹¡å¼µã® linter ã¯ä½¿ã‚ãªã„ï¼ˆRuff ã«ä¸€æœ¬åŒ–ï¼‰
  "python.linting.enabled": false,

  // Ruff VSCode æ‹¡å¼µã®æŒ™å‹•
  "ruff.enable": true,
  "ruff.run": "onSave",

  // Mypy æ‹¡å¼µã‚’å…¥ã‚Œã¦ã„ã‚‹å ´åˆã ã‘æœ‰åŠ¹ã«ãªã‚‹è¨­å®šï¼ˆãªã‘ã‚Œã°ç„¡è¦–ã•ã‚Œã‚‹ï¼‰
  "mypy-type-checker.args": [
    "--config-file",
    "${workspaceFolder}/mypy.ini"
  ]
}
"""

# ==== ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ› ====
# ruff.toml
( root / "ruff.toml" ).write_text(ruff_toml.lstrip(), encoding="utf-8")
print("[OK] ruff.toml ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

# mypy.ini
( root / "mypy.ini" ).write_text(mypy_ini.lstrip(), encoding="utf-8")
print("[OK] mypy.ini ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

# .vscode/settings.json
vscode_dir = root / ".vscode"
vscode_dir.mkdir(exist_ok=True)
( vscode_dir / "settings.json" ).write_text(settings_json.lstrip(), encoding="utf-8")
print("[OK] .vscode/settings.json ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
'@

python -c "$code"



=== file: è¨­å®š/ä»®æƒ³ç’°å¢ƒ.txt ===

D:\venv\fxbot-pca\.venv\Scripts\Activate.ps1


=== file: è³ªå•.txt ===

PS C:\Users\macht\OneDrive\fxbot> python -m tools.backtest_run `
>>   --csv data\USDJPY\ohlcv\USDJPY_M5.csv `
>>   --start 2024-01-01 --end 2025-11-09 `
>>   --capital 100000 `
>>   --mode wfo `
>>   --symbol USDJPY `
>>   --timeframe M5 `
>>   --layout per-symbol `
>>   --train-ratio 0.7
[main] mode=wfo csv=C:\Users\macht\OneDrive\fxbot\data\USDJPY\ohlcv\USDJPY_M5.csv
[wfo] start
[wfo] read_csv C:\Users\macht\OneDrive\fxbot\data\USDJPY\ohlcv\USDJPY_M5.csv
[wfo] slice 2024-01-01 .. 2025-11-09
[wfo] equity compute train
[wfo] AI model not used (C:\Users\macht\OneDrive\fxbot\active_model.json not found.) -> fallback to buy&hold
[wfo] equity compute test
[wfo] AI model not used (C:\Users\macht\OneDrive\fxbot\active_model.json not found.) -> fallback to buy&hold
[wfo] done
C:\Users\macht\OneDrive\fxbot\logs\backtest\USDJPY\M5\equity_curve.csv
