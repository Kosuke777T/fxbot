############################################################
# project_snapshot
############################################################
generated_at: 2025-11-24 17:28:31
project_root: D:\fxbot

NOTE:
  - このファイルは ChatGPT にプロジェクト構造と主要ファイルを伝えるためのスナップショットです。
  - ログ・データ・モデル・.git・.venv などは除外しています。
  - ディレクトリツリーにはファイルごとの最終更新日時 (updated: ...) を含みます。

========================================
=== DIRECTORY TREE =====================
========================================

fxbot/
app/
  __init__.py (updated: 2025-11-23 07:59:35)
  core/
    config_loader.py (updated: 2025-11-23 07:59:35)
    data_finder.py (updated: 2025-11-23 07:59:35)
    logger.py (updated: 2025-11-23 07:59:35)
    market.py (updated: 2025-11-23 07:59:35)
    mt5_client.py (updated: 2025-11-23 07:59:35)
    strategy_profile.py (updated: 2025-11-23 07:59:35)
  gui/
    __init__.py (updated: 2025-11-23 07:59:35)
    ai_tab.py (updated: 2025-11-23 07:59:35)
    backtest_tab.py (updated: 2025-11-23 07:59:35)
    control_tab.py (updated: 2025-11-23 07:59:35)
    dashboard_tab.py (updated: 2025-11-23 07:59:35)
    dashboard_tab_qt.py (updated: 2025-11-23 07:59:35)
    history_tab.py (updated: 2025-11-23 07:59:35)
    main.py (updated: 2025-11-23 07:59:35)
    settings_tab.py (updated: 2025-11-23 07:59:35)
    widgets/
      feature_importance.py (updated: 2025-11-23 07:59:35)
      shap_bar.py (updated: 2025-11-23 07:59:35)
  main_tk.py (updated: 2025-11-23 07:59:35)
  services/
    __init__.py (updated: 2025-11-23 07:59:35)
    ai_service.py (updated: 2025-11-23 21:09:14)
    aisvc_loader.py (updated: 2025-11-23 07:59:35)
    circuit_breaker.py (updated: 2025-11-23 07:59:35)
    data_guard.py (updated: 2025-11-23 07:59:35)
    decision_log.py (updated: 2025-11-23 07:59:35)
    event_store.py (updated: 2025-11-23 07:59:35)
    execution_stub.py (updated: 2025-11-23 20:16:07)
    feature_importance.py (updated: 2025-11-23 07:59:35)
    metrics.py (updated: 2025-11-23 07:59:35)
    mt5_account_store.py (updated: 2025-11-23 07:59:35)
    mt5_selftest.py (updated: 2025-11-23 07:59:35)
    mt5_service.py (updated: 2025-11-23 07:59:35)
    orderbook_stub.py (updated: 2025-11-23 07:59:35)
    recent_kpi.py (updated: 2025-11-23 07:59:35)
    shap_service.py (updated: 2025-11-23 07:59:35)
    trade_service.py (updated: 2025-11-23 20:49:59)
    trade_state.py (updated: 2025-11-23 07:59:35)
    trailing.py (updated: 2025-11-23 07:59:35)
    trailing_hook.py (updated: 2025-11-23 07:59:35)
  strategies/
    __init__.py (updated: 2025-11-23 07:59:35)
    ai_strategy.py (updated: 2025-11-23 07:59:35)
apply_order.py (updated: 2025-11-23 07:59:35)
config/
configs/
core/
  __init__.py (updated: 2025-11-23 07:59:35)
  ai/
    __init__.py (updated: 2025-11-23 07:59:35)
    calibration.py (updated: 2025-11-23 07:59:35)
    features.py (updated: 2025-11-23 07:59:35)
    loader.py (updated: 2025-11-24 17:20:14)
    service.py (updated: 2025-11-23 07:59:35)
  config.py (updated: 2025-11-23 07:59:35)
  indicators.py (updated: 2025-11-23 07:59:35)
  metrics/
  metrics.py (updated: 2025-11-23 07:59:35)
    __init__.py (updated: 2025-11-23 07:59:35)
    fi_extractor.py (updated: 2025-11-23 07:59:35)
    registry.py (updated: 2025-11-23 07:59:35)
  position_guard.py (updated: 2025-11-23 07:59:35)
  risk.py (updated: 2025-11-23 07:59:35)
  utils/
    __init__.py (updated: 2025-11-23 07:59:35)
    clock.py (updated: 2025-11-23 07:59:35)
    hashing.py (updated: 2025-11-23 07:59:35)
    runtime.py (updated: 2025-11-23 07:59:35)
    timeutil.py (updated: 2025-11-23 07:59:35)
fxbot_path.py (updated: 2025-11-23 07:59:35)
runtime/
scripts/
  cb_smoke.py (updated: 2025-11-23 07:59:35)
  diagnose_symbol.py (updated: 2025-11-23 07:59:35)
  dryrun_smoke.py (updated: 2025-11-23 07:59:35)
  export_mt5_history.py (updated: 2025-11-23 07:59:35)
  export_val_probs.py (updated: 2025-11-23 07:59:35)
  live_runner.py (updated: 2025-11-23 21:26:06)
  make_csv_from_mt5.py (updated: 2025-11-23 07:59:35)
  make_project_snapshot.py (updated: 2025-11-23 17:53:12)
  make_project_snapshot2.py (updated: 2025-11-23 18:36:49)
  make_toy_model.py (updated: 2025-11-23 07:59:35)
  mt5_export_csv.py (updated: 2025-11-23 07:59:35)
  mt5_smoke.py (updated: 2025-11-23 07:59:35)
  print_runtime.py (updated: 2025-11-23 07:59:35)
  promote_model.py (updated: 2025-11-23 07:59:35)
  rollback_model.py (updated: 2025-11-23 07:59:35)
  selftest_mt5.py (updated: 2025-11-23 07:59:35)
  selftest_order_flow.py (updated: 2025-11-23 07:59:35)
  sim_trailing.py (updated: 2025-11-23 07:59:35)
  swap_model.py (updated: 2025-11-23 07:59:35)
  train_calibrator.py (updated: 2025-11-23 07:59:35)
  walkforward_retrain.py (updated: 2025-11-23 07:59:35)
  walkforward_train.py (updated: 2025-11-23 07:59:35)
  weekly_retrain.py (updated: 2025-11-23 07:59:35)
temp_equity_check.py (updated: 2025-11-23 07:59:35)
tests/
  test_sanity.py (updated: 2025-11-23 07:59:35)
tmp_view.py (updated: 2025-11-23 07:59:35)
tools/
  __init__.py (updated: 2025-11-23 07:59:35)
  backtest_equity_curve.py (updated: 2025-11-23 07:59:35)
  backtest_run.py (updated: 2025-11-23 07:59:35)
  dump_feature_importance.py (updated: 2025-11-23 07:59:35)
  inspect_report.py (updated: 2025-11-23 07:59:35)
  list_wfo_reports.py (updated: 2025-11-23 07:59:35)
  train_lightgbm.py (updated: 2025-11-23 07:59:35)
  train_scaler.py (updated: 2025-11-23 07:59:35)


========================================
=== FILE CONTENTS ======================
========================================


=== file: app/__init__.py ===




=== file: app/core/config_loader.py ===

from pathlib import Path
from typing import Any, Dict

import yaml


def _deep_merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    for k, v in (b or {}).items():
        if isinstance(v, dict) and isinstance(a.get(k), dict):
            a[k] = _deep_merge(a[k], v)
        else:
            a[k] = v
    return a


def load_config() -> Dict[str, Any]:
    base: Dict[str, Any] = {}
    base_path = Path("configs/config.yaml")
    if base_path.exists():
        base = yaml.safe_load(base_path.read_text(encoding="utf-8")) or {}
    local_path = Path("configs/config.local.yaml")
    if local_path.exists():
        local: Dict[str, Any] = yaml.safe_load(local_path.read_text(encoding="utf-8")) or {}
        base = _deep_merge(base, local)
    return base



=== file: app/core/data_finder.py ===

# app/core/data_finder.py
from __future__ import annotations
import os, glob
from pathlib import Path
from typing import Iterable, Optional, Tuple
import pandas as pd

def _expand_path(p: str) -> Iterable[Path]:
    # 環境変数・%APPDATA% を解決し、glob を展開
    p = os.path.expandvars(p)
    p = os.path.expanduser(p)
    for hit in glob.glob(p, recursive=True):
        yield Path(hit)

def _load_csv(path: Path) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        low = {c.lower(): c for c in df.columns}
        need = ["time","open","high","low","close"]
        if not all(k in low for k in need):
            return None
        return df
    except Exception:
        return None

def resolve_csv(symbol: str, timeframe: str, search_paths: Iterable[str]) -> Tuple[Optional[Path], Optional[pd.DataFrame]]:
    """search_paths から {SYMBOL}_{TF}.csv を探して返す。最初に見つかったものを採用。"""
    target_name = f"{symbol.upper()}_{timeframe.upper()}.csv"
    for base in search_paths:
        for base_path in _expand_path(base):
            if not base_path.exists():
                continue
            # 1) 直下にある場合
            p1 = base_path / target_name
            if p1.exists():
                df = _load_csv(p1)
                if df is not None:
                    return p1, df
            # 2) サブフォルダも探索
            for hit in base_path.rglob(target_name):
                df = _load_csv(hit)
                if df is not None:
                    return hit, df
    return None, None



=== file: app/core/logger.py ===

# app/core/logger.py
from loguru import logger
import sys
from pathlib import Path

LOG_DIR = Path(__file__).resolve().parents[2] / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)

def setup():
    """Loguru ロガーの共通設定"""
    logger.remove()
    logger.add(sys.stdout, level="INFO", format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}")
    logger.add(
        LOG_DIR / "app.log",
        rotation="10 MB",
        retention="10 days",
        compression="zip",
        level="INFO",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    )
    return logger

# すぐ利用できるように初期化
setup()



=== file: app/core/market.py ===

import MetaTrader5 as mt5
from typing import Any, Optional, Tuple


def _pip_from_symbol_info(si: Any) -> float:
    # 例: USDJPY なら point=0.001 → pip=0.01（= point*10）
    return float(si.point) * 10.0 if si and si.point else 0.0

def select_symbol(symbol: str) -> bool:
    si = mt5.symbol_info(symbol)
    if si is None:
        return False
    if not si.visible:
        if not mt5.symbol_select(symbol, True):
            return False
    return True

def spread_pips(symbol: str) -> Optional[float]:
    if not select_symbol(symbol):
        return None
    si = mt5.symbol_info(symbol)
    tick = mt5.symbol_info_tick(symbol)
    pip = _pip_from_symbol_info(si)
    if tick and pip > 0:
        return (float(tick.ask) - float(tick.bid)) / pip
    if si and pip > 0 and si.ask and si.bid:
        return (float(si.ask) - float(si.bid)) / pip
    if si and pip > 0 and si.spread:
        try:
            return (float(si.spread) * float(si.point)) / pip
        except Exception:
            pass
    return None

def tick(symbol: str) -> Optional[Tuple[float, float]]:
    """(bid, ask) を返す。取得失敗で None。"""
    if not select_symbol(symbol):
        return None
    t = mt5.symbol_info_tick(symbol)
    if t is None:
        return None
    return float(t.bid), float(t.ask)

def pips_to_price(symbol: str, pips: float) -> Optional[float]:
    if not select_symbol(symbol):
        return None
    si = mt5.symbol_info(symbol)
    pip = _pip_from_symbol_info(si)
    return pips * pip if pip > 0 else None



=== file: app/core/mt5_client.py ===

import time
import MetaTrader5 as MT5
import pandas as pd
from loguru import logger
from typing import Optional, Dict, Any
from typing import NamedTuple



POSITION_COLUMNS = [
    "ticket",
    "time",
    "time_msc",
    "time_update",
    "time_update_msc",
    "symbol",
    "magic",
    "volume",
    "price_open",
    "sl",
    "tp",
    "price_current",
    "swap",
    "profit",
    "comment",
]

class TickSpec(NamedTuple):
    tick_size: float   # 1ティックの価格幅（例: 0.01 JPY）
    tick_value: float  # 1ティック動いたときの損益（1ロットあたりの通貨）


class MT5Client:
    """MT5 発注・接続ラッパー（最小構成）"""

    def __init__(self, login: int, password: str, server: str, timeout: float = 5.0):
        self.login = login
        self.password = password
        self.server = server
        self.timeout = timeout
        self.connected = False
        self.logger = logger

    # ------------------------
    # 接続系
    # ------------------------
    def initialize(self) -> bool:
        """MT5ターミナルの初期化（ログインは login_account()）"""
        logger.info("MT5 initialize() called...")

        if not MT5.initialize():
            err = MT5.last_error()
            logger.error(f"MT5 initialize() failed: {err}")
            self.connected = False
            return False

        logger.info("MT5 initialize() succeeded")
        self.connected = True
        return True

    def login_account(self) -> bool:
        """設定されたログイン情報で MT5.login() を実行"""
        logger.info(
            f"MT5 login() called with login={self.login}, server={self.server}"
        )

        ok = MT5.login(
            self.login,
            password=self.password,
            server=self.server,
        )
        if not ok:
            err = MT5.last_error()
            logger.error(f"MT5 login() failed: {err}")
            return False

        logger.info("MT5 login() succeeded")
        return True

    def shutdown(self):
        """MT5 をシャットダウン"""
        logger.info("MT5 shutdown()")
        MT5.shutdown()
        self.connected = False

    # ------------------------
    # 発注
    # ------------------------
    def order_send(
        self,
        symbol: str,
        order_type: str,
        lot: float,
        sl: Optional[float] = None,
        tp: Optional[float] = None,
        retries: int = 3,
    ) -> Optional[int]:
        """
        成行発注（BUY / SELL）

        Parameters
        ----------
        symbol : str
        order_type : "BUY" or "SELL"
        lot : float
        sl, tp : Optional[float]
        retries : int

        Returns
        -------
        Optional[int]
            成功: チケット番号（int）
            失敗: None
        """

        if order_type not in ("BUY", "SELL"):
            raise ValueError(f"order_type must be BUY/SELL: got {order_type}")

        # --- 1) シンボル情報をチェック ---
        info = MT5.symbol_info(symbol)
        if info is None:
            logger.error(f"[order_send] symbol_info({symbol}) が None。シンボルが存在しない可能性")
            return None

        if not info.visible:
            logger.info(f"[order_send] {symbol} が非表示なので symbol_select() します")
            if not MT5.symbol_select(symbol, True):
                logger.error(f"[order_send] symbol_select({symbol}, True) に失敗")
                return None

        # --- 2) 最新ティック ---
        tick = MT5.symbol_info_tick(symbol)
        if tick is None:
            logger.error(f"[order_send] symbol_info_tick({symbol}) が None。ティックが取得できない")
            return None

        # --- 3) 注文種別と価格 ---
        if order_type == "BUY":
            mt_type = MT5.ORDER_TYPE_BUY
            price = tick.ask
        else:
            mt_type = MT5.ORDER_TYPE_SELL
            price = tick.bid

        # --- 4) 注文リクエスト ---
        request: Dict[str, Any] = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": float(lot),
            "type": mt_type,
            "price": float(price),
            "sl": float(sl) if sl is not None else 0.0,
            "tp": float(tp) if tp is not None else 0.0,
            "magic": 123456,
            "comment": "fxbot_test_order",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        last_error: Optional[tuple[int, str]] = None

        # --- 5) リトライ付き order_send ---
        for attempt in range(1, retries + 1):
            logger.info(
                f"[order_send] Try {attempt}/{retries}: {order_type} {lot} lot @ {price} {symbol}"
            )

            result = MT5.order_send(request)

            if result is None:
                last_error = MT5.last_error()
                logger.error(f"[order_send] result is None, last_error={last_error}")

            else:
                logger.info(
                    "[order_send] retcode=%s, order=%s, deal=%s, comment=%s",
                    getattr(result, "retcode", None),
                    getattr(result, "order", None),
                    getattr(result, "deal", None),
                    getattr(result, "comment", None),
                )

                # 成行なので DONE = 成功
                if result.retcode == MT5.TRADE_RETCODE_DONE:
                    ticket = int(result.order or result.deal or 0)
                    if ticket > 0:
                        logger.info(f"[order_send] 成功: ticket={ticket}")
                        return ticket
                    else:
                        logger.warning(f"[order_send] DONE だが ticket が取得できない: {result}")

                else:
                    logger.warning(
                        f"[order_send] 失敗 retcode={result.retcode}。再試行する場合があります"
                    )

            if attempt < retries:
                time.sleep(1.0)

        logger.error(f"[order_send] 全 {retries} 回リトライしても失敗。last_error={last_error}")
        return None

    # ------------------------
    # 決済（クローズ）
    # ------------------------
    def close_position(self, ticket: int, symbol: str, retries: int = 3) -> bool:
        """指定チケットの成行クローズ"""

        pos = MT5.positions_get(ticket=ticket)
        if not pos:
            logger.error(f"ticket={ticket} のポジションが存在しません")
            return False

        position = pos[0]
        lot = position.volume

        # position.type: 0=BUY, 1=SELL
        order_type = MT5.ORDER_TYPE_SELL if position.type == 0 else MT5.ORDER_TYPE_BUY

        # クローズ価格
        t = MT5.symbol_info_tick(symbol)
        if t is None:
            logger.error(f"[close_position] symbol_info_tick({symbol}) が None")
            return False

        price = t.bid if order_type == MT5.ORDER_TYPE_SELL else t.ask

        request = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": lot,
            "type": order_type,
            "position": ticket,
            "price": price,
            "magic": 123456,
            "comment": "fxbot_test_close",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        for attempt in range(1, retries + 1):
            logger.info(f"[close_position] Try {attempt}: ticket={ticket}")
            result = MT5.order_send(request)

            if result and result.retcode == MT5.TRADE_RETCODE_DONE:
                logger.info(f"クローズ成功: ticket={ticket}")
                return True

            logger.error(
                f"retcode={result.retcode if result else None}, err={MT5.last_error()}"
            )
            time.sleep(1.0)

        logger.error("[close_position] 全リトライ失敗")
        return False

    # ------------------------
    # ポジション一覧
    # ------------------------
    def get_positions(self):
        try:
            pos = MT5.positions_get()
            if pos is None:
                self.logger.warning("positions_get() returned None")
                return []
            return list(pos)
        except Exception as exc:
            self.logger.exception(f"positions_get() failed: {exc}")
            return []

    def get_positions_by_symbol(self, symbol: str):
        rows = self.get_positions()
        out = [p for p in rows if getattr(p, "symbol", None) == symbol]
        self.logger.info(f"get_positions_by_symbol: {symbol} count={len(out)}")
        return out

    def get_positions_df(self, symbol: Optional[str] = None):
        rows = self.get_positions()
        if symbol:
            rows = [p for p in rows if getattr(p, "symbol", None) == symbol]

        if not rows:
            return pd.DataFrame(columns=POSITION_COLUMNS)

        data = []
        for p in rows:
            data.append(
                {
                    "ticket": p.ticket,
                    "time": p.time,
                    "time_msc": p.time_msc,
                    "time_update": p.time_update,
                    "time_update_msc": p.time_update_msc,
                    "symbol": p.symbol,
                    "magic": p.magic,
                    "volume": p.volume,
                    "price_open": p.price_open,
                    "sl": p.sl,
                    "tp": p.tp,
                    "price_current": p.price_current,
                    "swap": p.swap,
                    "profit": p.profit,
                    "comment": p.comment,
                }
            )

        return pd.DataFrame(data, columns=POSITION_COLUMNS)

    def get_equity(self) -> float:
        """現在口座の有効証拠金（equity）を返す。"""
        info = MT5.account_info()
        if info is None:
            raise RuntimeError("account_info() が None を返しました（MT5 接続を確認してください）")

        return float(info.equity)

    def get_tick_spec(self, symbol: str) -> TickSpec:
        """
        指定シンボルの tick_size / tick_value を返す。
        - tick_size: 価格が 1 tick 動く幅
        - tick_value: その 1 tick で 1 ロットあたりの損益
        """
        info = MT5.symbol_info(symbol)
        if info is None:
            raise RuntimeError(f"symbol_info({symbol!r}) が None を返しました（シンボル名を確認してください）")

        # broker によっては trade_tick_size / trade_tick_value を使う場合もあります。
        # ここでは point / trade_tick_value を使う想定です。
        tick_size = float(getattr(info, "point", 0.0))
        tick_value = float(getattr(info, "trade_tick_value", 0.0))

        if tick_size <= 0:
            raise RuntimeError(f"{symbol!r} の tick_size が 0 以下です: {tick_size}")
        if tick_value <= 0:
            raise RuntimeError(f"{symbol!r} の tick_value が 0 以下です: {tick_value}")

        return TickSpec(tick_size=tick_size, tick_value=tick_value)



=== file: app/core/strategy_profile.py ===

"""
app.core.strategy_profile

ミチビキの「戦略プロファイル」を一元管理するモジュール。

- プロファイル ID (profile.name) は
  - config
  - backtests/{profile}/...
  - JobScheduler のコマンド
  などで共通して使うキーとする。

まずは M-A2 対応として「michibiki_std」（ミチビキ標準プロファイル）だけを定義する。
"""

from __future__ import annotations

from dataclasses import dataclass
from core.risk import LotSizingResult, compute_lot_size_from_atr

from pathlib import Path
from typing import Dict


@dataclass(frozen=True)
class StrategyProfile:
    """戦略プロファイル 1 件分の定義。

    将来的には、ここに
    - 使用するモデル名
    - 特徴量セット名
    - 追加フィルタ設定
    などを足していく。
    """

    # プロファイル ID（フォルダ名 / 設定キーに使う）
    name: str

    # 人間が読む用の説明
    description: str

    # 取引対象
    symbol: str  # 例: "USDJPY" / "USDJPY-"
    timeframe: str  # 例: "M5", "M15"

    # KPI 目標
    target_monthly_return: float  # 例: 0.03 (= +3%)
    max_monthly_dd: float  # 例: -0.20 (= -20%)

    # ATR ベース戦略の主要パラメータ
    atr_period: int
    atr_mult_entry: float
    atr_mult_sl: float

    # Walk-Forward 窓
    wf_train_months: int
    wf_test_months: int

    @property
    def backtest_root(self) -> Path:
        """バックテスト結果を置くルートフォルダを返す。

        例:
            backtests/michibiki_std
        """
        return Path("backtests") / self.name

    @property
    def monthly_returns_path(self) -> Path:
        """monthly_returns.csv の標準パス。

        M-A1 で決めた「backtests/{profile}/monthly_returns.csv」と整合させる。
        """
        return self.backtest_root / "monthly_returns.csv"

    def compute_lot_size_from_atr(
        self,
        *,
        equity: float,
        atr: float,
        tick_value: float,
        tick_size: float,
        expected_trades_per_month: int = 40,
        worst_case_trades_for_dd: int = 10,
        avg_r_multiple: float = 0.6,
        min_lot: float = 0.01,
        max_lot: float = 1.0,
    ) -> LotSizingResult:
        """
        このプロファイルに設定された target_monthly_return / max_monthly_dd / atr_mult_sl を使用して
        推奨ロットを計算するヘルパーメソッド。

        Parameters
        ----------
        equity:
            現在の口座残高 or 有効証拠金。
        atr:
            現在の ATR 値（価格単位）。
        tick_value, tick_size:
            MT5 の symbol_info(...).trade_tick_value / trade_tick_size 等から取得した値。
        expected_trades_per_month, worst_case_trades_for_dd, avg_r_multiple, min_lot, max_lot:
            ロット計算の前提パラメータ。必要なら外から上書き可能。

        Returns
        -------
        LotSizingResult
        """
        return compute_lot_size_from_atr(
            equity=equity,
            atr=atr,
            atr_mult_sl=self.atr_mult_sl,
            target_monthly_return=self.target_monthly_return,
            max_monthly_dd=self.max_monthly_dd,
            tick_value=tick_value,
            tick_size=tick_size,
            expected_trades_per_month=expected_trades_per_month,
            worst_case_trades_for_dd=worst_case_trades_for_dd,
            avg_r_multiple=avg_r_multiple,
            min_lot=min_lot,
            max_lot=max_lot,
        )


# ==== ミチビキ標準プロファイル (M-A2) ======================================


MICHIBIKI_STD = StrategyProfile(
    name="michibiki_std",
    description="ミチビキ標準プロファイル v1（USDJPY M5 / ATRベース / 月次3％目標）",
    symbol="USDJPY",
    timeframe="M5",
    # KPI 目標
    target_monthly_return=0.03,  # +3%/月を狙う
    max_monthly_dd=-0.20,  # -20% 以内に収めたい
    # ATR戦略パラメータ（暫定値：あとでWFOでチューニング）
    atr_period=14,
    atr_mult_entry=1.5,
    atr_mult_sl=3.0,
    # Walk-Forward 窓（12ヶ月訓練 -> 1ヶ月テスト を基本とする）
    wf_train_months=12,
    wf_test_months=1,
)


# 今後プロファイルを増やす場合はここに追加していく
_PROFILES: Dict[str, StrategyProfile] = {
    MICHIBIKI_STD.name: MICHIBIKI_STD,
}


def get_profile(name: str = "michibiki_std") -> StrategyProfile:
    """プロファイル ID から StrategyProfile を取得する。

    未定義の名前が来た場合は KeyError ではなく ValueError にして、
    GUI や CLI でメッセージを出しやすくしておく。
    """
    try:
        return _PROFILES[name]
    except KeyError as exc:  # pragma: no cover - 単純なエラーパス
        known = ", ".join(sorted(_PROFILES.keys()))
        raise ValueError(f"未知のプロファイル名です: {name!r} (known: {known})") from exc


def list_profiles() -> Dict[str, StrategyProfile]:
    """定義済みプロファイル一覧を dict で返す（読み取り専用想定）。"""
    return dict(_PROFILES)



=== file: app/gui/__init__.py ===




=== file: app/gui/ai_tab.py ===

# app/gui/ai_tab.py
from __future__ import annotations
from pathlib import Path
from PyQt6.QtWidgets import (
    QWidget,
    QVBoxLayout,
    QGroupBox,
    QFormLayout,
    QLabel,
    QSpinBox,
    QPushButton,
    QHBoxLayout,
    QTabWidget,
)
import joblib

from loguru import logger

from app.services.ai_service import AISvc
from app.services.recent_kpi import compute_recent_kpi_from_decisions
from app.gui.widgets.feature_importance import FeatureImportanceWidget
from app.gui.widgets.shap_bar import ShapBarWidget


class AITab(QWidget):
    def __init__(self, ai_service: AISvc | None = None, parent: QWidget | None = None) -> None:
        super().__init__(parent)
        self.ai_service = ai_service or AISvc()

        try:
            p = Path("models/LightGBM_clf.pkl")
            if p.exists():
                obj = joblib.load(p)
                model = obj.get("model", obj) if isinstance(obj, dict) else obj
                self.ai_service.models.setdefault("lgbm_cls", model)
        except Exception as e:
            print(f"[AITab] model autoload skipped: {e}")

        main_layout = QVBoxLayout(self)
        self.tab_widget = QTabWidget(self)
        main_layout.addWidget(self.tab_widget, 1)

        self.tab_kpi = QWidget(self.tab_widget)
        kpi_layout = QVBoxLayout(self.tab_kpi)

        self.recent_kpi_group = QGroupBox("Recent Trades KPI", self.tab_kpi)
        kpi_form = QFormLayout(self.recent_kpi_group)

        self.spin_recent_n = QSpinBox(self.recent_kpi_group)
        self.spin_recent_n.setRange(1, 100000)
        self.spin_recent_n.setValue(100)

        self.lbl_n_trades = QLabel("-", self.recent_kpi_group)
        self.lbl_win_rate = QLabel("-", self.recent_kpi_group)
        self.lbl_profit_factor = QLabel("-", self.recent_kpi_group)
        self.lbl_max_drawdown = QLabel("0.0", self.recent_kpi_group)
        self.lbl_max_dd_ratio = QLabel("N/A", self.recent_kpi_group)
        self.lbl_best_streaks = QLabel("0 / 0", self.recent_kpi_group)

        self.btn_refresh_kpi = QPushButton("Refresh", self.recent_kpi_group)

        kpi_form.addRow("Window (N trades)", self.spin_recent_n)
        kpi_form.addRow("Total trades", self.lbl_n_trades)
        kpi_form.addRow("Win rate", self.lbl_win_rate)
        kpi_form.addRow("Profit factor", self.lbl_profit_factor)
        kpi_form.addRow("Max drawdown", self.lbl_max_drawdown)
        kpi_form.addRow("Max DD ratio", self.lbl_max_dd_ratio)
        kpi_form.addRow("Best win / loss streak", self.lbl_best_streaks)

        btn_row = QHBoxLayout()
        btn_row.addStretch(1)
        btn_row.addWidget(self.btn_refresh_kpi)
        kpi_form.addRow(btn_row)

        kpi_layout.addWidget(self.recent_kpi_group)
        kpi_layout.addStretch(1)

        self.tab_widget.addTab(self.tab_kpi, "KPI")

        self.tab_fi = QWidget(self.tab_widget)
        fi_layout = QVBoxLayout(self.tab_fi)
        self.feature_importance = FeatureImportanceWidget(self.ai_service, self.tab_fi)
        fi_layout.addWidget(self.feature_importance)
        self.tab_widget.addTab(self.tab_fi, "Feature Importance")

        self.tab_shap = QWidget(self.tab_widget)
        shap_layout = QVBoxLayout(self.tab_shap)

        self.shap_group = QGroupBox("SHAP Global Importance", self.tab_shap)
        shap_group_layout = QVBoxLayout(self.shap_group)

        self.shap_widget = ShapBarWidget(self.ai_service, self.shap_group)
        shap_group_layout.addWidget(self.shap_widget)

        shap_layout.addWidget(self.shap_group)
        shap_layout.addStretch(1)

        self.tab_widget.addTab(self.tab_shap, "SHAP")

        self.btn_refresh_kpi.clicked.connect(self.refresh_kpi)
        self.refresh_kpi()

    def refresh_kpi(self) -> None:
        """
        recent_kpi.compute_recent_kpi_from_decisions を呼び出し、
        ラベルに KPI を表示する。
        """
        limit = self.spin_recent_n.value()

        try:
            result = compute_recent_kpi_from_decisions(
                limit=limit,
                starting_equity=100000.0,
            )
        except Exception as e:
            self.lbl_n_trades.setText("Error")
            self.lbl_win_rate.setText("Error")
            self.lbl_profit_factor.setText("Error")
            self.lbl_max_drawdown.setText("Error")
            self.lbl_max_dd_ratio.setText("Error")
            self.lbl_best_streaks.setText("Error")

            print(f"[AITab] refresh_kpi error: {e!r}")
            return

        self.lbl_n_trades.setText(str(result.n_trades))

        if result.win_rate is not None:
            self.lbl_win_rate.setText(f"{result.win_rate * 100:.1f} %")
        else:
            self.lbl_win_rate.setText("N/A")

        if result.profit_factor is not None:
            self.lbl_profit_factor.setText(f"{result.profit_factor:.2f}")
        else:
            self.lbl_profit_factor.setText("N/A")

        self.lbl_max_drawdown.setText(f"{result.max_drawdown:.1f}")

        if result.max_drawdown_ratio is not None:
            self.lbl_max_dd_ratio.setText(f"{result.max_drawdown_ratio * 100:.2f} %")
        else:
            self.lbl_max_dd_ratio.setText("N/A")

        self.lbl_best_streaks.setText(
            f"{result.best_win_streak} / {result.best_loss_streak}"
        )

        if getattr(self, "shap_widget", None) is not None:
            try:
                self.shap_widget.refresh(force=True)
            except Exception as e:
                logger.exception("failed to refresh SHAP bar: %s", e)



=== file: app/gui/backtest_tab.py ===

# app/gui/backtest_tab.py
from __future__ import annotations

import json
import pathlib
import sys
import re
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import pandas as pd
import numpy as np
from PyQt6 import QtCore, QtWidgets
from PyQt6.QtCore import Qt, QProcess
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as Canvas
from matplotlib.backends.backend_qtagg import NavigationToolbar2QT as Toolbar
from matplotlib.dates import AutoDateLocator, ConciseDateFormatter
from matplotlib.figure import Figure
from matplotlib.ticker import FuncFormatter, AutoLocator
from matplotlib.widgets import SpanSelector
import matplotlib.gridspec as gridspec
from pandas.api.types import is_datetime64_any_dtype
from app.services.data_guard import ensure_data
from functools import partial
from datetime import datetime, timedelta

def _thousands(x, pos):
    try:
        return f"{int(x):,}"
    except Exception:
        return str(x)

def _find_trades_csv(equity_csv: Path):
    """equity_curve.csv と同じフォルダにある trades*.csv を探す（優先: test -> train -> trades）"""
    cand = [
        equity_csv.with_name("trades_test.csv"),
        equity_csv.with_name("trades_train.csv"),
        equity_csv.with_name("trades.csv"),
    ]
    for c in cand:
        if c.exists():
            return c
    return None

def plot_equity_with_markers_to_figure(fig: Figure, csv_path: str, note: str = ""):
    """equity_curve.csv を描画。signal変化点でマーク。変化が無ければ trades*.csv の entry_time でマーク。"""
    p = Path(csv_path)
    try:
        df = pd.read_csv(p)
    except Exception as e:
        print(f"[gui] plot error: failed to read {csv_path}: {e}")
        return

    if "equity" not in df.columns:
        print(f"[gui] plot error: CSVに 'equity' 列がありません。columns={df.columns.tolist()}")
        return

    # 日時化
    if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
        df["time"] = pd.to_datetime(df["time"], errors="coerce")

    # ベース線
    fig.clear()
    ax = fig.add_subplot(111)
    ax.set_title("Equity Curve & Trade Markers" + (f" — {p.name}" if p.name else ""))
    ax.set_xlabel("time")
    ax.set_ylabel("equity (JPY)")
    ax.plot(df["time"], df["equity"], lw=1.4, antialiased=True, label="Equity", zorder=2)
    ax.margins(x=0.01, y=0.06)
    ax.grid(True, which="major", alpha=0.25)
    ax.yaxis.set_major_formatter(FuncFormatter(_thousands))
    ax.yaxis.set_major_locator(AutoLocator())

    # --- マーカー①: signal の変化点 ---
    buys = sells = pd.DataFrame()
    if "signal" in df.columns:
        sig = pd.to_numeric(df["signal"], errors="coerce").fillna(0).astype(int)
        chg = sig.ne(sig.shift(1)).fillna(sig.iloc[0] != 0)
        if len(sig) > 0 and sig.iloc[0] != 0:
            chg.iloc[0] = True
        buys  = df[(sig ==  1) & chg]
        sells = df[(sig == -1) & chg]

    # --- マーカー②: trades*.csv の entry_time でフォールバック ---
    if (buys.empty and sells.empty):
        tcsv = _find_trades_csv(p)
        if tcsv is not None:
            try:
                tdf = pd.read_csv(tcsv)
                # 必須列チェック
                if "entry_time" in tdf.columns:
                    tdf["entry_time"] = pd.to_datetime(tdf["entry_time"], errors="coerce")
                    # 方向の推定：directionが無ければ pnl の符号で代用
                    if "direction" in tdf.columns:
                        dirv = pd.to_numeric(tdf["direction"], errors="coerce").fillna(0).astype(int)
                    else:
                        dirv = np.sign(pd.to_numeric(tdf.get("pnl", 0.0), errors="coerce").fillna(0.0)).astype(int)
                    # エクイティ上の Y 座標は近い時刻の equity を使う
                    if "time" in df.columns:
                        eq = df[["time", "equity"]].dropna().copy()
                        eq["time"] = pd.to_datetime(eq["time"], errors="coerce")
                        eq = eq.set_index("time").sort_index()

                        # entry の最近傍 equity を拾う
                        def _y_at(t):
                            try:
                                idx = eq.index.searchsorted(t, side="left")
                                if idx == len(eq):
                                    idx -= 1
                                return float(eq.iloc[idx, 0])
                            except Exception:
                                return np.nan

                        tdf["y"] = tdf["entry_time"].map(_y_at)
                        tb = tdf[(dirv ==  1)].copy()
                        ts = tdf[(dirv == -1)].copy()
                        tb = tb.dropna(subset=["y"])
                        ts = ts.dropna(subset=["y"])
                        buys  = pd.DataFrame({"time": tb["entry_time"], "equity": tb["y"]})
                        sells = pd.DataFrame({"time": ts["entry_time"], "equity": ts["y"]})
                        print(f"[gui] fallback markers from trades: buys={len(buys)} sells={len(sells)} ({tcsv.name})")
            except Exception as e:
                print(f"[gui] trades fallback error: {e}")

    # マーカー描画
    if not buys.empty:
        ax.scatter(buys["time"], buys["equity"],
                   marker="o", s=48, facecolors="tab:blue", edgecolors="black",
                   linewidths=0.7, label="Buy", zorder=5)
    if not sells.empty:
        ax.scatter(sells["time"], sells["equity"],
                   marker="x", s=64, c="tab:orange", linewidths=1.4,
                   label="Sell", zorder=6)

    # 日付目盛り
    if "time" in df.columns:
        loc = AutoDateLocator()
        ax.xaxis.set_major_locator(loc)
        ax.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        fig.autofmt_xdate()

    ax.legend(loc="upper left")

PROJECT_ROOT = Path(__file__).resolve().parents[2]

class PlotWindow(QtWidgets.QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("FXBot — Chart")
        self.resize(1000, 650)

        self.figure = Figure(figsize=(10, 6), tight_layout=True)
        self.canvas = Canvas(self.figure)
        self.toolbar = Toolbar(self.canvas, self)
        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.toolbar)
        layout.addWidget(self.canvas)

        # 2段のaxes（上：拡大、下：全体ナビ）
        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.08, figure=self.figure)
        self.ax_main = self.figure.add_subplot(gs[0])
        self.ax_nav  = self.figure.add_subplot(gs[1], sharex=self.ax_main)
        self.span = None
        self._last_kind: str | None = None
        self._last_csv: str | None = None
        self._overlay_lines: list = []

    def plot_equity_csv(self, csv_path: str):
        # 上段をクリアして本描画（既存の描画関数を再利用）
        self.figure.clear()
        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.08, figure=self.figure)
        ax_main = self.figure.add_subplot(gs[0])
        ax_nav  = self.figure.add_subplot(gs[1], sharex=ax_main)

        # 読み込み
        df = pd.read_csv(csv_path)
        if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
            df["time"] = pd.to_datetime(df["time"], errors="coerce")

        # 上段：本描画（マーカー付き）
        ax_main.plot(df["time"], df["equity"], lw=1.4, label="Equity")
        ax_main.grid(True, alpha=0.25)
        sig = df.get("signal", pd.Series(0, index=df.index)).astype(int)
        chg = sig.ne(sig.shift(1)).fillna(False)
        if len(sig) > 0 and sig.iloc[0] != 0:
            chg.iloc[0] = True
        buys  = df[(sig == 1)  & chg]
        sells = df[(sig == -1) & chg]
        if not buys.empty:
            ax_main.scatter(buys["time"], buys["equity"], marker="o", s=48,
                            facecolors="tab:blue", edgecolors="black", linewidths=0.7, label="Buy", zorder=5)
        if not sells.empty:
            ax_main.scatter(sells["time"], sells["equity"], marker="x", s=64,
                            c="tab:orange", linewidths=1.4, label="Sell", zorder=6)
        ax_main.legend(loc="upper left")
        ax_main.margins(x=0.01, y=0.05)

        # 目盛り・タイトル
        from matplotlib.ticker import AutoLocator, FuncFormatter
        ax_main.set_title("Equity Curve & Trade Markers — " + Path(csv_path).name)
        ax_main.set_ylabel("equity (JPY)")
        ax_main.yaxis.set_major_locator(AutoLocator())
        ax_main.yaxis.set_major_formatter(FuncFormatter(_thousands))

        # 下段：全体ナビ（薄い線）
        ax_nav.plot(df["time"], df["equity"], lw=1.0, alpha=0.5)
        ax_nav.grid(True, alpha=0.2)

        # SpanSelector で範囲選択 → 上段の xlim を同期
        def onselect(xmin, xmax):
            ax_main.set_xlim(xmin, xmax)
            self.canvas.draw_idle()

        self.span = SpanSelector(ax_nav, onselect, "horizontal", useblit=True,
                                 interactive=True, props=dict(alpha=0.15))

        # 目盛り体裁
        loc = AutoDateLocator()
        ax_nav.xaxis.set_major_locator(loc)
        ax_nav.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        ax_main.xaxis.set_major_locator(loc)
        ax_main.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        self.figure.autofmt_xdate()
        self.canvas.draw_idle()

        # 保存（期間ジャンプ用に参照）
        self.ax_main = ax_main
        self.ax_nav  = ax_nav
        self._last_kind = "equity"
        self._last_csv = str(csv_path)

    def overlay_wfo_equity(
        self,
        df_train: Optional[pd.DataFrame],
        df_test: Optional[pd.DataFrame],
    ) -> None:
        """Overlay WFO train/test equity lines on this window's axes."""
        axes = self.figure.get_axes()
        if not axes:
            return
        ax_main = axes[0]

        for ln in getattr(self, "_overlay_lines", []):
            try:
                ln.remove()
            except Exception:
                pass
        self._overlay_lines = []

        def _prepare(df: Optional[pd.DataFrame]) -> Optional[Tuple[pd.Series, pd.Series]]:
            if df is None or "equity" not in df.columns:
                return None
            y = pd.to_numeric(df["equity"], errors="coerce")
            if "time" in df.columns:
                x = pd.to_datetime(df["time"], errors="coerce")
            else:
                x = pd.Series(df.index)
            mask = x.notna() & y.notna()
            if not mask.any():
                return None
            return x[mask], y[mask]

        train_data = _prepare(df_train)
        if train_data is not None:
            ln1 = ax_main.plot(
                train_data[0],
                train_data[1],
                linestyle="--",
                linewidth=1,
                color="tab:blue",
                alpha=0.7,
                label="WFO Train",
            )[0]
            self._overlay_lines.append(ln1)

        test_data = _prepare(df_test)
        if test_data is not None:
            ln2 = ax_main.plot(
                test_data[0],
                test_data[1],
                linestyle="-",
                linewidth=2,
                color="tab:red",
                alpha=0.9,
                label="WFO Test",
            )[0]
            self._overlay_lines.append(ln2)

        if self._overlay_lines:
            ax_main.legend(loc="upper left")
        self.canvas.draw_idle()

    def plot_price_preview(self, csv_path: str, note: str = ""):
        import pandas as pd
        from matplotlib.dates import AutoDateLocator, ConciseDateFormatter
        df = pd.read_csv(csv_path)
        if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
            df["time"] = pd.to_datetime(df["time"], errors="coerce")
        if "close" not in df.columns:
            raise ValueError("CSVに 'close' 列がありません。")

        self.figure.clear()
        ax = self.figure.add_subplot(111)
        price = pd.to_numeric(df["close"], errors="coerce").ffill()
        norm = price / price.iloc[0] * 100.0
        ax.plot(df["time"] if "time" in df.columns else range(len(norm)),
                norm.values, lw=1.4, label="Price (close, =100@start)")
        ax.set_title("Price Preview (from OHLCV)" + (f" — {note}" if note else ""))
        ax.set_ylabel("index (=100@start)")
        ax.set_xlabel("time")
        ax.grid(True, alpha=0.25)
        if "time" in df.columns:
            loc = AutoDateLocator()
            ax.xaxis.set_major_locator(loc)
            ax.xaxis.set_major_formatter(ConciseDateFormatter(loc))
            self.figure.autofmt_xdate()
        ax.legend(loc="best")
        self.canvas.draw_idle()
        self.ax_main = ax
        self.ax_nav = None
        self._last_kind = "price"
        self._last_csv = str(csv_path)

    def plot_heatmap(self, df, note: str = ""):
        """
        tools/backtest_run が生成する monthly_returns_*.csv の形式に対応したヒートマップ描画。

        期待カラム:
            - 'year'
            - 'month'
            - 値列: 'return' or 'ret' or 'pnl' or 'pnl_pct' のいずれか
        """
        import pandas as pd
        from matplotlib.ticker import PercentFormatter

        self.figure.clear()
        ax = self.figure.add_subplot(111)

        if "year" not in df.columns or "month" not in df.columns:
            QtWidgets.QMessageBox.information(
                self, "情報",
                "月次リターンCSVに 'year' または 'month' 列がありません。"
            )
            return

        value_col = None
        for cand in ("return", "ret", "pnl_pct", "pnl"):
            if cand in df.columns:
                value_col = cand
                break

        df = df.copy()

        if value_col is not None:
            df["value"] = pd.to_numeric(df[value_col], errors="coerce")
        else:
            candidate_cols = [c for c in df.columns if c not in ("year", "month")]
            auto_col = None
            for c in candidate_cols:
                s = pd.to_numeric(df[c], errors="coerce")
                if s.notna().any():
                    auto_col = c
                    df["value"] = s
                    break

            if auto_col is None:
                QtWidgets.QMessageBox.information(
                    self,
                    "情報",
                    "月次リターンCSVに数値のリターン列が見つかりません。\n"
                    f"列一覧: {list(df.columns)}"
                )
                return

            print(f"[heatmap] auto-selected value column: {auto_col}")
        df = df.dropna(subset=["value"])

        if df.empty:
            QtWidgets.QMessageBox.information(self, "情報", "月次リターンに有効なデータがありません。")
            return

        pivot = df.pivot_table(
            index="year",
            columns="month",
            values="value",
            aggfunc="sum"
        ).reindex(columns=range(1, 13)).fillna(0.0)

        if pivot.empty:
            QtWidgets.QMessageBox.information(self, "情報", "月次リターンが空です。")
            return

        im = ax.imshow(pivot.values, aspect="auto", interpolation="nearest")

        ax.set_xticks(range(12))
        ax.set_xticklabels([f"{m:02d}" for m in range(1, 13)])
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels(pivot.index.astype(str))

        ax.set_title("Monthly Return Heatmap" + (f" — {note}" if note else ""))
        ax.set_xlabel("month")
        ax.set_ylabel("year")

        cbar = self.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.ax.yaxis.set_major_formatter(PercentFormatter(1.0))

        self.canvas.draw_idle()

    # === ���ԃW�����vAPI�i1W / 1M / ALL�j ===
    def jump_range(self, mode: str) -> None:
        """
        ポップアウト表示で 1W / 1M / ALL の X 範囲を切り替える。
        1) CSV の time 列 → 2) 描画中 xdata → 3) ファイル名のTF推定でバー数ズーム。
        """
        valid_modes = {"1W", "1M", "ALL"}
        mode = (mode or "").upper()
        if mode not in valid_modes:
            raise ValueError(f"Unknown mode: {mode}")

        if self._last_kind not in {"equity", "price"} or not self._last_csv:
            raise RuntimeError("直近に価格/エクイティを表示してから操作してください。")

        csv_path = Path(self._last_csv)
        if not csv_path.exists():
            raise RuntimeError(f"直近のCSVが見つかりません: {csv_path}")

        def _target_axes():
            axes = []
            seen: set[int] = set()
            for candidate in (getattr(self, "ax_main", None), getattr(self, "ax_nav", None)):
                if candidate is None or candidate not in self.figure.axes:
                    continue
                ident = id(candidate)
                if ident in seen:
                    continue
                seen.add(ident)
                axes.append(candidate)
            if not axes:
                for ax in self.figure.axes:
                    ident = id(ax)
                    if ident in seen:
                        continue
                    seen.add(ident)
                    axes.append(ax)
            return axes

        def _apply_xlim(xmin, xmax):
            axes = _target_axes()
            if not axes:
                raise RuntimeError("描画済みのAxesが無いためジャンプできません。")
            for ax in axes:
                ax.set_xlim(xmin, xmax)
            self.canvas.draw_idle()

        def _extract_xdata():
            for ax in (getattr(self, "ax_nav", None), getattr(self, "ax_main", None)):
                if ax is None:
                    continue
                lines = ax.get_lines()
                if lines:
                    data = lines[0].get_xdata()
                    if data is not None and len(data) >= 2:
                        return data
            for ax in self.figure.axes:
                lines = ax.get_lines()
                if lines:
                    data = lines[0].get_xdata()
                    if data is not None and len(data) >= 2:
                        return data
            return None

        def _apply_time_window(series: pd.Series) -> bool:
            series = pd.Series(series).dropna()
            if len(series) < 2:
                return False
            series = series.sort_values()
            latest = pd.Timestamp(series.iloc[-1])
            if mode == "ALL":
                earliest = pd.Timestamp(series.iloc[0])
            elif mode == "1W":
                earliest = latest - pd.Timedelta(days=7)
            else:
                earliest = latest - pd.Timedelta(days=31)
            first = pd.Timestamp(series.iloc[0])
            earliest = max(first, earliest)
            _apply_xlim(earliest.to_pydatetime(), latest.to_pydatetime())
            return True

        tf_keywords = {
            "M1": 1, "M3": 3, "M5": 5, "M10": 10, "M15": 15, "M30": 30,
            "H1": 60, "H2": 120, "H3": 180, "H4": 240, "H6": 360, "H8": 480, "H12": 720,
            "D1": 1440, "D2": 2880, "W1": 10080
        }

        def _infer_tf_minutes(path: Path) -> int | None:
            chunks = [path.stem.upper(), *(part.upper() for part in path.parts)]
            for name in chunks:
                for token, minutes in tf_keywords.items():
                    if re.search(rf"(?<![A-Z0-9]){token}(?![A-Z0-9])", name):
                        return minutes
                for part in re.split(r"[^A-Z0-9]+", name):
                    if not part:
                        continue
                    if part in tf_keywords:
                        return tf_keywords[part]
                    m = re.fullmatch(r"(M|H|D|W)(\d{1,3})", part)
                    if m:
                        unit = m.group(1)
                        value = int(m.group(2))
                        if value == 0:
                            continue
                        if unit == "M":
                            return value
                        if unit == "H":
                            return value * 60
                        if unit == "D":
                            return value * 1440
                        if unit == "W":
                            return value * 10080
            return None

        def _bars_for_mode(tf_minutes: int, total: int) -> int:
            if mode == "ALL":
                return total
            days = 7 if mode == "1W" else 31
            bars = int(days * 24 * 60 / max(tf_minutes, 1))
            return max(1, bars)

        try:
            df_time = pd.read_csv(csv_path, usecols=["time"])
        except ValueError:
            df_time = None
        except Exception as e:
            print(f"[pop] jump_range warn: failed to read time column from {csv_path}: {e}")
            df_time = None

        if df_time is not None and "time" in df_time.columns:
            times = pd.to_datetime(df_time["time"], errors="coerce")
            if _apply_time_window(times):
                return

        xdata = _extract_xdata()
        if xdata is None:
            raise RuntimeError("描画済みのXデータが無いためジャンプできません。")

        arr = np.asarray(xdata)
        if arr.size < 2:
            raise RuntimeError("描画済みデータが少なすぎます。")

        is_datetime = np.issubdtype(arr.dtype, np.datetime64)
        if not is_datetime and arr.dtype == object:
            sample = next((v for v in arr if v is not None), None)
            if isinstance(sample, (pd.Timestamp, datetime, np.datetime64)):
                is_datetime = True

        if is_datetime:
            if _apply_time_window(pd.Series(arr)):
                return

        tf_minutes = _infer_tf_minutes(csv_path)
        if tf_minutes is None:
            raise RuntimeError("ファイル名からタイムフレームを推定できません。例: *_M5_*.csv")

        total = arr.size
        bars = _bars_for_mode(tf_minutes, total)
        start_idx = 0 if mode == "ALL" else max(0, total - bars)
        xmin = arr[start_idx]
        xmax = arr[-1]
        if isinstance(xmin, np.generic):
            xmin = xmin.item()
        if isinstance(xmax, np.generic):
            xmax = xmax.item()
        _apply_xlim(xmin, xmax)

class BacktestTab(QtWidgets.QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)

        # === 入力フォーム ===
        self.symbol_edit = QtWidgets.QLineEdit("USDJPY")
        self.tf_combo = QtWidgets.QComboBox(); self.tf_combo.addItems(["M5", "M15", "H1"])

        # 実行／表示モード（Backtest / Walk-Forward / Overlay）
        self.mode_bt = QtWidgets.QRadioButton("Backtest")
        self.mode_wfo = QtWidgets.QRadioButton("Walk-Forward")
        self.mode_overlay = QtWidgets.QRadioButton("Overlay")

        self.mode_bt.setChecked(True)  # デフォルトは Backtest

        self.mode_group = QtWidgets.QButtonGroup(self)
        self.mode_group.addButton(self.mode_bt)
        self.mode_group.addButton(self.mode_wfo)
        self.mode_group.addButton(self.mode_overlay)

        # From/To 日付ピッカー
        self.start_edit = QtWidgets.QDateEdit(QtCore.QDate(2024, 1, 1))
        self.start_edit.setDisplayFormat("yyyy-MM-dd")
        self.start_edit.setCalendarPopup(True)

        self.end_edit = QtWidgets.QDateEdit(QtCore.QDate.currentDate())
        self.end_edit.setDisplayFormat("yyyy-MM-dd")
        self.end_edit.setCalendarPopup(True)

        self.capital_edit = QtWidgets.QLineEdit("100000")
        self.layout_combo = QtWidgets.QComboBox(); self.layout_combo.addItems(["per-symbol", "flat"])
        self.train_ratio_edit = QtWidgets.QLineEdit("0.7")

        form = QtWidgets.QGridLayout()
        r = 0
        form.addWidget(QtWidgets.QLabel("Symbol"), r, 0); form.addWidget(self.symbol_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("Timeframe"), r, 2); form.addWidget(self.tf_combo, r, 3); r += 1

        # Mode 行（ラジオボタンを横並び）
        mode_layout = QtWidgets.QHBoxLayout()
        mode_layout.addWidget(self.mode_bt)
        mode_layout.addWidget(self.mode_wfo)
        mode_layout.addWidget(self.mode_overlay)
        mode_layout.addStretch(1)

        form.addWidget(QtWidgets.QLabel("Mode"), r, 0)
        form.addLayout(mode_layout, r, 1, 1, 3)
        r += 1

        form.addWidget(QtWidgets.QLabel("Layout"), r, 0); form.addWidget(self.layout_combo, r, 1); r += 1
        form.addWidget(QtWidgets.QLabel("Start"), r, 0); form.addWidget(self.start_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("End"),   r, 2); form.addWidget(self.end_edit,   r, 3); r += 1
        form.addWidget(QtWidgets.QLabel("Initial Capital (JPY)"), r, 0); form.addWidget(self.capital_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("Train Ratio (WFO)"), r, 2); form.addWidget(self.train_ratio_edit, r, 3); r += 1

        # === ボタン ===
        self.btn_update = QtWidgets.QPushButton("データ確認＆更新")
        self.btn_run    = QtWidgets.QPushButton("テスト実行")
        self.btn_popout = QtWidgets.QPushButton("別ウインドウで表示")
        self.btn_export = QtWidgets.QPushButton("結果をエクスポート(JSON)")
        self.btn_savepng= QtWidgets.QPushButton("グラフをPNG保存")
        self.btn_heatmap= QtWidgets.QPushButton("月次ヒートマップ")

        btns = QtWidgets.QHBoxLayout()
        btns.addWidget(self.btn_update); btns.addWidget(self.btn_run)
        btns.addWidget(self.btn_popout); btns.addWidget(self.btn_heatmap)
        btns.addStretch(1)
        btns.addWidget(self.btn_savepng); btns.addWidget(self.btn_export)

        # === 期間ジャンプ（1W / 1M / ALL） ===
        range_box = QtWidgets.QHBoxLayout()
        range_box.setContentsMargins(0, 0, 0, 0)
        range_box.setSpacing(6)
        range_box.addWidget(QtWidgets.QLabel("期間:"))

        self.btn_1w  = QtWidgets.QPushButton("1W")
        self.btn_1m  = QtWidgets.QPushButton("1M")
        self.btn_all = QtWidgets.QPushButton("ALL")
        for b in (self.btn_1w, self.btn_1m, self.btn_all):
            b.setFixedHeight(26)

        self.btn_1w.clicked.connect(partial(self._on_range_jump, "1W"))
        self.btn_1m.clicked.connect(partial(self._on_range_jump, "1M"))
        self.btn_all.clicked.connect(partial(self._on_range_jump, "ALL"))

        range_box.addWidget(self.btn_1w)
        range_box.addWidget(self.btn_1m)
        range_box.addWidget(self.btn_all)
        range_box.addStretch(1)

        # === プロット・メトリクス ===
        self.fig = Figure(figsize=(6,3), constrained_layout=True)
        self.canvas = Canvas(self.fig)
        self.label_meta = QtWidgets.QLabel("未実行"); self.label_meta.setWordWrap(True)

        # モデル情報（active_model.json）
        self.model_info = QtWidgets.QLabel("model: -")
        self.model_info.setStyleSheet("color: #666;")

        # 出力パス
        self.path_edit = QtWidgets.QLineEdit(""); self.path_edit.setReadOnly(True)
        self.btn_open  = QtWidgets.QPushButton("CSVを開く…")
        path_line = QtWidgets.QHBoxLayout()
        path_line.addWidget(QtWidgets.QLabel("Equity CSV:")); path_line.addWidget(self.path_edit, 1); path_line.addWidget(self.btn_open)

        # 進捗ログ
        self.progress_box = QtWidgets.QPlainTextEdit(); self.progress_box.setReadOnly(True); self.progress_box.setMaximumBlockCount(1000)

        # メトリクス表
        self.table = QtWidgets.QTableWidget(0, 2)
        self.table.setHorizontalHeaderLabels(["Metric", "Value"])
        self.table.horizontalHeader().setStretchLastSection(True)
        self.table.setMinimumHeight(140)

        # Walk-Forward Stats パネル（後で metrics_wfo.json から更新）
        self.wfo_pf_label = QtWidgets.QLabel("WFO PF: -")
        self.wfo_winrate_label = QtWidgets.QLabel("WFO Win%: -")
        self.wfo_trades_label = QtWidgets.QLabel("WFO Trades: -")
        self.wfo_maxdd_label = QtWidgets.QLabel("WFO MaxDD: -")

        wfo_stats_layout = QtWidgets.QVBoxLayout()
        wfo_stats_layout.addWidget(self.wfo_pf_label)
        wfo_stats_layout.addWidget(self.wfo_winrate_label)
        wfo_stats_layout.addWidget(self.wfo_trades_label)
        wfo_stats_layout.addWidget(self.wfo_maxdd_label)

        self.wfo_stats_group = QtWidgets.QGroupBox("Walk-Forward Stats")
        self.wfo_stats_group.setLayout(wfo_stats_layout)

        # 全体配置
        lay = QtWidgets.QVBoxLayout(self)
        lay.addLayout(form)
        lay.addLayout(btns)
        lay.addLayout(range_box)          # ← 期間ボタン列を追加
        lay.addWidget(self.canvas, 1)
        lay.addWidget(self.label_meta)
        lay.addLayout(path_line)
        lay.addWidget(QtWidgets.QLabel("モデル情報:")); lay.addWidget(self.model_info)
        lay.addWidget(QtWidgets.QLabel("メトリクス:")); lay.addWidget(self.table)
        lay.addWidget(QtWidgets.QLabel("進捗ログ:")); lay.addWidget(self.progress_box, 2)
        lay.addWidget(self.wfo_stats_group)

        # 内部状態
        self.proc: QProcess | None = None
        self._last_plot_kind = None   # "equity" or "price"
        self._last_plot_data = None
        self._last_plot_note = ""
        self._pop = None              # PlotWindow
        self.plot_window: PlotWindow | None = None
        self._wfo_train_df: Optional[pd.DataFrame] = None
        self._wfo_test_df: Optional[pd.DataFrame] = None
        self._wfo_overlay_lines: list = []
        self._last_monthly_returns: Path | None = None

        # シグナル
        self.btn_update.clicked.connect(self._update_data)
        self.btn_run.clicked.connect(self._run_test)
        self.btn_open.clicked.connect(self._pick_file)
        self.btn_popout.clicked.connect(self._pop_out)
        self.btn_export.clicked.connect(self._export_result_json)
        self.btn_savepng.clicked.connect(self._save_png)
        self.btn_heatmap.clicked.connect(self._show_heatmap)
        # ダブルクリックでポップアウト
        def _on_canvas_dbl(ev):
            if ev.dblclick: self._pop_out()
        self.canvas.mpl_connect("button_press_event", _on_canvas_dbl)

        # 初回モデル情報表示
        self._load_model_info()
        # モード変更時に Walk-Forward 固有の入力を ON/OFF
        self.mode_bt.toggled.connect(self._on_mode_changed)
        self.mode_wfo.toggled.connect(self._on_mode_changed)
        self.mode_overlay.toggled.connect(self._on_mode_changed)

        # 初期状態の反映
        self._on_mode_changed()

    # ------------------ ヘルパ ------------------
    def _append_progress(self, text: str):
        self.progress_box.appendPlainText(text.rstrip())

    def _on_mode_changed(self, checked: bool = False):
        # Walk-Forward ラジオが ON のときだけ train_ratio を有効にする
        is_wfo = self.mode_wfo.isChecked()
        self.train_ratio_edit.setEnabled(is_wfo)

    def _current_mode_text(self) -> str:
        """UI 上のモード文字列を返す（Backtest / Walk-Forward / Overlay）。"""
        if self.mode_wfo.isChecked():
            return "Walk-Forward"
        if self.mode_overlay.isChecked():
            return "Overlay"
        return "Backtest"

    def _find_latest_wfo_dir(self) -> Optional[pathlib.Path]:
        base = pathlib.Path("logs") / "backtest"
        if not base.exists():
            return None

        candidates = list(base.rglob("metrics_wfo.json"))
        if not candidates:
            return None

        latest = max(candidates, key=lambda p: p.stat().st_mtime)
        return latest.parent

    def _load_latest_wfo_data(self) -> Optional[Dict[str, object]]:
        wfo_dir = self._find_latest_wfo_dir()
        if wfo_dir is None:
            print("[gui] no WFO dir found under logs/backtest")
            return None

        metrics_path = wfo_dir / "metrics_wfo.json"
        train_path = wfo_dir / "equity_train.csv"
        test_path = wfo_dir / "equity_test.csv"

        if not (metrics_path.exists() and train_path.exists() and test_path.exists()):
            print("[gui] missing WFO files:", metrics_path, train_path, test_path)
            return None

        with metrics_path.open("r", encoding="utf-8") as f:
            metrics = json.load(f)

        df_train = pd.read_csv(train_path)
        df_test = pd.read_csv(test_path)

        return {"metrics": metrics, "train": df_train, "test": df_test}

    def _update_wfo_stats_panel(self, metrics: Dict[str, object]) -> None:
        test = metrics.get("test") or {}
        pf = test.get("profit_factor") or test.get("pf")
        win_rate = test.get("win_rate")
        trades = test.get("trades") or test.get("num_trades")
        max_dd = test.get("max_drawdown") or test.get("max_dd")

        if isinstance(pf, (int, float)):
            self.wfo_pf_label.setText(f"WFO PF: {pf:.2f}")
        else:
            self.wfo_pf_label.setText("WFO PF: -")

        if isinstance(win_rate, (int, float)):
            val = win_rate * 100 if win_rate <= 1 else win_rate
            self.wfo_winrate_label.setText(f"WFO Win%: {val:.1f}%")
        else:
            self.wfo_winrate_label.setText("WFO Win%: -")

        self.wfo_trades_label.setText(f"WFO Trades: {trades}" if trades is not None else "WFO Trades: -")

        if isinstance(max_dd, (int, float)):
            self.wfo_maxdd_label.setText(f"WFO MaxDD: {max_dd:.2f}")
        else:
            self.wfo_maxdd_label.setText("WFO MaxDD: -")

    def _overlay_wfo_equity(
        self,
        df_train: Optional[pd.DataFrame],
        df_test: Optional[pd.DataFrame],
    ) -> None:
        """Overlay Train/Test equity lines onto the current plot axes."""
        lines = getattr(self, "_wfo_overlay_lines", [])
        for line in lines:
            try:
                line.remove()
            except Exception:
                pass
        self._wfo_overlay_lines = []

        if not self.fig.axes:
            return
        ax = self.fig.axes[0]

        def _prepare(df: Optional[pd.DataFrame]) -> Optional[Tuple[pd.Series, pd.Series]]:
            if df is None or "equity" not in df.columns:
                return None
            y = pd.to_numeric(df["equity"], errors="coerce")
            if "time" in df.columns:
                x = pd.to_datetime(df["time"], errors="coerce")
            else:
                x = pd.Series(df.index)
            mask = x.notna() & y.notna()
            if not mask.any():
                return None
            return x[mask], y[mask]

        train_data = _prepare(df_train)
        if train_data is not None:
            line, = ax.plot(
                train_data[0], train_data[1],
                linestyle="--", linewidth=1.0, color="tab:blue",
            )
            self._wfo_overlay_lines.append(line)

        test_data = _prepare(df_test)
        if test_data is not None:
            line, = ax.plot(
                test_data[0], test_data[1],
                linestyle="-", linewidth=2.2, color="tab:red",
            )
            self._wfo_overlay_lines.append(line)

        self.canvas.draw_idle()

    class _WFOResult(QtCore.QObject):
        """
        Walk-Forward 検証の結果セットをまとめて持つだけの小さな入れ物。
        report_json: logs/retrain/report_*.json の中身
        equity_train: equity_train_*.csv → DataFrame
        equity_test:  equity_test_*.csv → DataFrame
        run_id: "report_XXXX.json" の XXXX 部分
        """

        def __init__(
            self,
            report_json: Dict[str, Any],
            equity_train: Optional[pd.DataFrame],
            equity_test: Optional[pd.DataFrame],
            run_id: str,
            parent: Optional[QtCore.QObject] = None,
        ) -> None:
            super().__init__(parent)
            self.report_json = report_json
            self.equity_train = equity_train
            self.equity_test = equity_test
            self.run_id = run_id

    # ---------------------------------------------------------
    # ここからが実際のヘルパーメソッド
    # ---------------------------------------------------------

    def _find_latest_wfo_files(self) -> Optional["_WFOResult"]:
        """
        logs/retrain/ 配下から最新の report_*.json を探し、
        対応する equity_train_*.csv / equity_test_*.csv を読み込んで返す。
        見つからなければ None を返す。
        """
        base_dir = Path("logs") / "retrain"
        if not base_dir.exists():
            # ログフォルダ自体がない場合
            print("[WFO] logs/retrain が存在しません")
            return None

        # report_*.json を更新日時順にソート
        report_files = sorted(
            base_dir.glob("report_*.json"),
            key=lambda p: p.stat().st_mtime,
        )
        if not report_files:
            print("[WFO] report_*.json が見つかりません")
            return None

        latest_report = report_files[-1]
        stem = latest_report.stem  # 例: "report_1762505757473982"
        parts = stem.split("_", 1)
        run_id = parts[1] if len(parts) == 2 else ""

        try:
            with latest_report.open("r", encoding="utf-8") as f:
                report_json = json.load(f)
        except Exception as e:  # noqa: BLE001
            print(f"[WFO] report JSON の読込に失敗: {latest_report} ({e})")
            return None

        def _load_equity_csv(prefix: str) -> Optional[pd.DataFrame]:
            # run_id があるときは equity_train_{id}.csv を優先
            # なければ equity_train.csv を見る
            if run_id:
                candidate = base_dir / f"{prefix}_{run_id}.csv"
                if candidate.exists():
                    path = candidate
                else:
                    # フォールバック
                    path = base_dir / f"{prefix}.csv"
            else:
                path = base_dir / f"{prefix}.csv"

            if not path.exists():
                print(f"[WFO] {path} が存在しません（スキップ）")
                return None

            try:
                df = pd.read_csv(path)
            except Exception as e:  # noqa: BLE001
                print(f"[WFO] {path} の読込に失敗 ({e})")
                return None

            # time カラムがあれば datetime にしておく（グラフ用）
            if "time" in df.columns:
                df["time"] = pd.to_datetime(df["time"], errors="coerce")

            return df

        equity_train = _load_equity_csv("equity_train")
        equity_test = _load_equity_csv("equity_test")

        return self._WFOResult(
            report_json=report_json,
            equity_train=equity_train,
            equity_test=equity_test,
            run_id=run_id,
            parent=self,
        )

    def _debug_print_wfo_summary(self, wfo: "_WFOResult") -> None:
        """
        とりあえず「ちゃんと読めたか」を確認するために、
        コンソールにざっくりサマリを吐く。
        あとでここから GUI のラベル更新などに差し替えればOK。
        """
        print("========== [WFO Latest Result] ==========")
        print(f"run_id       : {wfo.run_id}")
        print(f"report keys  : {list(wfo.report_json.keys())}")

        train_len = len(wfo.equity_train) if wfo.equity_train is not None else 0
        test_len = len(wfo.equity_test) if wfo.equity_test is not None else 0
        print(f"equity_train : {train_len} rows")
        print(f"equity_test  : {test_len} rows")

        # よくありそうなキーがあればチラ見せ（なければスキップでOK）
        for k in ("symbol", "timeframe", "train_period", "test_period", "pf_train", "pf_test", "win_rate_train", "win_rate_test"):
            if k in wfo.report_json:
                print(f"{k:>15}: {wfo.report_json[k]}")

        print("=========================================")

    def _load_model_info(self):
        p = PROJECT_ROOT / "active_model.json"
        if p.exists():
            try:
                j = json.loads(p.read_text(encoding="utf-8"))
                name = j.get("model_name") or j.get("name") or "-"
                th   = j.get("best_threshold") or j.get("threshold") or "-"
                fh   = j.get("features_hash") or "-"
                self.model_info.setText(f"name={name}  best_threshold={th}  features_hash={fh}")
            except Exception as e:
                self.model_info.setText(f"(model info error: {e})")
        else:
            self.model_info.setText("(active_model.json が見つかりません)")

    # ------------------ データ更新 ------------------
    def _update_data(self):
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()
        mode_text = self._current_mode_text()
        layout = self.layout_combo.currentText()

        # start/end を日付ピッカーから "YYYY-MM-DD" 形式で取得
        if isinstance(self.start_edit, QtWidgets.QDateEdit):
            start = self.start_edit.date().toString("yyyy-MM-dd")
        else:
            start = self.start_edit.text().strip()

        if isinstance(self.end_edit, QtWidgets.QDateEdit):
            end = self.end_edit.date().toString("yyyy-MM-dd")
        else:
            end = self.end_edit.text().strip()

        # ここで ensure_data のログを出す
        self._append_progress(
            f"[gui] ensure_data sym={sym} tf={tf} start={start} end={end} layout={layout}"
        )

        try:
            csv = ensure_data(sym, tf, start, end, env="laptop", layout=layout)
            self._load_plot(csv)  # OHLCVプレビュー
            self.label_meta.setText(f"データOK: {csv}")
        except Exception as e:
            self.label_meta.setText(f"データ更新失敗: {e}")
            self._append_progress(f"[update] error: {e}")

    # ------------------ 実行（QProcess） ------------------
    def _run_test(self):
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()

        # モード文字列（Backtest / Walk-Forward / Overlay）
        mode_text = self._current_mode_text()
        # CLI に渡すモードフラグ（tools.backtest_run 側の想定）
        mode = "wfo" if mode_text == "Walk-Forward" else "bt"

        # 日付は QDateEdit なら date() から、そうでなければ text() から
        if isinstance(self.start_edit, QtWidgets.QDateEdit):
            start = self.start_edit.date().toString("yyyy-MM-dd")
        else:
            start = self.start_edit.text().strip()

        if isinstance(self.end_edit, QtWidgets.QDateEdit):
            end = self.end_edit.date().toString("yyyy-MM-dd")
        else:
            end = self.end_edit.text().strip()

        capital = self.capital_edit.text().strip()
        layout  = self.layout_combo.currentText()
        train_ratio = self.train_ratio_edit.text().strip() or "0.7"

        # 実行前にデータを保証
        try:
            csv = ensure_data(sym, tf, start, end, env="laptop", layout=layout)
        except Exception as e:
            self.label_meta.setText(f"データ不足: {e}")
            self._append_progress(f"[ensure_data] error before run: {e}")
            return

        # 既存プロセスがあれば殺す
        if self.proc:
            self.proc.kill()
            self.proc = None

        # “-m tools.backtest_run” でモジュール実行
        args = [
            "-m", "tools.backtest_run",
            "--csv", str(csv),
            "--start-date", start,
            "--end-date", end,
            "--capital", capital,
            "--mode", mode,
            "--symbol", sym,
            "--timeframe", tf,
            "--layout", layout,
        ]
        if mode == "wfo":
            args += ["--train-ratio", train_ratio]

        self._append_progress("[gui] run: python " + " ".join(args))

        self.proc = QProcess(self)
        self.proc.setProgram(sys.executable)
        self.proc.setArguments(args)
        self.proc.setWorkingDirectory(str(PROJECT_ROOT))  # 重要：プロジェクト直下をCWDに

        self.proc.setProcessChannelMode(QProcess.ProcessChannelMode.MergedChannels)
        self.proc.readyReadStandardOutput.connect(self._on_proc_output)
        self.proc.finished.connect(
            lambda code, status: self._on_proc_finished(code, status, sym, tf, mode)
        )
        self.label_meta.setText("実行中…")
        self.proc.start()

        # Walk-Forward の場合は、学習側WFOの最新レポートをコンソールにサマリ表示
        if mode_text == "Walk-Forward":
            try:
                wfo = self._find_latest_wfo_files()
                if wfo is None:
                    print("[WFO] 結果ファイルが見つかりませんでした")
                else:
                    self._debug_print_wfo_summary(wfo)
            except Exception as e:  # 念のためここで例外を潰しておくとGUIごと落ちない
                print(f"[WFO] summary error: {e}")

    def _on_proc_output(self):
        if not self.proc: return
        out = bytes(self.proc.readAllStandardOutput()).decode("utf-8", errors="replace")
        if out:
            for line in out.splitlines():
                self._append_progress(line)

    def _on_proc_finished(self, code: int, status: QtCore.QProcess.ExitStatus, sym: str, tf: str, mode: str):
        if code != 0:
            self.label_meta.setText(f"失敗(code={code})")
            self._append_progress(f"[gui] process failed code={code}")
            return

        out_dir = PROJECT_ROOT / "logs" / "backtest" / sym / tf
        out_csv = out_dir / "equity_curve.csv"
        self.path_edit.setText(str(out_csv))
        if not out_csv.exists():
            self.label_meta.setText(f"出力CSVが見つかりません: {out_csv}")
            self._append_progress(f"[gui] missing file: {out_csv}")
            return

        self._load_plot(out_csv)  # Equity描画

        # まずは毎回クリアしておく
        self._wfo_train_df = None
        self._wfo_test_df = None

        if mode == "wfo":
            wfo = self._load_latest_wfo_data()
            if wfo:
                self._update_wfo_stats_panel(wfo["metrics"])
                self._overlay_wfo_equity(wfo["train"], wfo["test"])
                # 最新の overlay データを属性として保持しておく
                self._wfo_train_df = wfo["train"]
                self._wfo_test_df = wfo["test"]
            else:
                # WFOだけど結果読めなかった → インライン overlay も消す
                self._overlay_wfo_equity(None, None)
        else:
            # Backtestモードなど → overlay は全部クリア
            self._overlay_wfo_equity(None, None)

        if hasattr(self, "plot_window") and self.plot_window is not None:
            try:
                self.plot_window.overlay_wfo_equity(self._wfo_train_df, self._wfo_test_df)
            except Exception as e:
                print("[gui] WFO overlay on new window failed:", e)

        metrics_path = out_dir / ("metrics.json" if mode=="bt" else "metrics_wfo.json")
        self._load_metrics(metrics_path)

        # 月次リターン保存パスを控える
        self._last_monthly_returns = out_dir / ("monthly_returns.csv" if mode=="bt" else "monthly_returns_test.csv")

        self.label_meta.setText("完了")

    # ------------------ ユーティリティUI ------------------
    def _pick_file(self):
        p, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Equity CSV", str(PROJECT_ROOT), "CSV (*.csv)")
        if p:
            self.path_edit.setText(p)
            self._load_plot(p)

    def _save_png(self):
        p, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Save PNG", str(PROJECT_ROOT / "logs"), "PNG (*.png)")
        if not p: return
        try:
            self.fig.savefig(p, dpi=150)
            self._append_progress(f"[gui] saved png: {p}")
        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "保存失敗", str(e))

    def _export_result_json(self):
        # 画面パラメータ＋メトリクス＋モデル情報をまとめて保存
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()
        mode = self._current_mode_text()
        payload = {
            "params": {
                "symbol": sym, "timeframe": tf,
                "mode": mode,
                "start": self.start_edit.text().strip(),
                "end": self.end_edit.text().strip(),
                "capital": float(self.capital_edit.text().strip() or 0),
                "layout": self.layout_combo.currentText(),
                "train_ratio": float(self.train_ratio_edit.text().strip() or 0.7)
            },
            "model": self.model_info.text(),
        }
        # metrics 表からも収集
        for r in range(self.table.rowCount()):
            k = self.table.item(r,0).text()
            v = self.table.item(r,1).text()
            payload.setdefault("metrics", {})[k] = v

        p, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Export JSON", str(PROJECT_ROOT / "logs"), "JSON (*.json)")
        if not p: return
        Path(p).write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        self._append_progress(f"[gui] exported: {p}")

    def _show_heatmap(self):
        if not self._last_monthly_returns or not self._last_monthly_returns.exists():
            QtWidgets.QMessageBox.information(
                self,
                "情報",
                "月次リターンがまだありません。先にテスト実行してください。"
            )
            return
        try:
            df = pd.read_csv(self._last_monthly_returns)
        except Exception as e:
            QtWidgets.QMessageBox.critical(
                self,
                "エラー",
                f"月次リターンCSVの読み込みに失敗しました。\n{e}"
            )
            self._append_progress(f"[gui] heatmap load error: {e}")
            return

        if self._pop is None:
            self._pop = PlotWindow(self)
        self.plot_window = self._pop
        self._pop.show(); self._pop.raise_(); self._pop.activateWindow()
        note = self._last_monthly_returns.name
        try:
            self._pop.plot_heatmap(df, note)
        except Exception as e:
            QtWidgets.QMessageBox.critical(
                self,
                "エラー",
                f"ヒートマップ描画中にエラーが発生しました。\n{e}"
            )
            self._append_progress(f"[gui] heatmap plot error: {e}")

    def _pop_out(self):
        if self._last_plot_kind is None or self._last_plot_data is None:
            self._append_progress("[gui] popout: 直近の描画データがありません")
            QtWidgets.QMessageBox.information(self, "情報", "まだ描画されていません。先に「データ確認＆更新」または「テスト実行」をしてください。")
            return
        if self._pop is None:
            self._pop = PlotWindow(self)
        self.plot_window = self._pop
        self._pop.show(); self._pop.raise_(); self._pop.activateWindow()
        if self._last_plot_kind == "equity":
            csvp = self._last_plot_data
            self._pop.plot_equity_csv(csvp)

            if self._wfo_train_df is not None or self._wfo_test_df is not None:
                try:
                    self._pop.overlay_wfo_equity(self._wfo_train_df, self._wfo_test_df)
                except Exception as e:
                    self._append_progress(f"[pop] WFO overlay failed: {e}")
        else:
            self._pop.plot_price_preview(self._last_plot_data, self._last_plot_note)

    # ------------------ 描画・メトリクス ------------------
    def _load_plot(self, path_or_csv):
        p = Path(path_or_csv)
        csv_path = str(p)
        ##
        if "equity_curve.csv" in csv_path:
            try:
                plot_equity_with_markers_to_figure(self.fig, csv_path, note=p.name)
                self.canvas.draw_idle()
                self._append_progress(f"[gui] plotted EQUITY (markers) from {p.name}")
                self._last_plot_kind = "equity"
                self._last_plot_data = str(p)  # CSVパスを記憶（期間ジャンプで利用）
                self._last_plot_note = p.name
            except Exception as e:
                self.label_meta.setText(f"描画失敗: {e}")
                self._append_progress(f"[gui] plot error: {e}")
            return

        ##
        try:
            df = pd.read_csv(p)
            self.fig.clear()
            ax = self.fig.add_subplot(111)

            if "close" in df.columns:
                price = pd.to_numeric(df["close"], errors="coerce").ffill()
                if len(price) == 0 or price.iloc[0] == 0:
                    raise ValueError("プレビュー用 'close' 列が空です。")
                norm = price / price.iloc[0] * 100.0
                ax.plot(norm.values, label="Price (close, =100@start)")
                ax.set_title("Price Preview (from OHLCV)")
                ax.set_ylabel("index (=100@start)")
                self._append_progress(f"[gui] plotted PRICE {len(df)} rows from {p.name}")
                self._last_plot_kind = "price"
                self._last_plot_data = str(p)
                self._last_plot_note = p.name
            else:
                raise ValueError(f"CSVに 'close' 列が含まれていません。columns={list(df.columns)}")

            ax.set_xlabel("bars")
            ax.legend()
            #self.fig.tight_layout()
            self.canvas.draw_idle()

        except Exception as e:
            self.label_meta.setText(f"描画失敗: {e}")
            self._append_progress(f"[gui] plot error: {e}")

    def _load_metrics(self, metrics_path: Path):
        self.table.setRowCount(0)
        try:
            txt = Path(metrics_path).read_text(encoding="utf-8")
            m = json.loads(txt)
            # WFO の場合は {train:{}, test:{}} 形式
            if "train" in m and "test" in m:
                # test 側を表に出す、train はログに
                ts = m["test"]; tr = m["train"]
                msg = (f"[WFO] Train ret={tr['total_return']*100:.2f}% mdd={tr['max_drawdown']*100:.2f}% sharpe≈{tr['sharpe_like']:.2f} "
                       f"| Test ret={ts['total_return']*100:.2f}% mdd={ts['max_drawdown']*100:.2f}% sharpe≈{ts['sharpe_like']:.2f}")
                self.label_meta.setText(msg)
                rows = ts
            else:
                rows = m
                self.label_meta.setText(
                    f"[BT] ret={m['total_return']*100:.2f}% mdd={m['max_drawdown']*100:.2f}% sharpe≈{m['sharpe_like']:.2f} bars={m['bars']}"
                )

            # 表へ
            for k in [
                "start_equity","end_equity","total_return","max_drawdown","max_dd_days","sharpe_like","bars",
                "trades","win_rate","avg_pnl","profit_factor","avg_holding_bars","avg_holding_days",
                "max_consec_win","max_consec_loss"
            ]:
                if k in rows:
                    r = self.table.rowCount()
                    self.table.insertRow(r)
                    self.table.setItem(r, 0, QtWidgets.QTableWidgetItem(k))
                    val = rows[k]
                    if isinstance(val, float):
                        if "return" in k or "drawdown" in k or k=="win_rate":
                            val = f"{val*100:.2f}%"
                        else:
                            val = f"{val:.4g}"
                    self.table.setItem(r, 1, QtWidgets.QTableWidgetItem(str(val)))

            self._append_progress("[gui] metrics loaded")
        except Exception as e:
            self.label_meta.setText(f"メトリクス読込失敗: {e}")
            self._append_progress(f"[gui] metrics error: {e}")

    # ------------------ 期間ジャンプ（本体） ------------------
    def _on_range_jump(self, mode: str) -> None:
        """
        Backtestタブのインライン描画と、ポップアウト済みウィンドウの両方に期間ジャンプを適用する。
        - インライン: equity だけでなく price プレビューにも対応。
        - ポップアウト: PlotWindow.jump_range() に委譲。
        """
        try:
            # 1) ポップアウト側（起動済みなら）
            if self._pop is not None:
                try:
                    self._pop.jump_range(mode)
                except Exception as e:
                    self._append_progress(f"[pop] jump_range warn: {e}")

            # 2) インライン側：equity
            if self._last_plot_kind == "equity" and isinstance(self._last_plot_data, str):
                csvp = Path(self._last_plot_data)
                if csvp.exists():
                    df = pd.read_csv(csvp)
                    if "time" in df.columns:
                        t = pd.to_datetime(df["time"], errors="coerce").dropna()
                        if len(t) >= 2:
                            tmax = t.iloc[-1]
                            if mode == "ALL":
                                tmin = t.iloc[0]
                            elif mode == "1W":
                                tmin = tmax - pd.Timedelta(days=7)
                            elif mode == "1M":
                                tmin = tmax - pd.Timedelta(days=31)
                            else:
                                raise ValueError(f"Unknown mode: {mode}")
                            tmin = max(t.iloc[0], tmin)
                            if self.fig.axes:
                                ax = self.fig.axes[0]
                                ax.set_xlim(tmin.to_pydatetime(), tmax.to_pydatetime())
                                self.canvas.draw_idle()
                                if mode == "ALL" and isinstance(self.start_edit, QtWidgets.QDateEdit):
                                    first = t.iloc[0]
                                    last = t.iloc[-1]
                                    self.start_edit.setDate(
                                        QtCore.QDate(first.year, first.month, first.day)
                                    )
                                    self.end_edit.setDate(
                                        QtCore.QDate(last.year, last.month, last.day)
                                    )
                                    self._append_progress(
                                        f"[gui] date pickers reset to ALL (equity): {first.date()} .. {last.date()}"
                                    )
                        else:
                            raise RuntimeError("エクイティの時系列が短すぎます。")
                    else:
                        raise RuntimeError("equity_curve.csv に 'time' 列がありません。")
                else:
                    raise RuntimeError("最後に描画したCSVが見つかりません。")

            # 3) インライン側：price（time列ありなら日付、なければバー数でズーム）
            elif self._last_plot_kind == "price" and isinstance(self._last_plot_data, str):
                csvp = Path(self._last_plot_data)
                if not csvp.exists():
                    raise RuntimeError("最後に描画した価格CSVが見つかりません。")
                df = pd.read_csv(csvp)

                if "time" in df.columns:
                    t = pd.to_datetime(df["time"], errors="coerce").dropna()
                    if len(t) < 2:
                        raise RuntimeError("価格プレビューの時系列が短すぎます。")
                    tmax = t.iloc[-1]
                    if mode == "ALL":
                        tmin = t.iloc[0]
                    elif mode == "1W":
                        tmin = tmax - pd.Timedelta(days=7)
                    elif mode == "1M":
                        tmin = tmax - pd.Timedelta(days=31)
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    tmin = max(t.iloc[0], tmin)
                    if self.fig.axes:
                        ax = self.fig.axes[0]
                        ax.set_xlim(tmin.to_pydatetime(), tmax.to_pydatetime())
                        self.canvas.draw_idle()
                        if mode == "ALL" and isinstance(self.start_edit, QtWidgets.QDateEdit) and "time" in df.columns:
                            try:
                                first = t.iloc[0]
                                last = t.iloc[-1]
                                self.start_edit.setDate(
                                    QtCore.QDate(first.year, first.month, first.day)
                                )
                                self.end_edit.setDate(
                                    QtCore.QDate(last.year, last.month, last.day)
                                )
                                self._append_progress(
                                    f"[gui] date pickers reset to ALL (price): {first.date()} .. {last.date()}"
                                )
                            except Exception as e:
                                self._append_progress(f"[gui] failed to sync date pickers on price: {e}")
                else:
                    # x軸がインデックス（0..n-1）の場合は「バー数」で近似
                    n = len(df)
                    if n < 2:
                        raise RuntimeError("価格プレビューの行数が不足しています。")
                    tf = (self.tf_combo.currentText() or "M5").upper()
                    tf_min = {"M1":1, "M5":5, "M15":15, "M30":30, "H1":60, "H4":240, "D1":1440}.get(tf, 5)
                    def bars_for_days(days: int) -> int:
                        return int(days * 24 * 60 / tf_min)
                    if mode == "ALL":
                        start_idx = 0
                    elif mode == "1W":
                        start_idx = max(0, n - bars_for_days(7))
                    elif mode == "1M":
                        start_idx = max(0, n - bars_for_days(31))
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    if self.fig.axes:
                        ax = self.fig.axes[0]
                        ax.set_xlim(start_idx, n - 1)
                        self.canvas.draw_idle()

            else:
                # それ以外（未描画など）
                self._append_progress("[gui] 期間ジャンプは直近の描画対象に対してのみ動作します。")

            # ステータス表示
            self.label_meta.setText(f"表示期間を {mode} に切り替えました")

        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "期間ジャンプエラー", str(e))
            self._append_progress(f"[range] error: {e}")
    



=== file: app/gui/control_tab.py ===

from typing import Optional
from PyQt6.QtCore import Qt
from PyQt6.QtWidgets import (
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QMessageBox,
    QPushButton,
    QSlider,
    QSpinBox,
    QVBoxLayout,
    QWidget,
)

from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_state
from app.services.orderbook_stub import orderbook


class ControlTab(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setLayout(QVBoxLayout())
        root_layout: Optional[QVBoxLayout] = self.layout()  # guard for static analyzers
        if root_layout is None:
            root_layout = QVBoxLayout()
            self.setLayout(root_layout)

        # 運転
        box_run = QGroupBox("運転")
        lay_run = QHBoxLayout()
        self.btn_toggle = QPushButton("取引：停止中（クリックで開始）")
        self.btn_toggle.setCheckable(True)
        self.btn_toggle.clicked.connect(self._toggle_trading)

        self.btn_close_all = QPushButton("全クローズ（ドライラン）")
        self.btn_close_all.clicked.connect(self._close_all_mock)

        self.btn_cb_reset = QPushButton("サーキット解除")
        self.btn_cb_reset.clicked.connect(self._cb_reset)

        lay_run.addWidget(self.btn_toggle)
        lay_run.addWidget(self.btn_close_all)
        lay_run.addWidget(self.btn_cb_reset)
        box_run.setLayout(lay_run)
        root_layout.addWidget(box_run)

        # しきい値
        box_thr = QGroupBox("エントリーしきい値（確信度）")
        lay_thr = QHBoxLayout()
        self.lbl_buy = QLabel("買い: 0.60")
        self.sld_buy = QSlider(Qt.Orientation.Horizontal)
        self.sld_buy.setRange(50, 80)
        self.sld_buy.setValue(60)
        self.sld_buy.valueChanged.connect(self._on_thr_changed)

        self.lbl_sell = QLabel("売り: 0.60")
        self.sld_sell = QSlider(Qt.Orientation.Horizontal)
        self.sld_sell.setRange(50, 80)
        self.sld_sell.setValue(60)
        self.sld_sell.valueChanged.connect(self._on_thr_changed)

        lay_thr.addWidget(self.lbl_buy)
        lay_thr.addWidget(self.sld_buy)
        lay_thr.addSpacing(12)
        lay_thr.addWidget(self.lbl_sell)
        lay_thr.addWidget(self.sld_sell)
        box_thr.setLayout(lay_thr)
        root_layout.addWidget(box_thr)

        # 決済
        box_exit = QGroupBox("決済（固定pips）")
        lay_exit = QHBoxLayout()
        self.sp_sl = QSpinBox()
        self.sp_sl.setRange(1, 200)
        self.sp_sl.setValue(10)
        self.sp_sl.valueChanged.connect(self._on_exit_changed)

        self.sp_tp = QSpinBox()
        self.sp_tp.setRange(1, 300)
        self.sp_tp.setValue(15)
        self.sp_tp.valueChanged.connect(self._on_exit_changed)

        lay_exit.addWidget(QLabel("SL"))
        lay_exit.addWidget(self.sp_sl)
        lay_exit.addSpacing(16)
        lay_exit.addWidget(QLabel("TP"))
        lay_exit.addWidget(self.sp_tp)
        box_exit.setLayout(lay_exit)
        root_layout.addWidget(box_exit)

        # 状態表示
        self.lbl_status = QLabel("")
        root_layout.addWidget(self.lbl_status)

        self._sync_from_state()

    def _sync_from_state(self):
        s = trade_state.get_settings()
        self.btn_toggle.setChecked(s.trading_enabled)
        self.btn_toggle.setText(
            "取引：稼働中（クリックで停止）" if s.trading_enabled else "取引：停止中（クリックで開始）"
        )
        self.sld_buy.setValue(int(s.threshold_buy * 100))
        self.sld_sell.setValue(int(s.threshold_sell * 100))
        self.sp_sl.setValue(int(s.sl_pips))
        self.sp_tp.setValue(int(s.tp_pips))
        self._refresh_status()

    def _refresh_status(self):
        s = trade_state.as_dict()
        state_txt = "稼働中" if s["trading_enabled"] else "停止中"
        self.lbl_status.setText(
            f"状態: {state_txt} / 買い閾値: {s['threshold_buy']:.2f} / 売り閾値: {s['threshold_sell']:.2f} / "
            f"SL: {s['sl_pips']} / TP: {s['tp_pips']}"
        )

    def _toggle_trading(self):
        try:
            enabled = self.btn_toggle.isChecked()
            trade_state.update(trading_enabled=enabled)
            self.btn_toggle.setText(
                "取引：稼働中（クリックで停止）" if enabled else "取引：停止中（クリックで開始）"
            )
            self._refresh_status()
        except Exception as e:
            self.btn_toggle.setChecked(not self.btn_toggle.isChecked())
            QMessageBox.critical(self, "Trading switch error", str(e))
            print("[control_tab] toggle failed:", e)

    def _on_thr_changed(self, *_):
        buy = self.sld_buy.value() / 100.0
        sell = self.sld_sell.value() / 100.0
        trade_state.update(threshold_buy=buy, threshold_sell=sell)
        self.lbl_buy.setText(f"買い: {buy:.2f}")
        self.lbl_sell.setText(f"売り: {sell:.2f}")
        self._refresh_status()

    def _on_exit_changed(self, *_):
        trade_state.update(sl_pips=int(self.sp_sl.value()), tp_pips=int(self.sp_tp.value()))
        self._refresh_status()

    def _close_all_mock(self):
        cfg = load_config()
        symbol = cfg.get("runtime", {}).get("symbol", "USDJPY")
        orderbook().close_all(symbol)

    def _cb_reset(self):
        circuit_breaker.reset()
        circuit_breaker.scan_and_update()
        self._refresh_status()



=== file: app/gui/dashboard_tab.py ===

# app/gui/dashboard_tab.py
import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parents[2]  # D:\macht\OneDrive\fxbot
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))
import json, os
from core.metrics import METRICS_JSON

import time
try:
    import tkinter as tk
    from tkinter import ttk
except ImportError:
    # Tkinterが無い環境用メッセージ（Windows公式Pythonなら入ってます）
    raise

from app.services import trade_service
from core.metrics import METRICS


class DashboardTab(ttk.Frame):
    """
    Realtime Metrics (ATR / Grace / Trail) を表示するだけの軽量パネル。
    別スレッドでMETRICSが更新される前提。self.afterで1秒ごとにpull。
    """
    def __init__(self, master, *args, **kwargs):
        super().__init__(master, *args, **kwargs)

        self._vars = {}

        box = ttk.LabelFrame(self, text="Realtime Metrics (ATR / Grace / Trail)")
        box.pack(fill="x", padx=8, pady=6)

        grid = ttk.Frame(box)
        grid.pack(fill="x", padx=8, pady=8)

        rows = [
            ("Last decision", "last_decision"),
            ("Reason", "last_reason"),
            ("ATR ref", "atr_ref"),
            ("ATR gate", "atr_gate_state"),
            ("Post-fill grace", "post_fill_grace"),
            ("Spread", "spread"),
            ("ADX / Min", "adx_min"),
            ("Prob threshold", "prob_threshold"),
            ("Min ATR %", "min_atr_pct"),
            ("Trail: activated", "trail_activated"),
            ("Trail: BE locked", "trail_be_locked"),
            ("Trail: layers", "trail_layers"),
            ("Trail: current SL", "trail_current_sl"),
            ("Guard/Open", "guard_open"),
            ("Guard/Inflight", "guard_inflight"),
            ("Guard/LastFix", "guard_last_fix"),
            ("CB/Tripped", "cb_tripped"),
            ("CB/Reason", "cb_reason"),
            ("CB/ConsecLoss", "cb_consec"),
            ("CB/DailyLossJPY", "cb_daily_loss"),
            ("Counts ENTRY/SKIP/BLOCK", "counts"),
            ("Updated (local)", "ts"),
        ]

        for i, (label, key) in enumerate(rows):
            ttk.Label(grid, text=label, width=22).grid(row=i, column=0, sticky="w", padx=4, pady=2)
            var = tk.StringVar(value="-")
            ttk.Label(grid, textvariable=var, width=28).grid(row=i, column=1, sticky="w", padx=4, pady=2)
            self._vars[key] = var

        # 最初の更新をセット
        self.after(500, self._refresh_metrics)

    def _refresh_metrics(self):
        # まずはファイルから読む（別プロセス更新に対応）
        kv = {}
        try:
            with open(METRICS_JSON, "r", encoding="utf-8") as f:
                kv = json.load(f)
        except Exception:
            # ファイルがまだ無い／壊れている場合はローカルKVSを参照
            from core.metrics import METRICS
            kv = METRICS.get()

        # 以降は同じ（値の反映処理）
        self._vars["last_decision"].set(str(kv.get("last_decision", "-")))
        self._vars["last_reason"].set(str(kv.get("last_reason", "-")))
        self._vars["atr_ref"].set(f"{float(kv.get('atr_ref', 0) or 0):.6f}")
        self._vars["atr_gate_state"].set(str(kv.get("atr_gate_state", "-")))
        self._vars["post_fill_grace"].set("ON" if kv.get("post_fill_grace") else "OFF")
        self._vars["spread"].set(str(kv.get("spread", "-")))
        self._vars["prob_threshold"].set(str(kv.get("prob_threshold", "-")))
        self._vars["min_atr_pct"].set(str(kv.get("min_atr_pct", "-")))
        adx = kv.get("adx"); m = kv.get("min_adx")
        self._vars["adx_min"].set(f"{adx} / {m}")
        self._vars["trail_activated"].set("ON" if kv.get("trail_activated") else "OFF")
        self._vars["trail_be_locked"].set("ON" if kv.get("trail_be_locked") else "OFF")
        self._vars["trail_layers"].set(str(kv.get("trail_layers", 0)))
        self._vars["trail_current_sl"].set(str(kv.get("trail_current_sl", "-")))
        cE = int(kv.get("count_entry", 0)); cS = int(kv.get("count_skip", 0)); cB = int(kv.get("count_blocked", 0))
        self._vars["counts"].set(f"{cE} / {cS} / {cB}")
        ts = kv.get("ts")
        local = "-" if not ts else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ts))
        self._vars["ts"].set(local)

        svc = getattr(trade_service, "SERVICE", None)
        guard_state = getattr(svc, "pos_guard", None)
        if guard_state:
            self._vars["guard_open"].set(str(guard_state.state.open_count))
            self._vars["guard_inflight"].set(str(len(guard_state.state.inflight_orders)))
            self._vars["guard_last_fix"].set(guard_state.state.last_fix_reason or "-")
        else:
            self._vars["guard_open"].set("-")
            self._vars["guard_inflight"].set("-")
            self._vars["guard_last_fix"].set("-")

        cb = getattr(svc, "cb", None) if svc else None
        cb_status = cb.status() if cb else {}
        self._vars["cb_tripped"].set(str(cb_status.get("tripped", False)))
        self._vars["cb_reason"].set(str(cb_status.get("reason", "-")))
        self._vars["cb_consec"].set(str(cb_status.get("consecutive_losses", "-")))
        self._vars["cb_daily_loss"].set(f"{float(cb_status.get('daily_loss_accum_jpy', 0.0)):.0f}")

        self.after(1000, self._refresh_metrics)



=== file: app/gui/dashboard_tab_qt.py ===

# app/gui/dashboard_tab_qt.py
from __future__ import annotations
from typing import Dict, Any
import json
import time

from PyQt6 import QtCore, QtWidgets
from core.metrics import METRICS_JSON, METRICS

class DashboardTab(QtWidgets.QWidget):
    """
    PyQt6版 Dashboard。runtime/metrics.json を1秒ごとに再読込し、
    値が無ければ core.metrics.METRICS(KVS) をフォールバック参照。
    """
    def __init__(self, parent: QtWidgets.QWidget | None = None) -> None:
        super().__init__(parent)
        self._labels: Dict[str, QtWidgets.QLabel] = {}

        group = QtWidgets.QGroupBox("Realtime Metrics (ATR / Grace / Trail)")
        grid = QtWidgets.QGridLayout()
        group.setLayout(grid)

        rows = [
            ("Last decision", "last_decision"),
            ("Reason", "last_reason"),
            ("ATR ref", "atr_ref"),
            ("ATR gate", "atr_gate_state"),
            ("Post-fill grace", "post_fill_grace"),
            ("Spread", "spread"),
            ("ADX / Min", "adx_min"),
            ("Prob threshold", "prob_threshold"),
            ("Min ATR %", "min_atr_pct"),
            ("Trail: activated", "trail_activated"),
            ("Trail: BE locked", "trail_be_locked"),
            ("Trail: layers", "trail_layers"),
            ("Trail: current SL", "trail_current_sl"),
            ("Guard/Open", "guard_open"),
            ("Guard/Inflight", "guard_inflight"),
            ("Guard/LastFix", "guard_last_fix"),
            ("CB/Tripped", "cb_tripped"),
            ("CB/Reason", "cb_reason"),
            ("CB/ConsecLoss", "cb_consec"),
            ("CB/DailyLossJPY", "cb_daily_loss"),
            ("Counts ENTRY/SKIP/BLOCK", "counts"),
            ("Updated (local)", "ts"),
        ]

        for r, (label, key) in enumerate(rows):
            grid.addWidget(QtWidgets.QLabel(label), r, 0, alignment=QtCore.Qt.AlignmentFlag.AlignLeft)
            val = QtWidgets.QLabel("-")
            val.setMinimumWidth(220)
            grid.addWidget(val, r, 1, alignment=QtCore.Qt.AlignmentFlag.AlignLeft)
            self._labels[key] = val

        lay = QtWidgets.QVBoxLayout(self)
        lay.addWidget(group)

        # タイマーで定期更新
        self._timer = QtCore.QTimer(self)
        self._timer.setInterval(1000)
        self._timer.timeout.connect(self._refresh_metrics)
        self._timer.start()

        self._refresh_metrics()

    def _refresh_metrics(self) -> None:
        kv: Dict[str, Any] = {}
        try:
            with open(METRICS_JSON, "r", encoding="utf-8") as f:
                kv = json.load(f)
        except Exception:
            kv = METRICS.get()  # 同一プロセスKVSのフォールバック

        # 値の整形と描画
        self._set("last_decision", str(kv.get("last_decision", "-")))
        self._set("last_reason", str(kv.get("last_reason", "-")))
        self._set("atr_ref", f"{float(kv.get('atr_ref', 0) or 0):.6f}")
        self._set("atr_gate_state", str(kv.get("atr_gate_state", "-")))
        self._set("post_fill_grace", "ON" if kv.get("post_fill_grace") else "OFF")
        self._set("spread", str(kv.get("spread", "-")))
        self._set("prob_threshold", str(kv.get("prob_threshold", "-")))
        self._set("min_atr_pct", str(kv.get("min_atr_pct", "-")))
        adx = kv.get("adx"); m = kv.get("min_adx")
        self._set("adx_min", f"{adx} / {m}")
        self._set("trail_activated", "ON" if kv.get("trail_activated") else "OFF")
        self._set("trail_be_locked", "ON" if kv.get("trail_be_locked") else "OFF")
        self._set("trail_layers", str(kv.get("trail_layers", 0)))
        self._set("trail_current_sl", str(kv.get("trail_current_sl", "-")))
        cE = int(kv.get("count_entry", 0)); cS = int(kv.get("count_skip", 0)); cB = int(kv.get("count_blocked", 0))
        self._set("counts", f"{cE} / {cS} / {cB}")
        ts = kv.get("ts")
        local = "-" if not ts else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ts))
        self._set("ts", local)

    def _set(self, key: str, val: str) -> None:
        lab = self._labels.get(key)
        if lab is not None:
            lab.setText(val)



=== file: app/gui/history_tab.py ===

from __future__ import annotations

from typing import List, Optional

from PyQt6 import QtCore, QtWidgets
from PyQt6.QtWidgets import QHeaderView

from app.services.event_store import EVENT_STORE, UiEvent

_COLUMNS = ["ts", "kind", "symbol", "side", "price", "sl", "tp", "profit_jpy", "reason", "notes"]


class HistoryTab(QtWidgets.QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.table = QtWidgets.QTableWidget(self)
        self.table.setColumnCount(len(_COLUMNS))
        self.table.setHorizontalHeaderLabels(_COLUMNS)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)
        h: Optional[QHeaderView] = self.table.horizontalHeader()
        if h is not None:
            h.setStretchLastSection(True)

        v: Optional[QHeaderView] = self.table.verticalHeader()
        if v is not None:
            v.setVisible(False)

        self.btnExport = QtWidgets.QPushButton("Export CSV")
        self.btnExport.clicked.connect(self._export_csv)

        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.table)
        layout.addWidget(self.btnExport)

        self._timer = QtCore.QTimer(self)
        self._timer.setInterval(1000)
        self._timer.timeout.connect(self.refresh)
        self._timer.start()

        self.refresh()

    def refresh(self) -> None:
        events: List[UiEvent] = EVENT_STORE.recent(300)
        self.table.setRowCount(len(events))
        for r, ev in enumerate(events):
            row = [getattr(ev, col) for col in _COLUMNS]
            for c, val in enumerate(row):
                item = QtWidgets.QTableWidgetItem("" if val is None else str(val))
                self.table.setItem(r, c, item)

    def _export_csv(self) -> None:
        path, _ = QtWidgets.QFileDialog.getSaveFileName(
            self, "Export history to CSV", "history.csv", "CSV Files (*.csv)"
        )
        if not path:
            return
        import csv

        events: List[UiEvent] = EVENT_STORE.recent(1000)
        with open(path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(_COLUMNS)
            for ev in events:
                writer.writerow([getattr(ev, col) for col in _COLUMNS])



=== file: app/gui/main.py ===

import sys
import traceback

from PyQt6.QtCore import QTimer
from PyQt6.QtWidgets import QApplication, QMainWindow, QTabWidget

from app.core import logger as app_logger
from app.gui.control_tab import ControlTab
from app.gui.dashboard_tab_qt import DashboardTab
from app.gui.history_tab import HistoryTab
from app.services.execution_stub import evaluate_and_log_once
from app.gui.ai_tab import AITab
from app.gui.backtest_tab import BacktestTab
from app.gui.settings_tab import SettingsTab

class MainWindow(QMainWindow):
    def __init__(self) -> None:
        super().__init__()
        self.setWindowTitle("FX AI Bot Control Panel")
        self.resize(980, 640)

        tabs = QTabWidget()
        tabs.addTab(DashboardTab(), "Dashboard")
        tabs.addTab(ControlTab(), "Control")
        tabs.addTab(HistoryTab(), "History")
        tabs.addTab(AITab(), "AI")
        tabs.addTab(BacktestTab(), "Backtest")
        tabs.addTab(SettingsTab(), "Settings")
        self.setCentralWidget(tabs)

        app_logger.setup()

        self.timer = QTimer(self)

        def _tick_safe():
            try:
                evaluate_and_log_once()
            except Exception:
                print("[gui.timer] evaluate failed:\n" + traceback.format_exc())

        self.timer.timeout.connect(_tick_safe)
        self.timer.start(3000)
        _tick_safe()


def main() -> None:
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()



=== file: app/gui/settings_tab.py ===

# app/gui/settings_tab.py
from __future__ import annotations

from typing import Optional

from PyQt6.QtCore import Qt
from PyQt6.QtWidgets import (
    QWidget,
    QVBoxLayout,
    QGroupBox,
    QFormLayout,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QComboBox,
    QPushButton,
    QMessageBox,
    QSpacerItem,
    QSizePolicy,
)

from app.services import mt5_account_store, mt5_selftest


class SettingsTab(QWidget):
    """MT5 口座設定タブ。

    - プロファイル（例: demo / real）ごとに login / password / server を保存
    - 「この口座に切り替え」ボタンで active_profile を変更し、
      カレントプロセスの環境変数 MT5_LOGIN/PASSWORD/SERVER も更新する
    """

    def __init__(self, parent: Optional[QWidget] = None) -> None:
        super().__init__(parent)

        self._setup_ui()
        self._load_profiles()

    # ------------------------------------------------------------------
    # UI 構築
    # ------------------------------------------------------------------
    def _setup_ui(self) -> None:
        root = QVBoxLayout(self)

        # プロファイル選択
        grp_profile = QGroupBox("MT5 口座プロファイル", self)
        lay_p = QFormLayout(grp_profile)

        self.cmb_profile = QComboBox(grp_profile)
        # 新しい名前（例: demo2, demo_2026 など）も入力できるよう editable に
        self.cmb_profile.setEditable(True)
        self.cmb_profile.setInsertPolicy(QComboBox.InsertPolicy.NoInsert)
        self.cmb_profile.currentTextChanged.connect(self._on_profile_changed)

        lay_p.addRow("プロファイル名（例: demo / real）:", self.cmb_profile)

        # 認証情報
        grp_auth = QGroupBox("ログイン情報", self)
        lay_auth = QFormLayout(grp_auth)

        self.ed_login = QLineEdit(grp_auth)
        self.ed_login.setPlaceholderText("口座番号（数字）")

        self.ed_password = QLineEdit(grp_auth)
        self.ed_password.setEchoMode(QLineEdit.EchoMode.Password)
        self.ed_password.setPlaceholderText("パスワード")

        self.ed_server = QLineEdit(grp_auth)
        self.ed_server.setPlaceholderText("サーバ名（例: GaitameFinest-Demo）")

        lay_auth.addRow("ログインID:", self.ed_login)
        lay_auth.addRow("パスワード:", self.ed_password)
        lay_auth.addRow("サーバ:", self.ed_server)

        # ボタン
        btn_row = QHBoxLayout()
        self.btn_save = QPushButton("保存（このプロファイルを更新）", self)
        self.btn_switch = QPushButton("この口座に切り替え", self)

        self.btn_save.clicked.connect(self._on_save_clicked)
        self.btn_switch.clicked.connect(self._on_switch_clicked)

        btn_row.addWidget(self.btn_save)
        btn_row.addWidget(self.btn_switch)
        btn_row.addStretch()

        # 情報表示
        self.lbl_active = QLabel("", self)
        self.lbl_active.setWordWrap(True)

        # 接続テストボタン（自己診断）
        self.btn_selftest = QPushButton("MT5 接続テスト（自己診断）", self)
        self.btn_selftest.setToolTip(
            "現在のアクティブ口座プロファイルを使って MT5 への接続とログイン状態を自己診断します。"
        )
        self.btn_selftest.clicked.connect(self._on_selftest_clicked)

        # テスト発注ボタン（selftest_order_flow）
        self.btn_orderflow_test = QPushButton("テスト発注（selftest_order_flow）", self)
        self.btn_orderflow_test.setToolTip(
            "scripts.selftest_order_flow を実行して、0.01 lot の成行発注→即決済フローをテストします。\n"
            "必ずデモ口座で実行してください。"
        )
        self.btn_orderflow_test.clicked.connect(self._on_orderflow_selftest_clicked)

        row_selftest = QHBoxLayout()
        row_selftest.addStretch()
        row_selftest.addWidget(self.btn_selftest)
        row_selftest.addWidget(self.btn_orderflow_test)

        # レイアウトに積む
        root.addWidget(grp_profile)
        root.addWidget(grp_auth)
        root.addLayout(btn_row)
        root.addWidget(self.lbl_active)
        root.addLayout(row_selftest)

        # 余白を下に追加
        root.addItem(QSpacerItem(0, 0, QSizePolicy.Policy.Minimum, QSizePolicy.Policy.Expanding))

    # ------------------------------------------------------------------
    # 内部ロジック
    # ------------------------------------------------------------------
    def _load_profiles(self) -> None:
        """設定ファイルからプロファイル一覧を読み込み、コンボボックスに反映。"""
        cfg = mt5_account_store.load_config()
        profiles = sorted(cfg["profiles"].keys())
        active = cfg.get("active_profile") or ""

        self.cmb_profile.blockSignals(True)
        self.cmb_profile.clear()

        # デモ・本口座の典型名をあらかじめ候補に入れておく
        base_candidates = ["demo", "real"]
        for name in base_candidates:
            if name not in profiles:
                profiles.append(name)

        for name in profiles:
            self.cmb_profile.addItem(name)

        # active_profile があれば選択
        if active and active in profiles:
            self.cmb_profile.setCurrentText(active)
        elif profiles:
            self.cmb_profile.setCurrentIndex(0)

        self.cmb_profile.blockSignals(False)

        # 選択中のプロファイル内容を反映
        self._apply_profile_to_fields(self.cmb_profile.currentText())
        self._refresh_active_label()

    def _apply_profile_to_fields(self, profile_name: str) -> None:
        """指定プロファイルの情報を入力欄に反映。"""
        if not profile_name:
            self.ed_login.clear()
            self.ed_password.clear()
            self.ed_server.clear()
            return

        acc = mt5_account_store.get_profile(profile_name)
        if acc is None:
            # 未保存プロファイルならフィールドは空に
            self.ed_login.clear()
            self.ed_password.clear()
            self.ed_server.clear()
            return

        self.ed_login.setText(str(acc.get("login", "")))
        self.ed_password.setText(acc.get("password", ""))
        self.ed_server.setText(acc.get("server", ""))

    def _on_profile_changed(self, name: str) -> None:
        self._apply_profile_to_fields(name)

    def _on_save_clicked(self) -> None:
        name = self.cmb_profile.currentText().strip()
        if not name:
            QMessageBox.warning(self, "保存エラー", "プロファイル名を入力してください。")
            return

        login_txt = self.ed_login.text().strip()
        password = self.ed_password.text()
        server = self.ed_server.text().strip()

        if not login_txt or not password or not server:
            QMessageBox.warning(self, "保存エラー", "ログインID・パスワード・サーバをすべて入力してください。")
            return

        try:
            login = int(login_txt)
        except ValueError:
            QMessageBox.warning(self, "保存エラー", "ログインID は数字のみを入力してください。")
            return

        mt5_account_store.upsert_profile(name, login=login, password=password, server=server)
        QMessageBox.information(self, "保存完了", f"プロファイル '{name}' を保存しました。")

        # 再読込して active/profile 表示を更新
        self._load_profiles()

    def _on_switch_clicked(self) -> None:
        name = self.cmb_profile.currentText().strip()
        if not name:
            QMessageBox.warning(self, "切り替えエラー", "プロファイル名を選択または入力してください。")
            return

        acc = mt5_account_store.get_profile(name)
        if acc is None:
            # 未保存なら「保存してから切り替える？」かを確認
            res = QMessageBox.question(
                self,
                "未保存プロファイル",
                "このプロファイルはまだ保存されていません。入力中の内容で保存してから切り替えますか？",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
            )
            if res == QMessageBox.StandardButton.Yes:
                self._on_save_clicked()
                acc = mt5_account_store.get_profile(name)
                if acc is None:
                    return
            else:
                return

        mt5_account_store.set_active_profile(name, apply_env=True)
        self._refresh_active_label()

        QMessageBox.information(
            self,
            "口座切り替え",
            f"アクティブ口座を '{name}' に切り替えました。\n\n"
            "このGUIプロセス内では MT5_LOGIN / MT5_PASSWORD / MT5_SERVER が\n"
            "選択した口座の情報に更新されています。",
        )

    def _on_selftest_clicked(self) -> None:
        """
        「MT5 接続テスト（自己診断）」ボタン押下時のハンドラ。
        """
        try:
            ok, log = mt5_selftest.run_mt5_selftest()
        except Exception as e:
            # サービス層でも例外を握っているが、GUI 側でも念のためガードしておく
            QMessageBox.critical(
                self,
                "MT5 接続テスト エラー",
                f"MT5 自己診断の実行中に予期しないエラーが発生しました。\n\n{e!r}",
            )
            return

        # 成功／失敗でアイコンとタイトルを変える
        if ok:
            icon = QMessageBox.Icon.Information
            title = "MT5 接続テスト 成功"
        else:
            icon = QMessageBox.Icon.Critical
            title = "MT5 接続テスト 失敗"

        # 詳細ログ（ログ全文）は detailedText に入れる
        first_line = log.splitlines()[0] if log else ""

        msg_box = QMessageBox(self)
        msg_box.setIcon(icon)
        msg_box.setWindowTitle(title)

        if ok:
            msg_box.setText(
                "MT5 への接続・ログインが正常に確認されました。\n"
                "取引準備は問題ありません。"
            )
        else:
            msg_box.setText(
                "MT5 への接続またはログインで問題が見つかりました。\n"
                "次の点を確認してください：\n"
                "  ・MT5 ターミナルは起動しているか\n"
                "  ・設定タブの口座ID / パスワード / サーバーは正しいか\n"
                "  ・デモ口座の有効期限が切れていないか\n"
                "  ・同時ログイン数の制限に引っかかっていないか\n"
                "\n詳細は「詳細」ボタンから確認できます。"
            )

        msg_box.setDetailedText(log)
        msg_box.exec()

    def _on_orderflow_selftest_clicked(self) -> None:
        """
        「テスト発注（selftest_order_flow）」ボタン押下時のハンドラ。
        """
        # まずは確認ダイアログ
        res = QMessageBox.question(
            self,
            "テスト発注の確認",
            (
                "scripts.selftest_order_flow を実行して、\n"
                "現在のアクティブMT5口座で 0.01 lot の成行注文→即決済テストを行います。\n\n"
                "※ 必ずデモ口座で実行してください。\n\n"
                "続行してよろしいですか？"
            ),
            QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
        )

        if res != QMessageBox.StandardButton.Yes:
            return

        try:
            ok, log = mt5_selftest.run_mt5_orderflow_selftest()
        except Exception as e:
            QMessageBox.critical(
                self,
                "テスト発注エラー",
                "selftest_order_flow 実行中に予期しないエラーが発生しました。\n\n"
                f"{e!r}",
            )
            return

        # 成功／失敗でアイコンとタイトルを変える
        if ok:
            icon = QMessageBox.Icon.Information
            title = "テスト発注 成功"
            text = (
                "0.01 lot の成行発注→即決済テストが正常に完了しました。\n"
                "詳細ログは「詳細」ボタンから確認できます。"
            )
        else:
            icon = QMessageBox.Icon.Critical
            title = "テスト発注 失敗"
            text = (
                "テスト発注中にエラーが発生したか、自己診断が失敗しました。\n"
                "詳細ログを確認して、口座設定やMT5ターミナルの状態を見直してください。"
            )

        msg_box = QMessageBox(self)
        msg_box.setIcon(icon)
        msg_box.setWindowTitle(title)
        msg_box.setText(text)
        msg_box.setDetailedText(log)
        msg_box.exec()

    def _refresh_active_label(self) -> None:
        cfg = mt5_account_store.load_config()
        active = cfg.get("active_profile") or "(未設定)"
        acc = mt5_account_store.get_profile(active)

        if acc is None:
            txt = f"現在のアクティブ口座: {active}（設定情報が見つかりません）"
        else:
            txt = (
                f"現在のアクティブ口座: {active}\n"
                f"  login={acc.get('login')} / server={acc.get('server')}"
            )
        self.lbl_active.setText(txt)



=== file: app/gui/widgets/feature_importance.py ===

# app/gui/widgets/feature_importance.py
from __future__ import annotations
from typing import Any, Dict, Optional, cast
import json
from pathlib import Path

from PyQt6 import QtWidgets
from PyQt6.QtWidgets import QHeaderView
import pandas as pd
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure

class FeatureImportanceWidget(QtWidgets.QWidget):
    def __init__(self, ai_service, parent: Optional[QtWidgets.QWidget] = None):
        super().__init__(parent)
        self.ai_service = ai_service
        self._df_cache: Optional[pd.DataFrame] = None
        self._alias: Dict[str, str] = self._load_alias()

        self.modelCombo = QtWidgets.QComboBox()
        self.methodCombo = QtWidgets.QComboBox()
        self.methodCombo.addItems(["gain", "split"])
        self.topSpin = QtWidgets.QSpinBox()
        self.topSpin.setRange(3, 100)
        self.topSpin.setValue(20)
        self.refreshBtn = QtWidgets.QPushButton("更新")

        ctrl = QtWidgets.QHBoxLayout()
        ctrl.addWidget(QtWidgets.QLabel("Model"))
        ctrl.addWidget(self.modelCombo, 1)
        ctrl.addWidget(QtWidgets.QLabel("Method"))
        ctrl.addWidget(self.methodCombo)
        ctrl.addWidget(QtWidgets.QLabel("TopN"))
        ctrl.addWidget(self.topSpin)
        ctrl.addWidget(self.refreshBtn)

        self.fig = Figure(figsize=(6, 4))
        self.canvas = FigureCanvas(self.fig)

        self.table = QtWidgets.QTableWidget()
        self.table.setColumnCount(3)
        self.table.setHorizontalHeaderLabels(["feature", "importance(%)", "model"])
        header: Optional[QHeaderView] = self.table.horizontalHeader()
        if header is not None:
            header.setStretchLastSection(True)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)

        lay = QtWidgets.QVBoxLayout(self)
        lay.addLayout(ctrl)
        lay.addWidget(self.canvas, 2)
        lay.addWidget(self.table, 1)

        self.refreshBtn.clicked.connect(self.refresh)
        self.methodCombo.currentTextChanged.connect(self.refresh)
        self.topSpin.valueChanged.connect(self.refresh)
        self.modelCombo.currentTextChanged.connect(self._plot_current)

        self.refresh()

    def refresh(self):
        method = self.methodCombo.currentText()
        top_n = int(self.topSpin.value())
        try:
            df = self.ai_service.get_feature_importance(method=method, top_n=top_n)
        except Exception as e:
            QtWidgets.QMessageBox.critical(self, "FI取得エラー", str(e))
            return

        if df is None or df.empty:
            self._df_cache = None
            self.modelCombo.clear()
            self._render_empty()
            return

        df = df.copy()
        if "feature" in df.columns and self._alias:
            df["feature"] = df["feature"].map(lambda x: self._alias.get(str(x), str(x)))

        self._df_cache = df
        models = sorted(df["model"].unique().tolist())
        prev = self.modelCombo.currentText()
        self.modelCombo.blockSignals(True)
        self.modelCombo.clear()
        self.modelCombo.addItems(models)
        self.modelCombo.blockSignals(False)
        if prev in models:
            self.modelCombo.setCurrentIndex(models.index(prev))
        self._plot_current()
        self._fill_table(df)

    def _plot_current(self):
        df = self._df_cache
        if df is None or df.empty:
            self._render_empty()
            return
        model = self.modelCombo.currentText()
        sub = df[df["model"] == model].sort_values("importance", ascending=True)
        self.fig.clear()
        ax = self.fig.add_subplot(111)
        ax.barh(sub["feature"], sub["importance"])
        ax.set_xlabel("importance (%)")
        ax.set_title(f"Feature Importance - {model} ({self.methodCombo.currentText()})")
        self.fig.tight_layout()
        self.canvas.draw_idle()

    def _fill_table(self, df: pd.DataFrame):
        rows = list(df.reset_index(drop=True).itertuples(index=False))
        self.table.setRowCount(len(rows))
        for r, row in enumerate(rows):
            importance_val = float(cast(Any, row.importance))
            self.table.setItem(r, 0, QtWidgets.QTableWidgetItem(str(row.feature)))
            self.table.setItem(r, 1, QtWidgets.QTableWidgetItem(f"{importance_val:.2f}"))
            self.table.setItem(r, 2, QtWidgets.QTableWidgetItem(str(row.model)))
        self.table.resizeColumnsToContents()

    def _render_empty(self):
        self.fig.clear()
        ax = self.fig.add_subplot(111)
        ax.text(0.5, 0.5, "No data", ha="center", va="center", transform=ax.transAxes)
        ax.axis("off")
        self.canvas.draw_idle()
        self.table.setRowCount(0)

    def _load_alias(self) -> Dict[str, str]:
        try:
            root = Path(__file__).resolve().parents[3]
            path = root / "config" / "feature_alias.json"
            if path.exists():
                raw = json.loads(path.read_text(encoding="utf-8"))
                if isinstance(raw, dict):
                    return {str(k): str(v) for k, v in raw.items()}
        except Exception:
            pass
        return {}



=== file: app/gui/widgets/shap_bar.py ===

# app/gui/widgets/shap_bar.py
from __future__ import annotations

from typing import Dict, Optional
import json
from pathlib import Path

import pandas as pd
from PyQt6 import QtWidgets
from PyQt6.QtWidgets import QHeaderView
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure

from app.services.ai_service import AISvc


class ShapBarWidget(QtWidgets.QWidget):
    """
    AISvc.get_shap_top_features() の結果を棒グラフ＋テーブルで表示するウィジェット。
    """

    def __init__(self, ai_service: AISvc, parent: Optional[QtWidgets.QWidget] = None) -> None:
        super().__init__(parent)
        self.ai_service = ai_service
        self._df_cache: Optional[pd.DataFrame] = None
        self._alias: Dict[str, str] = self._load_alias()

        self.spin_top_n = QtWidgets.QSpinBox()
        self.spin_top_n.setRange(1, 200)
        self.spin_top_n.setValue(20)

        self.spin_cache_sec = QtWidgets.QSpinBox()
        self.spin_cache_sec.setRange(0, 24 * 3600)
        self.spin_cache_sec.setSingleStep(60)
        self.spin_cache_sec.setValue(300)

        self.btn_refresh = QtWidgets.QPushButton("Recalc SHAP")

        ctrl_layout = QtWidgets.QHBoxLayout()
        ctrl_layout.addWidget(QtWidgets.QLabel("Top N:"))
        ctrl_layout.addWidget(self.spin_top_n)
        ctrl_layout.addSpacing(12)
        ctrl_layout.addWidget(QtWidgets.QLabel("Cache TTL (sec):"))
        ctrl_layout.addWidget(self.spin_cache_sec)
        ctrl_layout.addStretch(1)
        ctrl_layout.addWidget(self.btn_refresh)

        self.lbl_top1 = QtWidgets.QLabel("-")
        self.lbl_top2 = QtWidgets.QLabel("-")
        self.lbl_top3 = QtWidgets.QLabel("-")
        for lbl in (self.lbl_top1, self.lbl_top2, self.lbl_top3):
            lbl.setWordWrap(True)

        top_box = QtWidgets.QGroupBox("Top 3 features")
        top_layout = QtWidgets.QVBoxLayout(top_box)
        top_layout.addWidget(self.lbl_top1)
        top_layout.addWidget(self.lbl_top2)
        top_layout.addWidget(self.lbl_top3)

        self.figure = Figure(figsize=(5, 3))
        self.canvas = FigureCanvas(self.figure)

        self.table = QtWidgets.QTableWidget()
        self.table.setColumnCount(4)
        self.table.setHorizontalHeaderLabels(["rank", "feature", "mean|SHAP|", "model"])
        self.table.verticalHeader().setVisible(False)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)
        self.table.horizontalHeader().setSectionResizeMode(QHeaderView.ResizeMode.ResizeToContents)
        self.table.horizontalHeader().setStretchLastSection(True)

        layout = QtWidgets.QVBoxLayout(self)
        layout.addLayout(ctrl_layout)
        layout.addWidget(top_box)
        layout.addWidget(self.canvas, 2)
        layout.addWidget(self.table, 1)

        self.btn_refresh.clicked.connect(self.refresh)
        self.spin_top_n.valueChanged.connect(self.refresh)
        self.spin_cache_sec.valueChanged.connect(self.refresh)

        self.refresh()

    def refresh(self, force: bool = False) -> None:
        """
        AISvc.get_shap_top_features() を呼び出して再描画する。
        """
        from loguru import logger

        top_n = int(self.spin_top_n.value())
        cache_sec = 0 if force else int(self.spin_cache_sec.value())

        try:
            df = self.ai_service.get_shap_top_features(
                top_n=top_n,
                cache_sec=cache_sec,
            )
        except Exception as e:
            logger.exception("failed to compute SHAP top features: %s", e)
            self._render_empty(f"SHAP error: {e}")
            return

        if df is None or df.empty:
            self._render_empty("No SHAP data.")
            return

        self._df_cache = df
        self._plot(df)
        self._fill_table(df)
        self._update_top3(df)

    def _plot(self, df: pd.DataFrame) -> None:
        """
        SHAPグローバル重要度の水平棒グラフを描画。
        """
        df_plot = df.copy()
        df_plot = df_plot.sort_values("mean_abs_shap", ascending=True)

        features_raw = df_plot["feature"].astype(str).tolist()
        features = [self._alias.get(f, f) for f in features_raw]
        values = pd.to_numeric(df_plot["mean_abs_shap"], errors="coerce").fillna(0.0).tolist()

        self.figure.clear()
        ax = self.figure.add_subplot(111)

        y_pos = range(len(features))
        ax.barh(y_pos, values)
        ax.set_yticks(list(y_pos))
        ax.set_yticklabels(features)

        ax.set_xlabel("mean |SHAP value|")
        ax.set_title("Global SHAP Feature Importance")

        self.figure.tight_layout()
        self.canvas.draw_idle()

    def _fill_table(self, df: pd.DataFrame) -> None:
        """
        テーブルに SHAP 順位を表示。
        """
        df = df.reset_index(drop=True)
        self.table.setRowCount(len(df))

        for row_idx, row in df.iterrows():
            rank = str(row.get("rank", row_idx + 1))
            feat = str(row.get("feature", ""))
            mean_abs = float(row.get("mean_abs_shap", 0.0))
            model = str(row.get("model", ""))

            items = [
                QtWidgets.QTableWidgetItem(rank),
                QtWidgets.QTableWidgetItem(feat),
                QtWidgets.QTableWidgetItem(f"{mean_abs:.6f}"),
                QtWidgets.QTableWidgetItem(model),
            ]

            for col_idx, item in enumerate(items):
                self.table.setItem(row_idx, col_idx, item)

        self.table.resizeColumnsToContents()

    def _update_top3(self, df: Optional[pd.DataFrame]) -> None:
        """
        上位3特徴量の簡易サマリをラベルに表示。
        """
        if df is None or df.empty:
            self.lbl_top1.setText("-")
            self.lbl_top2.setText("-")
            self.lbl_top3.setText("-")
            return

        if "rank" in df.columns:
            df_sorted = df.sort_values("rank", ascending=True)
        else:
            df_sorted = df.sort_values("mean_abs_shap", ascending=False)

        top3 = df_sorted.head(3).reset_index(drop=True)

        labels = [self.lbl_top1, self.lbl_top2, self.lbl_top3]
        for idx in range(3):
            if idx < len(top3):
                row = top3.iloc[idx]
                raw_name = str(row.get("feature", ""))
                alias = self._alias.get(raw_name, raw_name)
                mean_abs = float(row.get("mean_abs_shap", 0.0))
                model = str(row.get("model", ""))
                text = f"{idx + 1}. {alias} (|SHAP|={mean_abs:.4f})"
                if model:
                    text += f"  [{model}]"
                labels[idx].setText(text)
            else:
                labels[idx].setText("-")

    def _render_empty(self, message: str) -> None:
        """
        データが無い or エラー時の簡単な表示。
        """
        self.figure.clear()
        ax = self.figure.add_subplot(111)
        ax.text(
            0.5,
            0.5,
            message,
            ha="center",
            va="center",
            transform=ax.transAxes,
        )
        ax.axis("off")
        self.canvas.draw_idle()
        self.table.setRowCount(0)
        self._update_top3(None)

    def _load_alias(self) -> Dict[str, str]:
        """
        configs/feature_alias.json から feature 名のエイリアスを読み出す。
        FeatureImportanceWidget でも使えるように共通処理を流用。
        """
        try:
            root = Path(__file__).resolve().parents[3]
            path = root / "configs" / "feature_alias.json"
            if not path.exists():
                return {}
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                return {str(k): str(v) for k, v in data.items()}
        except Exception as e:
            print(f"[ShapBarWidget] _load_alias failed: {e!r}")
        return {}



=== file: app/main_tk.py ===

# app/main.py
import tkinter as tk
from tkinter import ttk
from app.gui.dashboard_tab import DashboardTab

def main() -> None:
    root = tk.Tk()
    root.title("FXBot Dashboard")
    root.geometry("520x540")

    nb = ttk.Notebook(root)
    nb.pack(fill="both", expand=True)

    dash = DashboardTab(nb)
    nb.add(dash, text="Dashboard")

    root.mainloop()

if __name__ == "__main__":
    main()



=== file: app/services/__init__.py ===




=== file: app/services/ai_service.py ===

from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, Optional
import time

import pandas as pd
from loguru import logger

from app.services.feature_importance import compute_feature_importance
from app.services.shap_service import (
    ShapFeatureImpact,
    compute_shap_feature_importance,
    shap_items_to_frame,
)


class AISvc:
    """
    既存の推論サービス想定。モデル群は self.models に格納されている想定。
    例: self.models = {"lgbm_cls": lgb_model, "xgb_cls": xgb_model}
    """

    def __init__(self) -> None:
        self.models: Dict[str, Any] = {}
        self._fi_cache: Optional[pd.DataFrame] = None
        self._fi_cache_key: Optional[str] = None
        self._fi_cache_ts: float = 0.0

        # SHAP用の高速キャッシュ
        self._shap_cache: Optional[pd.DataFrame] = None
        self._shap_cache_key: Optional[str] = None
        self._shap_cache_ts: float = 0.0

        self.expected_features: Optional[list[str]] = None
        # ... （既存の初期化）

    # ... （既存のメソッド： load_models(), predict(), など）

    def get_feature_importance(
        self,
        method: str = "gain",
        top_n: int = 20,
        cache_sec: int = 300,
    ) -> pd.DataFrame:
        """
        GUI から呼び出して Feature Importance を取得する API。

        現状 method 引数はプレースホルダで、
        LightGBM / XGBoost の「デフォルトの重要度（おおむね gain ベース）」を返す。

        戻り値:
            columns = ["model", "feature", "importance"]
            importance は「割合(%)」を想定。FeatureImportanceWidget 側の
            軸ラベル "importance(%)" と対応させる。
        """
        model_key = ",".join(f"{name}:{id(model)}" for name, model in sorted(self.models.items()))
        key = f"{model_key}|{method}|{top_n}"
        now = time.time()

        if (
            self._fi_cache is not None
            and self._fi_cache_key == key
            and (now - self._fi_cache_ts) < cache_sec
        ):
            return self._fi_cache.copy()

        rows: list[dict[str, Any]] = []

        for name, model in self.models.items():
            if model is None:
                continue

            try:
                items = compute_feature_importance(
                    model=model,
                    feature_names=None,
                    top_n=top_n,
                )
            except Exception as e:
                print(f"[AISvc] compute_feature_importance failed for {name}: {e}")
                continue

            for item in items:
                rows.append(
                    {
                        "model": name,
                        "feature": item.name,
                        "importance": item.importance_pct,
                    }
                )

        if not rows:
            df = pd.DataFrame(columns=["model", "feature", "importance"])
        else:
            df = pd.DataFrame(rows)

        self._fi_cache = df.copy()
        self._fi_cache_key = key
        self._fi_cache_ts = now
        return df

    def _load_shap_background_features(
        self,
        max_rows: int = 2000,
        *,
        csv_path: Path | None = None,
    ) -> pd.DataFrame:
        """
        SHAP計算用の背景特徴量を読み込むヘルパ。

        暫定仕様：
        - data/USDJPY/features_for_shap.csv に特徴量CSVがある前提。
          （今後、weekly_retrain 側から自動出力させる予定）
        - self.expected_features があれば、その列順に揃える。
        """
        if csv_path is None:
            csv_path = Path("data") / "USDJPY" / "features_for_shap.csv"

        if not csv_path.exists():
            raise FileNotFoundError(
                f"SHAP用特徴量CSVが見つかりません: {csv_path}\n"
                "一時的には手動で特徴量CSVを用意してください。"
            )

        logger.info(
            "SHAP背景特徴量を読み込み: path={path}", path=csv_path.as_posix()
        )
        df = pd.read_csv(csv_path)

        if self.expected_features:
            missing = set(self.expected_features) - set(df.columns)
            if missing:
                raise ValueError(
                    "SHAP背景特徴量に expected_features の列が足りません: "
                    f"{sorted(missing)}"
                )
            df = df.loc[:, list(self.expected_features)]

        if len(df) > max_rows:
            df = df.sample(n=max_rows, random_state=42)

        return df

    def get_shap_top_features(
        self,
        *,
        top_n: int = 20,
        max_background: int = 2000,
        csv_path: Path | None = None,
        cache_sec: int = 300,
    ) -> pd.DataFrame:
        """
        LightGBMモデルに対する SHAP グローバル重要度（平均絶対SHAP）を計算し、
        DataFrame (rank, feature, mean_abs_shap, model) を返す。

        - 現状は LightGBM 系モデル（キー名に 'lgb' を含むもの）を対象。
        - 背景データは _load_shap_background_features() で読み込む。
        - cache_sec 秒以内に同じ条件で呼ばれた場合は前回結果を再利用する。
        """
        model_key = ",".join(
            f"{name}:{id(model)}"
            for name, model in sorted(self.models.items())
        )

        if csv_path is None:
            csv_real = Path("data") / "USDJPY" / "features_for_shap.csv"
        else:
            csv_real = csv_path

        try:
            stat = csv_real.stat()
            csv_sig = f"{csv_real.resolve()}|{int(stat.st_mtime)}|{stat.st_size}"
        except FileNotFoundError:
            csv_sig = f"{csv_real.resolve()}|missing"

        key = f"{model_key}|{csv_sig}|top={top_n}|bg={max_background}"
        now = time.time()

        if (
            self._shap_cache is not None
            and self._shap_cache_key == key
            and (now - self._shap_cache_ts) < cache_sec
        ):
            return self._shap_cache.copy()

        target_name: Optional[str] = None
        target_model: Any | None = None

        for name, model in self.models.items():
            if "lgb" in name.lower():
                target_name = name
                target_model = model
                break

        if target_model is None:
            raise RuntimeError(
                "SHAP計算対象の LightGBM モデルが見つかりませんでした。"
                "AISvc.models に 'lgb' を含むキーで LightGBM を登録してください。"
            )

        logger.info(
            "SHAP計算対象モデル: name={name}, type={typ}",
            name=target_name,
            typ=type(target_model).__name__,
        )

        df_bg = self._load_shap_background_features(
            max_rows=max_background,
            csv_path=csv_real,
        )

        feature_names = (
            list(self.expected_features)
            if self.expected_features
            else list(df_bg.columns)
        )

        items: list[ShapFeatureImpact] = compute_shap_feature_importance(
            target_model,
            df_bg,
            feature_names=feature_names,
            top_n=top_n,
            max_background=max_background,
        )

        df_result = shap_items_to_frame(items)
        df_result.insert(0, "model", target_name)

        self._shap_cache = df_result.copy()
        self._shap_cache_key = key
        self._shap_cache_ts = now

        return df_result

    def get_live_probs(self, symbol: str) -> dict[str, float]:
        """
        Live 用：execution_stub と同じ特徴量パイプラインを使って
        確率と atr_for_lot を返す簡易版。
        """
        from app.core import market
        from app.core.config_loader import load_config
        from app.services.execution_stub import _collect_features

        # tick が取れない場合は素直に全部 SKIP に倒す
        try:
            tick = market.tick(symbol)
        except Exception:
            tick = None

        if not tick:
            return {
                "p_buy": 0.0,
                "p_sell": 0.0,
                "p_skip": 1.0,
                "atr_for_lot": 0.0,
            }

        # 設定から base_features を取得（execution_stub と揃える）
        try:
            cfg = load_config()
        except Exception:
            cfg = {}

        ai_cfg = cfg.get("ai", {}) if isinstance(cfg, dict) else {}
        base_features = tuple(ai_cfg.get("features", {}).get("base", []))

        # spread を market から取得（なければ 0.0）
        try:
            spr_callable = getattr(market, "spread", None)
            spread_pips = spr_callable(symbol) if callable(spr_callable) else 0.0
        except Exception:
            spread_pips = 0.0

        # 現在のオープンポジション数は、とりあえず 0 として扱う
        open_positions = 0

        # execution_stub と同じダミー特徴量生成ロジックを使う
        features = _collect_features(
            symbol,
            base_features,
            tick,
            spread_pips,
            open_positions,
        )

        # モデルで確率予測
        prob = self.predict(features)

        # ロット計算用 ATR（price 単位）＝特徴量 atr_14 と同じもの
        atr_for_lot = float(features.get("atr_14", 0.0))

        return {
            "p_buy": float(prob.p_buy),
            "p_sell": float(prob.p_sell),
            "p_skip": float(prob.p_skip),
            "atr_for_lot": atr_for_lot,
        }


    def build_decision_from_probs(self, probs: dict, symbol: str) -> dict:
        """
        Live 用：execution_stub の ENTRY/SKIP 判定を最小限で再現。
        ATR や threshold は設定ファイルを参照する。
        """
        from app.core.config_loader import load_config
        cfg = load_config()
        thr = float(cfg.get("entry", {}).get("prob_threshold", 0.5))

        p_buy = probs["p_buy"]
        p_sell = probs["p_sell"]

        # SKIP 条件
        if p_buy < thr and p_sell < thr:
            return {"action": "SKIP", "reason": "ai_threshold"}

        # どちらを選ぶか
        if p_buy >= p_sell:
            side = "BUY"
            prob = p_buy
        else:
            side = "SELL"
            prob = p_sell

        return {
            "action": "ENTRY",
            "signal": {
                "side": side,
                "atr_for_lot": probs.get("atr_for_lot"),
                "prob": prob,
            },
            "reason": "entry_ok",
        }



=== file: app/services/aisvc_loader.py ===

# app/services/aisvc_loader.py
import json
from pathlib import Path

ROOT = Path(r"C:\fxbot")  # 運用固定
ACTIVE = ROOT / "active_model.json"
MODELS = ROOT / "models_store"


class ActiveModelInfo(dict):
    @property
    def model_path(self) -> Path:
        return MODELS / self["model_name"]


def load_active_model_meta() -> ActiveModelInfo | None:
    if not ACTIVE.exists():
        return None
    meta = json.loads(ACTIVE.read_text(encoding="utf-8"))
    return ActiveModelInfo(meta)


def resolve_model_path() -> Path | None:
    meta = load_active_model_meta()
    if not meta:
        return None
    p = meta.model_path
    return p if p.exists() else None


# 例：GUI起動時
def load_model_for_inference():
    p = resolve_model_path()
    if not p:
        print("[AISvc] no active model; fallback to bundled default")
        # ここで同梱のデフォルトをロードするなど
        return None
    print(f"[AISvc] loading: {p.name}")
    # 実際は joblib/pickle/onnxruntime 等でロード
    # return joblib.load(p)
    return None



=== file: app/services/circuit_breaker.py ===

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Optional


JST = timezone(timedelta(hours=9))


@dataclass
class CBState:
    tripped: bool = False
    reason: Optional[str] = None
    consecutive_losses: int = 0
    last_trip_ts: Optional[float] = None
    daily_loss_accum_jpy: float = 0.0
    day_key: str = ""


class CircuitBreaker:
    """
    Resettable circuit breaker that combines consecutive-loss and daily-loss budgets
    with a cool-down period.
    """

    def __init__(
        self,
        max_consecutive_losses: int = 5,
        daily_loss_limit_jpy: float = 0.0,
        cooldown_min: int = 30,
    ):
        self.max_consecutive_losses = int(max_consecutive_losses)
        self.daily_loss_limit_jpy = float(daily_loss_limit_jpy)
        self.cooldown_min = int(cooldown_min)
        self.state = CBState()

    # ------------------------------------------------------------------ #
    # Public API
    # ------------------------------------------------------------------ #
    def on_trade_result(self, profit_jpy: float) -> None:
        """Record a trade result and trip if thresholds are violated."""
        self._rollover_if_new_day()
        if profit_jpy <= 0:
            self.state.consecutive_losses += 1
        else:
            self.state.consecutive_losses = 0

        self.state.daily_loss_accum_jpy += float(profit_jpy)

        if self.max_consecutive_losses > 0 and self.state.consecutive_losses >= self.max_consecutive_losses:
            self._trip("consecutive_losses")

        if (
            self.daily_loss_limit_jpy
            and self.state.daily_loss_accum_jpy <= -abs(self.daily_loss_limit_jpy)
        ):
            self._trip("daily_loss_limit")

    def can_trade(self) -> bool:
        """Return True if trading is allowed (not tripped or cool-down finished)."""
        if self.state.tripped and self.state.last_trip_ts:
            elapsed = datetime.now(tz=timezone.utc).timestamp() - self.state.last_trip_ts
            if elapsed < self.cooldown_min * 60:
                return False
            self.reset()
        return True

    def reset(self) -> None:
        """Reset trip status (but keep daily accumulator)."""
        self.state.tripped = False
        self.state.reason = None
        self.state.consecutive_losses = 0
        self.state.last_trip_ts = None

    def status(self) -> dict:
        """Return a serialisable snapshot of the breaker state."""
        return {
            "tripped": self.state.tripped,
            "reason": self.state.reason,
            "consecutive_losses": self.state.consecutive_losses,
            "daily_loss_accum_jpy": self.state.daily_loss_accum_jpy,
            "day_key": self.state.day_key,
            "cooldown_min": self.cooldown_min,
        }

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #
    def _trip(self, reason: str) -> None:
        self.state.tripped = True
        self.state.reason = reason
        self.state.last_trip_ts = datetime.now(tz=timezone.utc).timestamp()

    def _rollover_if_new_day(self) -> None:
        now = datetime.now(JST)
        key = now.strftime("%Y-%m-%d")
        if key != self.state.day_key:
            self.state.day_key = key
            self.state.daily_loss_accum_jpy = 0.0



=== file: app/services/data_guard.py ===

# app/services/data_guard.py
from __future__ import annotations
import subprocess
from pathlib import Path
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parents[2]  # app/services/ → app → プロジェクトルート
DATA_DIR = PROJECT_ROOT / "data"

def csv_path(symbol_tag: str, timeframe: str, layout: str="per-symbol") -> Path:
    """
    symbol_tag は接尾辞なし（例: USDJPY）
    layout: "flat" or "per-symbol"
    """
    if layout == "per-symbol":
        return DATA_DIR / symbol_tag / "ohlcv" / f"{symbol_tag}_{timeframe}.csv"
    return DATA_DIR / f"{symbol_tag}_{timeframe}.csv"

def ensure_data(symbol_tag: str, timeframe: str, start_date: str, end_date: str,
                env: str="laptop", layout: str="per-symbol") -> Path:
    """
    指定の [start_date, end_date] を満たすCSVが存在するか確認し、足りなければ scripts.make_csv_from_mt5 を呼んで追記する。
    戻り値: CSVのフルパス
    """
    out_csv = csv_path(symbol_tag, timeframe, layout)
    need_fetch = True

    if out_csv.exists():
        try:
            df = pd.read_csv(out_csv, parse_dates=["time"])
            if not df.empty:
                has_start = (df["time"].min() <= pd.Timestamp(start_date))
                has_end   = (df["time"].max() >= pd.Timestamp(end_date))
                need_fetch = not (has_start and has_end)
        except Exception:
            need_fetch = True

    if need_fetch:
        # make_csv_from_mt5 を呼ぶ（不足分は自動追記）
        cmd = [
            str((PROJECT_ROOT / "scripts" / "make_csv_from_mt5.py").resolve()),
            "--symbol", symbol_tag,
            "--timeframes", timeframe,
            "--start", start_date,
            "--layout", layout,
            "--env", env,
        ]
        # Windows では python 経由で実行
        subprocess.check_call(["python", *cmd], cwd=str(PROJECT_ROOT))

    # 最終チェック
    if not out_csv.exists():
        raise FileNotFoundError(f"CSV not found after update: {out_csv}")
    return out_csv



=== file: app/services/decision_log.py ===

# app/services/decision_log.py
from __future__ import annotations

import json
from collections.abc import Mapping, Sequence
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List, Optional

import pandas as pd
import fxbot_path

# プロジェクトルート = app/services/ から 2 つ上
_PROJECT_ROOT = Path(__file__).resolve().parents[2]
_LOG_DIR = _PROJECT_ROOT / "logs" / "decisions"


@dataclass
class DecisionRecord:
    """
    decisions_*.jsonl の 1 行を、GUI や KPI 計算から使いやすい形に薄くラップしたもの。

    必要に応じてフィールドは増やせるようにしておく。
    """
    ts_jst: str
    symbol: str
    action: str
    side: Optional[str]
    reason: Optional[str]
    meta: Optional[str]
    blocked: Optional[str]
    raw: dict[str, Any]


def _iter_jsonl(path: Path) -> Iterable[dict[str, Any]]:
    """JSONL ファイルを 1 行ずつ dict として返すジェネレータ。壊れた行はスキップ。"""
    try:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    yield json.loads(line)
                except Exception:
                    # 壊れた1行があっても全体は止めない
                    continue
    except FileNotFoundError:
        return


def _extract_decision_record(j: dict[str, Any]) -> DecisionRecord:
    """
    �� JSON ����AAI�^�u/KPI �ł悭�g���������������� DecisionRecord �����B
    """
    ts = str(j.get("ts_jst") or j.get("ts") or "")
    symbol = str(j.get("symbol") or "")

    decision_raw = j.get("decision")
    if isinstance(decision_raw, dict):
        decision = decision_raw
    elif isinstance(decision_raw, str):
        decision = {"action": decision_raw}
    else:
        decision = {}

    action = str(decision.get("action") or "")

    inner_dec_raw = decision.get("dec")
    if isinstance(inner_dec_raw, dict):
        inner_dec = inner_dec_raw
    else:
        inner_dec = {}

    side = inner_dec.get("side") or decision.get("side")
    if side is not None:
        side = str(side)

    meta = j.get("meta")
    if meta is None:
        if isinstance(decision_raw, dict):
            meta = decision_raw.get("meta")
        elif isinstance(decision_raw, str):
            meta = decision_raw
    if meta is not None:
        meta = str(meta)

    reason = decision.get("reason")
    if reason is not None:
        reason = str(reason)

    filters_raw = j.get("filters") or {}
    filters = filters_raw if isinstance(filters_raw, dict) else {}
    blocked = filters.get("blocked")
    if blocked is not None:
        blocked = str(blocked)

    return DecisionRecord(
        ts_jst=ts,
        symbol=symbol,
        action=action,
        side=side,
        reason=reason,
        meta=meta,
        blocked=blocked,
        raw=j,
    )


def _find_first_numeric_by_keys(
    container: Any,
    key_candidates: tuple[str, ...],
) -> float | None:
    """
    任意にネストした dict/list 構造の中から、
    指定したキー名のいずれかに対応する「最初の数値」を返す。
    見つからなければ None。
    """
    if isinstance(container, Mapping):
        for k in key_candidates:
            if k in container and container[k] is not None:
                try:
                    return float(container[k])
                except (TypeError, ValueError):
                    pass
        for value in container.values():
            val = _find_first_numeric_by_keys(value, key_candidates)
            if val is not None:
                return val
    elif isinstance(container, Sequence) and not isinstance(container, (str, bytes, bytearray)):
        for item in container:
            val = _find_first_numeric_by_keys(item, key_candidates)
            if val is not None:
                return val
    return None


def _ensure_pnl_column(df: pd.DataFrame) -> pd.DataFrame:
    """
    decisions_* の生ログ DataFrame に「pnl 列」が無ければ、
    exit_plan / decision_detail / ai / meta などの dict の中から
    それっぽいキー名を再帰的に探し、最初に見つかった数値を pnl とする。
    見つからない行は NaN のまま。
    """
    if "pnl" in df.columns:
        return df

    KEY_CANDIDATES: tuple[str, ...] = (
        "pnl",
        "profit",
        "pl_jpy",
        "pl",
        "pips",
    )

    TARGET_COLS: tuple[str, ...] = (
        "exit_plan",
        "decision_detail",
        "ai",
        "meta",
    )

    def _row_pnl(row: pd.Series) -> float | None:
        for col_name in TARGET_COLS:
            if col_name not in row:
                continue
            container = row[col_name]
            val = _find_first_numeric_by_keys(container, KEY_CANDIDATES)
            if val is not None:
                return val
        return None

    df = df.copy()
    df["pnl"] = df.apply(_row_pnl, axis=1)
    return df


def _get_decision_log_dir() -> Path:
    """
    決定ログのルートディレクトリを返す。

    例: <project_root>/logs/decisions
    """
    root = fxbot_path.get_project_root()
    return root / "logs" / "decisions"


def load_recent_decisions(limit: int | None = None) -> pd.DataFrame:
    """
    decisions_*.jsonl から最新の N レコードを pandas.DataFrame で読み込む。
    """
    log_dir = _get_decision_log_dir()
    files = sorted(log_dir.glob("decisions_*.jsonl"))
    if not files:
        return pd.DataFrame()

    df_list: list[pd.DataFrame] = []
    for f in files:
        try:
            df_list.append(pd.read_json(f, lines=True))
        except Exception:
            continue

    if not df_list:
        return pd.DataFrame()

    df = pd.concat(df_list, ignore_index=True)

    if "ts_jst" in df.columns:
        df = df.sort_values("ts_jst", ascending=False)
    elif "timestamp" in df.columns:
        df = df.sort_values("timestamp", ascending=False)

    if limit is not None and limit > 0:
        df = df.head(limit)

    df = _ensure_pnl_column(df)

    return df.reset_index(drop=True)



=== file: app/services/event_store.py ===

from __future__ import annotations

import json
import os
import threading
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from typing import Any, Deque, List, Optional
from pathlib import Path

# プロジェクトルート = app/services/ から 2 つ上
_PROJECT_ROOT = Path(__file__).resolve().parents[2]

_LOG_DIR = _PROJECT_ROOT / "logs"
_LOG_FILE = _LOG_DIR / "ui_events.jsonl"
_LOG_DIR.mkdir(parents=True, exist_ok=True)

_lock = threading.Lock()


@dataclass
class UiEvent:
    ts: str
    kind: str
    symbol: str
    side: Optional[str] = None
    price: Optional[float] = None
    sl: Optional[float] = None
    tp: Optional[float] = None
    profit_jpy: Optional[float] = None
    reason: Optional[str] = None
    notes: Optional[str] = None


def _now() -> str:
    return datetime.now(timezone.utc).astimezone().isoformat(timespec="seconds")


class _EventStore:
    def __init__(self, maxlen: int = 1000):
        self._buf: Deque[UiEvent] = deque(maxlen=maxlen)

    def append(self, ev: UiEvent) -> None:
        with _lock:
            self._buf.appendleft(ev)
            with open(_LOG_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(ev), ensure_ascii=False) + "\n")

    def add(self, **kwargs: Any) -> None:
        kwargs.setdefault("ts", _now())
        self.append(UiEvent(**kwargs))

    def recent(self, n: int = 200) -> List[UiEvent]:
        with _lock:
            return list(list(self._buf)[:n])


EVENT_STORE = _EventStore()



=== file: app/services/execution_stub.py ===

﻿from __future__ import annotations

import json
import os
import re
import statistics
from collections import deque, defaultdict
from datetime import datetime
from dataclasses import dataclass
from typing import Any, DefaultDict, Dict, Optional, Tuple
from zoneinfo import ZoneInfo
from pathlib import Path

from loguru import logger

from app.core import market, mt5_client
from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_service, trade_state
from app.services.orderbook_stub import orderbook
from app.services.trailing import AtrTrailer, TrailConfig, TrailState
from app.services.trailing_hook import apply_trailing_update
from core import position_guard
from core.ai.service import AISvc, ProbOut
from core.metrics import METRICS
from core.utils.timeutil import now_jst_iso
from app.services.event_store import EVENT_STORE
from app.services.metrics import publish_metrics

# プロジェクトルート = app/services/ から 2 つ上
_PROJECT_ROOT = Path(__file__).resolve().parents[2]

LOG_DIR = _PROJECT_ROOT / "logs" / "decisions"
LOG_DIR.mkdir(parents=True, exist_ok=True)

_ATR_MED: deque[float] = deque(maxlen=128)
_ATR_LAST_PASS: bool = False
_ATR_LAST_REF: Optional[float] = None
_ATR_LAST_ENABLE: Optional[float] = None
_ATR_LAST_DISABLE: Optional[float] = None

# Trailing state store shared across dryrun/production per symbol
runtime_trail_states: DefaultDict[str, Dict[str, Any]] = defaultdict(dict)


def _load_runtime_threshold(default: float = 0.5) -> float:
    try:
        meta_path = _PROJECT_ROOT / "models" / "active_model.json"
        if meta_path.exists():
            with open(meta_path, "r", encoding="utf-8") as fh:
                meta = json.load(fh)
            threshold = meta.get("best_threshold")
            if isinstance(threshold, (int, float)) and 0.0 < threshold < 1.0:
                print(f"[exec] using best_threshold from active_model.json: {threshold}")
                return float(threshold)
    except Exception as exc:
        print(f"[exec][warn] failed to load best_threshold: {exc}")
    print(f"[exec] using default threshold: {default}")
    return float(default)


BEST_THRESHOLD = _load_runtime_threshold(0.5)
print(f"[exec] active BEST_THRESHOLD={BEST_THRESHOLD}", flush=True)

def reset_atr_gate_state() -> None:
    """???/????????ATR????????????"""
    global _ATR_MED, _ATR_LAST_PASS
    _ATR_MED.clear()
    _ATR_LAST_PASS = False


def _atr_gate_ok(atr_pct_now: float, runtime_cfg: Dict[str, Any]) -> bool:
    """Hysteresis-enabled ATR gate to avoid rapid flip-flops around thresholds."""
    global _ATR_LAST_PASS, _ATR_LAST_REF, _ATR_LAST_ENABLE, _ATR_LAST_DISABLE

    filters_cfg: Dict[str, Any] = {}
    if isinstance(runtime_cfg, dict):
        filters_cfg = (runtime_cfg.get("filters") or {})

    if not filters_cfg:
        try:
            cfg = load_config()
            filters_cfg = cfg.get("filters", {})
        except Exception:
            filters_cfg = {}

    hy = (filters_cfg.get("atr_hysteresis") or {}) if isinstance(filters_cfg, dict) else {}

    default_min = 0.00055
    if isinstance(runtime_cfg, dict):
        default_min = float(runtime_cfg.get("min_atr_pct", default_min))

    en = float(hy.get("enable_min_pct", default_min))
    de = float(hy.get("disable_min_pct", min(en, 0.00045)))
    _ATR_LAST_ENABLE = en
    _ATR_LAST_DISABLE = de
    lb = int(hy.get("lookback", 12)) or 1
    if lb <= 0:
        lb = 1

    _ATR_MED.append(float(atr_pct_now))
    window = list(_ATR_MED)[-lb:] or [atr_pct_now]
    try:
        ref = statistics.median(window)
    except Exception:
        ref = float(window[-1])
    _ATR_LAST_REF = ref

    if _ATR_LAST_PASS:
        if ref < de:
            _ATR_LAST_PASS = False
        return _ATR_LAST_PASS or ref >= de
    else:
        if ref >= en:
            _ATR_LAST_PASS = True
        return _ATR_LAST_PASS or ref >= en


def _tick_to_dict(tick: Any) -> Optional[Dict[str, float]]:
    if tick is None:
        return None

    if isinstance(tick, dict):
        bid = tick.get("bid")
        ask = tick.get("ask")
    else:
        try:
            bid, ask = tick
        except (TypeError, ValueError):
            return None
    try:
        bid_f = float(bid) if bid is not None else 0.0
        ask_f = float(ask) if ask is not None else 0.0
    except (TypeError, ValueError):
        return None
    return {"bid": bid_f, "ask": ask_f, "mid": (bid_f + ask_f) / 2.0}

def _pip_size_for(symbol: str) -> float:
    return 0.01 if symbol.endswith("JPY") else 0.0001

def _point_for(symbol: str) -> float:
    return 0.001 if symbol.endswith("JPY") else 0.0001

def _mid_price(tick_dict: Optional[Dict[str, float]]) -> Optional[float]:
    if tick_dict is None:
        return None
    return tick_dict.get("mid")

def _current_price_for_side(tick_dict: Optional[Dict[str, float]], side: str, price_source: str) -> Optional[float]:
    if tick_dict is None:
        return None
    ps = (price_source or "mid").lower()
    if ps == "bid":
        return tick_dict.get("bid") if side == "BUY" else tick_dict.get("ask")
    if ps == "ask":
        return tick_dict.get("ask") if side == "BUY" else tick_dict.get("bid")
    return tick_dict.get("mid")

def _register_trailing_state(symbol: str, signal: Dict[str, Any], tick_dict: Optional[Dict[str, float]]) -> None:
    xp = signal.get("exit_plan") or {}
    if xp.get("mode") != "atr":
        runtime_trail_states.pop(symbol, None)
        return

    trailing = xp.get("trailing") or {}
    if not trailing.get("enabled", True):
        runtime_trail_states.pop(symbol, None)
        return

    atr_val = float(xp.get("atr") or 0.0)
    if atr_val <= 0.0:
        return

    side = signal.get("side")
    if not side:
        return

    pip_size = float(_pip_size_for(symbol))
    point = float(_point_for(symbol))
    price_source = (trailing.get("price_source") or "mid").lower()

    entry_price = signal.get("entry_price")
    if entry_price is None and tick_dict is not None:
        entry_price = _current_price_for_side(tick_dict, side, price_source)
    if entry_price is None:
        entry_price = _mid_price(tick_dict) if tick_dict else None
    if entry_price is None:
        return

    state = {
        "mode": "atr",
        "side": side,
        "symbol": symbol,
        "entry": float(entry_price),
        "atr": atr_val,
        "pip_size": pip_size,
        "point": point,
        "activate_atr_mult": float(trailing.get("activate_atr_mult", 0.5)),
        "step_atr_mult": float(trailing.get("step_atr_mult", 0.25)),
        "lock_be_atr_mult": float(trailing.get("lock_be_atr_mult", 0.3)),
        "hard_floor_pips": float(trailing.get("hard_floor_pips", 5.0)),
        "only_in_profit": bool(trailing.get("only_in_profit", True)),
        "max_layers": int(trailing.get("max_layers", 20)),
        "price_source": price_source,
        "activated": False,
        "be_locked": False,
        "layers": 0,
        "current_sl": None,
    }

    trail = runtime_trail_states.setdefault(symbol, {})
    trail.clear()
    trail.update(state)
    signal["trail_state"] = {
        "activated": False,
        "be_locked": False,
        "layers": 0,
        "current_sl": None,
        "atr": atr_val,
        "activate_atr_mult": state["activate_atr_mult"],
        "step_atr_mult": state["step_atr_mult"],
        "lock_be_atr_mult": state["lock_be_atr_mult"],
        "hard_floor_pips": state["hard_floor_pips"],
        "price_source": price_source,
        "max_layers": state["max_layers"],
        "only_in_profit": state["only_in_profit"],
        "side": side,
        "symbol": symbol,
        "entry": float(entry_price),
    }
    publish_metrics({
        "trail_activated": False,
        "trail_be_locked": False,
        "trail_layers":    0,
        "trail_current_sl": None,
    })
    signal["entry_price"] = float(entry_price)

def _update_trailing_state(symbol: str, tick_dict: Optional[Dict[str, float]]) -> Optional[Dict[str, Any]]:
    if tick_dict is None:
        return None

    state = runtime_trail_states.setdefault(symbol, {})
    if not state or state.get("mode") != "atr":
        return None

    side = state.get("side")
    entry = state.get("entry")
    atr_val = float(state.get("atr") or 0.0)
    if not side or entry is None or atr_val <= 0.0:
        return None

    price_source = (state.get("price_source") or "mid").lower()
    current_price = _current_price_for_side(tick_dict, side, price_source)
    if current_price is None:
        return None

    cfg = TrailConfig(
        pip_size=float(state.get("pip_size", _pip_size_for(symbol))),
        point=float(state.get("point", _point_for(symbol))),
        atr=atr_val,
        activate_mult=float(state.get("activate_atr_mult", 0.5)),
        step_mult=float(state.get("step_atr_mult", 0.25)),
        lock_be_mult=float(state.get("lock_be_atr_mult", 0.3)),
        hard_floor_pips=float(state.get("hard_floor_pips", 5.0)),
        only_in_profit=bool(state.get("only_in_profit", True)),
        max_layers=int(state.get("max_layers", 20)),
    )
    trail_state = TrailState(
        side=side,
        entry=float(entry),
        activated=bool(state.get("activated", False)),
        be_locked=bool(state.get("be_locked", False)),
        layers=int(state.get("layers", 0)),
        current_sl=state.get("current_sl"),
    )

    trailer = AtrTrailer(cfg, trail_state)
    new_sl = trailer.suggest_sl(float(current_price))

    state.update(
        {
            "activated": trail_state.activated,
            "be_locked": trail_state.be_locked,
            "layers": trail_state.layers,
            "current_sl": trail_state.current_sl,
        }
    )
    runtime_trail_states[symbol] = state

    if new_sl is None:
        return None

    return {
        "new_sl": new_sl,
        "price": current_price,
        "state": {
            "activated": trail_state.activated,
            "be_locked": trail_state.be_locked,
            "layers": trail_state.layers,
            "current_sl": trail_state.current_sl,
            "price_source": price_source,
            "atr": atr_val,
            "max_layers": int(state.get("max_layers", 20)),
            "only_in_profit": bool(state.get("only_in_profit", True)),
            "side": side,
            "symbol": state.get("symbol", symbol),
        },
    }

def _session_hour_allowed() -> bool:
    """
    config.session.allow_hours_jst ??????????????????????
    ????????/???/???????????
    """
    try:
        from core.config import cfg as _cfg
    except Exception:
        _cfg = {}

    session_cfg = {}
    if isinstance(_cfg, dict):
        raw = _cfg.get("session")
        session_cfg = raw if isinstance(raw, dict) else {}

    allow = session_cfg.get("allow_hours_jst", [])
    if not isinstance(allow, (list, tuple, set)) or len(allow) == 0:
        return True

    try:
        import pytz
        from datetime import datetime
        jst = pytz.timezone("Asia/Tokyo")
        hour = datetime.now(jst).hour
    except Exception:
        return True

    return hour in set(allow)

def _symbol_to_filename(symbol: str) -> str:
    safe = re.sub(r"[^A-Za-z0-9_]+", "_", symbol)
    return safe.strip("_") or "UNKNOWN"


def _write_decision_log(symbol: str, record: Dict[str, Any]) -> None:
    fname = LOG_DIR / f"decisions_{_symbol_to_filename(symbol)}.jsonl"
    with open(fname, "a", encoding="utf-8") as fp:
        fp.write(json.dumps(record, ensure_ascii=False) + "\n")


def _build_decision_trace(
    *,
    ts_jst: str,
    symbol: str,
    ai_out: "ProbOut",
    cb_status: Dict[str, Any],
    filters_ctx: Dict[str, Any],
    decision: Dict[str, Any],
    prob_threshold: float,
    calibrator_name: str,
) -> Dict[str, Any]:
    """Assemble a structured trace record for downstream analysis."""
    if isinstance(decision, dict):
        action = str(decision.get("action") or "").upper()
        if action in {"BUY", "SELL", "LONG", "SHORT"}:
            decision_label = "ENTRY"
        else:
            decision_label = str(decision.get("action") or "")
    else:
        decision_label = str(decision)

    trace = {
        "ts_jst": ts_jst,
        "type": "decision",
        "symbol": symbol,
        "filters": filters_ctx,
        "probs": {
            "buy": round(ai_out.p_buy, 6),
            "sell": round(ai_out.p_sell, 6),
            "skip": round(ai_out.p_skip, 6),
        },
        "calibrator": calibrator_name,
        "meta": ai_out.meta,
        "threshold": float(prob_threshold),
        "decision": decision_label,
        "ai": ai_out.model_dump(),
        "cb": cb_status,
        "features_hash": ai_out.features_hash,
        "model": ai_out.model_name,
    }
    if isinstance(decision, dict):
        trace["decision_detail"] = decision
    exit_plan = decision.get("signal", {}).get("exit_plan") if isinstance(decision, dict) else None
    trace["exit_plan"] = exit_plan or {"mode": "none"}
    return trace

def _collect_features(
    symbol: str,
    base_features: Tuple[str, ...],
    tick: Optional[Tuple[float, float]],
    spread_pips: Optional[float],
    open_positions: int,
) -> Dict[str, float]:
    bid, ask = tick if tick else (None, None)
    mid = (float(bid) + float(ask)) / 2 if bid is not None and ask is not None else 0.0
    spr = float(spread_pips) if spread_pips is not None else 0.0

    features: Dict[str, float] = {}
    if not base_features:
        features["bias"] = 1.0
        return features

    for name in base_features:
        if name == "ema_5":
            features[name] = mid
        elif name == "ema_20":
            features[name] = mid
        elif name == "rsi_14":
            features[name] = 50.0
        elif name == "atr_14":
            features[name] = spr
        elif name == "adx_14":
            features[name] = 20.0 + min(20.0, spr * 5.0)
        elif name == "bbp":
            features[name] = 0.5 if spr == 0 else max(0.0, min(1.0, spr / 5.0))
        elif name == "vol_chg":
            features[name] = float(open_positions)
        elif name == "wick_ratio":
            features[name] = 0.5
        else:
            features[name] = 0.0
    return features


@dataclass
class ExecutionStub:
    """
    ドライラン用の実行スタブ：
    - AI確率（AISvc.predict）を呼び出して意思決定だけ行い、約定はしない
    - サーキットブレーカー（self.cb）発動中は BLOCKED を記録
    """
    cb: circuit_breaker.CircuitBreaker
    ai: AISvc

    def __post_init__(self) -> None:
        try:
            self.ai.threshold = float(BEST_THRESHOLD)
        except Exception:
            pass
        try:
            sell_threshold = max(min(1.0 - BEST_THRESHOLD, 1.0), 0.0)
            trade_state.update(
                prob_threshold=float(BEST_THRESHOLD),
                threshold_buy=float(BEST_THRESHOLD),
                threshold_sell=float(sell_threshold),
            )
        except Exception:
            pass


    def on_tick(
        self,
        symbol: str,
        features: Dict[str, float],
        runtime_cfg: Dict[str, Any],
    ) -> Dict[str, Any]:
        ts = now_jst_iso()

        cb_status = self.cb.status()
        ai_out = self.ai.predict(features)

        tick_dict = _tick_to_dict(runtime_cfg.get("tick"))

        spread_limit = float(runtime_cfg.get("spread_limit_pips", 1.5))
        min_adx = float(runtime_cfg.get("min_adx", 15.0))
        min_atr_pct = float(runtime_cfg.get("min_atr_pct", 0.0003))
        disable_adx_gate = bool(runtime_cfg.get("disable_adx_gate", False))
        prob_threshold = float(BEST_THRESHOLD)
        runtime_cfg["prob_threshold"] = prob_threshold
        side_bias = runtime_cfg.get("side_bias")

        raw_spread = runtime_cfg.get("spread_pips", 0.0)
        try:
            cur_spread = float(raw_spread)
        except (TypeError, ValueError):
            cur_spread = 0.0
        cur_spread = round(cur_spread, 5)

        cur_adx = round(float(features.get("adx_14", 0.0)), 5)
        cur_atr_pct = round(float(features.get("atr_14", 0.0)), 8)
        # ロット計算用：価格単位の ATR（特徴量 atr_14 と同じもの）
        atr_for_lot = float(features.get("atr_14", 0.0))

        base_filters: Dict[str, Any] = {
            "spread": cur_spread,
            "spread_limit": spread_limit,
            "adx": cur_adx,
            "min_adx": min_adx,
            "adx_disabled": disable_adx_gate,
            "atr_pct": cur_atr_pct,
            "min_atr_pct": min_atr_pct,
            "prob_threshold": prob_threshold,
            # ログ用に、ロット計算で使う ATR も入れておく
            "atr_for_lot": atr_for_lot,
        }
        if side_bias is not None:
            base_filters["side_bias"] = side_bias

        atr_gate_ok = _atr_gate_ok(cur_atr_pct, runtime_cfg)
        if _ATR_LAST_REF is not None:
            base_filters["atr_ref"] = round(float(_ATR_LAST_REF), 8)
        base_filters["atr_gate_state"] = "open" if _ATR_LAST_PASS else "closed"
        if _ATR_LAST_ENABLE is not None:
            base_filters["atr_enable_min"] = float(_ATR_LAST_ENABLE)
        if _ATR_LAST_DISABLE is not None:
            base_filters["atr_disable_min"] = float(_ATR_LAST_DISABLE)

        grace_active = trade_service.post_fill_grace_active()
        base_filters["post_fill_grace"] = grace_active

        def _emit(decision: Any, filters_ctx: Dict[str, Any], level: str = "info") -> None:
            # decision が str ("SKIP" など) の場合は dict として扱わずに抜ける
            if not isinstance(decision, dict):
                print("decision は dict ではありません:", decision)
                return

            action = decision.get("action")
            reason = decision.get("reason")

            gate_state = filters_ctx.get("atr_gate_state")
            atr_ref = float(filters_ctx.get("atr_ref", filters_ctx.get("atr_pct", 0.0)) or 0.0)
            post_grace = bool(filters_ctx.get("post_fill_grace", False))

            # --- カウンタは KVS から安全に読み出して加算 ---
            cur = METRICS.get()  # dictコピーが返る想定
            ce = int(cur.get("count_entry", 0))
            cs = int(cur.get("count_skip", 0))
            cb = int(cur.get("count_blocked", 0))
            if action == "ENTRY":
                ce += 1
            elif action == "SKIP":
                cs += 1
            elif action == "BLOCKED":
                cb += 1

            # --- まとめて publish（KVS更新＋runtime/metrics.json原子的書き換え） ---
            publish_metrics({
                "last_decision": action,
                "last_reason":   reason,
                "atr_ref":       float(atr_ref),
                "atr_gate_state": gate_state,
                "post_fill_grace": bool(post_grace),
                "spread":          filters_ctx.get("spread"),
                "adx":             filters_ctx.get("adx"),
                "min_adx":         filters_ctx.get("min_adx"),
                "prob_threshold":  filters_ctx.get("prob_threshold"),
                "min_atr_pct":     filters_ctx.get("min_atr_pct"),
                "count_entry":     ce,
                "count_skip":      cs,
                "count_blocked":   cb,
                # ts は publish_metrics 側でも自動付与するが、ここで入れても良い
            })


            trail_signal = decision.get("signal") if isinstance(decision, dict) else None
            if isinstance(trail_signal, dict) and "trail_state" in trail_signal:
                trail_state = trail_signal.get("trail_state") or {}
                new_sl_val = trail_state.get("current_sl")
                trail_side = trail_state.get("side") or trail_signal.get("side") or decision.get("side")
                trail_symbol = trail_state.get("symbol") or trail_signal.get("symbol") or symbol
                ticket = trail_state.get("ticket") if isinstance(trail_state, dict) else None
                if new_sl_val is not None and trail_side and trail_symbol:
                    try:
                        apply_trailing_update(
                            ticket=ticket if isinstance(ticket, int) else None,
                            side=str(trail_side),
                            symbol=str(trail_symbol),
                            new_sl=float(new_sl_val),
                            reason=str(action or "trail"),
                        )
                    except Exception as exc:
                        logger.debug(f"[TRAIL][HOOK][ERR] {exc}")

            trace = _build_decision_trace(
                ts_jst=ts,
                symbol=symbol,
                ai_out=ai_out,
                cb_status=cb_status,
                filters_ctx=filters_ctx,
                decision=decision,
                prob_threshold=prob_threshold,
                calibrator_name=self.ai.calibrator_name,
            )
            trace["runtime"] = runtime_cfg
            _write_decision_log(symbol, trace)

            ai_payload = ai_out.model_dump()
            ai_payload["best_threshold"] = BEST_THRESHOLD
            ai_payload.setdefault("threshold", getattr(self.ai, "threshold", prob_threshold))
            payload = {
                "mode": "dryrun",
                "symbol": symbol,
                "decision": decision.get("action"),
                "reason": decision.get("reason"),
                "ai": ai_payload,
                "filters": filters_ctx,
                "cb": cb_status,
            }
            log = logger.bind(event="dryrun", ts=ts)
            if level == "warning":
                log.warning(payload)
            elif level == "error":
                log.error(payload)
            else:
                log.info(payload)

        trail_info = _update_trailing_state(symbol, tick_dict)
        if trail_info:
            filters_ctx = dict(base_filters)
            filters_ctx["trail_state"] = trail_info["state"]
            filters_ctx["trail_new_sl"] = trail_info["new_sl"]
            filters_ctx["trail_price"] = trail_info["price"]
            publish_metrics({
                "trail_activated": bool(trail_info["state"].get("activated")),
                "trail_be_locked": bool(trail_info["state"].get("be_locked")),
                "trail_layers":    int(trail_info["state"].get("layers") or 0),
                "trail_current_sl": trail_info["state"].get("current_sl"),
            })

            decision_payload = {
                "action": "TRAIL_UPDATE",
                "reason": None,
                "signal": {
                    "trail_state": trail_info["state"],
                    "trail_new_sl": trail_info["new_sl"],
                    "trail_price": trail_info["price"],
                    "side": trail_info["state"].get("side"),
                    "symbol": trail_info["state"].get("symbol", symbol),
                },
            }
            _emit(decision_payload, filters_ctx, level="info")

        if not _session_hour_allowed():
            filters_ctx = dict(base_filters)
            filters_ctx["session"] = "closed"
            decision_payload = {"action": "SKIP", "reason": "session_closed"}
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        if not grace_active and cur_spread and cur_spread > spread_limit:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "spread"
            decision_payload = {"action": "BLOCKED", "reason": "spread"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not grace_active and not disable_adx_gate and cur_adx < min_adx:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "adx_low"
            decision_payload = {"action": "BLOCKED", "reason": "adx_low"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not grace_active and not atr_gate_ok:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "atr_low"
            decision_payload = {"action": "BLOCKED", "reason": "atr_low"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not self.cb.can_trade():
            cb_status = self.cb.status()
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "circuit_breaker"
            decision_payload = {"action": "BLOCKED", "reason": cb_status.get("reason", "circuit_breaker")}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        cb_status = self.cb.status()

        if cb_status.get("tripped"):
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "circuit_breaker"
            decision_payload = {"action": "BLOCKED", "reason": cb_status.get("reason", "circuit_breaker")}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        config = load_config()
        entry_cfg = config.get("entry", {}) if isinstance(config, dict) else {}
        edge = float(entry_cfg.get("entry_min_edge", entry_cfg.get("min_edge", 0.0)))
        buy_threshold = prob_threshold
        sell_threshold = max(min(1.0 - prob_threshold, 1.0), 0.0)

        base_filters["threshold_buy"] = buy_threshold
        base_filters["threshold_sell"] = sell_threshold
        base_filters["edge"] = edge

        filters_ctx = dict(base_filters)
        filters_ctx["blocked"] = None

        if side_bias is None:
            side_bias = (entry_cfg.get("side_bias") or "auto").lower()
        else:
            side_bias = str(side_bias).lower()
        filters_ctx["side_bias"] = side_bias

        p_buy = float(ai_out.p_buy)
        p_sell = float(ai_out.p_sell)

        buy_ok = p_buy >= buy_threshold
        sell_ok = p_sell >= sell_threshold

        chosen_side = None
        chosen_prob = 0.0
        other_prob = 0.0

        if buy_ok and (not sell_ok or p_buy >= p_sell):
            chosen_side = "BUY"
            chosen_prob = p_buy
            other_prob = p_sell
        elif sell_ok:
            chosen_side = "SELL"
            chosen_prob = p_sell
            other_prob = p_buy

        if chosen_side is not None and p_buy == p_sell:
            if side_bias == "buy":
                chosen_side = "BUY"
                chosen_prob = p_buy
                other_prob = p_sell
            elif side_bias == "sell":
                chosen_side = "SELL"
                chosen_prob = p_sell
                other_prob = p_buy

        decision_info: Dict[str, Any] = {
            "threshold_buy": buy_threshold,
            "threshold_sell": sell_threshold,
            "edge": edge,
            "prob_buy": p_buy,
            "prob_sell": p_sell,
        }

        if chosen_side is None:
            decision_info.update({"decision": "SKIP", "reason": "ai_threshold"})
            decision_payload = {
                "action": "SKIP",
                "reason": "ai_threshold",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        if (chosen_prob - other_prob) < edge:
            decision_info.update({"decision": "SKIP", "reason": "ai_low_edge"})
            decision_payload = {
                "action": "SKIP",
                "reason": "ai_low_edge",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        decision_info.update(
            {
                "decision": "ENTRY",
                "side": chosen_side,
                "prob": chosen_prob,
                "edge_delta": chosen_prob - other_prob,
            }
        )

        if not trade_service.can_open_new_position(symbol):
            blocked_filters = dict(base_filters)
            blocked_filters["blocked"] = "pos_guard"
            decision_payload = {
                "action": "BLOCKED",
                "reason": "pos_guard",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, blocked_filters, level="warning")
            position_guard.on_order_rejected_or_canceled(symbol)
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        signal = {
            "side": chosen_side,
            "prob": chosen_prob,
            "meta": chosen_side,
            "best_threshold": buy_threshold,
        }

        recent_ohlc = globals().get("get_recent_ohlc")
        ohlc_tail = None
        if callable(recent_ohlc):
            try:
                ohlc_tail = recent_ohlc(symbol, bars=64)
            except Exception:
                ohlc_tail = None

        exit_plan = None
        try:
            exit_plan = trade_service.build_exit_plan(symbol, ohlc_tail)
        except Exception:
            exit_plan = None

        if not exit_plan:
            exit_builder = globals().get("_build_exit_plan")
            decision_exit_builder = globals().get("_build_decision_exit_plan")
            if callable(exit_builder) and ohlc_tail is not None:
                if callable(decision_exit_builder):
                    exit_plan = decision_exit_builder(symbol, ohlc_tail)
                else:
                    exit_plan = exit_builder(symbol, ohlc_tail)

        signal["exit_plan"] = exit_plan or {"mode": "none"}
        # ロット計算用 ATR をシグナルにも載せて Live 側で参照できるようにする
        if atr_for_lot is not None:
            signal["atr_for_lot"] = float(atr_for_lot)

        _register_trailing_state(symbol, signal, tick_dict)

        trade_service.mark_filled_now()
        filters_ctx = dict(base_filters)
        filters_ctx["blocked"] = None
        decision_payload = {
            "action": "ENTRY",
            "reason": decision_info.get("reason","entry_ok"),
            "ai_meta": ai_out.meta,
            "signal": signal,
            "dec": decision_info,
        }
        _emit(decision_payload, filters_ctx, level="info")
        return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}


def evaluate_and_log_once() -> None:
    """Dry-run evaluation that mirrors the live decision path."""
    cfg = load_config()
    runtime_cfg = cfg.get("runtime", {})
    ai_cfg = cfg.get("ai", {})
    entry_cfg = cfg.get("entry", {})
    filters_cfg = cfg.get("filters", {})

    best_threshold = float(BEST_THRESHOLD)
    sell_threshold = max(min(1.0 - best_threshold, 1.0), 0.0)

    trade_state.update(
        threshold_buy=best_threshold,
        threshold_sell=sell_threshold,
        prob_threshold=best_threshold,
        side_bias=str(entry_cfg.get("side_bias", "auto") or "auto"),
        trading_enabled=True,  # ★ dryrun 評価では常にトレードONにする
    )
    settings = trade_state.get_settings()

    symbol = runtime_cfg.get("symbol", "USDJPY")
    spread_limit_pips = float(runtime_cfg.get("spread_limit_pips", runtime_cfg.get("spread_limit", 1.5)))
    max_pos = int(runtime_cfg.get("max_positions", 1))
    min_adx = float(filters_cfg.get("adx_min", 15.0))
    disable_adx_gate = bool(filters_cfg.get("adx_disable", False))
    min_atr_pct = float(filters_cfg.get("min_atr_pct", 0.0003))

    if not settings.trading_enabled:
        logger.bind(event="dryrun", ts=now_jst_iso()).info(
            {"mode": "dryrun", "enabled": False, "reason": "trading_disabled"}
        )
        return

    '''
    try:
        from app.core.mt5_client import MT5Client
        client = MT5Client()
        client.initialize()
        client.login_account()
    except Exception:
        # ドライランなのでMT5につながらなくてもOK
        logger.bind(event="dryrun", ts=now_jst_iso()).warning(
            {"mode": "dryrun", "enabled": True, "error": "mt5_init_skipped"}
        )
    '''

    try:
        spr_callable = getattr(market, "spread", None)
        spr = spr_callable(symbol) if callable(spr_callable) else 0.0

        ob_obj = orderbook() if callable(orderbook) else orderbook
        get_maybe = getattr(ob_obj, "get", None)
        ob = get_maybe(symbol) if callable(get_maybe) else None

        open_cnt = 0
        if ob is not None:
            updater = getattr(ob, "update_with_market_and_close_if_hit", None)
            if callable(updater):
                updater(symbol)
            cnt_getter = getattr(ob, "count_open", None)
            if callable(cnt_getter):
                try:
                    open_cnt = int(cnt_getter(symbol))
                except Exception:
                    open_cnt = 0

        tick = market.tick(symbol)
        tick_dict = None
        if tick:
            try:
                bid, ask = tick
                tick_dict = {"bid": float(bid), "ask": float(ask)}
            except (TypeError, ValueError):
                tick_dict = None

        base_features = tuple(ai_cfg.get("features", {}).get("base", []))
        features = _collect_features(symbol, base_features, tick, spr, open_cnt)

        cb_cfg = cfg.get("circuit_breaker", {}) if isinstance(cfg, dict) else {}
        cb = circuit_breaker.CircuitBreaker(
            max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", 5)),
            daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
            cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
        )
        #
        # --- DryRun 用 AISvc：モデル読み込み失敗を回避する ---
        try:
            ai = AISvc(threshold=best_threshold)
        except Exception:
            print("[exec] AISvc model loading failed → using dummy model for dryrun")

            class DummyProbOut:
                def __init__(self, p_buy: float, p_sell: float, p_skip: float, meta: str = "dummy") -> None:
                    # ExecutionStub や _build_decision_trace から参照される属性だけ持っておけばOK
                    self.p_buy = float(p_buy)
                    self.p_sell = float(p_sell)
                    self.p_skip = float(p_skip)
                    self.meta = meta
                    self.model_name = "dummy"
                    self.calibrator_name = "dummy"
                    self.features_hash = "dummy"

                def model_dump(self) -> dict:
                    # 本物の ProbOut.model_dump() っぽい辞書を返す
                    return {
                        "p_buy": self.p_buy,
                        "p_sell": self.p_sell,
                        "p_skip": self.p_skip,
                        "meta": self.meta,
                        "model_name": self.model_name,
                        "calibrator_name": self.calibrator_name,
                        "features_hash": self.features_hash,
                    }

            class DummyAISvc:
                def __init__(self, threshold: float) -> None:
                    self.threshold = float(threshold)
                    self.calibrator_name = "dummy"
                    self.model_name = "dummy"

                def predict(self, feats: dict) -> "DummyProbOut":
                    # 適当な確率を返すダミーモデル
                    return DummyProbOut(
                        p_buy=0.33,
                        p_sell=0.33,
                        p_skip=0.34,
                        meta="dummy",
                    )

            ai = DummyAISvc(threshold=best_threshold)
        #
        print(f"[exec] AISvc model: {getattr(ai, 'model_name', 'unknown')} (threshold={best_threshold})")
        stub = ExecutionStub(cb=cb, ai=ai)

        runtime_payload = {
            "threshold_buy": best_threshold,
            "threshold_sell": sell_threshold,
            "prob_threshold": best_threshold,
            "spread_limit_pips": spread_limit_pips,
            "max_positions": max_pos,
            "spread_pips": spr,
            "open_positions": open_cnt,
            "ai_threshold": stub.ai.threshold,
            "min_adx": min_adx,
            "disable_adx_gate": disable_adx_gate,
            "min_atr_pct": min_atr_pct,
            "tick": tick,
            "side_bias": settings.side_bias,
        }

        result = stub.on_tick(symbol, features, runtime_payload)
        _ = result
    finally:
        try:
            shutdown = getattr(mt5_client, "shutdown", None)
            if callable(shutdown):
                shutdown()
        except Exception:
            pass




=== file: app/services/feature_importance.py ===

# app/services/feature_importance.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Sequence, Tuple

import numpy as np


# LightGBM / XGBoost は「あるなら使う」スタイルにしておく
try:  # type: ignore[unused-ignore]
    import lightgbm as lgb
except Exception:  # ランタイムで LightGBM 未インストールでも死なないように
    lgb = None  # type: ignore[assignment]


try:  # type: ignore[unused-ignore]
    import xgboost as xgb
except Exception:
    xgb = None  # type: ignore[assignment]


@dataclass
class FeatureImportanceItem:
    """1つの特徴量についての FI 情報."""

    name: str
    importance: float
    importance_pct: float
    rank: int


def _unwrap_model(model: Any) -> Any:
    """
    CalibratedClassifierCV やラッパーに包まれている場合、
    中身の base_estimator / estimator をできるだけ剥がす。
    """
    for attr in ("base_estimator_", "base_estimator", "estimator_", "estimator"):
        inner = getattr(model, attr, None)
        if inner is not None:
            return inner
    return model


def _detect_model_type(model: Any) -> str:
    """
    LightGBM / XGBoost / その他 をざっくり判定する。
    """
    m = _unwrap_model(model)

    # LightGBM
    if lgb is not None:
        try:
            if isinstance(m, lgb.Booster):
                return "lightgbm"
        except Exception:
            pass
        try:
            from lightgbm.sklearn import LGBMModel  # type: ignore
        except Exception:
            LGBMModel = tuple()  # type: ignore[assignment]

        try:
            if isinstance(m, LGBMModel):
                return "lightgbm"
        except Exception:
            pass

    # XGBoost
    if xgb is not None:
        try:
            from xgboost import XGBModel  # type: ignore
        except Exception:
            XGBModel = tuple()  # type: ignore[assignment]

        try:
            if isinstance(m, XGBModel):
                return "xgboost"
        except Exception:
            pass

        try:
            if isinstance(m, xgb.Booster):
                return "xgboost"
        except Exception:
            pass

    return "unknown"


def _fi_lightgbm(
    model: Any,
    feature_names: Optional[Sequence[str]] = None,
) -> Tuple[np.ndarray, List[str]]:
    """
    LightGBM 用の raw FI 抽出.
    戻り値: (importances, names)
    """
    booster = None
    m = _unwrap_model(model)

    # sklearn API: LGBMClassifier / LGBMRegressor など
    if hasattr(m, "feature_importances_"):
        importances = np.asarray(getattr(m, "feature_importances_"), dtype=float)

        # feature_name_ があれば優先、それがなければ引数の feature_names
        names: List[str]
        fn = getattr(m, "feature_name_", None)
        if fn is not None:
            names = list(fn)
        elif feature_names is not None:
            names = list(feature_names)
        else:
            names = [f"f{i}" for i in range(len(importances))]

        return importances, names

    # Booster インスタンスを直接持っている場合
    if hasattr(m, "booster_"):
        booster = getattr(m, "booster_")
    elif lgb is not None and isinstance(m, lgb.Booster):
        booster = m

    if booster is not None:
        try:
            importances = np.asarray(
                booster.feature_importance(importance_type="gain"), dtype=float
            )
        except Exception:
            importances = np.asarray(
                booster.feature_importance(importance_type="split"), dtype=float
            )

        fn = getattr(booster, "feature_name", None)
        names: List[str]
        if callable(fn):
            names = list(fn())
        elif feature_names is not None:
            names = list(feature_names)
        else:
            names = [f"f{i}" for i in range(len(importances))]

        return importances, names

    raise ValueError("LightGBM モデルから feature_importance を取得できませんでした。")


def _fi_xgboost(
    model: Any,
    feature_names: Optional[Sequence[str]] = None,
) -> Tuple[np.ndarray, List[str]]:
    """
    XGBoost 用の raw FI 抽出.
    戻り値: (importances, names)
    """
    m = _unwrap_model(model)

    booster = None
    # sklearn API: XGBClassifier / XGBRegressor
    if hasattr(m, "get_booster"):
        booster = m.get_booster()
    elif xgb is not None and isinstance(m, xgb.Booster):
        booster = m

    if booster is None:
        raise ValueError("XGBoost モデルから Booster を取得できませんでした。")

    # gain ベースを優先、無ければ weight
    try:
        score_dict: Dict[str, float] = booster.get_score(importance_type="gain")
    except Exception:
        score_dict = {}

    if not score_dict:
        score_dict = booster.get_score(importance_type="weight")

    if not score_dict:
        raise ValueError("XGBoost Booster.get_score() が空でした。")

    # key は "f0", "f1" … のことが多い
    names: List[str] = []
    importances: List[float] = []

    for key, val in score_dict.items():
        # key が f0 形式なら index を解釈して feature_names と合わせる
        if feature_names is not None and key.startswith("f") and key[1:].isdigit():
            idx = int(key[1:])
            if 0 <= idx < len(feature_names):
                fname = str(feature_names[idx])
            else:
                fname = key
        else:
            fname = key

        names.append(fname)
        importances.append(float(val))

    return np.asarray(importances, dtype=float), names


def compute_feature_importance(
    model: Any,
    feature_names: Optional[Sequence[str]] = None,
    top_n: Optional[int] = 30,
) -> List[FeatureImportanceItem]:
    """
    LightGBM / XGBoost モデルから Feature Importance を取り出し、
    降順ソート＋割合付きのリストにして返す。

    Parameters
    ----------
    model:
        LightGBM / XGBoost の学習済みモデル
        （calibration ラッパー付きでも OK）
    feature_names:
        特徴量名のシーケンス。
        モデル側から取れない場合にここで補う。
    top_n:
        上位いくつまで返すか。None なら全件。
    """
    model_type = _detect_model_type(model)

    if model_type == "lightgbm":
        importances, names = _fi_lightgbm(model, feature_names)
    elif model_type == "xgboost":
        importances, names = _fi_xgboost(model, feature_names)
    else:
        raise ValueError(
            f"未対応のモデルタイプです: {_detect_model_type(model)} "
            "(LightGBM / XGBoost 以外は compute_feature_importance() では扱いません)"
        )

    if len(importances) == 0:
        raise ValueError("feature_importance の長さが 0 です。")

    # 負の値などが来ても一応扱えるように abs を取る
    imp = np.asarray(importances, dtype=float)
    imp = np.nan_to_num(imp, nan=0.0)
    total = float(np.sum(np.abs(imp)))
    if total <= 0:
        # すべて 0 の場合は一律 0%
        pct = np.zeros_like(imp, dtype=float)
    else:
        pct = (np.abs(imp) / total) * 100.0

    items: List[FeatureImportanceItem] = []
    for name, v, p in zip(names, imp, pct):
        items.append(
            FeatureImportanceItem(
                name=str(name),
                importance=float(v),
                importance_pct=float(p),
                rank=-1,  # ここでは仮。あとでソートして rank を振る。
            )
        )

    # importance 降順でソート
    items.sort(key=lambda x: x.importance, reverse=True)

    # rank を振り直し
    for i, it in enumerate(items, start=1):
        it.rank = i

    # top_n で切る
    if top_n is not None and top_n > 0:
        items = items[:top_n]

    return items



=== file: app/services/metrics.py ===

# app/services/metrics.py
from __future__ import annotations
from pathlib import Path
from typing import Dict, Any
import json, os, time, tempfile
import shutil,time
from core.metrics import METRICS_JSON, METRICS  # METRICS_JSON はファイルパス、METRICS はKVS

def publish_metrics(kv: Dict[str, Any]) -> None:
    """
    Dashboardが読むランタイム指標を KVS と JSON(atomic write) に出力する。
    必要なキー例は下記の通り（全部でなくてOK）:
      last_decision, last_reason, atr_ref, atr_gate_state, post_fill_grace,
      spread, prob_threshold, min_atr_pct, adx, min_adx,
      trail_activated, trail_be_locked, trail_layers, trail_current_sl,
      count_entry, count_skip, count_blocked, cb_tripped, cb_reason, ts
    """
    # KVS（同一プロセス向けフォールバック）
    METRICS.update(**kv)

    # JSON（別プロセス連携／Dashboard標準入力）
    path = Path(METRICS_JSON)
    path.parent.mkdir(parents=True, exist_ok=True)
    data = dict(kv)
    # ts（ローカル更新時刻）はここで保証
    data.setdefault("ts", int(time.time()))

    txt = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
    tmp_path = Path(tempfile.mkstemp(prefix="metrics_", suffix=".json", dir=str(path.parent))[1])
    tmp_path.write_text(txt, encoding="utf-8")

    # --- safe replace with retry ---
    for i in range(10):
        try:
            shutil.move(tmp_path, path)
            break
        except PermissionError:
            time.sleep(0.5)
    else:
        print(f"[metrics][warn] could not update {path} (still locked). skipped.")
    


=== file: app/services/mt5_account_store.py ===

# app/services/mt5_account_store.py
from __future__ import annotations

from typing import Dict, Any, Optional
from pathlib import Path
import json
import os

from loguru import logger

import fxbot_path

# 設定ファイルのパス: <project_root>/config/mt5_accounts.json
_CONFIG_DIR = fxbot_path.get_project_root() / "config"
_CONFIG_FILE = _CONFIG_DIR / "mt5_accounts.json"


def _default_config() -> Dict[str, Any]:
    """設定ファイルが存在しない場合の初期値。"""
    return {
        "active_profile": "",
        "profiles": {},  # name -> {login, password, server}
    }


def load_config() -> Dict[str, Any]:
    """JSON 設定ファイルを読み込んで dict を返す。"""
    try:
        if not _CONFIG_FILE.exists():
            return _default_config()
        with _CONFIG_FILE.open("r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, dict):
            logger.warning(f"[mt5_account_store] config is not dict: {_CONFIG_FILE}")
            return _default_config()
        # 必須キーのフォールバック
        data.setdefault("active_profile", "")
        data.setdefault("profiles", {})
        if not isinstance(data["profiles"], dict):
            data["profiles"] = {}
        return data
    except Exception as e:
        logger.error(f"[mt5_account_store] failed to load config: {e}")
        return _default_config()


def save_config(cfg: Dict[str, Any]) -> None:
    """設定ファイルを保存する。"""
    try:
        _CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        with _CONFIG_FILE.open("w", encoding="utf-8") as f:
            json.dump(cfg, f, ensure_ascii=False, indent=2)
        logger.info(f"[mt5_account_store] saved config: {_CONFIG_FILE}")
    except Exception as e:
        logger.error(f"[mt5_account_store] failed to save config: {e}")


def get_profile(name: str) -> Optional[Dict[str, Any]]:
    """プロファイル名から設定を取得する。存在しなければ None。"""
    cfg = load_config()
    profiles = cfg.get("profiles", {})
    if not isinstance(profiles, dict):
        return None
    acc = profiles.get(name)
    if isinstance(acc, dict):
        return acc
    return None


def upsert_profile(name: str, *, login: int, password: str, server: str) -> None:
    """プロファイルを追加または更新する。"""
    cfg = load_config()
    profiles = cfg.setdefault("profiles", {})
    if not isinstance(profiles, dict):
        profiles = {}
        cfg["profiles"] = profiles

    profiles[name] = {
        "login": int(login),
        "password": str(password),
        "server": str(server),
    }
    save_config(cfg)


def set_active_profile(name: str, *, apply_env: bool = True) -> None:
    """アクティブプロファイルを変更する。

    apply_env=True のとき、os.environ の MT5_LOGIN/PASSWORD/SERVER も更新。
    """
    cfg = load_config()
    profiles = cfg.get("profiles", {})
    if not isinstance(profiles, dict) or name not in profiles:
        logger.warning(f"[mt5_account_store] profile {name!r} not found; active_profile not changed")
        return

    cfg["active_profile"] = name
    save_config(cfg)

    if apply_env:
        acc = profiles[name]
        os.environ["MT5_LOGIN"] = str(acc.get("login", ""))
        os.environ["MT5_PASSWORD"] = str(acc.get("password", ""))
        os.environ["MT5_SERVER"] = str(acc.get("server", ""))
        logger.info(f"[mt5_account_store] applied env for profile {name!r}")


def get_active_profile_name() -> str:
    cfg = load_config()
    active = cfg.get("active_profile")
    return str(active or "")



=== file: app/services/mt5_selftest.py ===

# app/services/mt5_selftest.py
from __future__ import annotations

from typing import Any, List, Tuple
import traceback
import subprocess
import sys
from pathlib import Path

from app.core import mt5_client


def _get_attr(obj: Any, name: str, default: Any = "(n/a)") -> Any:
    """
    dict / MT5 の AccountInfo のどちらでも安全に属性を取り出すヘルパー。

    scripts/selftest_mt5.py と同じ挙動にしておく。
    """
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def run_mt5_selftest() -> Tuple[bool, str]:
    """
    MT5 自己診断を実行して、(成功フラグ, ログ文字列) を返す。

    - GUI から呼び出すことを前提に、print ではなく文字列を組み立てる
    - 例外はここでキャッチし、False とスタックトレースを返す
    """
    lines: List[str] = []
    lines.append("=== MT5 self test (GUI) ===")
    lines.append("このテストは、現在の設定タブで選択されている口座プロファイルに基づき、")
    lines.append("MT5 への接続とログイン状態を確認します。")
    lines.append("")

    try:
        # 念のため毎回クリーンな状態から始める
        try:
            mt5_client.shutdown()
        except Exception:
            # 失敗しても致命的ではないので無視
            pass

        # 1) initialize
        lines.append("[1] mt5_client.initialize() ...")
        ok = mt5_client.initialize()
        lines.append(f"    -> initialize() returned: {ok!r}")
        if not ok:
            lines.append("")
            lines.append("ERROR: MT5 の初期化に失敗しました。")
            lines.append(" - MT5 ターミナルが起動しているか？")
            lines.append(" - 設定タブで選択した口座ID / サーバー / パスワードは正しいか？")
            return False, "\n".join(lines)

        # 2) account_info
        lines.append("")
        lines.append("[2] mt5_client.get_account_info() ...")
        info = mt5_client.get_account_info()
        if not info:
            lines.append("ERROR: get_account_info() が None / False を返しました。")
            lines.append("      ログイン情報やサーバー設定を確認してください。")
            return False, "\n".join(lines)

        login = _get_attr(info, "login")
        name = _get_attr(info, "name")
        server = _get_attr(info, "server")
        balance = _get_attr(info, "balance")
        equity = _get_attr(info, "equity")
        trade_mode = _get_attr(info, "trade_mode")

        lines.append("  --- Account Info ---")
        lines.append(f"  login      : {login}")
        lines.append(f"  name       : {name}")
        lines.append(f"  server     : {server}")
        lines.append(f"  balance    : {balance}")
        lines.append(f"  equity     : {equity}")
        lines.append(f"  trade_mode : {trade_mode}")
        lines.append("  --------------------")

        # 3) positions (raw)
        lines.append("")
        lines.append("[3] mt5_client.get_positions() ...")
        positions = mt5_client.get_positions()
        n_pos = len(positions) if positions is not None else 0
        lines.append(f"    -> open positions: {n_pos}")
        if positions:
            # 先頭数件だけざっくり表示
            lines.append("    sample positions (up to 3):")
            for i, pos in enumerate(positions[:3]):
                lines.append(f"      [{i}] {pos!r}")

        # 4) positions_df (DataFrame)
        lines.append("")
        lines.append("[4] mt5_client.get_positions_df() ...")
        df = mt5_client.get_positions_df()
        if df is None:
            lines.append("    -> positions_df: None")
        else:
            try:
                shape = getattr(df, "shape", None)
                lines.append(f"    -> positions_df.shape = {shape}")
                # 行数だけ軽く表示
                lines.append(f"    -> positions_df.head():")
                lines.append(df.head(5).to_string())
            except Exception as e:
                lines.append(f"    -> positions_df の表示中にエラー: {e!r}")

        lines.append("")
        lines.append("MT5 self test completed successfully.")
        return True, "\n".join(lines)

    except Exception:
        # ここで例外を全部飲み込んで、GUI 側には文字列で返す
        lines.append("")
        lines.append("ERROR: MT5 自己診断中に例外が発生しました。")
        lines.append("")
        lines.append(traceback.format_exc())
        return False, "\n".join(lines)

    finally:
        # 毎回 shutdown しておくことで、次回テストもクリーンに実行できるようにする
        try:
            mt5_client.shutdown()
        except Exception:
            pass

def run_mt5_orderflow_selftest() -> Tuple[bool, str]:
    """
    scripts/selftest_order_flow.py をサブプロセスとして実行し、
    (成功フラグ, ログ文字列) を返す。

    - GUI から呼び出すことを前提に、print ではなく文字列を組み立てる
    - 例外はここでキャッチし、False とスタックトレースを返す
    """
    lines: List[str] = []
    lines.append("=== MT5 order flow self test (GUI) ===")
    lines.append(
        "このテストは、現在のアクティブMT5口座で 0.01 lot の成行BUY → 即クローズを行い、"
        "発注フローが正常に動くかを確認します。"
    )
    lines.append("")
    lines.append("※ 必ずデモ口座で実行してください。")
    lines.append("")

    try:
        # プロジェクトルートを推定（.../app/services/ から2つ上）
        project_root = Path(__file__).resolve().parents[2]

        cmd = [sys.executable, "-m", "scripts.selftest_order_flow"]
        lines.append(f"[INFO] 実行コマンド: {' '.join(cmd)}")
        lines.append(f"[INFO] cwd: {project_root}")
        lines.append("")

        result = subprocess.run(
            cmd,
            cwd=project_root,
            capture_output=True,
            text=True,
            check=False,
        )

        stdout = (result.stdout or "").strip()
        stderr = (result.stderr or "").strip()

        lines.append("--- stdout ---")
        if stdout:
            lines.append(stdout)
        else:
            lines.append("(出力なし)")

        if stderr:
            lines.append("")
            lines.append("--- stderr ---")
            lines.append(stderr)

        lines.append("")
        lines.append(f"[INFO] returncode = {result.returncode}")

        ok = (result.returncode == 0)
        return ok, "\n".join(lines)

    except Exception:
        # ここで例外を握って、False + スタックトレースを返す
        lines.append("")
        lines.append("ERROR: selftest_order_flow 実行中に例外が発生しました。")
        lines.append(traceback.format_exc())
        return False, "\n".join(lines)



=== file: app/services/mt5_service.py ===

# app/services/mt5_service.py
from __future__ import annotations
import time
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple

import MetaTrader5 as mt5

@dataclass
class BrokerConstraints:
    # 価格刻み
    digits: int
    point: float
    tick_size: float
    # 制約
    stop_level_points: int     # 最小SL/TP距離（ポイント）
    freeze_level_points: int   # 凍結レベル（ポイント）
    trade_stops_level: int     # 旧称/互換（ポイント）
    # 価格制約（スリッページ等は発注側で取り回し）
    min_sl_step_points: int = 1

def _symbol_props(symbol: str) -> BrokerConstraints:
    info = mt5.symbol_info(symbol)
    if info is None:
        raise RuntimeError(f"symbol_info({symbol}) failed: {mt5.last_error()}")
    # 一部ブローカは stop_level と trade_stops_level どちらかしか意味がないことがある
    stop_level = getattr(info, "stop_level", 0) or getattr(info, "trade_stops_level", 0) or 0
    trade_stops = getattr(info, "trade_stops_level", 0) or stop_level
    freeze = getattr(info, "freeze_level", 0) or 0
    return BrokerConstraints(
        digits=info.digits,
        point=info.point,
        tick_size=getattr(info, "trade_tick_size", info.point),
        stop_level_points=int(stop_level),
        freeze_level_points=int(freeze),
        trade_stops_level=int(trade_stops),
        min_sl_step_points=1
    )

def _round_to_point(price: float, point: float) -> float:
    # MT5に合わせた丸め（ポイント単位）
    return round(price / point) * point

def _sl_min_distance_ok(side: str, price_now: float, sl_price: float, min_points: int, point: float) -> bool:
    # side=BUY → SLは price_now より下、side=SELL → SLは上
    dist_points = abs(price_now - sl_price) / point
    if side == "BUY":
        return sl_price < price_now and dist_points >= min_points
    else:
        return sl_price > price_now and dist_points >= min_points

def _freeze_level_ok(side: str, price_now: float, sl_price: float, freeze_points: int, point: float) -> bool:
    # FreezeLevel以内だと変更不可
    dist_points = abs(price_now - sl_price) / point
    return dist_points > freeze_points

def _snap_sl_to_rules(side: str, price_now: float, desired_sl: float, bc: BrokerConstraints) -> Optional[float]:
    """
    望ましいSLを、StopLevel/FreezeLevel/丸めに収まるよう調整。
    条件を満たせない場合は None（＝更新スキップ）。
    """
    sl = _round_to_point(desired_sl, bc.point)

    # StopLevelを満たすよう少し離す
    if not _sl_min_distance_ok(side, price_now, sl, bc.stop_level_points, bc.point):
        # 何ポイントずらせば良いか計算して、min_sl_step_points単位で押し出す
        need = bc.stop_level_points - abs(price_now - sl) / bc.point
        # 端数切り上げ
        steps = int(max(0, need)) + 1
        delta = steps * bc.min_sl_step_points * bc.point
        if side == "BUY":
            sl = price_now - delta
        else:
            sl = price_now + delta
        sl = _round_to_point(sl, bc.point)

    # FreezeLevelチェック（満たせなければ諦める）
    if not _freeze_level_ok(side, price_now, sl, bc.freeze_level_points, bc.point):
        return None

    # 方向性の安全（BUYなら下、SELLなら上）
    if side == "BUY" and sl >= price_now:
        return None
    if side == "SELL" and sl <= price_now:
        return None

    return sl

def _price_for_side(tick: Dict[str, float], side: str) -> float:
    # 更新判定に使う現在レート（SLは逆サイドに付くので BUY→bid、SELL→ask を基準に）
    if side == "BUY":
        return tick["bid"]
    else:
        return tick["ask"]

def _position_of(ticket: int) -> Any:
    # ticketから現在のポジション情報を取る（なければNone）
    pos = mt5.positions_get(ticket=ticket)
    if pos is None:
        raise RuntimeError(f"positions_get failed: {mt5.last_error()}")
    return pos[0] if len(pos) > 0 else None

def _current_tick(symbol: str) -> Dict[str, float]:
    t = mt5.symbol_info_tick(symbol)
    if t is None:
        raise RuntimeError(f"symbol_info_tick({symbol}) failed: {mt5.last_error()}")
    return {"bid": t.bid, "ask": t.ask, "last": getattr(t, "last", (t.bid + t.ask)/2)}

class MT5Service:
    """
    本線：安全なSL更新（OrderModify）
    """
    def __init__(self, max_retries: int = 3, backoff_sec: float = 0.3, min_change_points: int = 2):
        self.max_retries = max_retries
        self.backoff_sec = backoff_sec
        self.min_change_points = min_change_points  # 現SLからこのポイント以上ずれた時のみ更新

    def safe_order_modify_sl(self, ticket: int, side: str, symbol: str, desired_sl: float, reason: str = "") -> Tuple[bool, Optional[float], str]:
        """
        返り値: (成功/失敗, 実際に送ったSL, 詳細メッセージ)
        """
        pos = _position_of(ticket)
        if pos is None:
            return (False, None, f"no-position ticket={ticket}")

        bc = _symbol_props(symbol)
        tick = _current_tick(symbol)
        price_now = _price_for_side(tick, side)

        # 既存SLとの差分が小さすぎるなら何もしない（チラつき抑制）
        current_sl = float(getattr(pos, "sl", 0.0) or 0.0)
        if current_sl > 0:
            dpoints = abs(current_sl - desired_sl) / bc.point
            if dpoints < self.min_change_points:
                return (True, None, f"skip: delta<{self.min_change_points}pt (current_sl={current_sl}, desired={desired_sl})")

        # 規約に沿ってSLをスナップ
        snapped = _snap_sl_to_rules(side, price_now, desired_sl, bc)
        if snapped is None:
            return (False, None, f"reject: violates stop/freeze/side rules (desired={desired_sl}, price_now={price_now})")

        # TRADE_ACTION_SLTP で変更
        request = {
            "action": mt5.TRADE_ACTION_SLTP,
            "position": ticket,
            "symbol": symbol,
            "sl": round(snapped, bc.digits),
            "tp": float(getattr(pos, "tp", 0.0) or 0.0),  # 既存TPは維持
            "deviation": 10,  # 形式上必要。SLTP変更では通常影響しない
            "comment": f"trail:{reason}"[:28],
            "type_time": mt5.ORDER_TIME_GTC,
            "type_filling": mt5.ORDER_FILLING_RETURN,
        }

        last_err = ""
        for i in range(self.max_retries + 1):
            res = mt5.order_send(request)
            if res is None:
                last_err = f"order_send None: {mt5.last_error()}"
            else:
                if res.retcode == mt5.TRADE_RETCODE_DONE or res.retcode == mt5.TRADE_RETCODE_DONE_PARTIAL:
                    return (True, request["sl"], f"OK retcode={res.retcode}")
                last_err = f"retcode={res.retcode}, comment={getattr(res, 'comment', '')}"

            time.sleep(self.backoff_sec)

            # 価格更新して再スナップ（Freeze/StopLevel変化に追随）
            tick = _current_tick(symbol)
            price_now = _price_for_side(tick, side)
            snapped = _snap_sl_to_rules(side, price_now, desired_sl, bc)
            if snapped is None:
                break
            request["sl"] = round(snapped, bc.digits)

        return (False, None, f"fail: {last_err}")



=== file: app/services/orderbook_stub.py ===

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Optional
from datetime import datetime, timezone, timedelta
from loguru import logger
from app.core import market

JST = timezone(timedelta(hours=9))
TIMEOUT_SECONDS = 0  # 本番寄せ：タイムアウトによる強制クローズを無効化

@dataclass
class MockPosition:
    id: int
    symbol: str
    side: str        # "BUY" or "SELL"
    lot: float
    entry: float
    sl: float
    tp: float
    open_time: datetime = field(default_factory=lambda: datetime.now(JST))
    close_time: Optional[datetime] = None
    closed: bool = False
    close_price: Optional[float] = None
    profit_pips: Optional[float] = None

class OrderBook:
    def __init__(self) -> None:
        self._next_id = 1
        self._positions: List[MockPosition] = []

    def count_open(self, symbol: Optional[str] = None) -> int:
        return sum(1 for p in self._positions if not p.closed and (symbol is None or p.symbol == symbol))

    def open(self, symbol: str, side: str, lot: float, entry: float, sl: float, tp: float) -> MockPosition:
        pos = MockPosition(
            id=self._next_id, symbol=symbol, side=side, lot=lot, entry=entry, sl=sl, tp=tp
        )
        self._next_id += 1
        self._positions.append(pos)
        logger.bind(event="dryrun_open").info({
            "mode":"dryrun", "action":"open", "id": pos.id, "symbol": symbol, "side": side,
            "lot": lot, "entry": entry, "sl": sl, "tp": tp, "ts": pos.open_time.isoformat(timespec="seconds")
        })
        return pos

    def _close(self, p: MockPosition, price: float, reason: str) -> None:
        if p.closed:
            return
        p.closed = True
        p.close_time = datetime.now(JST)
        p.close_price = price
        pip_delta = market.pips_to_price(p.symbol, 1.0)
        if pip_delta and pip_delta > 0:
            p.profit_pips = (p.close_price - p.entry)/pip_delta if p.side == "BUY" else (p.entry - p.close_price)/pip_delta
        else:
            p.profit_pips = None
        logger.bind(event="dryrun_close").info({
            "mode":"dryrun", "action":"close", "id": p.id, "symbol": p.symbol, "side": p.side,
            "entry": p.entry, "close": p.close_price, "profit_pips": p.profit_pips,
            "reason": reason, "ts": p.close_time.isoformat(timespec="seconds")
        })

    def update_with_market_and_close_if_hit(self, symbol: str) -> None:
        """現在の価格で SL/TP 到達、またはTIMEOUTでクローズ。"""
        tk = market.tick(symbol)
        for p in list(self._positions):
            if p.closed or p.symbol != symbol:
                continue
            # TIMEOUT
            if TIMEOUT_SECONDS and (datetime.now(JST) - p.open_time).total_seconds() >= TIMEOUT_SECONDS:
                price = (tk[1] if p.side == "BUY" else tk[0]) if tk else p.entry
                self._close(p, price, "TIMEOUT")
                continue
            if not tk:
                continue
            bid, ask = tk
            price = ask if p.side == "BUY" else bid
            hit_tp = (price >= p.tp) if p.side == "BUY" else (price <= p.tp)
            hit_sl = (price <= p.sl) if p.side == "BUY" else (price >= p.sl)
            if hit_tp:
                self._close(p, price, "TP")
            elif hit_sl:
                self._close(p, price, "SL")

    def close_all(self, symbol: Optional[str] = None) -> None:
        """現在値で全クローズ（ドライラン）。"""
        tk = None
        if symbol:
            tk = market.tick(symbol)
        for p in list(self._positions):
            if p.closed: 
                continue
            if symbol and p.symbol != symbol: 
                continue
            price = None
            if tk:
                bid, ask = tk
                price = ask if p.side == "BUY" else bid
            self._close(p, price if price is not None else p.entry, "FORCE_CLOSE")

# シングルトン
_orderbook = OrderBook()
def orderbook() -> OrderBook:
    return _orderbook



=== file: app/services/recent_kpi.py ===

# app/services/recent_kpi.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, Mapping, Optional, Sequence, Union

from app.services.decision_log import load_recent_decisions

import math

try:
    import pandas as pd
    from pandas import DataFrame, Series
except ImportError:  # pandas 未インストール環境向けの保険
    pd = None
    DataFrame = object  # type: ignore
    Series = object  # type: ignore


Number = Union[int, float]


@dataclass
class RecentKpiResult:
    """
    直近 N トレードの簡易 KPI 集計結果。

    単位はすべて「pnl の単位」（= 通貨 or 円）に揃える。
    """

    n_trades: int
    n_wins: int
    n_losses: int
    win_rate: Optional[float]  # 0.0–1.0, データ不足などの場合は None

    gross_profit: float        # 勝ちトレードの合計損益（>=0）
    gross_loss: float          # 負けトレードの合計損益（<=0）
    profit_factor: Optional[float]  # gross_profit / abs(gross_loss)

    net_profit: float          # 総損益（= gross_profit + gross_loss）

    max_drawdown: float        # 最大ドローダウン（>0: 金額）
    max_drawdown_ratio: Optional[float]  # 開始残高が与えられた場合の割合（0.1 で 10%）

    best_win_streak: int       # 連勝の最大値
    best_loss_streak: int      # 連敗の最大値


def _extract_pnl_series(
    trades: Union["DataFrame", Sequence[Mapping[str, Number]]],
    profit_field: str,
) -> Sequence[float]:
    """
    汎用的に「pnl の列」を取り出すヘルパー。

    - pandas.DataFrame なら該当列を float にして NaN を除外
    - list[dict] 的な構造なら profit_field キーで取り出す
    """
    if pd is not None and isinstance(trades, pd.DataFrame):
        if profit_field not in trades.columns:
            raise KeyError(f"profit_field '{profit_field}' not in DataFrame columns")
        series = trades[profit_field].astype(float)
        series = series.dropna()
        return series.to_list()

    pnl_list: list[float] = []
    for i, t in enumerate(trades):
        if profit_field not in t:
            raise KeyError(f"profit_field '{profit_field}' not in trade[{i}] keys")
        pnl_list.append(float(t[profit_field]))  # type: ignore[arg-type]
    return pnl_list

def compute_kpi_from_trades(
    trades: Union["DataFrame", Sequence[Mapping[str, Number]]],
    *,
    profit_field: str = "pnl",
    starting_equity: Optional[float] = None,
) -> RecentKpiResult:
    """
    直近 N トレードの KPI を計算するメイン関数。

    Parameters
    ----------
    trades:
        - pandas.DataFrame または
        - dict のシーケンス（各要素が 1 トレード）を想定。
        profit_field で指定したキー/列に損益（pnl）が入っていること。
    profit_field:
        1 トレードあたりの損益を表す列名 / キー名。
        例: "pnl", "profit"
    starting_equity:
        最大 DD を「残高ベース」で見たい場合の開始残高。
        None の場合は、「0 からスタートした累積損益」に対する DD を返す。
    """
    pnl_list = _extract_pnl_series(trades, profit_field)
    n_trades = len(pnl_list)

    if n_trades == 0:
        # 取引がない場合は全部ゼロ/None で返す
        return RecentKpiResult(
            n_trades=0,
            n_wins=0,
            n_losses=0,
            win_rate=None,
            gross_profit=0.0,
            gross_loss=0.0,
            profit_factor=None,
            net_profit=0.0,
            max_drawdown=0.0,
            max_drawdown_ratio=None,
            best_win_streak=0,
            best_loss_streak=0,
        )

    # --- 勝ち / 負け / 引き分け 判定 ---
    wins = [p for p in pnl_list if p > 0]
    losses = [p for p in pnl_list if p < 0]
    n_wins = len(wins)
    n_losses = len(losses)

    # 引き分け（pnl == 0）は win_rate / PF の分母からは外す
    n_effective = n_wins + n_losses

    if n_effective > 0:
        win_rate = n_wins / n_effective
    else:
        win_rate = None

    gross_profit = float(sum(wins)) if wins else 0.0
    gross_loss = float(sum(losses)) if losses else 0.0  # <=0
    net_profit = gross_profit + gross_loss

    if gross_loss < 0.0:
        profit_factor = gross_profit / abs(gross_loss)
    else:
        # 負けトレードが無い場合は PF 無限大とみなす / None とするかは好み。
        # ここでは None にして GUI 側で "∞" 表示などに委ねる。
        profit_factor = None

    # --- 最大ドローダウン ---
    # 累積損益から DD を計算する。starting_equity があればそれをベースにする。
    equity_series: list[float] = []
    cumulative = 0.0
    base = starting_equity or 0.0

    for p in pnl_list:
        cumulative += p
        equity_series.append(base + cumulative)

    max_equity = equity_series[0]
    max_dd = 0.0  # 正の値（下方向の幅）
    for eq in equity_series:
        if eq > max_equity:
            max_equity = eq
        dd = max_equity - eq  # max_equity >= eq のとき dd >= 0
        if dd > max_dd:
            max_dd = dd

    if starting_equity is not None and starting_equity > 0:
        max_dd_ratio: Optional[float] = max_dd / float(starting_equity)
    else:
        max_dd_ratio = None

    # --- 連勝 / 連敗 ストリーク ---
    best_win_streak = 0
    best_loss_streak = 0
    current_win_streak = 0
    current_loss_streak = 0

    for p in pnl_list:
        if p > 0:
            current_win_streak += 1
            current_loss_streak = 0
        elif p < 0:
            current_loss_streak += 1
            current_win_streak = 0
        else:
            # 引き分けはどちらのストリークも中断させる
            current_win_streak = 0
            current_loss_streak = 0

        best_win_streak = max(best_win_streak, current_win_streak)
        best_loss_streak = max(best_loss_streak, current_loss_streak)

    return RecentKpiResult(
        n_trades=n_trades,
        n_wins=n_wins,
        n_losses=n_losses,
        win_rate=win_rate,
        gross_profit=gross_profit,
        gross_loss=gross_loss,
        profit_factor=profit_factor,
        net_profit=net_profit,
        max_drawdown=max_dd,
        max_drawdown_ratio=max_dd_ratio,
        best_win_streak=best_win_streak,
        best_loss_streak=best_loss_streak,
    )


def compute_recent_kpi_from_decisions(
    limit: Optional[int] = None,
    *,
    profit_field: str = "pnl",
    starting_equity: Optional[float] = None,
) -> RecentKpiResult:
    """
    Read logs/decisions/decisions_*.jsonl, filter trades with numeric pnl, and compute recent KPI.

    Parameters
    ----------
    limit : int | None
        Number of pnl-qualified trades to include from the end of the log. None means all trades.
    profit_field : str
        Column name for the profit/loss value. Defaults to "pnl".
    starting_equity : float | None
        Initial equity used for drawdown ratio. None keeps the ratio None.

    Returns
    -------
    RecentKpiResult
        KPI result even when there are zero pnl trades.
    """
    df = load_recent_decisions(limit=None)

    if df.empty or profit_field not in df.columns:
        return compute_kpi_from_trades(
            df,
            profit_field=profit_field,
            starting_equity=starting_equity,
        )

    pnl = pd.to_numeric(df[profit_field], errors="coerce")
    mask = pnl.notna()
    trades = df.loc[mask].copy()

    if trades.empty:
        return compute_kpi_from_trades(
            trades,
            profit_field=profit_field,
            starting_equity=starting_equity,
        )

    if "ts_jst" in trades.columns:
        trades = trades.sort_values("ts_jst")

    if limit is not None:
        trades = trades.tail(limit)

    return compute_kpi_from_trades(
        trades,
        profit_field=profit_field,
        starting_equity=starting_equity,
    )



=== file: app/services/shap_service.py ===

# app/services/shap_service.py

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Sequence, Optional

import numpy as np
import pandas as pd
import shap
from loguru import logger


@dataclass
class ShapFeatureImpact:
    """1つの特徴量に対するSHAP影響度情報"""

    name: str
    mean_abs_shap: float
    rank: int


def _normalize_background_frame(
    X: pd.DataFrame,
    max_background: int = 2000,
    feature_names: Optional[Sequence[str]] = None,
) -> pd.DataFrame:
    """
    背景サンプル用に DataFrame を整理するヘルパ。
    - 列順を feature_names にそろえる（指定があれば）
    - 行数が多すぎる場合は max_background までサンプリング
    """
    if X is None or X.empty:
        raise ValueError("SHAP計算用の背景データが空です。")

    df = X.copy()

    if feature_names is not None:
        missing = set(feature_names) - set(df.columns)
        if missing:
            raise ValueError(
                f"SHAP背景データに不足している特徴量があります: {sorted(missing)}"
            )
        # 列順を揃える
        df = df.loc[:, list(feature_names)]

    if len(df) > max_background:
        logger.info(
            "SHAP背景サンプルを {orig} 行 → {sub} 行にサンプリングします。",
            orig=len(df),
            sub=max_background,
        )
        df = df.sample(n=max_background, random_state=42)

    return df


def compute_shap_feature_importance(
    model,
    X: pd.DataFrame,
    *,
    feature_names: Optional[Sequence[str]] = None,
    top_n: int = 20,
    max_background: int = 2000,
) -> List[ShapFeatureImpact]:
    """
    LightGBM などツリーモデルに対して SHAP (TreeExplainer) を使って
    グローバルな特徴量重要度（平均絶対SHAP値）を計算する。

    Parameters
    ----------
    model : 学習済みモデル（LightGBMClassifier想定だが、ツリーモデルなら概ねOK）
    X : 背景データの特徴量 DataFrame
    feature_names : 列順を明示したい場合の特徴量名リスト
    top_n : 上位何個まで返すか
    max_background : 背景サンプルの最大行数（重くなるのを防ぐフィルタ）

    Returns
    -------
    List[ShapFeatureImpact]
    """
    df_bg = _normalize_background_frame(
        X, max_background=max_background, feature_names=feature_names
    )

    logger.info(
        "SHAP計算開始: rows={rows}, cols={cols}, top_n={top_n}",
        rows=len(df_bg),
        cols=df_bg.shape[1],
        top_n=top_n,
    )

    # LightGBM/sklearn 互換のツリーモデルなら TreeExplainer が速い
    explainer = shap.TreeExplainer(model)

    shap_values = explainer.shap_values(df_bg)

    # shap_values の形はモデルによって違うので頑張って正規化する
    if isinstance(shap_values, list):
        # クラスごとの配列リスト（n_class, n_sample, n_feature）想定
        arr = np.stack(shap_values, axis=0)  # (n_class, n_sample, n_feature)
        mean_abs = np.mean(np.abs(arr), axis=(0, 1))  # feature 次元に集約
    else:
        arr = np.asarray(shap_values)  # (n_sample, n_feature)
        mean_abs = np.mean(np.abs(arr), axis=0)

    # 念のため shape を確認
    if mean_abs.shape[0] != df_bg.shape[1]:
        raise RuntimeError(
            f"SHAP重要度の次元数 ({mean_abs.shape[0]}) と特徴量数 "
            f"({df_bg.shape[1]}) が一致しません。"
        )

    features = list(df_bg.columns)
    # 大きい順にソート
    order = np.argsort(-mean_abs)
    items: List[ShapFeatureImpact] = []

    for idx, feat_idx in enumerate(order):
        if top_n is not None and idx >= top_n:
            break
        items.append(
            ShapFeatureImpact(
                name=features[int(feat_idx)],
                mean_abs_shap=float(mean_abs[int(feat_idx)]),
                rank=idx + 1,
            )
        )

    logger.info("SHAP計算完了: 返却件数={cnt}", cnt=len(items))
    return items


def shap_items_to_frame(items: List[ShapFeatureImpact]) -> pd.DataFrame:
    """
    ShapFeatureImpact のリストを pandas.DataFrame に変換するヘルパ。
    GUI や CLI で扱いやすくするため。
    """
    if not items:
        return pd.DataFrame(columns=["rank", "feature", "mean_abs_shap"])

    data = {
        "rank": [it.rank for it in items],
        "feature": [it.name for it in items],
        "mean_abs_shap": [it.mean_abs_shap for it in items],
    }
    return pd.DataFrame(data).sort_values("rank").reset_index(drop=True)



=== file: app/services/trade_service.py ===

from __future__ import annotations

import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, Iterable, Optional
import logging

from app.core import mt5_client
from app.core.config_loader import load_config
from app.services import trade_state
from app.services.circuit_breaker import CircuitBreaker
from app.services.event_store import EVENT_STORE
from core.config import cfg
from core.indicators import atr as _atr
from core.position_guard import PositionGuard
from core.utils.clock import now_jst

from app.core.mt5_client import MT5Client, TickSpec
from app.core.strategy_profile import StrategyProfile, get_profile
from core.risk import LotSizingResult
#from app.core.risk import LotSizingResult


@dataclass
class LotRule:
    base_equity_per_0p01: int = 10_000
    min_lot: float = 0.01
    max_lot: float = 1.00
    step: float = 0.01


def round_to_step(x: float, step: float) -> float:
    return (int(x / step)) * step


def calc_lot(equity: float, rule: LotRule = LotRule()) -> float:
    raw = (equity / rule.base_equity_per_0p01) * 0.01
    lot = max(rule.min_lot, min(rule.max_lot, round_to_step(raw, rule.step)))
    return float(f"{lot:.2f}")


def snapshot_account() -> Optional[dict]:
    if not mt5_client.initialize():
        return None
    try:
        return mt5_client.get_account_info()
    finally:
        mt5_client.shutdown()


class TradeService:
    """Facade that coordinates guards, circuit breaker, and decision helpers."""

    def __init__(
        self,
        mt5_client: MT5Client | None = None,
        profile: StrategyProfile | None = None,
    ) -> None:
        self._mt5 = mt5_client
        self._profile = profile or get_profile()
        self._last_lot_result: LotSizingResult | None = None
        self._logger = logging.getLogger(__name__)
        self.pos_guard = PositionGuard()
        self.cb = CircuitBreaker()
        self._reconcile_interval = 15
        self._desync_fix = True
        self._last_reconcile = 0.0
        self.state = trade_state.get_runtime()
        self.reload()

    def _compute_lot_for_entry(self, symbol: str, atr: float) -> LotSizingResult:
        """
        1トレードあたりのロット数を、現在の equity / ATR / tick 情報から計算する。

        atr: エントリー直前の足で計算した ATR 値（価格単位）
        """
        if atr is None or atr <= 0:
            # ATR が変な場合は、プロファイルのデフォルトロットにフォールバック
            default_lot = getattr(self._profile, "default_lot", None)
            if default_lot is None:
                default_lot = float(self._config.trade.default_lot)  # プロジェクト側の設定名に合わせてください

            self._logger.warning(
                "ATR が無効 (atr=%s) のため、default_lot=%.2f を使用します。",
                atr,
                default_lot,
            )
            return LotSizingResult(
                lot=default_lot,
                capped_by_max_risk=False,
                effective_risk_pct=None,
                note="fallback_default_lot_due_to_invalid_atr",
            )

        equity = self._mt5.get_equity()
        tick_spec: TickSpec = self._mt5.get_tick_spec(symbol)

        result = self._profile.compute_lot_size_from_atr(
            equity=equity,
            atr=atr,
            tick_size=tick_spec.tick_size,
            tick_value=tick_spec.tick_value,
        )

        self._logger.info(
            "ロット計算: equity=%.2f atr=%.5f tick_size=%.5f tick_value=%.5f -> lot=%.2f (capped=%s risk=%.3f)",
            equity,
            atr,
            tick_spec.tick_size,
            tick_spec.tick_value,
            result.lot,
            result.capped_by_max_risk,
            (result.effective_risk_pct or 0.0),
        )

        return result

    # ------------------------------------------------------------------ #
    # Configuration & helpers
    # ------------------------------------------------------------------ #
    def reload(self) -> None:
        conf = cfg
        g = conf.get("guard", {}) or {}
        cb_cfg = conf.get("circuit_breaker", {}) or {}

        max_positions = int(g.get("max_positions", conf.get("runtime", {}).get("max_positions", 1)))
        inflight_timeout = int(g.get("inflight_timeout_sec", 20))
        self.pos_guard = PositionGuard(max_positions=max_positions, inflight_timeout_sec=inflight_timeout)

        self.cb = CircuitBreaker(
            max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", conf.get("risk", {}).get("max_consecutive_losses", 5))),
            daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
            cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
        )
        self._reconcile_interval = int(g.get("reconcile_interval_sec", 15))
        self._desync_fix = bool(g.get("desync_fix", True))
        self._last_reconcile = 0.0
        self.state = trade_state.get_runtime()

    def _periodic_reconcile(self, symbol: str) -> None:
        now = time.time()
        if now - self._last_reconcile >= self._reconcile_interval:
            self._last_reconcile = now
            self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=self._desync_fix)

    # ------------------------------------------------------------------ #
    # Decisions & guards
    # ------------------------------------------------------------------ #
    def can_open(self, symbol: Optional[str]) -> bool:
        if symbol:
            self._periodic_reconcile(symbol)
        return self.pos_guard.can_open()

    def decide_entry_from_probs(self, p_buy: float, p_sell: float) -> Dict:
        conf = load_config()
        entry_cfg = conf.get("entry", {}) if isinstance(conf, dict) else {}
        th = float(entry_cfg.get("prob_threshold", entry_cfg.get("threshold_buy", 0.60)))
        edge = float(entry_cfg.get("entry_min_edge", entry_cfg.get("min_edge", 0.0)))
        bias = (entry_cfg.get("side_bias") or "auto").lower()

        pmax = p_buy if p_buy >= p_sell else p_sell
        p2nd = p_sell if p_buy >= p_sell else p_buy
        side = "BUY" if p_buy >= p_sell else "SELL"

        if pmax < th:
            return {"decision": "SKIP", "meta": "SKIP", "side": None, "reason": "ai_skip", "threshold": th}

        if (pmax - p2nd) < edge:
            return {
                "decision": "SKIP",
                "meta": "SKIP",
                "side": None,
                "reason": "ai_low_edge",
                "threshold": th,
                "edge": edge,
            }

        if p_buy == p_sell:
            if bias == "buy":
                side = "BUY"
            elif bias == "sell":
                side = "SELL"

        return {"decision": "ENTRY", "meta": side, "side": side, "threshold": th, "edge": edge}

    def decide_entry(self, p_buy: float, p_sell: float) -> Optional[str]:
        result = self.decide_entry_from_probs(p_buy, p_sell)
        return result["side"] if result.get("decision") == "ENTRY" else None

    def can_trade(self) -> bool:
        return self.cb.can_trade()


    def open_position(
        self,
        symbol: str,
        side: str,
        lot: float | None = None,
        atr: float | None = None,
        sl: float | None = None,
        tp: float | None = None,
        comment: str = "",
    ) -> None:
        """
        MT5 への発注。ATR を元に lot 計算を優先し、なければ ATR なしのフォールバック lot で送信。
        """
        if self._mt5 is None:
            raise RuntimeError("MT5 client is not configured on TradeService.")
        if self._profile is None:
            raise RuntimeError("Strategy profile is not configured on TradeService.")

        side_up = side.upper()
        if side_up not in {"BUY", "SELL"}:
            raise ValueError('side must be "buy" or "sell"')

        equity = float(self._mt5.get_equity())
        tick_spec: TickSpec = self._mt5.get_tick_spec(symbol)

        lot_result: LotSizingResult | None = None
        lot_val = lot

        # ATR が指定されていて lot が決まっていない場合は ATR ベースで計算
        if (lot_val is None or lot_val == 0) and atr is not None and atr > 0:
            lot_result = self._profile.compute_lot_size_from_atr(
                equity=equity,
                atr=atr,
                tick_size=tick_spec.tick_size,
                tick_value=tick_spec.tick_value,
            )
            raw_volume = getattr(lot_result, "lot", None)
            if raw_volume is None:
                raw_volume = getattr(lot_result, "volume", None)
            if raw_volume is not None:
                lot_val = float(raw_volume)

        # フォールバック: ATR が無い/0 のときはデフォルト lot を使う
        if lot_val is None or lot_val <= 0:
            default_lot = getattr(self._profile, "default_lot", None)
            if default_lot is None:
                default_lot = float((cfg.get("trade", {}) or {}).get("default_lot", 0.01))
            lot_val = float(default_lot)

        self._last_lot_result = lot_result

        self._mt5.order_send(
            symbol=symbol,
            order_type=side_up,
            lot=float(lot_val),
            sl=sl,
            tp=tp,
            comment=comment,
        )

    def mark_order_inflight(self, order_id: str) -> None:
        self.pos_guard.mark_inflight(order_id)

    def on_order_result(self, *, order_id: str, ok: bool, symbol: str) -> None:
        self.pos_guard.clear_inflight(order_id)
        if ok:
            self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=True)

    def on_order_success(self, *, ticket: Optional[int], side: str, symbol: str, price: Optional[float] = None) -> None:
        self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=True)
        runtime = self.state
        runtime.last_ticket = ticket
        runtime.last_side = side
        runtime.last_symbol = symbol
        EVENT_STORE.add(kind="ENTRY", symbol=symbol, side=side, price=price, sl=None, notes=f"ticket={ticket}")

    def on_broker_sync(self, symbol: Optional[str], fix: bool = True) -> None:
        self.pos_guard.reconcile_with_broker(symbol, desync_fix=fix)

    def record_trade_result(
        self,
        *,
        symbol: str,
        side: str,
        profit_jpy: float,
        info: Optional[dict[str, Any]] = None,
    ) -> None:
        resolved_symbol = symbol or self.state.last_symbol or "-"
        resolved_side = side or self.state.last_side
        notes = "settled"
        if info:
            if "notes" in info:
                notes = str(info["notes"])
            else:
                notes = str(info)
        EVENT_STORE.add(
            kind="CLOSE",
            symbol=resolved_symbol,
            side=resolved_side,
            profit_jpy=float(profit_jpy),
            notes=notes,
        )
        self.cb.on_trade_result(profit_jpy)


# ------------------------------------------------------------------ #
# Module-level helpers (backwards compatibility)
# ------------------------------------------------------------------ #
SERVICE = TradeService()


def execute_decision(
    decision: Dict[str, Any],
    *,
    symbol: Optional[str] = None,
    service: Optional[TradeService] = None,
) -> None:
    """
    Live 用のヘルパ:
    decision dict から TradeService.open_position(...) を呼び出す。

    期待する decision 形式の例::
        {
            "action": "ENTRY",
            "reason": "entry_ok",
            "signal": {
                "side": "BUY",
                "atr_for_lot": 0.0042,
                ...
            },
            "dec": {...},
        }

    - action != "ENTRY" の場合は何もしない
    - side や symbol が足りなければ何もしない
    - atr_for_lot はそのまま open_position(atr=...) に渡す
      （lot=None として渡し、TradeService 側で ATR ベースのロット計算を使う）
    """
    if not isinstance(decision, dict):
        # "SKIP" などの str が来た場合は黙って終了
        return

    action = decision.get("action")
    if action != "ENTRY":
        # エントリー以外（SKIP/BLOCKED/TRAIL_UPDATE）はここでは何もしない
        return

    signal = decision.get("signal") or {}
    if not isinstance(signal, dict):
        return

    side = signal.get("side")
    if not side:
        # どっちに建てるか不明なら何もしない
        return

    atr_for_lot = signal.get("atr_for_lot")

    svc = service or SERVICE

    # symbol が指定されていなければ設定ファイルから拾う（なければ何もしない）
    sym = symbol
    if not sym:
        try:
            from app.core.config_loader import load_config  # 遅延 import
            cfg = load_config()
            runtime_cfg = cfg.get("runtime", {}) if isinstance(cfg, dict) else {}
            sym = runtime_cfg.get("symbol")
        except Exception:
            sym = None

    if not sym:
        # シンボルが決まらない場合はエントリーしない
        return

    # lot=None + atr=atr_for_lot で呼び出し
    # open_position 側で StrategyProfile.compute_lot_size_from_atr を使って
    # ATR ベースの自動ロット計算が走る（既に実装済み）
    svc.open_position(
        symbol=str(sym),
        side=str(side),
        lot=None,
        atr=float(atr_for_lot) if atr_for_lot is not None else None,
    )


def can_open_new_position(symbol: Optional[str] = None) -> bool:
    settings = trade_state.get_settings()
    if not settings.trading_enabled:
        return False
    sym = symbol or load_config().get("runtime", {}).get("symbol")
    return SERVICE.can_open(sym)


def decide_entry(p_buy: float, p_sell: float) -> Optional[str]:
    return SERVICE.decide_entry(p_buy, p_sell)


def decide_entry_from_probs(p_buy: float, p_sell: float) -> dict:
    return SERVICE.decide_entry_from_probs(p_buy, p_sell)


def get_account_summary() -> dict[str, Any] | None:
    return mt5_client.get_account_info()


def build_exit_plan(symbol: str, ohlc_tail: Optional[Iterable[dict[str, Any]]]) -> dict[str, Any]:
    conf = load_config()
    ex_cfg = conf.get("exits", {}) if isinstance(conf, dict) else {}
    mode = (ex_cfg.get("mode") or "fixed").lower()

    if mode == "none":
        return {"mode": "none"}

    if mode == "fixed":
        fx = ex_cfg.get("fixed", {}) or {}
        return {
            "mode": "fixed",
            "tp_pips": float(fx.get("tp_pips", 10)),
            "sl_pips": float(fx.get("sl_pips", 10)),
        }

    if mode == "atr":
        ax = ex_cfg.get("atr", {}) or {}
        period = int(ax.get("period", 14))
        tp_mult = float(ax.get("tp_mult", 1.2))
        sl_mult = float(ax.get("sl_mult", 1.0))
        trailing = ax.get("trailing", {}) or {}

        highs: list[float] = []
        lows: list[float] = []
        closes: list[float] = []
        for row in ohlc_tail or []:
            h = row.get("h") or row.get("high")
            l = row.get("l") or row.get("low")
            c = row.get("c") or row.get("close")
            if h is not None:
                highs.append(float(h))
            if l is not None:
                lows.append(float(l))
            if c is not None:
                closes.append(float(c))

        atr_value = _atr(highs, lows, closes, period)
        return {
            "mode": "atr",
            "atr": atr_value,
            "tp_mult": tp_mult,
            "sl_mult": sl_mult,
            "trailing": {
                "enabled": bool(trailing.get("enabled", True)),
                "activate_atr_mult": float(trailing.get("activate_atr_mult", 0.5)),
                "step_atr_mult": float(trailing.get("step_atr_mult", 0.25)),
                "lock_be_atr_mult": float(trailing.get("lock_be_atr_mult", 0.3)),
                "hard_floor_pips": float(trailing.get("hard_floor_pips", 5)),
                "only_in_profit": bool(trailing.get("only_in_profit", True)),
                "max_layers": int(trailing.get("max_layers", 20)),
                "price_source": (trailing.get("price_source") or "mid").lower(),
            },
        }

    return {"mode": "fixed", "tp_pips": 10, "sl_pips": 10}


_trade_last_fill_ts: Optional[datetime] = None


def mark_filled_now() -> None:
    """Record the timestamp of the latest successful fill."""
    global _trade_last_fill_ts
    _trade_last_fill_ts = now_jst()


def post_fill_grace_active() -> bool:
    """Return True when the post-fill grace window is active."""
    if _trade_last_fill_ts is None:
        return False

    conf = load_config()
    runtime_cfg = conf.get("runtime", {}) if isinstance(conf, dict) else {}
    grace_sec = int((runtime_cfg or {}).get("post_fill_grace_sec", 0) or 0)
    if grace_sec <= 0:
        return False

    return (now_jst() - _trade_last_fill_ts) <= timedelta(seconds=grace_sec)


def mark_order_inflight(order_id: str) -> None:
    SERVICE.mark_order_inflight(order_id)


def on_order_result(order_id: str, ok: bool, symbol: str) -> None:
    SERVICE.on_order_result(order_id=order_id, ok=ok, symbol=symbol)


def reconcile_positions(symbol: Optional[str] = None, desync_fix: bool = True) -> None:
    SERVICE.on_broker_sync(symbol, fix=desync_fix)


def on_order_success(ticket: Optional[int], side: str, symbol: str, price: Optional[float] = None) -> None:
    SERVICE.on_order_success(ticket=ticket, side=side, symbol=symbol, price=price)


def record_trade_result(
    *,
    symbol: str,
    side: str,
    profit_jpy: float,
    info: Optional[dict[str, Any]] = None,
) -> None:
    SERVICE.record_trade_result(symbol=symbol, side=side, profit_jpy=profit_jpy, info=info)


def circuit_breaker_can_trade() -> bool:
    return SERVICE.can_trade()



=== file: app/services/trade_state.py ===

from dataclasses import dataclass, asdict
from typing import Any, Optional

@dataclass
class TradeSettings:
    trading_enabled: bool = False
    threshold_buy: float = 0.60
    threshold_sell: float = 0.60
    prob_threshold: float = 0.60
    side_bias: str = "auto"
    tp_pips: int = 15
    sl_pips: int = 10

# シングルトン的にプロセス内共有
_settings = TradeSettings()



@dataclass
class TradeRuntime:
    last_ticket: Optional[int] = None
    last_side: Optional[str] = None
    last_symbol: Optional[str] = None


_runtime_state = TradeRuntime()


def get_runtime() -> TradeRuntime:
    return _runtime_state


def update_runtime(**kwargs: Any) -> None:
    for k, v in kwargs.items():
        if hasattr(_runtime_state, k):
            setattr(_runtime_state, k, v)

def get_settings() -> TradeSettings:
    return _settings

def update(**kwargs: Any) -> None:
    for k, v in kwargs.items():
        if hasattr(_settings, k):
            setattr(_settings, k, v)

def as_dict() -> dict[str, Any]:
    return asdict(_settings)



=== file: app/services/trailing.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional


@dataclass
class TrailConfig:
    pip_size: float
    point: float
    atr: float
    activate_mult: float
    step_mult: float
    lock_be_mult: float
    hard_floor_pips: float
    only_in_profit: bool
    max_layers: int


@dataclass
class TrailState:
    side: str  # "BUY" or "SELL"
    entry: float
    activated: bool = False
    be_locked: bool = False
    layers: int = 0
    current_sl: Optional[float] = None


def _round_to_point(price: float, point: float) -> float:
    k = round(price / point)
    return k * point


def _pips(price_diff: float, pip_size: float) -> float:
    return price_diff / pip_size


def _price_from_pips(pips: float, pip_size: float) -> float:
    return pips * pip_size


def _profit_side(side: str, entry: float, price: float) -> float:
    return (price - entry) if side == "BUY" else (entry - price)


class AtrTrailer:
    def __init__(self, cfg: TrailConfig, state: TrailState):
        self.cfg = cfg
        self.st = state

    def activation_threshold(self) -> float:
        return self.cfg.atr * self.cfg.activate_mult

    def step_size(self) -> float:
        return self.cfg.atr * self.cfg.step_mult

    def be_threshold(self) -> float:
        return self.cfg.atr * self.cfg.lock_be_mult

    def suggest_sl(self, current_price: float) -> Optional[float]:
        profit = _profit_side(self.st.side, self.st.entry, current_price)
        if profit <= 0:
            return None

        if not self.st.activated and profit >= self.activation_threshold():
            self.st.activated = True
            sl = self._hard_floor_sl()
            applied = self._apply_if_better(sl)
            if applied is not None:
                return applied

        if not self.st.activated:
            return None

        if (not self.st.be_locked) and (profit >= self.be_threshold()):
            self.st.be_locked = True
            be_sl = self._breakeven_sl()
            hf_sl = self._hard_floor_sl()
            if self.st.side == "BUY":
                new_sl = max(be_sl, hf_sl)
            else:
                new_sl = min(be_sl, hf_sl)
            return self._apply_if_better(new_sl)

        step = self.step_size()
        if step <= 0:
            return None

        layers_should = int(profit // step)
        layers_should = min(layers_should, self.cfg.max_layers)
        if layers_should <= self.st.layers:
            return None

        move_layers = layers_should - self.st.layers
        new_sl = self._layer_sl(move_layers, current_price)
        self.st.layers = layers_should
        return self._apply_if_better(new_sl)

    def _hard_floor_sl(self) -> float:
        delta = _price_from_pips(self.cfg.hard_floor_pips, self.cfg.pip_size)
        if self.st.side == "BUY":
            sl = self.st.entry + delta
        else:
            sl = self.st.entry - delta
        return _round_to_point(sl, self.cfg.point)

    def _breakeven_sl(self) -> float:
        return _round_to_point(self.st.entry, self.cfg.point)

    def _layer_sl(self, move_layers: int, current_price: float) -> float:
        step = self.step_size() * move_layers
        if self.st.side == "BUY":
            sl = current_price - step
        else:
            sl = current_price + step
        sl = self._ensure_profit_side(sl)
        return _round_to_point(sl, self.cfg.point)

    def _ensure_profit_side(self, sl: float) -> float:
        hf = self._hard_floor_sl()
        current = self.st.current_sl
        if self.st.side == "BUY":
            sl = max(sl, hf)
            if self.cfg.only_in_profit and current is not None:
                sl = max(sl, current)
        else:
            sl = min(sl, hf)
            if self.cfg.only_in_profit and current is not None:
                sl = min(sl, current)
        return sl

    def _apply_if_better(self, new_sl: float) -> Optional[float]:
        cur = self.st.current_sl
        if cur is None:
            self.st.current_sl = new_sl
            return new_sl
        if self.st.side == "BUY" and new_sl > cur:
            self.st.current_sl = new_sl
            return new_sl
        if self.st.side == "SELL" and new_sl < cur:
            self.st.current_sl = new_sl
            return new_sl
        return None



=== file: app/services/trailing_hook.py ===

from __future__ import annotations

import logging
from typing import Any, Optional

from app.services.event_store import EVENT_STORE
from core.metrics import METRICS
from core.utils.runtime import is_live

logger = logging.getLogger(__name__)

_mt5svc: Optional[MT5Service] = None

try:
    from app.services.mt5_service import MT5Service
except Exception:
    _mt5svc = None
else:
    _mt5svc = MT5Service(max_retries=3, backoff_sec=0.3, min_change_points=2)


def apply_trailing_update(
    *,
    ticket: Optional[int],
    side: str,
    symbol: str,
    new_sl: float,
    reason: str = "trail",
) -> bool:
    """
    Apply trailing-stop loss updates (dry-run logs or live MT5 OrderModify).
    """

    METRICS.set({"trail_proposed_sl": new_sl, "trail_reason": reason})

    if not is_live():
        EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="DRYRUN")
        logger.info(f"[TRAIL][DRYRUN] side={side} symbol={symbol} new_sl={new_sl} reason={reason}")
        return True

    if _mt5svc is None or ticket is None:
        EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="SKIP")
        logger.warning(
            f"[TRAIL][LIVE][SKIP] ticket={ticket} svc={_mt5svc} side={side} symbol={symbol} new_sl={new_sl}"
        )
        return False

    ok, sent_sl, msg = _mt5svc.safe_order_modify_sl(
        ticket=ticket,
        side=side,
        symbol=symbol,
        desired_sl=new_sl,
        reason=reason,
    )
    sl_val = float(sent_sl) if sent_sl is not None else None

    if ok:
        EVENT_STORE.add(
            kind="TRAIL",
            symbol=symbol,
            side=side,
            sl=sl_val,
            reason=reason,
            notes="OK",
        )
        logger.info(f"[TRAIL][OK] ticket={ticket} side={side} sl={sent_sl} reason={reason} {msg}")
        if sl_val is not None:
            METRICS.set({"trail_current_sl": sl_val})
        METRICS.set({"trail_last_ok": True})
        return True

    if sl_val is not None:
        METRICS.set({"trail_current_sl": sl_val})

    EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="NG")
    logger.warning(f"[TRAIL][NG] ticket={ticket} side={side} desired={new_sl} reason={reason} {msg}")
    METRICS.set({"trail_last_ok": False})
    return False



=== file: app/strategies/__init__.py ===




=== file: app/strategies/ai_strategy.py ===

# app/strategies/ai_strategy.py
from __future__ import annotations
from pathlib import Path
import json
import pandas as pd
import numpy as np
import math
import joblib
import pickle
import binascii
from typing import Tuple, Dict, Any

PROJECT_ROOT = Path(__file__).resolve().parents[2]

def _load_model_generic(path_str: str):
    """
    1) joblib.load()
    2) pickle.load()
    3) LightGBM Booster（.txtやバイナリ）
       └ pklが失敗したら「同名.txt」にも自動フォールバック
    全滅時は先頭バイトをdumpして原因特定メッセージを返す。
    """
    p = Path(path_str)

    # 1) joblib
    try:
        import joblib
        m = joblib.load(p)
        print(f"[wfo] model loaded via joblib: {p.name}", flush=True)
        return m
    except Exception as e1:
        e1_msg = str(e1)

    # 2) pickle
    try:
        with open(p, "rb") as f:
            m = pickle.load(f)
        print(f"[wfo] model loaded via pickle: {p.name}", flush=True)
        return m
    except Exception as e2:
        e2_msg = str(e2)

    # 3) Booster
    def _try_booster(mp: Path):
        import lightgbm as lgb
        booster = lgb.Booster(model_file=str(mp))
        class _BoosterWrapper:
            def __init__(self, bst): self.bst = bst
            def predict_proba(self, X):
                prob1 = self.bst.predict(X)
                prob1 = np.asarray(prob1).reshape(-1)
                prob0 = 1.0 - prob1
                return np.vstack([prob0, prob1]).T
        print(f"[wfo] model loaded via booster: {mp.name}", flush=True)
        return _BoosterWrapper(booster)

    e3_msg = ""
    try:
        return _try_booster(p)
    except Exception as e3:
        e3_msg = str(e3)

    # pkl→txt フォールバック
    alt_txt = p.with_suffix(".txt")
    if alt_txt.exists():
        try:
            return _try_booster(alt_txt)
        except Exception as e4:
            e3_msg += f" | alt_txt='{alt_txt.name}': {e4}"

    # 先頭バイトを表示
    try:
        head = p.read_bytes()[:16]
        head_hex = binascii.hexlify(head).decode("ascii")
    except Exception:
        head_hex = "unreadable"

    raise RuntimeError(
        f"model load failed: joblib='{e1_msg}' | pickle='{e2_msg}' | booster='{e3_msg}' | head={head_hex}"
    )

    
# =====================================================
# active_model.json の読み込み
# =====================================================
def load_active_model() -> Tuple[str, str, float, Dict[str, Any]]:
    meta = PROJECT_ROOT / "active_model.json"
    if not meta.exists():
        raise FileNotFoundError(f"{meta} not found.")

    j = json.loads(meta.read_text(encoding="utf-8"))
    model_name = j.get("model_name", "").strip()
    threshold = float(j.get("best_threshold", 0.5))
    params = j.get("params", {}) or {}

    if model_name.startswith("builtin_"):
        return ("builtin", model_name, threshold, params)

    # 外部モデル
    pkl = PROJECT_ROOT / "models" / f"{model_name}.pkl"
    txt = PROJECT_ROOT / "models" / f"{model_name}.txt"

    # ここを txt 優先にする
    if txt.exists():
        return ("pickle", str(txt), threshold, params)
    if pkl.exists():
        return ("pickle", str(pkl), threshold, params)

    raise FileNotFoundError(f"Model file not found: {pkl} nor {txt}")


# =====================================================
# 特徴量レシピ
# =====================================================
def _rsi(x: pd.Series, period: int = 14) -> pd.Series:
    delta = x.diff()
    up = (delta.clip(lower=0)).rolling(period).mean()
    down = (-delta.clip(upper=0)).rolling(period).mean()
    rs = up / (down + 1e-12)
    return 100 - (100 / (1 + rs))

def _ema(x: pd.Series, span: int) -> pd.Series:
    return x.ewm(span=span, adjust=False).mean()

def _bbands(x: pd.Series, window: int = 20, n_sigma: float = 2.0):
    ma = x.rolling(window).mean()
    sd = x.rolling(window).std(ddof=0)
    upper = ma + n_sigma * sd
    lower = ma - n_sigma * sd
    return upper, lower

def _stoch(high: pd.Series, low: pd.Series, close: pd.Series, k_win: int = 14, d_win: int = 3):
    ll = low.rolling(k_win).min()
    hh = high.rolling(k_win).max()
    k = (close - ll) / (hh - ll + 1e-12) * 100
    d = k.rolling(d_win).mean()
    return k, d

def build_features_recipe(df: pd.DataFrame, name: str) -> pd.DataFrame:
    """
    内蔵レシピで特徴量を作成。time列は残します。
    name:
      - "ohlcv_tech_v1": 代表的なテクニカル群
    """
    out = df.copy()
    close = out["close"].astype(float)
    high = out["high"].astype(float)
    low  = out["low"].astype(float)

    if name == "ohlcv_tech_v1":
        out["ret1"]  = close.pct_change()
        out["ret5"]  = close.pct_change(5)
        out["ret20"] = close.pct_change(20)

        out["sma_10"] = close.rolling(10, min_periods=1).mean()
        out["sma_50"] = close.rolling(50, min_periods=1).mean()
        out["ema_20"] = _ema(close, 20)

        out["rsi_14"] = _rsi(close, 14)
        u,l = _bbands(close, 20, 2.0)
        out["bb_high_20_2"] = u
        out["bb_low_20_2"]  = l

        k,d = _stoch(high, low, close, 14, 3)
        out["stoch_k_14_3"] = k
        out["stoch_d_14_3"] = d

        # シンプルなATR風（高低差のEMA）
        tr = (high - low).abs()
        out["atr_14"] = tr.rolling(14).mean()

        # ボラ指標
        out["vol_pct_20"] = close.pct_change().rolling(20).std() * math.sqrt(20)

    else:
        raise ValueError(f"unknown feature recipe: {name}")

    out = out.dropna().reset_index(drop=True)
    return out

def build_features(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
    """外部モデル・内蔵双方で使う特徴量ビルドの統一入口"""
    recipe = (params or {}).get("feature_recipe", "ohlcv_tech_v1")
    feat = build_features_recipe(df, recipe)
    return feat

# =====================================================
# 予測とシグナル
# =====================================================
def _load_scaler_if_any(params: Dict[str, Any]):
    name = params.get("scaler_name")
    if not name:
        return None
    p = PROJECT_ROOT / "models" / "scalers" / f"{name}.pkl"
    if not p.exists():
        raise FileNotFoundError(f"Scaler not found: {p}")

    # まずヘッダを見て形式推定
    head_hex = None
    try:
        b = p.read_bytes()
        head_hex = binascii.hexlify(b[:8]).decode("ascii")
        # NumPy .npy の典型ヘッダは \x93NUMPY (= 93 4e 55 4d 50 59)
        is_npy = b[:6] == b"\x93NUMPY"
    except Exception:
        is_npy = False

    # 1) 標準pickle
    if not is_npy:
        try:
            with open(p, "rb") as f:
                scaler = pickle.load(f)
            typename = type(scaler).__name__
            print(f"[wfo] using scaler: {name} ({typename})", flush=True)
            return scaler
        except Exception as e1:
            # 2) joblib
            try:
                import joblib
                scaler = joblib.load(p)
                typename = type(scaler).__name__
                print(f"[wfo] using scaler: {name} ({typename}) via joblib", flush=True)
                return scaler
            except Exception as e2:
                # 3) 最後に NumPy ローダ
                try:
                    arr = np.load(str(p), allow_pickle=True)
                    # .npz の場合は最初のキーを取り出す
                    if hasattr(arr, "files"):
                        key0 = arr.files[0]
                        arr = arr[key0]
                    print(f"[wfo] using scaler: {name} (ndarray via np.load)", flush=True)
                    return arr
                except Exception as e3:
                    raise RuntimeError(
                        f"Scaler load failed: pickle='{e1}' | joblib='{e2}' | numpy='{e3}' | head={head_hex}"
                    )

    # ヘッダで .npy と判定された場合は最初から NumPy
    try:
        arr = np.load(str(p), allow_pickle=True)
        if hasattr(arr, "files"):
            key0 = arr.files[0]
            arr = arr[key0]
        print(f"[wfo] using scaler: {name} (ndarray via np.load)", flush=True)
        return arr
    except Exception as e:
        raise RuntimeError(f"Scaler load failed (npy fast-path): {e} | head={head_hex}")


def _ensure_feature_order(feat_df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
    cols = params.get("feature_cols")
    if cols:
        # 明示された列順に合わせ、不足はエラー、余分は削除
        missing = [c for c in cols if c not in feat_df.columns]
        if missing:
            raise ValueError(f"Missing features for model: {missing}")
        return feat_df.loc[:, cols]
    # 明示なしなら、price系を除いた派生列をアルファベット順で安定化
    skip = {"time","open","high","low","close","tick_volume","real_volume","spread"}
    cols = [c for c in feat_df.columns if c not in skip]
    return feat_df.loc[:, sorted(cols)]

def _sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def _predict_proba_generic(model, X: np.ndarray) -> np.ndarray:
    """
    LightGBM / XGBoost / Sklearn いずれでも陽性確率を返す汎用ハンドラー
    """
    import lightgbm as lgb

    # Xは常にndarray化して安全側に倒す
    if isinstance(X, pd.DataFrame):
        X = X.values
    elif not isinstance(X, np.ndarray):
        X = np.asarray(X)

    # LightGBM Booster（生boosterまたはラッパー）
    if isinstance(model, lgb.Booster):
        try:
            prob1 = model.predict(X)
        except Exception as e:
            print(f"[wfo] warn: Booster.predict failed: {e}  -> fallback sigmoid(raw score)", flush=True)
            raw = model.predict(X, raw_score=True)
            prob1 = 1.0 / (1.0 + np.exp(-raw))
        prob1 = np.asarray(prob1).reshape(-1)
        prob0 = 1.0 - prob1
        return np.vstack([prob0, prob1]).T

    # ラッパー（_BoosterWrapper）なら predict_proba を直接呼ぶ
    if hasattr(model, "bst") and hasattr(model.bst, "predict"):
        try:
            prob1 = model.bst.predict(X)
            prob1 = np.asarray(prob1).reshape(-1)
            prob0 = 1.0 - prob1
            return np.vstack([prob0, prob1]).T
        except Exception as e:
            print(f"[wfo] warn: BoosterWrapper predict failed: {e}", flush=True)

    # Sklearn系：predict_proba
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X)
        if proba.ndim == 1:
            return proba
        if proba.shape[1] == 2:
            return proba[:, 1]
        return proba[:, -1]

    # decision_function（SVMなど）
    if hasattr(model, "decision_function"):
        score = model.decision_function(X)
        return _sigmoid(score)

    # それ以外は predict() の結果を確率扱い
    pred = model.predict(X)
    return np.asarray(pred).astype(float)


def predict_signals(kind: str, payload, df_feat: pd.DataFrame, threshold: float = 0.0, params=None) -> pd.Series:
    """
    - builtin_sma: fast/slow のクロスで +1/-1 を返す
    - pickle: 外部モデルの proba > threshold → +1、それ以外 → -1（long_short）等、params['mode'] で制御
    """
    params = params or {}
    mode = params.get("mode", "long_short")  # long_only / short_only / long_flat / long_short

    if kind == "builtin":
        name = str(payload)
        if name == "builtin_sma":
            fast = int(params.get("fast", 10))
            slow = int(params.get("slow", 50))
            sma_fast = df_feat["close"].rolling(fast, min_periods=1).mean()
            sma_slow = df_feat["close"].rolling(slow, min_periods=1).mean()
            raw = np.where(sma_fast > sma_slow, 1, -1)
        else:
            raise ValueError(f"unknown builtin model: {name}")

    else:
        # 外部モデル推論
        model = _load_model_generic(payload)
        X = _ensure_feature_order(df_feat, params)
        scaler = _load_scaler_if_any(params)
        if scaler is not None:
            Xv = X.values
            try:
                # 標準のsklearn系（StandardScaler など）
                Xv = scaler.transform(Xv)
            except AttributeError:
                # dict / (mean, scale) / ndarray を許容
                if isinstance(scaler, dict) and ("mean" in scaler or "scale" in scaler):
                    mean = np.asarray(scaler.get("mean", np.zeros(Xv.shape[1])))
                    scale = np.asarray(scaler.get("scale", np.ones(Xv.shape[1])))
                    Xv = (Xv - mean) / (scale + 1e-12)
                elif isinstance(scaler, (tuple, list)) and len(scaler) >= 2:
                    mean = np.asarray(scaler[0])
                    scale = np.asarray(scaler[1])
                    Xv = (Xv - mean) / (scale + 1e-12)
                elif isinstance(scaler, np.ndarray):
                    # 平均だけが保存されているケース
                    mean = scaler
                    Xv = (Xv - mean)
                else:
                    # 想定外の型は未スケールで続行
                    print(f"[wfo] warn: unknown scaler type -> skip scaling ({type(scaler).__name__})", flush=True)
            # DataFrameに戻す（列名は維持）
            X = pd.DataFrame(Xv, index=X.index, columns=X.columns)
        ##
        proba = _predict_proba_generic(model, X.values)

        # 2次元(=確率2列)なら陽性側だけを採用
        if proba.ndim == 2 and proba.shape[1] == 2:
            proba = proba[:, 1]

        raw = (proba > float(threshold)).astype(int)  # 1 or 0
        ##
        # raw(1/0) → モードごとの最終signal
        if mode == "long_only" or mode == "long_flat":
            # 1=long, 0=flat
            sig = np.where(raw==1, 1, 0)
            return pd.Series(sig, index=df_feat.index, name="signal")
        elif mode == "short_only":
            # 1=flat, 0=short
            sig = np.where(raw==1, 0, -1)
            return pd.Series(sig, index=df_feat.index, name="signal")
        else:
            # long_short: 1=long, 0=short
            sig = np.where(raw==1, 1, -1)
            return pd.Series(sig, index=df_feat.index, name="signal")

    # builtinのモード切替
    if mode == "long_only" or mode == "long_flat":
        sig = np.where(raw==1, 1, 0)
    elif mode == "short_only":
        sig = np.where(raw==1, 0, -1)
    else:
        sig = raw
    return pd.Series(sig, index=df_feat.index, name="signal")

# =====================================================
# トレード生成（動的サイズ・ロング/ショート対応）
# =====================================================
def trades_from_signals(df_feat: pd.DataFrame, initial_capital: float, params=None) -> pd.DataFrame:
    """
    signal列（+1/-1/0）に基づいて IN/OUT/反転。
    ポジションサイズは equity * risk_pct / price を lot_step で丸め。
    PnL = (exit - entry) * dir * units - (spread+commission) * units
    """
    if "signal" not in df_feat.columns:
        raise ValueError("signal列が必要です。")

    p = params or {}
    spread_pips = float(p.get("spread_pips", 0.2))
    risk_pct = float(p.get("risk_pct", 0.01))
    lot_step = int(p.get("lot_step", 1000))
    min_units = int(p.get("min_units", lot_step))
    max_units = int(p.get("max_units", 200000))
    commission_per_unit = float(p.get("commission_per_unit", 0.0))

    spread_yen_per_unit = spread_pips * 0.01
    fee_yen_per_unit = commission_per_unit
    cost_yen_per_unit = spread_yen_per_unit + fee_yen_per_unit

    position = 0           # +1 / -1 / 0
    entry_price = None
    entry_time = None
    units = 0
    equity = float(initial_capital)

    trades = []
    idx_time = df_feat["time"].tolist()
    prices  = df_feat["close"].astype(float).tolist()
    sigs    = df_feat["signal"].astype(int).tolist()

    def _round_units(u: float) -> int:
        if u <= 0:
            return 0
        u = int(u // lot_step * lot_step)
        return max(min_units, min(u, max_units)) if u > 0 else 0

    # entry_time の行番号検索を高速化するための辞書
    time_to_index = {t:i for i,t in enumerate(idx_time)}

    for i in range(len(df_feat)):
        sig = sigs[i]
        price = prices[i]
        t = idx_time[i]

        if sig != position:
            if position != 0 and entry_price is not None and units > 0:
                pnl = (price - entry_price) * position * units - cost_yen_per_unit * units
                equity += pnl
                et_idx = time_to_index.get(entry_time, i)
                trades.append({
                    "entry_time": entry_time,
                    "exit_time": t,
                    "pnl": float(pnl),
                    "direction": "LONG" if position > 0 else "SHORT",
                    "units": int(units),
                    "entry_price": float(entry_price),
                    "exit_price": float(price),
                    "holding_bars": i - et_idx,
                    "holding_days": (pd.Timestamp(t) - pd.Timestamp(entry_time)).days,
                    "win": int(pnl > 0),
                    "equity_after": float(equity)
                })

            position = sig
            entry_price = price
            entry_time = t
            if position != 0:
                raw_units = (equity * risk_pct) / max(price, 1e-9)
                units = _round_units(raw_units)
            else:
                units = 0

    if position != 0 and entry_price is not None and units > 0:
        price = prices[-1]
        t = idx_time[-1]
        pnl = (price - entry_price) * position * units - cost_yen_per_unit * units
        equity += pnl
        et_idx = time_to_index.get(entry_time, len(df_feat)-1)
        trades.append({
            "entry_time": entry_time,
            "exit_time": t,
            "pnl": float(pnl),
            "direction": "LONG" if position > 0 else "SHORT",
            "units": int(units),
            "entry_price": float(entry_price),
            "exit_price": float(price),
            "holding_bars": (len(df_feat)-1) - et_idx,
            "holding_days": (pd.Timestamp(t) - pd.Timestamp(entry_time)).days,
            "win": int(pnl > 0),
            "equity_after": float(equity)
        })

    cols = [
        "entry_time","exit_time","pnl","direction","units",
        "entry_price","exit_price","holding_bars","holding_days","win","equity_after"
    ]
    return pd.DataFrame(trades, columns=cols)




=== file: apply_order.py ===

from pathlib import Path

# ここに「完全に復元したい mt5_client.py の中身」をそのまま埋め込む
NEW_MT5_CLIENT_SOURCE = """import time
import MetaTrader5 as MT5
import pandas as pd
from loguru import logger
from typing import Optional, Dict, Any


POSITION_COLUMNS = [
    "ticket",
    "time",
    "time_msc",
    "time_update",
    "time_update_msc",
    "symbol",
    "magic",
    "volume",
    "price_open",
    "sl",
    "tp",
    "price_current",
    "swap",
    "profit",
    "comment",
]


class MT5Client:
    \"\"\"MT5 発注・接続ラッパー（最小構成）\"\"\"

    def __init__(self, login: int, password: str, server: str, timeout: float = 5.0):
        self.login = login
        self.password = password
        self.server = server
        self.timeout = timeout
        self.connected = False
        self.logger = logger

    # ------------------------
    # 接続系
    # ------------------------
    def initialize(self) -> bool:
        \"\"\"MT5ターミナルの初期化（ログインは login_account()）\"\"\"
        logger.info("MT5 initialize() called...")

        if not MT5.initialize():
            err = MT5.last_error()
            logger.error(f"MT5 initialize() failed: {err}")
            self.connected = False
            return False

        logger.info("MT5 initialize() succeeded")
        self.connected = True
        return True

    def login_account(self) -> bool:
        \"\"\"設定されたログイン情報で MT5.login() を実行\"\"\"
        logger.info(
            f"MT5 login() called with login={self.login}, server={self.server}"
        )

        ok = MT5.login(
            self.login,
            password=self.password,
            server=self.server,
        )
        if not ok:
            err = MT5.last_error()
            logger.error(f"MT5 login() failed: {err}")
            return False

        logger.info("MT5 login() succeeded")
        return True

    def shutdown(self):
        \"\"\"MT5 をシャットダウン\"\"\"
        logger.info("MT5 shutdown()")
        MT5.shutdown()
        self.connected = False

    # ------------------------
    # 発注
    # ------------------------
    def order_send(
        self,
        symbol: str,
        order_type: str,
        lot: float,
        sl: Optional[float] = None,
        tp: Optional[float] = None,
        retries: int = 3,
    ) -> Optional[int]:
        \"\"\"成行発注（BUY / SELL）

        Parameters
        ----------
        symbol : str
        order_type : "BUY" or "SELL"
        lot : float
        sl, tp : Optional[float]
        retries : int

        Returns
        -------
        Optional[int]
            成功: チケット番号（int）
            失敗: None
        \"\"\"

        if order_type not in ("BUY", "SELL"):
            raise ValueError(f"order_type must be BUY/SELL: got {order_type}")

        # --- 1) シンボル情報をチェック ---
        info = MT5.symbol_info(symbol)
        if info is None:
            logger.error(f"[order_send] symbol_info({symbol}) が None。シンボルが存在しない可能性")
            return None

        if not info.visible:
            logger.info(f"[order_send] {symbol} が非表示なので symbol_select() します")
            if not MT5.symbol_select(symbol, True):
                logger.error(f"[order_send] symbol_select({symbol}, True) に失敗")
                return None

        # --- 2) 最新ティック ---
        tick = MT5.symbol_info_tick(symbol)
        if tick is None:
            logger.error(f"[order_send] symbol_info_tick({symbol}) が None。ティックが取得できない")
            return None

        # --- 3) 注文種別と価格 ---
        if order_type == "BUY":
            mt_type = MT5.ORDER_TYPE_BUY
            price = tick.ask
        else:
            mt_type = MT5.ORDER_TYPE_SELL
            price = tick.bid

        # --- 4) 注文リクエスト ---
        request: Dict[str, Any] = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": float(lot),
            "type": mt_type,
            "price": float(price),
            "sl": float(sl) if sl is not None else 0.0,
            "tp": float(tp) if tp is not None else 0.0,
            "magic": 123456,
            "comment": "fxbot_test_order",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        last_error: Optional[tuple[int, str]] = None

        # --- 5) リトライ付き order_send ---
        for attempt in range(1, retries + 1):
            logger.info(
                f"[order_send] Try {attempt}/{retries}: {order_type} {lot} lot @ {price} {symbol}"
            )

            result = MT5.order_send(request)

            if result is None:
                last_error = MT5.last_error()
                logger.error(f"[order_send] result is None, last_error={last_error}")

            else:
                logger.info(
                    "[order_send] retcode=%s, order=%s, deal=%s, comment=%s",
                    getattr(result, "retcode", None),
                    getattr(result, "order", None),
                    getattr(result, "deal", None),
                    getattr(result, "comment", None),
                )

                # 成行なので DONE = 成功
                if result.retcode == MT5.TRADE_RETCODE_DONE:
                    ticket = int(result.order or result.deal or 0)
                    if ticket > 0:
                        logger.info(f"[order_send] 成功: ticket={ticket}")
                        return ticket
                    else:
                        logger.warning(f"[order_send] DONE だが ticket が取得できない: {result}")

                else:
                    logger.warning(
                        f"[order_send] 失敗 retcode={result.retcode}。再試行する場合があります"
                    )

            if attempt < retries:
                time.sleep(1.0)

        logger.error(f"[order_send] 全 {retries} 回リトライしても失敗。last_error={last_error}")
        return None

    # ------------------------
    # 決済（クローズ）
    # ------------------------
    def close_position(self, ticket: int, symbol: str, retries: int = 3) -> bool:
        \"\"\"指定チケットの成行クローズ\"\"\"

        pos = MT5.positions_get(ticket=ticket)
        if not pos:
            logger.error(f"ticket={ticket} のポジションが存在しません")
            return False

        position = pos[0]
        lot = position.volume

        # position.type: 0=BUY, 1=SELL
        order_type = MT5.ORDER_TYPE_SELL if position.type == 0 else MT5.ORDER_TYPE_BUY

        # クローズ価格
        t = MT5.symbol_info_tick(symbol)
        if t is None:
            logger.error(f"[close_position] symbol_info_tick({symbol}) が None")
            return False

        price = t.bid if order_type == MT5.ORDER_TYPE_SELL else t.ask

        request = {
            "action": MT5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "volume": lot,
            "type": order_type,
            "position": ticket,
            "price": price,
            "magic": 123456,
            "comment": "fxbot_test_close",
            "type_time": MT5.ORDER_TIME_GTC,
            "type_filling": MT5.ORDER_FILLING_FOK,
        }

        for attempt in range(1, retries + 1):
            logger.info(f"[close_position] Try {attempt}: ticket={ticket}")
            result = MT5.order_send(request)

            if result and result.retcode == MT5.TRADE_RETCODE_DONE:
                logger.info(f"クローズ成功: ticket={ticket}")
                return True

            logger.error(
                f"retcode={result.retcode if result else None}, err={MT5.last_error()}"
            )
            time.sleep(1.0)

        logger.error("[close_position] 全リトライ失敗")
        return False

    # ------------------------
    # ポジション一覧
    # ------------------------
    def get_positions(self):
        try:
            pos = MT5.positions_get()
            if pos is None:
                self.logger.warning("positions_get() returned None")
                return []
            return list(pos)
        except Exception as exc:
            self.logger.exception(f"positions_get() failed: {exc}")
            return []

    def get_positions_by_symbol(self, symbol: str):
        rows = self.get_positions()
        out = [p for p in rows if getattr(p, "symbol", None) == symbol]
        self.logger.info(f"get_positions_by_symbol: {symbol} count={len(out)}")
        return out

    def get_positions_df(self, symbol: Optional[str] = None):
        rows = self.get_positions()
        if symbol:
            rows = [p for p in rows if getattr(p, "symbol", None) == symbol]

        if not rows:
            return pd.DataFrame(columns=POSITION_COLUMNS)

        data = []
        for p in rows:
            data.append(
                {
                    "ticket": p.ticket,
                    "time": p.time,
                    "time_msc": p.time_msc,
                    "time_update": p.time_update,
                    "time_update_msc": p.time_update_msc,
                    "symbol": p.symbol,
                    "magic": p.magic,
                    "volume": p.volume,
                    "price_open": p.price_open,
                    "sl": p.sl,
                    "tp": p.tp,
                    "price_current": p.price_current,
                    "swap": p.swap,
                    "profit": p.profit,
                    "comment": p.comment,
                }
            )

        return pd.DataFrame(data, columns=POSITION_COLUMNS)
"""


def main() -> None:
    """mt5_client.py を完全復元するワンショットスクリプト"""
    path = Path("app/core/mt5_client.py")
    path.write_text(NEW_MT5_CLIENT_SOURCE, encoding="utf-8")
    print("mt5_client.py を NEW_MT5_CLIENT_SOURCE で上書きしました。")


if __name__ == "__main__":
    main()



=== file: core/__init__.py ===




=== file: core/ai/__init__.py ===

# Core AI package initializer.



=== file: core/ai/calibration.py ===

from __future__ import annotations

import pickle
from dataclasses import dataclass
from typing import Any, Literal, Optional, Sequence, Tuple, cast

import numpy as np
from numpy.typing import NDArray

FloatArray = NDArray[np.float64]

from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import log_loss

CalibMethod = Literal["platt", "isotonic"]

# 互換API: 既存コード（core.ai.service 等）から呼ばれることを想定
def load_calibrator(path: str) -> Any:
    """
    互換ローダー。昔のコードが期待しているシグネチャに合わせる。
    core.ai.service から import される想定。
    """
    with open(path, "rb") as f:
        return pickle.load(f)

def apply_calibration(p: Sequence[float] | FloatArray, calib: Calibrator | None) -> FloatArray:
    """
    �݊��K�p�w���p�B�m���z�� p �ɑ΂��āAcalibrator ������� transform ��K�p�B
    calibrator �� None �Ȃ� p �����̂܂ܕԂ��B
    """
    data = cast(FloatArray, np.asarray(p, dtype=float))
    if calib is None:
        return data
    # Calibrator.transform �� 1�����m���x�N�g�����󂯎���� 1������Ԃ��݌v
    return calib.transform(data)


@dataclass
class Calibrator:
    method: CalibMethod
    model: LogisticRegression | IsotonicRegression

    def transform(self, p: FloatArray) -> FloatArray:
        # LogisticRegression は入力 x=logit(p)
        if self.method == "platt":
            eps = 1e-12
            x = np.clip(p, eps, 1 - eps)
            logit = np.log(x / (1 - x)).reshape(-1, 1)
            probs = self.model.predict_proba(logit)[:, 1]
            return cast(FloatArray, np.asarray(probs, dtype=float))
        elif self.method == "isotonic":
            transformed = self.model.transform(p)
            return cast(FloatArray, np.asarray(transformed, dtype=float))
        else:
            raise ValueError(f"unknown method: {self.method}")


def fit_platt(y_valid: np.ndarray, p_valid: np.ndarray) -> Calibrator:
    eps = 1e-12
    x = np.clip(p_valid, eps, 1 - eps)
    logit = np.log(x / (1 - x)).reshape(-1, 1)
    lr = LogisticRegression(solver="liblinear")
    lr.fit(logit, y_valid.astype(int))
    return Calibrator(method="platt", model=lr)


def fit_isotonic(y_valid: np.ndarray, p_valid: np.ndarray) -> Calibrator:
    ir = IsotonicRegression(out_of_bounds="clip")
    ir.fit(p_valid, y_valid.astype(float))
    return Calibrator(method="isotonic", model=ir)


def choose_best_calibrator(
    y_valid: np.ndarray, p_valid: np.ndarray
) -> Tuple[Optional[Calibrator], dict[str, Any]]:
    """
    Compare logloss across available calibration strategies and return the best.
    """
    scores: dict[str, float] = {}
    y_arr = np.asarray(y_valid, dtype=float)
    p_arr = cast(FloatArray, np.asarray(p_valid, dtype=float))
    base_ll = log_loss(y_arr, p_arr, labels=[0, 1])
    scores["none"] = base_ll

    # platt
    try:
        platt = fit_platt(y_arr, p_arr)
        p_platt = platt.transform(p_arr)
        scores["platt"] = log_loss(y_arr, p_platt, labels=[0, 1])
    except Exception:
        scores["platt"] = np.inf
        platt = None

    # isotonic
    try:
        iso = fit_isotonic(y_arr, p_arr)
        p_iso = iso.transform(p_arr)
        scores["isotonic"] = log_loss(y_arr, p_iso, labels=[0, 1])
    except Exception:
        scores["isotonic"] = np.inf
        iso = None

    best = min(scores, key=lambda k: scores[k])
    meta: dict[str, Any] = {"valid_logloss": scores, "baseline": base_ll, "selected": best}

    if best == "none":
        return None, meta
    if best == "platt" and platt is not None and scores["platt"] < base_ll:
        return platt, meta
    if best == "isotonic" and iso is not None and scores["isotonic"] < base_ll:
        return iso, meta
    return None, meta


def save_calibrator(path: str, calib: Calibrator) -> None:
    with open(path, "wb") as f:
        pickle.dump(calib, f)



=== file: core/ai/features.py ===

import numpy as np
import pandas as pd


def _rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = delta.clip(lower=0)
    down = -delta.clip(upper=0)
    roll_up = up.ewm(alpha=1 / period, adjust=False).mean()
    roll_down = down.ewm(alpha=1 / period, adjust=False).mean()
    rs = roll_up / (roll_down + 1e-12)
    rsi = 100 - (100 / (1 + rs))
    return rsi


def _true_range(high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:
    prev_close = close.shift(1)
    tr1 = high - low
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)


def _adx(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    up_arr = np.asarray(high.diff(), dtype=float)
    down_arr = np.asarray(-low.diff(), dtype=float)
    plus_dm = np.where((up_arr > down_arr) & (up_arr > 0), up_arr, 0.0)
    minus_dm = np.where((down_arr > up_arr) & (down_arr > 0), down_arr, 0.0)

    tr = _true_range(high, low, close).astype(float)
    tr_smooth = tr.rolling(period).sum()
    plus_series = pd.Series(plus_dm, index=df.index)
    minus_series = pd.Series(minus_dm, index=df.index)
    plus_di = 100 * plus_series.rolling(period).sum() / (tr_smooth + 1e-12)
    minus_di = 100 * minus_series.rolling(period).sum() / (tr_smooth + 1e-12)

    dx = ((plus_di - minus_di).abs() / ((plus_di + minus_di) + 1e-12)) * 100
    adx = dx.rolling(period).mean()
    return adx


def _bb_percent_b(close: pd.Series, period: int = 20, k: float = 2.0) -> pd.Series:
    ma = close.rolling(period).mean()
    sd = close.rolling(period).std(ddof=0)
    upper = ma + k * sd
    lower = ma - k * sd
    bbp = (close - lower) / ((upper - lower) + 1e-12)
    return bbp.clip(0, 1)


def _wick_body_ratios(df: pd.DataFrame) -> pd.DataFrame:
    open_, high, low, close = df["open"], df["high"], df["low"], df["close"]
    body = (close - open_).abs()
    upper_wick = (high - np.maximum(open_, close)).clip(lower=0)
    lower_wick = (np.minimum(open_, close) - low).clip(lower=0)
    total = (high - low).replace(0, np.nan)
    return pd.DataFrame(
        {
            "upper_wick_ratio": (upper_wick / total).fillna(0),
            "lower_wick_ratio": (lower_wick / total).fillna(0),
            "body_ratio": (body / total).fillna(0),
        }
    )


def _zscore(series: pd.Series, win: int = 20) -> pd.Series:
    mean = series.rolling(win).mean()
    sd = series.rolling(win).std(ddof=0)
    return (series - mean) / (sd + 1e-12)


def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    入力: df には ["open","high","low","close","volume"] が必須
    出力: 元のOHLCV + 追加特徴列（NaNはdropnaで最終的に落としてください）
    """
    out = df.copy()

    for p in (1, 3, 5, 10):
        out[f"ret_{p}"] = df["close"].pct_change(p)

    out["ret_std_10"] = out["ret_1"].rolling(10).std(ddof=0)
    out["ret_std_20"] = out["ret_1"].rolling(20).std(ddof=0)

    out["tr"] = _true_range(df["high"], df["low"], df["close"])
    out["atr_14"] = out["tr"].rolling(14).mean()

    out["rsi_14"] = _rsi(df["close"], 14)
    out["adx_14"] = _adx(df, 14)
    out["bbp_20"] = _bb_percent_b(df["close"], 20, 2.0)

    wick = _wick_body_ratios(df)
    out = pd.concat([out, wick], axis=1)

    out["vol_zscore_20"] = _zscore(df["volume"], 20)

    return out

# --- ここから追記（任意） ---
def build_Xy(df_raw: pd.DataFrame, label_col: str = "label") -> tuple[pd.DataFrame, pd.Series, list[str]]:
    """
    build_features() で作った特徴群から、学習用の X(DF) と y(Series) を返す。
    - label_col はあなたの既存ラベル列名に合わせて変更
    """
    feat_df = build_features(df_raw)

    # 学習に使う列をここで明示化（あなたの実際の特徴列に合わせて調整）
    feature_cols = [
        "open","high","low","close","volume",
        "ret_1","ret_3","ret_5","ret_10",
        "ret_std_10","ret_std_20",
        "tr","atr_14",
        "rsi_14","adx_14","bbp_20",
        "upper_wick_ratio","lower_wick_ratio","body_ratio",
        "vol_zscore_20",
    ]

    # ラベルがまだ無い場合は外で作ってから渡す想定
    if label_col not in feat_df.columns:
        raise ValueError(f"label col '{label_col}' not in DataFrame. 先にラベル作成を行ってください。")

    # 欠損落とし＆インデックス同期
    X_df = feat_df[feature_cols].copy()
    y_ser = feat_df[label_col].copy()
    mask = ~X_df.isna().any(axis=1)
    X_df = X_df.loc[mask]
    y_ser = y_ser.loc[X_df.index]

    return X_df, y_ser, feature_cols
# --- 追記ここまで ---



=== file: core/ai/loader.py ===

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, cast, Sequence, Union

import json
import joblib
import numpy as np
import pandas as pd
from numpy.typing import NDArray

FloatArray = NDArray[np.float64]

ArrayLike = Union[np.ndarray, Sequence[float]]

class ModelWrapper:
    """dict/ラッパーの多段ネストを再帰で“ほどき”、predict_proba/predictを安定提供する薄いラッパー。"""

    def __init__(self, obj_or_path: Union[str, Path, Any]) -> None:
        if isinstance(obj_or_path, (str, Path)):
            loaded = joblib.load(str(obj_or_path))
        else:
            loaded = obj_or_path

        self.base_model = self._unwrap(loaded, depth=0)
        self.classes_ = getattr(self.base_model, "classes_", None)
        self.model_name = getattr(self.base_model, "model_name", None) or \
                          getattr(self.base_model, "__class__", type("X",(object,),{})).__name__
        if isinstance(self.base_model, dict):
            try:
                print(f"[ModelWrapper][warn] still dict after unwrap. keys={list(self.base_model.keys())[:10]}")
            except Exception:
                print("[ModelWrapper][warn] still dict after unwrap (keys unavailable).")

    def _unwrap(self, obj: Any, depth: int = 0) -> Any:
        if depth > 5:
            return obj
        if hasattr(obj, "predict_proba") or hasattr(obj, "predict") or hasattr(obj, "decision_function"):
            return obj
        if isinstance(obj, dict):
            for key in ("model", "estimator", "clf", "base_model", "wrapped", "inner", "object"):
                if key in obj and obj[key] is not None:
                    out = self._unwrap(obj[key], depth + 1)
                    if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                        return out
            for v in obj.values():
                out = self._unwrap(v, depth + 1)
                if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                    return out
        if isinstance(obj, (list, tuple)):
            for v in obj:
                out = self._unwrap(v, depth + 1)
                if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                    return out
        return obj

    def predict_proba(self, X: ArrayLike) -> np.ndarray:
        X_arr = np.asarray(X)
        return self.base_model.predict_proba(X_arr)

    def predict(self, X: ArrayLike) -> np.ndarray:
        X_arr = np.asarray(X)
        if hasattr(self.base_model, "predict"):
            return self.base_model.predict(X_arr)
        if hasattr(self.base_model, "decision_function"):
            scores = np.asarray(self.base_model.decision_function(X_arr), dtype=float)
            probs = 1.0 / (1.0 + np.exp(-scores))
            return (probs >= 0.5).astype(int)
        raise AttributeError("The underlying model has neither predict nor decision_function.")

def _load_pickle_or_joblib(path: str) -> Any:
    return joblib.load(path)

def _apply_calibration(calibrator: Any, p1: FloatArray) -> FloatArray:
    data = np.asarray(p1, dtype=float)
    if calibrator is None:
        return data
    if hasattr(calibrator, "transform"):
        transformed = calibrator.transform(data)
        return np.asarray(transformed, dtype=float)
    if hasattr(calibrator, "predict_proba"):
        proba = calibrator.predict_proba(data)
        return np.asarray(proba, dtype=float)
    return data

# ------------------------------------------------------------
# モデルバンドル（必要なら使う。未使用なら残しても害なし）
# ------------------------------------------------------------
@dataclass
class LGBBundle:
    name: str
    version: str
    clf: object            # predict_proba を持つ推論器
    feature_order: List[str]
    ready: bool = True

# ------------------------------------------------------------
# 校正付きラッパ（Booster / clf 両対応版）
# ------------------------------------------------------------
class _CalibratedWrapper:
    """
    base_model:
        - 通常: sklearn 系の clf (predict_proba を持つ)
        - 古いモデル: LightGBM Booster (predict のみ)
    calibrator:
        - None なら何もしない
        - transform / predict_proba を持っていればそれを適用
    """

    def __init__(self, base_model: Any, calibrator: Any, model_name: str = "(unknown)") -> None:
        self.base_model = base_model
        self.calibrator = calibrator
        self.model_name = model_name
        self.calibrator_name = getattr(calibrator, "method", "none") if calibrator else "none"
        # AISvc 側から埋められる（feature_order）
        self.expected_features: Optional[list[str]] = None

    def __getattr__(self, item: str) -> Any:
        # その他の属性は元モデルに委譲
        return getattr(self.base_model, item)

    def _raw_p1_from_model(self, X_arr: np.ndarray) -> FloatArray:
        """
        モデルから「クラス1の確率 or スコア」を 1次元配列で取り出す。

        - predict_proba があれば、それを優先
        - なければ predict をそのまま確率扱い（Booster想定）
        """
        # 1) 通常パス: predict_proba
        if hasattr(self.base_model, "predict_proba"):
            raw = self.base_model.predict_proba(X_arr)
            raw = np.asarray(raw, dtype=float)

            # (n, ) or (n, 1) or (n, 2) などを全部 1次元に落とす
            if raw.ndim == 1:
                p1 = raw.reshape(-1)
            elif raw.ndim == 2:
                if raw.shape[1] == 1:
                    p1 = raw[:, 0]
                else:
                    # 2列以上ある場合は「最後の列」を陽線クラスとして扱う
                    p1 = raw[:, -1]
            else:
                raise ValueError(f"unexpected predict_proba shape: {raw.shape}")
            return p1

        # 2) フォールバック: predict のみ
        if hasattr(self.base_model, "predict"):
            raw = self.base_model.predict(X_arr)
            p1 = np.asarray(raw, dtype=float).reshape(-1)
            return p1

        raise AttributeError("base_model has neither predict_proba nor predict")

    def predict_proba(self, X: Any) -> FloatArray:
        X_arr = np.asarray(X, dtype=float)

        # モデルから生の p1 を取得
        p1 = self._raw_p1_from_model(X_arr)

        # 校正器があれば適用（vector → (n,1) → (n,1) → vector）
        p1_2d = p1.reshape(-1, 1)
        p1_cal = _apply_calibration(self.calibrator, p1_2d).reshape(-1)

        # 数値安全のためクリップ
        p1_cal = np.clip(p1_cal, 1e-6, 1.0 - 1e-6)
        p0 = 1.0 - p1_cal

        # shape: (n, 2) [クラス0, クラス1]
        return np.stack([p0, p1_cal], axis=1)

def _read_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as fh:
        data = json.load(fh)
    if isinstance(data, dict):
        return cast(Dict[str, Any], data)
    raise ValueError(f"JSON at {path} is not an object")

# ------------------------------------------------------------
# active_model.json / 校正ファイルの解決
# ------------------------------------------------------------
def _load_active_meta() -> Dict[str, Any]:
    meta_path = Path("models") / "active_model.json"
    if meta_path.exists():
        try:
            return _read_json(str(meta_path))
        except Exception as exc:
            print(f"[core.ai.loader][warn] active_model.json read failed: {exc}")
    return {}

def _resolve_calib_path(calib_path: str | None) -> Optional[str]:
    return calib_path if calib_path else None

def _maybe_load_calibrator_from_meta(meta: Dict[str, Any]) -> Any:
    cpath = _resolve_calib_path(meta.get("calibrator_path"))
    if cpath:
        try:
            return _load_pickle_or_joblib(cpath)
        except Exception as exc:
            print(f"[core.ai.loader][warn] calibrator load failed: {exc}")
    return None

# ------------------------------------------------------------
# パブリックAPI：モデルローダ
# ------------------------------------------------------------

def load_lgb_clf(model_path: str | None = None, *, meta_path: str | None = None) -> Any:
    model_file = model_path or "models/LightGBM_clf.pkl"
    base_model = _load_pickle_or_joblib(model_file)

    # ★ここを追加：保存物が dict/ラッパでも推定器本体を取り出す
    try:
        # ModelWrapper を一時的に使って「中身の推定器」を取り出す
        _tmp = ModelWrapper(base_model)
        base_model = _tmp.base_model  # predict_proba / predict を持つ本体
    except Exception:
        # 失敗してもそのまま進める（あとで _CalibratedWrapper でまた拾う）
        pass

    meta: Dict[str, Any] = {}
    if meta_path and Path(meta_path).is_file():
        try:
            meta = _read_json(meta_path)
        except Exception as exc:
            print(f"[core.ai.loader][warn] meta read failed: {exc}")
    else:
        try:
            meta = _load_active_meta()
        except Exception as exc:
            print(f"[core.ai.loader][warn] active meta read failed: {exc}")
            meta = {}

    # ★追加：active_model.json の feature_order / features を不足分としてマージ
    try:
        active_meta = _load_active_meta()
    except Exception as exc:
        print(f"[core.ai.loader][warn] active meta merge failed: {exc}")
        active_meta = {}

    for key in ("feature_order", "features"):
        if not meta.get(key) and active_meta.get(key):
            meta[key] = active_meta[key]

    # 旧仕様 {"calibration": {...}} → 新仕様 calibrator_path へ
    if "calibration" in meta and "calibrator_path" not in meta:
        calib_meta = meta.get("calibration") or {}
        meta["calibrator_path"] = calib_meta.get("path")

    calibrator = _maybe_load_calibrator_from_meta(meta or {})
    wrapper = _CalibratedWrapper(base_model, calibrator, model_name=Path(model_file).name)

    expected = meta.get("feature_order") or meta.get("features")
    if expected:
        wrapper.expected_features = list(expected)

    return wrapper


def build_feature_vector(features: dict, order: list[str]) -> pd.DataFrame:
    row = [features.get(k, 0.0) for k in order]
    return pd.DataFrame([row], columns=order)



=== file: core/ai/service.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional

import glob
import json
import os
from pathlib import Path

import numpy as np
import pandas as pd
from loguru import logger

from core.ai.loader import ModelWrapper, load_lgb_clf


def _read_active_model_path(default: str = "models/LightGBM_clf.pkl") -> str:
    """Resolve model path from active_model.json (target_path -> source_path -> default)."""
    meta_path = Path("models") / "active_model.json"
    if meta_path.exists():
        try:
            with meta_path.open("r", encoding="utf-8") as fh:
                meta = json.load(fh)
            return meta.get("target_path") or meta.get("source_path") or default
        except Exception as exc:
            logger.warning(f"failed to read active_model.json: {exc}")
    return default


def _as_2d_frame(X: Any) -> pd.DataFrame:
    """任意XをLightGBMに渡せる2D DataFrameに整形する。"""
    if isinstance(X, pd.DataFrame):
        return X.copy()
    if isinstance(X, dict):
        return pd.DataFrame([X])
    if isinstance(X, (list, tuple, np.ndarray)):
        arr = np.asarray(X, dtype=float)
        if arr.ndim == 1:
            arr = arr.reshape(1, -1)
        return pd.DataFrame(arr)
    return pd.DataFrame([[X]])

class _ProbOut:
    __slots__ = ("p_buy", "p_sell", "p_skip", "meta", "model_name", "version", "features_hash")

    def __init__(
        self,
        p_buy: float,
        p_sell: float,
        *,
        model_name: str = "(unknown)",
        version: str = "na",
        features_hash: str = "",
    ) -> None:
        buy_val = float(p_buy)
        sell_val = float(p_sell)
        self.p_buy = buy_val
        self.p_sell = sell_val
        self.p_skip = float(min(buy_val, sell_val))
        self.meta = "BUY" if buy_val >= sell_val else "SELL"
        self.model_name = model_name
        self.version = version
        self.features_hash = features_hash

    def model_dump(self) -> Dict[str, float | str]:
        return {
            "model_name": self.model_name,
            "version": self.version,
            "features_hash": self.features_hash,
            "p_buy": self.p_buy,
            "p_sell": self.p_sell,
            "p_skip": self.p_skip,
            "meta": self.meta,
        }


ProbOut = _ProbOut  # backward compatibility for external imports

__all__ = ["AISvc", "ProbOut", "_as_2d_frame", "_read_active_model_path"]


@dataclass
class AISvc:
    threshold: float = 0.52
    model: Optional[object] = None
    model_name: str = "unknown"
    calibrator_name: str = "none"
    is_dummy: bool = False
    expected_features: Optional[list[str]] = None

    def __post_init__(self) -> None:
        self._initialize_model()

    def _resolve_model_path(self) -> str:
        """
        優先順：
          1) self.model_path（あれば）
          2) runtime/active_model.json の "path"
          3) 既定: models/LightGBM_clf.pkl
        """
        if getattr(self, "model_path", None):
            return str(self.model_path)

        try:
            p = Path("runtime/active_model.json")
            if p.exists():
                d = json.loads(p.read_text(encoding="utf-8"))
                m = d.get("path")
                if m:
                    return str(m)
        except Exception:
            pass

        return "models/LightGBM_clf.pkl"

    def _initialize_model(self, model_path: str | None = None):
        from core.ai.loader import ModelWrapper
        from pathlib import Path

        model_path = model_path or self._resolve_model_path()

        # 何が返っても最終的に推定器へ到達できるよう ModelWrapper で統一
        bundle = load_lgb_clf(model_path)
        self.model = ModelWrapper(bundle)

        # 表示名
        self.model_name = getattr(self.model, "model_name", None) or Path(model_path).name or "(unknown)"
        print(f"[AISvc] loaded model: {self.model_name}")

        # 期待特徴量のロード（既存メソッド）
        self._load_expected_features()

    def _load_expected_features(self) -> None:
        """最新レポートの features を expected_features として保持"""
        try:
            meta_path = os.path.join("models", "active_model.json")
            report = None
            if os.path.isfile(meta_path):
                with open(meta_path, encoding="utf-8") as f:
                    meta = json.load(f)
                report = meta.get("best_threshold_source_report")
            if not report or not os.path.isfile(report or ""):
                candidates = sorted(glob.glob(os.path.join("logs", "retrain", "report_*.json")))
                if candidates:
                    report = candidates[-1]
            if report and os.path.isfile(report):
                with open(report, encoding="utf-8") as f:
                    data = json.load(f)
                feats = data.get("features")
                if isinstance(feats, list) and feats:
                    self.expected_features = feats
                    print(f"[AISvc] expected_features loaded ({len(feats)} cols) from {report}")
                    return
        except Exception as exc:
            print(f"[AISvc][warn] expected_features load failed: {exc}")
        self.expected_features = None

    def predict(self, X: Any) -> _ProbOut:
        if self.model is None:
            return _ProbOut(0.5, 0.5, model_name=self.model_name, version="na", features_hash="")

        df = _as_2d_frame(X)
        if self.expected_features:
            for col in self.expected_features:
                if col not in df.columns:
                    df[col] = 0.0
            df = df[self.expected_features]

        values = df.to_numpy(dtype=float, copy=False)
        if hasattr(self.model, "predict_proba"):
            probs = np.asarray(self.model.predict_proba(values), dtype=float)
        elif hasattr(self.model, "predict"):
            preds = np.asarray(self.model.predict(values), dtype=float).reshape(-1, 1)
            probs = np.column_stack([1.0 - preds, preds])
        else:
            probs = np.zeros((len(values), 2), dtype=float)

        features_hash = ""
        try:
            if not df.empty:
                features_hash = str(hash(tuple(df.iloc[0].astype(float).values.tolist())))
        except Exception:
            features_hash = ""

        if probs.ndim == 2:
            p_buy = probs[0, 1]
            p_sell = probs[0, 0]
        else:
            p_buy = float(probs)
            p_sell = 1.0 - p_buy

        version = getattr(getattr(self.model, "clf", self.model), "version", "na")
        return _ProbOut(
            p_buy,
            p_sell,
            model_name=self.model_name,
            version=str(version),
            features_hash=features_hash,
        )



=== file: core/config.py ===

from __future__ import annotations

from functools import lru_cache
from typing import Any, Dict

from app.core.config_loader import load_config


@lru_cache(maxsize=1)
def _load() -> Dict[str, Any]:
    return load_config()


cfg: Dict[str, Any] = _load()


def reload() -> Dict[str, Any]:
    """
    Reload configuration from disk and update cached reference.
    """
    global cfg
    cfg = load_config()
    _load.cache_clear()
    return cfg



=== file: core/indicators.py ===

import math
from typing import Sequence


def true_range(h: float, l: float, prev_close: float) -> float:
    """Return Wilder's true range for a single bar."""
    return max(h - l, abs(h - prev_close), abs(prev_close - l))


def atr(highs: Sequence[float], lows: Sequence[float], closes: Sequence[float], period: int) -> float:
    """Compute a simple average true range over the trailing window."""
    n = len(closes)
    if n < period + 1:
        return math.nan

    trs = []
    for i in range(n - period, n):
        trs.append(true_range(highs[i], lows[i], closes[i - 1]))
    return sum(trs) / len(trs)



=== file: core/metrics.py ===

# core/metrics.py
from __future__ import annotations
from dataclasses import dataclass, field
from threading import Lock
from typing import Any, Dict
import time, os, json, tempfile

RUNTIME_DIR = os.path.join(os.getcwd(), "runtime")
METRICS_JSON = os.path.join(RUNTIME_DIR, "metrics.json")

def _atomic_write_json(path: str, obj: dict):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    fd, tmp = tempfile.mkstemp(prefix="metrics_", suffix=".json", dir=os.path.dirname(path))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, separators=(",", ":"))
        os.replace(tmp, path)
    except Exception:
        try:
            os.remove(tmp)
        except Exception:
            pass

@dataclass
class _MetricsStore:
    _lock: Lock = field(default_factory=Lock)
    _kv: Dict[str, Any] = field(default_factory=dict)

    def set(self, **kwargs):
        with self._lock:
            self._kv.update(kwargs)
            self._kv["ts"] = time.time()
            _atomic_write_json(METRICS_JSON, self._kv)

    def inc(self, key: str, by: int = 1):
        with self._lock:
            self._kv[key] = int(self._kv.get(key, 0)) + by
            self._kv["ts"] = time.time()
            _atomic_write_json(METRICS_JSON, self._kv)

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._kv)

METRICS = _MetricsStore()



=== file: core/metrics/__init__.py ===

# core/metrics/__init__.py
from .registry import METRICS_JSON, METRICS



=== file: core/metrics/fi_extractor.py ===

from __future__ import annotations
from typing import Any, Dict, Iterable, List, Optional, cast

import numpy as np
import pandas as pd
from numpy.typing import NDArray


def _norm_importance(vals: Iterable[float]) -> List[float]:
    arr: NDArray[np.float64] = np.asarray(list(vals), dtype=float)
    if arr.size == 0:
        return []
    total = float(arr.sum())
    if total == 0.0:
        zero_dist = (np.ones_like(arr, dtype=float) / len(arr) * 100.0).astype(float).tolist()
        return cast(List[float], zero_dist)
    normalized = (arr / total * 100.0).astype(float).tolist()
    return cast(List[float], normalized)


def _lgbm_importance(model: Any, method: str = "gain") -> Optional[pd.DataFrame]:
    # LightGBM sklearn API or Booster を想定
    # method: "gain" or "split"
    booster = None
    feature_names = None
    if hasattr(model, "booster_"):
        booster = model.booster_
    elif hasattr(model, "booster"):
        booster = model.booster()
    elif hasattr(model, "model"):
        booster = getattr(model.model, "booster_", None)

    if booster is None:
        # fallback: sklearn APIの feature_importances_（=split相当が多い）
        if hasattr(model, "feature_importances_") and hasattr(model, "feature_name_"):
            vals = list(map(float, getattr(model, "feature_importances_")))
            feature_names = list(getattr(model, "feature_name_"))
            imp = _norm_importance(vals)
            return pd.DataFrame({"feature": feature_names, "importance": imp})
        return None

    # Booster から
    try:
        vals = booster.feature_importance(importance_type=method)
        feature_names = booster.feature_name()
        imp = _norm_importance(vals)
        return pd.DataFrame({"feature": feature_names, "importance": imp})
    except Exception:
        return None


def _xgb_importance(model: Any, method: str = "gain") -> Optional[pd.DataFrame]:
    # XGBoost：booster.get_score(importance_type=method) -> dict {feat: score}
    booster = None
    if hasattr(model, "get_booster"):
        booster = model.get_booster()
    elif hasattr(model, "booster"):
        booster = model.booster
    if booster is None:
        # fallback: sklearn APIの feature_importances_（仕様上gain相当ではない時もある）
        if hasattr(model, "feature_importances_"):
            vals = list(map(float, getattr(model, "feature_importances_")))
            # XGBのsklearnラッパは feature_names_in_ を持つ
            feats = getattr(model, "feature_names_in_", None)
            if feats is None:
                feats = [f"f{i}" for i in range(len(vals))]
            imp = _norm_importance(vals)
            return pd.DataFrame({"feature": feats, "importance": imp})
        return None

    try:
        score: Dict[str, float] = booster.get_score(importance_type=method)
        if not score:
            return None
        feats = list(score.keys())
        vals = list(score.values())
        imp = _norm_importance(vals)
        return pd.DataFrame({"feature": feats, "importance": imp})
    except Exception:
        return None


def extract_feature_importance(
    models: Dict[str, Any],
    method: str = "gain",
    top_n: int = 30,
) -> pd.DataFrame:
    """
    models: {"lgbm_cls": model_obj, "xgb_cls": model_obj, ...}
    method: "gain" | "split"（LightGBM/XGBoost両対応。XGBは"weight"=split相当）
    top_n : 上位Nのみ返す（モデルごとにtop_nを抽出→縦結合）

    return columns: ["feature","importance","model","method"]
    importanceは各モデル内で正規化（合計=100）後、top_n抽出。
    """
    frames: List[pd.DataFrame] = []
    for name, m in models.items():
        df = None
        # 判別ざっくり：文字列に"lightgbm" or "xgboost"が含まれるか、属性で判定
        module_name = type(m).__module__.lower()
        if "lightgbm" in module_name:
            df = _lgbm_importance(m, method=method)
        elif "xgboost" in module_name:
            # XGBoostの"split"相当はimportance_type="weight"
            xgb_method = method
            if method == "split":
                xgb_method = "weight"
            df = _xgb_importance(m, method=xgb_method)
        else:
            # 最後の手段：feature_importances_があれば使う
            if hasattr(m, "feature_importances_"):
                vals = list(map(float, getattr(m, "feature_importances_")))
                feats = getattr(m, "feature_names_in_", None)
                if feats is None:
                    feats = [f"f{i}" for i in range(len(vals))]
                imp = _norm_importance(vals)
                df = pd.DataFrame({"feature": feats, "importance": imp})

        if df is None or df.empty:
            continue

        df = df.sort_values("importance", ascending=False).head(top_n).copy()
        df["model"] = name
        df["method"] = method
        frames.append(df)

    if not frames:
        return pd.DataFrame(columns=["feature", "importance", "model", "method"])

    out = pd.concat(frames, axis=0, ignore_index=True)
    # 表示安定化のためimportanceを丸める
    out["importance"] = out["importance"].round(2)
    return out.sort_values(["model", "importance"], ascending=[True, False]).reset_index(drop=True)



=== file: core/metrics/registry.py ===

# core/metrics/registry.py
from __future__ import annotations
from pathlib import Path
import json
import threading
from typing import Any, Dict

# プロジェクトルート .../fxbot
ROOT = Path(__file__).resolve().parents[2]

# ランタイム出力: .../fxbot/runtime/metrics.json
_RUNTIME_DIR = ROOT / "runtime"
_RUNTIME_DIR.mkdir(parents=True, exist_ok=True)
METRICS_JSON = str(_RUNTIME_DIR / "metrics.json")  # GUI側が open(METRICS_JSON) する前提

class _MetricsKV:
    """
    簡易KVS: GUI側がファイル読めなかったときのフォールバック。
    trade_service 等が同プロセス内で set()/update() を呼べば get() で返る。
    """
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self._kv: Dict[str, Any] = {}

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._kv)

    def set(self, kv: Dict[str, Any]) -> None:
        with self._lock:
            self._kv.update(kv)

    def update(self, **kwargs: Any) -> None:
        with self._lock:
            self._kv.update(kwargs)

# 外部公開
METRICS = _MetricsKV()



=== file: core/position_guard.py ===

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import Dict, Optional

try:
    import MetaTrader5 as mt5
except Exception:  # pragma: no cover - MT5 unavailable in dryrun/tests
    mt5 = None


@dataclass
class PositionGuardState:
    inflight_orders: Dict[str, float] = field(default_factory=dict)
    last_reconcile_ts: float = 0.0
    open_count: int = 0
    last_fix_reason: Optional[str] = None


class PositionGuard:
    """
    Track live positions/in-flight orders and reconcile with broker state.
    The guard focuses on aggregate count (per account) to keep logic simple.
    """

    def __init__(self, max_positions: int = 1, inflight_timeout_sec: int = 20):
        self.max_positions = max_positions
        self.inflight_timeout_sec = inflight_timeout_sec
        self.state = PositionGuardState()

    # === public API ===

    def mark_inflight(self, order_id: str) -> None:
        """Mark an order as in-flight (before send)."""
        self.state.inflight_orders[str(order_id)] = time.time()

    def clear_inflight(self, order_id: str) -> None:
        """Clear an order from in-flight tracking."""
        self.state.inflight_orders.pop(str(order_id), None)

    def can_open(self) -> bool:
        """Return True when within allowed max positions (after GC)."""
        self._gc_inflight()
        return self.state.open_count < self.max_positions

    def reset(self) -> None:
        """Reset guard state."""
        self.state = PositionGuardState()

    def reconcile_with_broker(self, symbol: Optional[str], desync_fix: bool = True) -> None:
        """
        Sync the local open count with broker positions.
        In dryrun (no MT5), it sticks with current open_count.
        """
        now = time.time()
        self.state.last_reconcile_ts = now
        count = self.state.open_count
        try:
            if mt5 is not None:
                if symbol:
                    poss = mt5.positions_get(symbol=symbol) or []
                else:
                    poss = mt5.positions_get() or []
                count = len(poss)
        except Exception:
            return

        if count != self.state.open_count:
            reason = f"desync(open_count={self.state.open_count} -> {count})"
            self.state.last_fix_reason = reason
            if desync_fix:
                self.state.open_count = count
            self._gc_inflight()

    # === helpers ===

    def _gc_inflight(self) -> None:
        now = time.time()
        dead = [k for k, ts in list(self.state.inflight_orders.items()) if now - ts > self.inflight_timeout_sec]
        for k in dead:
            self.state.inflight_orders.pop(k, None)


# ----------------------------------------------------------------------
# Backwards-compatible procedural helpers (legacy call sites still expect these)
# ----------------------------------------------------------------------
_DEFAULT_GUARD = PositionGuard()


def get_default_guard() -> PositionGuard:
    return _DEFAULT_GUARD


def can_open_new(symbol: Optional[str], max_positions: int) -> bool:
    guard = get_default_guard()
    if guard.max_positions != max_positions:
        guard.max_positions = max_positions
    guard._gc_inflight()
    return guard.can_open()


def mark_inflight(symbol: Optional[str], flag: bool) -> None:
    guard = get_default_guard()
    key = symbol or "GLOBAL"
    if flag:
        guard.mark_inflight(key)
    else:
        guard.clear_inflight(key)


def reset() -> None:
    get_default_guard().reset()


def on_order_rejected_or_canceled(symbol: Optional[str] = None, ticket: Optional[int] = None) -> None:
    mark_inflight(symbol, False)



=== file: core/risk.py ===

# core/risk.py
from __future__ import annotations

from dataclasses import dataclass
from math import sqrt
from typing import Optional


@dataclass
class LotSizingResult:
    """
    target_monthly_return / max_monthly_dd と ATR ストップから
    「推奨ロット」と「月間想定ボラ」を計算した結果をまとめたデータクラス。
    """

    lot: float  # 実際に使うロット（min/max でクランプ済み）
    per_trade_risk_pct: float  # 1トレードあたりのリスク（％）
    est_monthly_volatility_pct: float  # 月間想定ボラ（ざっくり標準偏差イメージ％）
    est_max_monthly_dd_pct: float  # 想定最大DD（％、max_monthly_dd に近い値になるよう設計）


def compute_lot_size_from_atr(
    *,
    equity: float,
    atr: float,
    atr_mult_sl: float,
    target_monthly_return: float,
    max_monthly_dd: float,
    tick_value: float,
    tick_size: float,
    expected_trades_per_month: int = 40,
    worst_case_trades_for_dd: int = 10,
    avg_r_multiple: float = 0.6,
    min_lot: float = 0.01,
    max_lot: float = 1.0,
) -> LotSizingResult:
    """
    target_monthly_return / max_monthly_dd と ATR ストップから
    自動ロットを計算するユーティリティ。

    Parameters
    ----------
    equity:
        現在の口座残高 or 有効証拠金（口座通貨）。
    atr:
        ATR 値（価格単位）。例: USDJPY なら 0.25 など。
    atr_mult_sl:
        ストップ幅の係数。SL 距離 = atr_mult_sl * atr。
    target_monthly_return:
        目標月次リターン (例: 0.03)。
    max_monthly_dd:
        許容最大月次 DD (例: -0.20)。符号付きでもよいが絶対値を使用する。
    tick_value:
        1ティック動いたときの損益（1 ロットあたり、口座通貨）。
        MT5 の symbol_info(...).trade_tick_value を想定。
    tick_size:
        1ティックの価格幅。symbol_info(...).trade_tick_size を想定。
    expected_trades_per_month:
        月あたり想定トレード回数。
    worst_case_trades_for_dd:
        「この回数連続で負けたら max_dd に達する」とみなす回数。
    avg_r_multiple:
        1トレードあたりの平均 R（リスクリワード）。0.6 などの経験値。
    min_lot, max_lot:
        ロットの下限・上限（ブローカー仕様に合わせて調整）。

    Returns
    -------
    LotSizingResult
    """

    if equity <= 0:
        raise ValueError("equity は正の値である必要があります。")
    if atr <= 0:
        raise ValueError("atr は正の値である必要があります。")
    if atr_mult_sl <= 0:
        raise ValueError("atr_mult_sl は正の値である必要があります。")
    if tick_value <= 0 or tick_size <= 0:
        raise ValueError("tick_value / tick_size は正の値である必要があります。")
    if expected_trades_per_month <= 0:
        raise ValueError("expected_trades_per_month は正の整数である必要があります。")
    if worst_case_trades_for_dd <= 0:
        raise ValueError("worst_case_trades_for_dd は正の整数である必要があります。")
    if avg_r_multiple <= 0:
        raise ValueError("avg_r_multiple は正の値である必要があります。")

    # DD は絶対値を使う（仕様書では -0.20 などになっている想定）
    max_dd_abs = abs(max_monthly_dd)

    # --- 1. DD 制約から見た 1トレードあたり許容リスク ---
    risk_from_dd = max_dd_abs / float(worst_case_trades_for_dd)

    # --- 2. 目標リターンから見た 1トレードあたり必要リスク ---
    expected_return_per_trade = target_monthly_return / float(expected_trades_per_month)
    risk_from_return = expected_return_per_trade / avg_r_multiple

    # --- 3. 実際に使う 1トレードのリスク％ ---
    # 安全側に振るため、DD 制約と 3% 目標のうち「小さい方」を採用する。
    per_trade_risk_pct = min(risk_from_dd, risk_from_return)

    # 念のため、極端な値をクランプ（0.01%〜10% の範囲に収める）
    per_trade_risk_pct = max(0.0001, min(per_trade_risk_pct, 0.10))

    # --- 4. ATR ストップから 1ロットあたりの損失額を計算 ---
    # 1ポイントあたり損益（1 ロット）
    value_per_point_per_lot = tick_value / tick_size

    # ストップまでのポイント数
    sl_points = (atr_mult_sl * atr) / tick_size

    # 1ロットあたりの損失額（口座通貨）
    risk_per_lot = sl_points * value_per_point_per_lot

    if risk_per_lot <= 0:
        raise ValueError("risk_per_lot が 0 以下です。tick_value / tick_size / atr の指定を確認してください。")

    # --- 5. 口座残高からロットを逆算 ---
    risk_per_trade_money = equity * per_trade_risk_pct
    raw_lot = risk_per_trade_money / risk_per_lot

    # ロットをクランプ（0.01〜max_lot）
    lot = max(min_lot, min(raw_lot, max_lot))

    # --- 6. 月間想定ボラと DD のざっくり推定 ---
    # ガウシアンっぽい近似で、「標準偏差 ~ sqrt(N) * per_trade_risk_pct」と置く。
    est_monthly_volatility_pct = per_trade_risk_pct * sqrt(float(expected_trades_per_month))

    # 「worst_case_trades_for_dd 回負けたらこの DD」とみなす。
    est_max_monthly_dd_pct = per_trade_risk_pct * float(worst_case_trades_for_dd)

    return LotSizingResult(
        lot=lot,
        per_trade_risk_pct=per_trade_risk_pct,
        est_monthly_volatility_pct=est_monthly_volatility_pct,
        est_max_monthly_dd_pct=est_max_monthly_dd_pct,
    )



=== file: core/utils/__init__.py ===

# Core utils package initializer.



=== file: core/utils/clock.py ===

from __future__ import annotations

from datetime import datetime, timedelta, timezone

JST = timezone(timedelta(hours=9), name="Asia/Tokyo")


def now_jst() -> datetime:
    """Return current datetime in JST (timezone-aware)."""
    return datetime.now(JST)



=== file: core/utils/hashing.py ===

# core/utils/hashing.py
from __future__ import annotations

import hashlib
import json
from typing import Any, Mapping, Sequence


def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def hash_features(features: Mapping[str, Any], order: Sequence[str] | None = None) -> str:
    """
    特徴量辞書を安定ハッシュ化する。
    order が与えられたらその順、無ければキーでソート。
    値は JSON にしてから sha256。
    """
    if order is None:
        order = sorted(features.keys())
    snapshot = {key: features.get(key, None) for key in order}
    payload = json.dumps(snapshot, ensure_ascii=False, sort_keys=True, separators=(",", ":"))
    return sha256_text(payload)



=== file: core/utils/runtime.py ===

from __future__ import annotations

import os
from functools import lru_cache


@lru_cache(maxsize=1)
def is_live() -> bool:
    """
    Returns True when the environment variable FXBOT_RUNTIME indicates live mode.
    """
    return os.environ.get("FXBOT_RUNTIME", "").strip().lower() in {"live", "prod", "production"}



=== file: core/utils/timeutil.py ===

# core/utils/timeutil.py
from __future__ import annotations

from datetime import datetime, timedelta, timezone

JST = timezone(timedelta(hours=9), name="Asia/Tokyo")


def now_jst_iso() -> str:
    return datetime.now(JST).isoformat(timespec="seconds")



=== file: fxbot_path.py ===

from __future__ import annotations

import os
from pathlib import Path


def get_project_root() -> Path:
    """
    fxbot プロジェクトのルートパスを返す。
    このファイル自身（fxbot_path.py）が置かれているディレクトリをルートとみなす。
    例:
      C:\\Users\\macht\\OneDrive\\fxbot
      D:\\macht\\OneDrive\\fxbot
      C:\\fxbot
    """
    return Path(__file__).resolve().parent


def get_data_root(cli_data_dir: str | os.PathLike | None = None) -> Path:
    """
    データディレクトリの候補を複数試して、最初に存在したディレクトリを採用する。
    優先順位:
      1) --data-dir 引数で明示されたパス
      2) 環境変数 FXBOT_DATA
      3) プロジェクトルート配下の data/
      4) カレントディレクトリ配下の data/
    どれも存在しない場合は、最後に project_root/data を返す（存在チェックには使える）。
    """
    candidates: list[Path] = []

    # 1) CLI 引数
    if cli_data_dir:
        candidates.append(Path(cli_data_dir))

    # 2) 環境変数
    env_dir = os.getenv("FXBOT_DATA")
    if env_dir:
        candidates.append(Path(env_dir))

    # 3) プロジェクトルートの data
    root = get_project_root()
    candidates.append(root / "data")

    # 4) カレントディレクトリの data
    candidates.append(Path.cwd() / "data")

    # 実在するもののうち先頭
    for p in candidates:
        try:
            if p.is_dir():
                return p.resolve()
        except Exception:
            # パス参照で問題が起きた場合は次へ
            continue

    # 全滅なら project_root/data をとりあえず返す（存在しない場合でも作成されうる）
    return (root / "data").resolve()


def _ensure_dir(p: Path) -> Path:
    """
    ディレクトリが存在しないなら作成して Path を返す。
    Windows/OneDrive 等でパスが特殊でも例外を上げずに済むようにする。
    """
    try:
        p.mkdir(parents=True, exist_ok=True)
    except Exception:
        # 作成できない場合でも Path を返す（呼び出し側でハンドリング）
        pass
    return p


def get_file_tag(symbol: str) -> str:
    """
    CSV ファイル名に使う接尾辞なしタグを決定する（英字のみ）。
    例: "USDJPY-" -> "USDJPY" , "USDJPY.m" -> "USDJPY"
    """
    tag = "".join([c for c in (symbol or "") if c.isalpha()])
    return tag or symbol


def get_ohlcv_csv_path(
    symbol: str,
    timeframe: str,
    data_root: str | os.PathLike | Path | None = None,
    layout: str = "per-symbol",
) -> Path:
    """
    統一された場所へ OHLCV CSV のパスを返す。必要なディレクトリは作成する。

    - `symbol`: ブローカー接尾辞を含む可能性あり（ここではそのまま受け取る）
    - `timeframe`: 例 'M5', 'H1'（そのままファイル名に使う）
    - `data_root`: None の場合は `get_data_root()` に委譲
    - `layout`: 'per-symbol' または 'flat'

    返り値例（per-symbol）: <data_root>/USDJPY/ohlcv/USDJPY_M5.csv
    返り値例（flat）: <data_root>/USDJPY_M5.csv
    """
    # data_root を決定
    if data_root is None:
        root = get_data_root()
    else:
        root = Path(data_root) if not isinstance(data_root, Path) else data_root
        if not root.is_absolute():
            root = (get_project_root() / root).resolve()

    root = root.resolve()

    # file tag
    tag = get_file_tag(symbol.upper())

    if layout == "per-symbol":
        ohlcv_dir = root / tag / "ohlcv"
        _ensure_dir(ohlcv_dir)
        csv_path = ohlcv_dir / f"{tag}_{timeframe}.csv"
    else:
        _ensure_dir(root)
        csv_path = root / f"{tag}_{timeframe}.csv"

    return csv_path.resolve()



=== file: scripts/cb_smoke.py ===

from __future__ import annotations

from typing import Any, Optional

import importlib


def main() -> None:
    cb_mod = importlib.import_module("app.services.circuit_breaker")
    scan_and_update = getattr(cb_mod, "scan_and_update", None)
    status = getattr(cb_mod, "status", None)

    if callable(scan_and_update):
        scan_and_update()
    if callable(status):
        s: Optional[dict[str, Any]] = status()
        print(s)
    else:
        print({"circuit": "unknown"})


if __name__ == "__main__":
    main()



=== file: scripts/diagnose_symbol.py ===

# scripts/diagnose_symbol.py
import MetaTrader5 as mt5


def main() -> None:
    if not mt5.initialize():
        print("MT5 init failed:", mt5.last_error())
        return
    try:
        # USDJPYで始まる全候補を列挙
        cands = mt5.symbols_get("USDJPY*")
        print("Candidates:", len(cands))
        for s in cands:
            print(f"- {s.name}  (select={s.select}, bid={s.bid}, ask={s.ask}, point={s.point})")
    finally:
        mt5.shutdown()


if __name__ == "__main__":
    main()



=== file: scripts/dryrun_smoke.py ===

# scripts/dryrun_smoke.py
from __future__ import annotations

import argparse
import math
import random
import sys
import time
from pathlib import Path
from typing import Dict

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from loguru import logger

from app.core import logger as app_logger
from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_state
from app.services.execution_stub import ExecutionStub, reset_atr_gate_state
from core.ai.service import AISvc
from core.utils.hashing import hash_features
from core.utils.timeutil import now_jst_iso


def parse_args(argv: list[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Dry-run smoke simulator")
    parser.add_argument("--sim", action="store_true", help="Use synthetic tick stream")
    parser.add_argument("--atr-open", action="store_true", help="Force ATR gate open")
    parser.add_argument("--n", type=int, default=200, help="Number of ticks to simulate")
    parser.add_argument("--dt", type=int, default=50, help="Tick interval in milliseconds")
    parser.add_argument("--base", type=float, default=150.20, help="Base mid price")
    parser.add_argument("--spread", type=float, default=0.5, help="Spread in pips")
    parser.add_argument("--atrpct", type=float, default=0.0005, help="ATR percentage baseline")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument("--symbol", type=str, default=None, help="Override runtime symbol")
    return parser.parse_args(argv)


def _build_features(base_price: float, spread_pips: float, tick_idx: int, rng: random.Random) -> Dict[str, float]:
    drift = math.sin(tick_idx / 6.0)
    noise = rng.uniform(-0.02, 0.02)
    ema_5 = base_price * (1 + drift * 0.002) + noise
    ema_20 = base_price * (1 - drift * 0.001) - noise
    rsi_14 = 60.0 + drift * 25.0 + rng.uniform(-3, 3)
    atr_14 = abs(drift) * 0.002 + spread_pips * 0.0001
    adx_14 = 22.0 + abs(drift) * 15.0 + rng.uniform(-1.5, 1.5)
    bbp = 0.5 + drift * 0.35 + rng.uniform(-0.05, 0.05)
    vol_chg = drift * 0.08 + rng.uniform(-0.02, 0.02)
    wick_ratio = 0.5 + drift * 0.3 + rng.uniform(-0.05, 0.05)
    return {
        "ema_5": float(ema_5),
        "ema_20": float(ema_20),
        "rsi_14": float(max(0.0, min(100.0, rsi_14))),
        "atr_14": float(abs(atr_14)),
        "adx_14": float(max(5.0, adx_14)),
        "bbp": float(max(0.0, min(1.0, bbp))),
        "vol_chg": float(vol_chg),
        "wick_ratio": float(max(0.0, min(1.0, wick_ratio))),
    }


def _ensure_config_overrides(cfg: dict, args: argparse.Namespace) -> None:
    filters_cfg = cfg.setdefault("filters", {})
    hy = filters_cfg.setdefault("atr_hysteresis", {})
    if args.atr_open:
        filters_cfg["min_atr_pct"] = 0.0
        hy["enable_min_pct"] = 0.0
        hy["disable_min_pct"] = 0.0


def prepare_state(cfg: dict, args: argparse.Namespace) -> ExecutionStub:
    runtime_cfg = cfg.get("runtime", {})
    entry_cfg = cfg.get("entry", {})
    prob_threshold = float(entry_cfg.get("prob_threshold", entry_cfg.get("threshold_buy", 0.60)))
    trade_state.update(
        trading_enabled=True,
        threshold_buy=float(entry_cfg.get("threshold_buy", prob_threshold)),
        threshold_sell=float(entry_cfg.get("threshold_sell", prob_threshold)),
        prob_threshold=prob_threshold,
        side_bias=str(entry_cfg.get("side_bias", "auto") or "auto"),
    )

    cb_cfg = cfg.get("circuit_breaker", {}) if isinstance(cfg, dict) else {}
    cb = circuit_breaker.CircuitBreaker(
        max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", cfg.get("risk", {}).get("max_consecutive_losses", 5))),
        daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
        cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
    )
    ai = AISvc(threshold=prob_threshold)
    try:
        reset_atr_gate_state()
    except Exception:
        pass
    return ExecutionStub(cb=cb, ai=ai)


def main(argv: list[str]) -> None:
    app_logger.setup()
    args = parse_args(argv)
    if not args.sim:
        args.sim = True  # default to simulation for smoke test

    rng = random.Random(args.seed)
    cfg = load_config()
    _ensure_config_overrides(cfg, args)

    import core.config as core_config

    core_config.cfg = cfg
    stub = prepare_state(cfg, args)

    runtime_cfg = cfg.get("runtime", {})
    symbol = args.symbol or runtime_cfg.get("symbol", "USDJPY")

    spread_limit = float(runtime_cfg.get("spread_limit_pips", runtime_cfg.get("spread_limit", 1.5)))
    max_positions = int(runtime_cfg.get("max_positions", 1))

    trail_logged = False

    for idx in range(args.n):
        features = _build_features(args.base, args.spread, idx, rng)
        runtime_payload = {
            "spread_pips": float(args.spread),
            "spread_limit_pips": spread_limit,
            "max_positions": max_positions,
            "open_positions": 0,
            "ai_threshold": stub.ai.threshold,
            "min_atr_pct": cfg.get("filters", {}).get("min_atr_pct", 0.0),
            "filters": cfg.get("filters", {}),
        }
        result = stub.on_tick(symbol, features, runtime_payload)
        if not trail_logged:
            logger.info("[TRAIL][DRYRUN] smoke trail ping features_hash={}", hash_features(features))
            trail_logged = True
        if args.dt > 0:
            time.sleep(min(args.dt / 1000.0, 0.1))

    logger.info(
        "[SMOKE] completed n={} dt_ms={} symbol={} time={}",
        args.n,
        args.dt,
        symbol,
        now_jst_iso(),
    )


if __name__ == "__main__":
    main(sys.argv[1:])



=== file: scripts/export_mt5_history.py ===

# --- project root on sys.path ---
import os, sys
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)
# --------------------------------

import MetaTrader5 as mt5
import pandas as pd
from datetime import datetime, timedelta
from typing import NoReturn

SYMBOL = os.environ.get("FXBOT_SYMBOL", "USDJPY-")
TIMEFRAME = mt5.TIMEFRAME_M5

# 端末パスを明示したい場合は環境変数 FXBOT_MT5_TERMINAL を使う
# 例: setx FXBOT_MT5_TERMINAL "C:\Program Files\MetaTrader 5\terminal64.exe"
TERM_PATH = os.environ.get("FXBOT_MT5_TERMINAL")

def die(msg: str) -> NoReturn:
    print(msg)
    mt5.shutdown()
    raise SystemExit(1)

def ensure_init() -> None:
    ok = mt5.initialize() if not TERM_PATH else mt5.initialize(path=TERM_PATH)
    if not ok:
        die(f"MT5 initialize() failed: last_error={mt5.last_error()} term_path={TERM_PATH!r}")
    ver = mt5.version()
    print(f"MT5 initialized. version={ver} term_path={TERM_PATH!r}")

def ensure_logged_in() -> None:
    ai = mt5.account_info()
    if ai is None:
        die(f"Not logged in or terminal not ready. last_error={mt5.last_error()}")
    print(f"Account: {ai.login} / {ai.server}")

def ensure_symbol(symbol: str) -> None:
    info = mt5.symbol_info(symbol)
    if info is None:
        die(f"symbol_info({symbol}) is None. last_error={mt5.last_error()}")
    if not info.visible:
        if not mt5.symbol_select(symbol, True):
            die(f"symbol_select({symbol}) failed. last_error={mt5.last_error()}")
    # 試しに最新ティックも触っておく
    _ = mt5.symbol_info_tick(symbol)
    print(f"Symbol {symbol} ready (visible={mt5.symbol_info(symbol).visible})")

def try_copy_small(symbol: str, timeframe: int, count: int = 1000) -> int:
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None:
        return 0
    return len(rates)

def export_range(symbol: str, timeframe: int, days: int = 365 * 5) -> str:
    to = datetime.now()
    frm = to - timedelta(days=days)

    # まず小さく取れるか診断
    small = try_copy_small(symbol, timeframe, 1000)
    print(f"diagnostic: copy_rates_from_pos count={small}")

    # --- 安全取得モード ---
    print(f"fetching {symbol} {days}days range in chunks ...")
    chunk_days = 30   # 1か月単位で遡る
    frames = []
    cursor_to = to
    while cursor_to > frm:
        cursor_from = cursor_to - timedelta(days=chunk_days)
        rates = mt5.copy_rates_range(symbol, timeframe, cursor_from, cursor_to)
        if rates is None or len(rates) == 0:
            print(f"chunk {cursor_from.date()}~{cursor_to.date()} => no data (skip)")
        else:
            df = pd.DataFrame(rates)
            frames.append(df)
            print(f"chunk {cursor_from.date()}~{cursor_to.date()} => {len(df)} bars")
        cursor_to = cursor_from

    if not frames:
        die("no data returned even by chunked fetch. Try shorter days or different symbol/timeframe.")

    df = pd.concat(frames).drop_duplicates(subset=["time"]).sort_values("time")
    df["Date"] = pd.to_datetime(df["time"], unit="s")
    df = df.rename(columns={
        "open": "open", "high": "high", "low": "low", "close": "close",
        "tick_volume": "volume"
    })
    df = df[["Date", "open", "high", "low", "close", "volume"]]
    df["label"] = (df["close"].shift(-1) > df["close"]).map({True: "BUY", False: "SELL"})

    outdir = os.path.join("data", "usdjpy")
    os.makedirs(outdir, exist_ok=True)
    out = os.path.join(outdir, "USDJPY_M5_mt5.csv")
    df.to_csv(out, index=False)
    print(f"wrote {out} rows:{len(df)}")
    return out


def main() -> None:
    ensure_init()
    ensure_logged_in()
    ensure_symbol(SYMBOL)
    export_range(SYMBOL, TIMEFRAME, days=365*5)
    mt5.shutdown()

if __name__ == "__main__":
    main()



=== file: scripts/export_val_probs.py ===

# scripts/export_val_probs.py
from __future__ import annotations

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import atexit, math, json
from datetime import datetime, timezone, timedelta
from typing import Any

import numpy as np
import pandas as pd
import MetaTrader5 as mt5
from loguru import logger

from core.ai.loader import load_lgb_clf  # 既存ローダを利用
from pathlib import Path

# ====== 設定 ======
SYMBOL = "USDJPY"                 # ブローカー接尾辞は自動吸収します
TIMEFRAME = mt5.TIMEFRAME_M5
START = "2025-08-01 00:00:00"     # 検証開始（JST）
END   = "2025-10-01 00:00:00"     # 検証終了（JST）
BARS_MIN = 400                    # 最低必要バー数（EMA/BB等のため）
OUT_PBUY = Path("logs/val_p_buy_raw.npy")
OUT_Y    = Path("logs/val_y_true.npy")
OUT_META = Path("logs/val_export_meta.json")

# ====== MT5初期化 ======
def _ensure_symbol(symbol: str) -> str:
    if mt5.symbol_select(symbol, True):
        return symbol
    upper = symbol.upper()
    cands = [s.name for s in mt5.symbols_get() if s.name.upper().startswith(upper)]
    if not cands:
        raise RuntimeError(f"no candidates for '{symbol}'")
    best = sorted(cands, key=len)[0]
    best = str(best)
    if not mt5.symbol_select(best, True):
        raise RuntimeError(f"symbol_select failed for '{best}'")
    return best


def _read_feature_order(meta_path: str) -> list[str]:
    try:
        with open(meta_path, "r", encoding="utf-8") as fh:
            meta: dict[str, Any] = json.load(fh)
        return list(meta.get("feature_order", []))
    except Exception:
        return []

def _to_utc(jst_str: str) -> datetime:
    jst = datetime.strptime(jst_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone(timedelta(hours=9)))
    return jst.astimezone(timezone.utc)

# ====== 特徴量（dryrunと同じ定義） ======
def make_features_df(df: pd.DataFrame) -> pd.DataFrame:
    close, high, low, open_ = df["close"], df["high"], df["low"], df["open"]

    ema_5  = close.ewm(span=5, adjust=False).mean()
    ema_20 = close.ewm(span=20, adjust=False).mean()

    delta = close.diff()
    up = delta.clip(lower=0.0)
    down = -delta.clip(upper=0.0)
    roll_up = up.ewm(alpha=1/14, adjust=False).mean()
    roll_down = down.ewm(alpha=1/14, adjust=False).mean()
    rs = roll_up / roll_down.replace(0, np.nan)
    rsi_14 = (100.0 - (100.0 / (1.0 + rs))).fillna(50.0)

    tr1 = (high - low).abs()
    tr2 = (high - close.shift()).abs()
    tr3 = (low - close.shift()).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr_14_abs = tr.ewm(alpha=1/14, adjust=False).mean()
    atr_14 = (atr_14_abs / close.replace(0, np.nan)).fillna(0.0)

    plus_dm  = (high.diff()).clip(lower=0.0)
    minus_dm = (-low.diff()).clip(lower=0.0)
    plus_dm[plus_dm < minus_dm] = 0.0
    minus_dm[minus_dm <= plus_dm] = 0.0
    tr_smooth = tr.ewm(alpha=1/14, adjust=False).mean()
    plus_di = 100 * (plus_dm.ewm(alpha=1/14, adjust=False).mean() / tr_smooth.replace(0, np.nan))
    minus_di = 100 * (minus_dm.ewm(alpha=1/14, adjust=False).mean() / tr_smooth.replace(0, np.nan))
    dx = 100 * ((plus_di - minus_di).abs() / (plus_di + minus_di).replace(0, np.nan))
    adx_14 = dx.ewm(alpha=1/14, adjust=False).mean().fillna(20.0)

    bb_ma = close.rolling(20).mean()
    bb_std = close.rolling(20).std(ddof=0)
    bb_upper = bb_ma + 2 * bb_std
    bb_lower = bb_ma - 2 * bb_std
    bbp = ((close - bb_lower) / (bb_upper - bb_lower)).replace([np.inf, -np.inf], np.nan).clip(0.0, 1.0).fillna(0.5)

    # volume由来の特徴は省略（学習時に使っていれば追加）
    body_high = np.maximum(open_, close)
    body_low  = np.minimum(open_, close)
    upper_wick = (high - body_high).clip(lower=0.0)
    lower_wick = (body_low - low).clip(lower=0.0)
    rng = (high - low).replace(0, np.nan)
    wick_ratio = ((upper_wick + lower_wick) / rng).clip(0.0, 1.0).fillna(0.0)

    out = pd.DataFrame({
        "ema_5": ema_5 - ema_20,
        "ema_20": (ema_20 - close) / close.replace(0, np.nan),
        "rsi_14": rsi_14,
        "atr_14": atr_14,
        "adx_14": adx_14,
        "bbp": bbp,
        "wick_ratio": wick_ratio,
    })
    return out.replace([np.inf, -np.inf], 0.0).fillna(0.0)

def main() -> None:
    if not mt5.initialize():
        raise SystemExit(f"MT5 init failed: {mt5.last_error()}")
    atexit.register(mt5.shutdown)

    symbol = _ensure_symbol(SYMBOL)

    # 余裕を持って過去から取得
    utc_from = _to_utc(START) - timedelta(days=7)
    utc_to   = _to_utc(END)
    rates = mt5.copy_rates_range(symbol, TIMEFRAME, utc_from, utc_to)
    if rates is None or len(rates) < BARS_MIN:
        raise SystemExit(f"not enough bars: {len(rates) if rates is not None else 0}")

    df = pd.DataFrame(rates)
    df["ts"] = pd.to_datetime(df["time"], unit="s", utc=True).dt.tz_convert("Asia/Tokyo")
    df = df[ (df["ts"] >= pd.Timestamp(START, tz="Asia/Tokyo")) & (df["ts"] < pd.Timestamp(END, tz="Asia/Tokyo")) ].copy()
    df = df.reset_index(drop=True)

    feats = make_features_df(df)
    # ラベル定義（次バーの上げ下げ、同値は0とする）
    y_true = (df["close"].shift(-1) > df["close"]).astype(int)[:-1].values
    feats = feats.iloc[:-1, :].copy()
#
    # ===== モデルと列順（返り値の型差を吸収） =====
    lm = load_lgb_clf()  # LoadedModel / sklearn LGBMClassifier / lightgbm.Booster 等

    # モデル本体候補を広く探索
    candidates = [lm]
    for attr in ("model", "clf", "estimator", "inner", "wrapped", "lgbm", "booster", "booster_"):
        if hasattr(lm, attr):
            candidates.append(getattr(lm, attr))

    model = None
    for cand in candidates:
        if hasattr(cand, "predict_proba"):
            model = cand  # sklearn 互換
            use_proba = "predict_proba"
            break
        if hasattr(cand, "predict"):
            model = cand  # Booster 等（predictで確率が返る想定）
            use_proba = "predict"
            # break しないで続けてもいいが、まずはこれで採用
            break

    if model is None:
        raise TypeError("No usable model found: neither predict_proba nor predict detected.")

    # 特徴量順（存在しなければ現在列で進む）
    try:
        order = _read_feature_order("models/LightGBM_clf.features.json")
    except Exception:
        order = None
    if not order:
        order = list(feats.columns)

    # 欠け列は0で補い、余分は落とす
    for col in order:
        if col not in feats.columns:
            feats[col] = 0.0
    X = feats[order].astype(float)

    # ===== 生BUY確率の取得 =====
    if use_proba == "predict_proba":
        proba = model.predict_proba(X)  # (N,2 or 3)
    else:
        # LightGBM Booster 等: predict で確率が返る（binary は陽性確率、multiclass は (N, K)）
        # raw_score=False を渡せる場合は渡す（無くてもOK）
        try:
            proba = model.predict(X, raw_score=False)
        except TypeError:
            proba = model.predict(X)

    # shape を正規化
    proba = np.asarray(proba)
    if proba.ndim == 1:
        # binary の陽性確率が 1 列で返ったケース
        p_buy_raw = proba
    elif proba.ndim == 2:
        # (N,2) or (N,K) を想定。BUY列（陽性）を列1と仮定（必要なら調整）
        if proba.shape[1] >= 2:
            p_buy_raw = proba[:, 1]
        else:
            # よほどの特殊形状。安全側で最大スコア列をBUYとみなす
            idx = np.argmax(proba, axis=1)
            p_buy_raw = (idx == 1).astype(float)
    else:
        raise ValueError(f"Unsupported prediction output shape: {proba.shape}")

    OUT_PBUY.parent.mkdir(parents=True, exist_ok=True)
    np.save(OUT_PBUY, p_buy_raw.astype(float))
    np.save(OUT_Y,    y_true.astype(int))
    OUT_META.write_text(json.dumps({
        "symbol": symbol,
        "timeframe": "M5",
        "start": START,
        "end": END,
        "N": int(len(p_buy_raw)),
        "feature_order_used": order,
        "inferred_api": use_proba,
    }, ensure_ascii=False, indent=2), encoding="utf-8")

    logger.info(f"wrote: {OUT_PBUY} ({len(p_buy_raw)}), {OUT_Y} ({len(y_true)}), api={use_proba}")

#
if __name__ == "__main__":
    main()



=== file: scripts/live_runner.py ===

# scripts/live_runner.py
from __future__ import annotations

from pathlib import Path
import sys
import time
from typing import Any, Dict

# --- プロジェクトルート(fxbot/)を sys.path に追加 ---
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from app.services.ai_service import AISvc
from app.services.trade_service import execute_decision
from app.core.config_loader import load_config

def main() -> None:
    cfg = load_config()
    symbol = cfg.get("runtime", {}).get("symbol", "USDJPY")

    ai = AISvc()

    print("=== Live runner started ===")
    while True:
        # → AISvc が自動で MT5 チャートから特徴量を取る前提（既存の get_live_features があるなら置き換える）
        probs = ai.get_live_probs(symbol)
        print("tick → probs:", probs)
        decision = ai.build_decision_from_probs(probs, symbol=symbol)
        print("decision →", decision)
        # ATR は execution_stub 互換の decision の中に入っている
        execute_decision(decision, symbol=symbol)

        time.sleep(1.0)  # 1秒間隔でループ（適宜調整）

if __name__ == "__main__":
    main()



=== file: scripts/make_csv_from_mt5.py ===

# scripts/make_csv_from_mt5.py
"""
MT5 から USDJPY の M5/M15/H1 を 2020-11-01 以降で CSV 化し、
以降は不足分のみを自動追記するユーティリティ。

- 保存先: <プロジェクトルート>/data （相対指定でも最終的に絶対パスへ解決）
- タイムゾーン: JST（Asia/Tokyo）で time 列を naive datetime64[ns] として保存
- 既存CSVがあれば末尾時刻以降を自動で追記（重複は除去）
- シンボル接尾辞（例: USDJPY-）は自動解決
- GaitameFinest 等で copy_rates_range() が失敗する環境に対して
  copy_rates_from() の「現在→過去へページング」フォールバックを搭載
- 実行環境名は --env で明示でき、未指定時は HOST_MAP とヒューリスティックで推定
- 保存レイアウトは --layout で切替（flat | per-symbol）
"""

from __future__ import annotations

import argparse
import os
import socket
import sys
from datetime import UTC
from datetime import datetime as pdt
from pathlib import Path

import pandas as pd

try:
    import MetaTrader5 as mt5
except Exception as e:
    raise SystemExit(
        "[fatal] MetaTrader5 パッケージが見つかりません。仮想環境で `pip install MetaTrader5 pandas` を実行してください。"
    ) from e


# =========================
# 設定（必要なら編集）
# =========================

SYMBOL_DEFAULT = "USDJPY"
TIMEFRAMES_DEFAULT = ["M5", "M15", "H1"]
START_DATE_DEFAULT = "2020-11-01"  # ここ以前は取得しない

# プロジェクトルート（このスクリプトの1つ上のディレクトリ）
PROJECT_ROOT = Path(__file__).resolve().parents[1]
# デフォルトのデータ保存ディレクトリ（最終的に PROJECT_ROOT/data に解決）
DATA_DIR_DEFAULT = "data"

# allow importing fxbot_path from project root
sys.path.insert(0, str(PROJECT_ROOT))
try:
    from fxbot_path import get_data_root, get_ohlcv_csv_path
except Exception:
    # import error will be surfaced when running main; keep module import-safe
    get_data_root = None  # type: ignore
    get_ohlcv_csv_path = None  # type: ignore

# MT5 の timeframe 定数マップ
TF_MAP = {
    "M1": mt5.TIMEFRAME_M1,
    "M5": mt5.TIMEFRAME_M5,
    "M15": mt5.TIMEFRAME_M15,
    "M30": mt5.TIMEFRAME_M30,
    "H1": mt5.TIMEFRAME_H1,
    "H4": mt5.TIMEFRAME_H4,
    "D1": mt5.TIMEFRAME_D1,
}

# CSV カラム順（MT5の戻り値に準拠）
CSV_COLS = [
    "time",
    "open",
    "high",
    "low",
    "close",
    "tick_volume",
    "spread",
    "real_volume",
]

# フォールバック時の1回あたり取得本数と最大ループ回数（必要に応じて調整可能）
PAGE = 20000
MAX_LOOPS = 300

# CSVファイル名で使う “接尾辞なしタグ” を main() 内で設定
FILE_TAG: str | None = None


# =========================
# ユーティリティ
# =========================


def log(msg: str):
    host = os.environ.get("COMPUTERNAME", socket.gethostname())
    ts = pdt.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}][{host}] {msg}")


def ensure_mt5_initialized(terminal_path: str | None = None):
    """
    MT5 を初期化。terminal_path を指定すればその exe に紐づけ。
    Windows で MT5 が起動していなくても、通常は自動で起動・接続可能。
    """
    ok = mt5.initialize(path=terminal_path) if terminal_path else mt5.initialize()
    if not ok:
        code, details = mt5.last_error()
        raise SystemExit(f"[fatal] MT5 initialize 失敗: {code} {details}")

    info = mt5.account_info()
    if info is None:
        log(
            "[warn] account_info() が None。未ログインの可能性。ターミナル側で口座ログインしてください。"
        )
    else:
        log(f"connected login={info.login} server={info.server} balance={info.balance}")


def resolve_symbol(base: str) -> str:
    """
    ブローカー接尾辞違いに対応してシンボル名を解決。
    成功した実在シンボル名を返す。全て失敗なら元の base を返す。
    """
    candidates = [
        base,
        base + "-",
        base + ".",
        base + ".r",
        base + ".m",
        base + ".mini",
        base + "_",
    ]
    tried = []
    for sym in candidates:
        tried.append(sym)
        info = mt5.symbol_info(sym)
        if info is not None:
            if not info.visible:
                mt5.symbol_select(sym, True)
            return sym
    log(f"[warn] symbol resolve failed. tried={tried}")
    return base


def jst_from_mt5_epoch(series):
    """
    MT5の 'time' (Unix秒, UTC) を JST の naive datetime64[ns] に変換。
    series は pandas Series でも DatetimeIndex でも両対応。
    """
    s = pd.to_datetime(series, unit="s", utc=True)
    if isinstance(s, pd.DatetimeIndex):
        return s.tz_convert("Asia/Tokyo").tz_localize(None)
    else:
        return s.dt.tz_convert("Asia/Tokyo").dt.tz_localize(None)


def merge_and_dedup(old: pd.DataFrame | None, new: pd.DataFrame) -> pd.DataFrame:
    """
    old と new を縦結合して time 重複を除去（後勝ち）→ time 昇順に揃える。
    """
    if old is None or old.empty:
        out = new.copy()
    else:
        out = pd.concat([old, new], axis=0, ignore_index=True)
        out = out.drop_duplicates(subset=["time"], keep="last")
    return out.sort_values("time").reset_index(drop=True)


# =========================
# データ取得コア
# =========================


def _to_utc_naive(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> UTC naive（tzinfoなし）"""
    return (
        ts_jst.tz_localize("Asia/Tokyo")
        .tz_convert("UTC")
        .to_pydatetime()
        .replace(tzinfo=None)
    )


def _to_local_naive(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> JST naive（ローカルnaiveを要求する環境向け）"""
    return ts_jst.to_pydatetime().replace(tzinfo=None)


def _to_utc_aware(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> UTC aware（timezone.utc）"""
    return (
        ts_jst.tz_localize("Asia/Tokyo")
        .tz_convert("UTC")
        .to_pydatetime()
        .replace(tzinfo=UTC)
    )


def _range_attempts(
    symbol: str, tf: int, start_ts: pd.Timestamp, end_ts: pd.Timestamp
) -> tuple[pd.DataFrame | None, str]:
    """
    copy_rates_range() を 3 方式（UTC-naive / local-naive / UTC-aware）で試す。
    成功時は (DataFrame, "ok:<tag>")、全滅なら (None, "fail")
    """
    variants = [
        ("utc_naive", _to_utc_naive(start_ts), _to_utc_naive(end_ts)),
        ("local_naive", _to_local_naive(start_ts), _to_local_naive(end_ts)),
        ("utc_aware", _to_utc_aware(start_ts), _to_utc_aware(end_ts)),
    ]
    for tag, dfrom, dto in variants:
        rates = mt5.copy_rates_range(symbol, tf, dfrom, dto)
        if rates is None:
            code, details = mt5.last_error()
            log(f"[try:{tag}] copy_rates_range returned None: {code} {details}")
            continue
        df = pd.DataFrame(rates)
        if len(df) == 0:
            log(f"[ok:{tag}] fetched rows=0")
            return pd.DataFrame(columns=CSV_COLS), f"ok:{tag}"
        df["time"] = jst_from_mt5_epoch(df["time"])
        df = df.sort_values("time").reset_index(drop=True)
        log(f"[ok:{tag}] fetched rows={len(df)}")
        return df[CSV_COLS], f"ok:{tag}"
    return None, "fail"


def fetch_rates(
    symbol: str, tf: int, start_ts: pd.Timestamp, end_ts: pd.Timestamp
) -> pd.DataFrame:
    """
    指定期間のレートを取得して DataFrame で返す（JSTに変換）。
    まず copy_rates_range() を試し、全滅したら copy_rates_from() のバックページングで補う。
    """
    if not isinstance(start_ts, pd.Timestamp):
        start_ts = pd.Timestamp(start_ts)
    if not isinstance(end_ts, pd.Timestamp):
        end_ts = pd.Timestamp(end_ts)
    if end_ts <= start_ts:
        raise ValueError(f"start >= end: {start_ts} .. {end_ts}")

    # 1) range 試行
    df_range, status = _range_attempts(symbol, tf, start_ts, end_ts)
    if status.startswith("ok"):
        return df_range

    # 2) フォールバック: from で過去にページング
    log("[fallback] using copy_rates_from() paging backward")
    dt_to = _to_utc_naive(end_ts)  # MT5は tzinfo なしの UTC を好む

    frames = []
    safety_loops = 0

    while safety_loops < MAX_LOOPS:
        safety_loops += 1
        rates = mt5.copy_rates_from(symbol, tf, dt_to, PAGE)
        if rates is None:
            code, details = mt5.last_error()
            log(f"[fallback] copy_rates_from returned None: {code} {details}")
            break
        if len(rates) == 0:
            log("[fallback] no more bars returned")
            break

        # 生のDataFrame（UTC epoch秒）
        df_raw = pd.DataFrame(rates)

        # まずはJSTへ
        df = df_raw.copy()
        df["time"] = jst_from_mt5_epoch(df["time"])
        df = df.sort_values("time").reset_index(drop=True)

        # 目標期間に重なる分だけ保持（JST基準でフィルタ）
        df_keep = df[(df["time"] >= start_ts) & (df["time"] <= end_ts)]
        if len(df_keep):
            frames.append(df_keep[CSV_COLS])

        # 次ページの終端（さらに過去へ）
        oldest_utc_epoch = int(df_raw["time"].min())  # epoch秒
        # DeprecationWarning 回避：UTC aware で作ってから tzinfo=None で naive UTC へ
        dt_to = pdt.fromtimestamp(oldest_utc_epoch - 1, tz=UTC).replace(tzinfo=None)

        # もう十分遡れたか？
        if len(df) and df["time"].min() <= start_ts:
            break

    if not frames:
        log("[fallback] collected 0 rows")
        return pd.DataFrame(columns=CSV_COLS)

    out = (
        pd.concat(frames, axis=0, ignore_index=True)
        .drop_duplicates(subset=["time"])
        .sort_values("time")
        .reset_index(drop=True)
    )
    log(
        f"[fallback] fetched rows={len(out)} (min={out['time'].min()} .. max={out['time'].max()})"
    )
    return out[CSV_COLS]


# =========================
# CSV 作成・更新
# =========================


def ensure_csv_for_timeframe(
    symbol: str,
    tf_name: str,
    start_date: str,
    data_dir: Path,
    end_date: str | None = None,
    layout: str = "per-symbol",
) -> Path:
    """
    単一タイムフレームのCSVを作成/更新する。
    - start_date ～ end_date の範囲で作成（end_date が None の場合は現在まで）
    - 既存があれば末尾以降のみ追記
    - 返り値: 保存した CSV のパス
    """
    log(f"=== begin timeframe={tf_name} ===")
    if tf_name not in TF_MAP:
        raise ValueError(f"未知のタイムフレーム: {tf_name}")

    tf_const = TF_MAP[tf_name]
    assert FILE_TAG is not None, "FILE_TAG が未設定です（main() で設定されます）"
    # 統一パス生成
    if get_ohlcv_csv_path is not None:
        csv_path = get_ohlcv_csv_path(
            symbol, tf_name, data_root=data_dir, layout=layout
        )
    else:
        # fall back to legacy behavior (data_dir / FILE_TAG_tf.csv)
        csv_path = data_dir / f"{FILE_TAG}_{tf_name}.csv"
        csv_path.parent.mkdir(parents=True, exist_ok=True)

    start_ts = pd.Timestamp(start_date)  # JST naive

    if end_date is not None:
        end_ts = pd.Timestamp(end_date)
    else:
        end_ts = pd.Timestamp.now(tz="Asia/Tokyo").tz_localize(None)

    # 既存CSVの読み込み
    if csv_path.exists():
        old = pd.read_csv(csv_path, parse_dates=["time"])
        old = old[CSV_COLS]
        last_time = old["time"].max()
        fetch_from = max(start_ts, last_time)
        log(
            f"{csv_path.name}: existing rows={len(old)} last={last_time} -> fetch_from={fetch_from}"
        )
    else:
        old = None
        fetch_from = start_ts
        log(f"{csv_path.name}: not found -> fresh export from {fetch_from}")

    # end_ts より進んでいたら、新規取得は行わない
    if end_ts <= fetch_from:
        log(
            f"{csv_path.name}: end_ts <= fetch_from ({end_ts} <= {fetch_from}) -> no new fetch"
        )
        merged = old if old is not None else pd.DataFrame(columns=CSV_COLS)
    else:
        # データ取得（少し余分に取り直して重複で吸収）
        df_new = fetch_rates(symbol, tf_const, fetch_from, end_ts)
        log(f"{csv_path.name}: fetched rows={len(df_new)} [{fetch_from} .. {end_ts}]")

        # マージ＆重複除去
        merged = merge_and_dedup(old, df_new)

    # 型最適化（省メモリ）
    if not merged.empty:
        for c in ["open", "high", "low", "close"]:
            merged[c] = merged[c].astype("float32")
        for c in ["tick_volume", "spread", "real_volume"]:
            merged[c] = merged[c].astype("int32")

    merged.to_csv(csv_path, index=False)
    log(f"{csv_path.name}: wrote rows={len(merged)}")
    log(f"filepath: {csv_path.resolve()}")
    log(f"=== end timeframe={tf_name} ===")
    return csv_path


# =========================
# エントリポイント
# =========================


def main():
    parser = argparse.ArgumentParser(
        description="Export/Update MT5 rates to CSV per timeframe."
    )
    parser.add_argument(
        "--symbol", default=SYMBOL_DEFAULT, help="シンボル（例: USDJPY）"
    )
    parser.add_argument(
        "--timeframes", nargs="+", default=TIMEFRAMES_DEFAULT, help="例: M5 M15 H1"
    )
    parser.add_argument(
        "--start", default=START_DATE_DEFAULT, help="開始日（例: 2020-11-01）"
    )
    parser.add_argument(
        "--end",
        default=None,
        help="終了日（例: 2024-07-10、省略時は現在時刻まで）",
    )
    parser.add_argument(
        "--data-dir",
        default=DATA_DIR_DEFAULT,
        help="保存先ディレクトリ（相対はプロジェクトルート基準）",
    )
    parser.add_argument(
        "--terminal", default=None, help="MT5 terminal.exe のフルパス（必要な場合のみ）"
    )

    # 環境と保存レイアウト
    parser.add_argument(
        "--env",
        choices=["laptop", "desktop", "vps"],
        default=None,
        help="環境名を明示（laptop/desktop/vps）。未指定ならホスト名ヒューリスティック。",
    )
    parser.add_argument(
        "--layout",
        choices=["flat", "per-symbol"],
        default="per-symbol",
        help="CSV保存レイアウト。flat= data/直下, per-symbol= data/<SYMBOL>/ohlcv/ 下に保存",
    )

    args = parser.parse_args()

    symbol = args.symbol.upper()
    tfs = [tf.upper() for tf in args.timeframes]

    # data_root を決定（FXBOT_DATA 環境変数、--data-dir を考慮）
    if get_data_root is not None:
        data_root = get_data_root(cli_data_dir=args.data_dir)
    else:
        raw_data_dir = Path(args.data_dir)
        data_root = (
            raw_data_dir
            if raw_data_dir.is_absolute()
            else (PROJECT_ROOT / raw_data_dir)
        )
        data_root = data_root.resolve()

    end_display = args.end or "NOW"
    log(
        f"start export: symbol={symbol} tfs={tfs} start={args.start} end={end_display} data_root={data_root}"
    )
    log(f"cwd={Path.cwd()} project_root={PROJECT_ROOT}")

    # 環境推定（明示指定優先 → HOST_MAP → ヒューリスティック）
    host = os.environ.get("COMPUTERNAME", socket.gethostname()).lower()
    HOST_MAP = {
        # 必要に応じて固定マッピングを追加
        # 例: "desktop-8rrd83d": "laptop",
        # "sakura-vps": "vps",
    }
    if args.env:
        env_resolved = args.env
    else:
        env_resolved = HOST_MAP.get(host)
        if not env_resolved:
            if (
                "vps" in host
                or "sakura" in host
                or "administrator" in str(Path.home()).lower()
            ):
                env_resolved = "vps"
            elif "desk" in host:
                env_resolved = "desktop"
            else:
                env_resolved = "laptop"
    log(f"環境: {env_resolved} (host={host})")

    # MT5 初期化
    ensure_mt5_initialized(terminal_path=args.terminal)

    # シンボル解決（USDJPY / USDJPY- など）
    resolved_symbol = resolve_symbol(symbol)
    if resolved_symbol != symbol:
        log(f"symbol resolved: {symbol} -> {resolved_symbol}")
    symbol = resolved_symbol
    mt5.symbol_select(symbol, True)

    # CSV 用の “接尾辞なしタグ” を作成（英字のみ抽出）
    global FILE_TAG
    FILE_TAG = "".join([c for c in symbol if c.isalpha()]) or symbol

    # 保存先ルートは data_root（一つの場所から get_ohlcv_csv_path で個別ファイルを決定）
    save_root = data_root
    log(f"save_root={save_root}")

    # 取得・保存
    created: list[Path] = []
    for tf_name in tfs:
        path = ensure_csv_for_timeframe(
            symbol,
            tf_name,
            args.start,
            save_root,
            end_date=args.end,
            layout=args.layout,
        )
        created.append(path)

    mt5.shutdown()
    log("done.")


if __name__ == "__main__":
    main()



=== file: scripts/make_project_snapshot.py ===

#!/usr/bin/env python
"""
プロジェクト全体のフォルダ構造と主要ファイルの内容を
1つの project_snapshot.txt にまとめて出力するスクリプト。

- プロジェクトルート = このファイルの 1 つ上のフォルダ
- 除外ディレクトリや対象拡張子は CONFIG のところで調整可能
- ディレクトリツリーにはファイルの最終更新日時も表示します
"""

from __future__ import annotations

import os
from pathlib import Path
from datetime import datetime
from typing import Iterable, List

# ========================
# 設定
# ========================

# 除外するディレクトリ名（部分一致）
EXCLUDE_DIR_NAMES = {
    ".git",
    ".venv",
    "__pycache__",
    ".mypy_cache",
    ".pytest_cache",
    ".idea",
    ".vscode",
    "logs",
    "data",
    "models",
    "設定",
    "dist",
    "build",
    ".ruff_cache",
}

# 中身を書き出す対象の拡張子
INCLUDE_EXTENSIONS = {
    ".py",
    ".txt",
    ".md",
    ".rst",
    ".yaml",
    ".yml",
    ".ini",
    ".toml",
    ".json",
    ".ps1",
    ".bat",
    ".sh",
}

# ファイルサイズの上限（バイト）。これを超えると中身はスキップしてヘッダだけ書く
MAX_FILE_SIZE_BYTES = 100_000  # 100KB

# 出力ファイル名
SNAPSHOT_FILENAME = "project_snapshot.txt"


# ========================
# ヘルパー関数
# ========================

def iter_tree(root: Path) -> Iterable[Path]:
    """root 配下のファイル・ディレクトリを walk するジェネレータ。
    除外ディレクトリをスキップする。
    """
    for dirpath, dirnames, filenames in os.walk(root):
        # dirnames を in-place でフィルタすると os.walk がそれを辿らなくなる
        dirnames[:] = [
            d for d in dirnames
            if d not in EXCLUDE_DIR_NAMES
        ]
        current_dir = Path(dirpath)
        # ディレクトリ自身
        yield current_dir
        # ファイル
        for fname in filenames:
            yield current_dir / fname


def make_tree_text(root: Path) -> str:
    """ディレクトリツリー文字列を生成する（ファイルには最終更新日時付き）。"""
    lines: List[str] = []

    root_str = root.name
    lines.append(f"{root_str}/")

    # root からの相対パスでソートして表示
    paths = sorted(
        (p for p in iter_tree(root)),
        key=lambda p: str(p.relative_to(root)).lower(),
    )

    seen_dirs = set()

    for p in paths:
        rel = p.relative_to(root)
        parts = rel.parts

        # ルート自身はもう書いているのでスキップ
        if rel == Path("."):
            continue

        indent = "  " * (len(parts) - 1)
        name = parts[-1]

        if p.is_dir():
            # ディレクトリ
            dir_key = rel
            if dir_key in seen_dirs:
                continue
            seen_dirs.add(dir_key)
            lines.append(f"{indent}{name}/")
        else:
            # ファイル（最終更新日時を付ける）
            try:
                mtime = datetime.fromtimestamp(p.stat().st_mtime)
                mtime_str = mtime.strftime("%Y-%m-%d %H:%M:%S")
            except OSError:
                mtime_str = "unknown"
            lines.append(f"{indent}{name} (updated: {mtime_str})")

    return "\n".join(lines)


def should_dump_content(path: Path) -> bool:
    """このファイルの中身をスナップショットに含めるか判定。"""
    if not path.is_file():
        return False

    if path.suffix.lower() not in INCLUDE_EXTENSIONS:
        return False

    try:
        size = path.stat().st_size
    except OSError:
        return False

    if size > MAX_FILE_SIZE_BYTES:
        return False

    return True


def read_text_safely(path: Path) -> str:
    """UTF-8 で読みつつ、読めない文字は置き換える。"""
    try:
        return path.read_text(encoding="utf-8", errors="replace")
    except Exception as e:  # noqa: BLE001
        return f"<< FAILED TO READ FILE: {e} >>"


def collect_files_for_dump(root: Path) -> List[Path]:
    """中身を抜き出す対象のファイル一覧を返す。"""
    files: List[Path] = []
    for p in iter_tree(root):
        if p.is_file() and should_dump_content(p):
            files.append(p)
    files.sort(key=lambda p: str(p.relative_to(root)).lower())
    return files


# ========================
# メイン処理
# ========================

def main() -> int:
    # このスクリプトの 1 つ上をプロジェクトルートとみなす
    script_path = Path(__file__).resolve()
    project_root = script_path.parent.parent
    snapshot_path = project_root / SNAPSHOT_FILENAME

    print(f"[INFO] script_path   = {script_path}")
    print(f"[INFO] project_root  = {project_root}")
    print(f"[INFO] snapshot_path = {snapshot_path}")

    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # 1) ヘッダ
    header_lines = [
        "#" * 60,
        "# project_snapshot",
        "#" * 60,
        f"generated_at: {now}",
        f"project_root: {project_root}",
        "",
        "NOTE:",
        "  - このファイルは ChatGPT にプロジェクト構造と主要ファイルを伝えるためのスナップショットです。",
        "  - ログ・データ・モデル・.git・.venv などは除外しています。",
        "  - ディレクトリツリーにはファイルごとの最終更新日時 (updated: ...) を含みます。",
        "",
        "========================================",
        "=== DIRECTORY TREE =====================",
        "========================================",
        "",
    ]

    tree_text = make_tree_text(project_root)

    # 2) ファイル内容
    files = collect_files_for_dump(project_root)

    content_lines: List[str] = []
    content_lines.append("")
    content_lines.append("")
    content_lines.append("========================================")
    content_lines.append("=== FILE CONTENTS ======================")
    content_lines.append("========================================")
    content_lines.append("")

    for fpath in files:
        rel = fpath.relative_to(project_root)
        content_lines.append("")
        content_lines.append(f"=== file: {rel.as_posix()} ===")
        content_lines.append("")
        txt = read_text_safely(fpath)
        content_lines.append(txt)
        content_lines.append("")  # 区切り

    # 3) スナップショットを書き出し
    all_text = "\n".join(header_lines) + "\n" + tree_text + "\n" + "\n".join(content_lines)

    try:
        snapshot_path.write_text(all_text, encoding="utf-8")
    except Exception as e:  # noqa: BLE001
        print(f"[ERROR] failed to write snapshot: {e}")
        return 1

    print(f"[OK] snapshot written: {snapshot_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/make_project_snapshot2.py ===

#!/usr/bin/env python
"""
プロジェクト全体のフォルダ構造と主要ファイルの内容を
1つの project_snapshot.txt にまとめて出力するスクリプト。

- プロジェクトルート = このファイルの 1 つ上のフォルダ
- 除外ディレクトリや対象拡張子は CONFIG のところで調整可能
- ディレクトリツリーにはファイルの最終更新日時も表示します
"""

from __future__ import annotations

import os
from pathlib import Path
from datetime import datetime
from typing import Iterable, List

# ========================
# 設定
# ========================

# 除外するディレクトリ名（部分一致）
EXCLUDE_DIR_NAMES = {
    ".git",
    ".venv",
    "__pycache__",
    ".mypy_cache",
    ".pytest_cache",
    ".idea",
    ".vscode",
    "logs",
    "data",
    "models",
    "設定",
    "dist",
    "build",
    ".ruff_cache",
}

# 中身を書き出す対象の拡張子
INCLUDE_EXTENSIONS = {
    ".py",
}

# ファイルサイズの上限（バイト）。これを超えると中身はスキップしてヘッダだけ書く
MAX_FILE_SIZE_BYTES = 100_000  # 100KB

# 出力ファイル名
SNAPSHOT_FILENAME = "project_snapshot.txt"


# ========================
# ヘルパー関数
# ========================

def iter_tree(root: Path) -> Iterable[Path]:
    """root 配下のファイル・ディレクトリを walk するジェネレータ。
    除外ディレクトリをスキップする。
    """
    for dirpath, dirnames, filenames in os.walk(root):
        # dirnames を in-place でフィルタすると os.walk がそれを辿らなくなる
        dirnames[:] = [
            d for d in dirnames
            if d not in EXCLUDE_DIR_NAMES
        ]
        current_dir = Path(dirpath)
        # ディレクトリ自身
        yield current_dir
        # ファイル
        for fname in filenames:
            yield current_dir / fname


def make_tree_text(root: Path) -> str:
    """ディレクトリツリー文字列を生成する（ファイルには最終更新日時付き）。"""
    lines: List[str] = []

    root_str = root.name
    lines.append(f"{root_str}/")

    # root からの相対パスでソートして表示
    paths = sorted(
        (p for p in iter_tree(root)),
        key=lambda p: str(p.relative_to(root)).lower(),
    )

    seen_dirs = set()

    for p in paths:
        rel = p.relative_to(root)
        parts = rel.parts

        # ルート自身はもう書いているのでスキップ
        if rel == Path("."):
            continue

        indent = "  " * (len(parts) - 1)
        name = parts[-1]

        if p.is_dir():
            # ディレクトリ
            dir_key = rel
            if dir_key in seen_dirs:
                continue
            seen_dirs.add(dir_key)
            lines.append(f"{indent}{name}/")
        else:
            # ファイル（最終更新日時を付ける）
            if p.suffix.lower() != ".py":
                continue
            try:
                mtime = datetime.fromtimestamp(p.stat().st_mtime)
                mtime_str = mtime.strftime("%Y-%m-%d %H:%M:%S")
            except OSError:
                mtime_str = "unknown"
            lines.append(f"{indent}{name} (updated: {mtime_str})")

    return "\n".join(lines)


def should_dump_content(path: Path) -> bool:
    """このファイルの中身をスナップショットに含めるか判定。"""
    if not path.is_file():
        return False

    if path.suffix.lower() not in INCLUDE_EXTENSIONS:
        return False

    try:
        size = path.stat().st_size
    except OSError:
        return False

    if size > MAX_FILE_SIZE_BYTES:
        return False

    return True


def read_text_safely(path: Path) -> str:
    """UTF-8 で読みつつ、読めない文字は置き換える。"""
    try:
        return path.read_text(encoding="utf-8", errors="replace")
    except Exception as e:  # noqa: BLE001
        return f"<< FAILED TO READ FILE: {e} >>"


def collect_files_for_dump(root: Path) -> List[Path]:
    """中身を抜き出す対象のファイル一覧を返す。"""
    files: List[Path] = []
    for p in iter_tree(root):
        if p.is_file() and should_dump_content(p):
            files.append(p)
    files.sort(key=lambda p: str(p.relative_to(root)).lower())
    return files


# ========================
# メイン処理
# ========================

def main() -> int:
    # このスクリプトの 1 つ上をプロジェクトルートとみなす
    script_path = Path(__file__).resolve()
    project_root = script_path.parent.parent
    snapshot_path = project_root / SNAPSHOT_FILENAME

    print(f"[INFO] script_path   = {script_path}")
    print(f"[INFO] project_root  = {project_root}")
    print(f"[INFO] snapshot_path = {snapshot_path}")

    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # 1) ヘッダ
    header_lines = [
        "#" * 60,
        "# project_snapshot",
        "#" * 60,
        f"generated_at: {now}",
        f"project_root: {project_root}",
        "",
        "NOTE:",
        "  - このファイルは ChatGPT にプロジェクト構造と主要ファイルを伝えるためのスナップショットです。",
        "  - ログ・データ・モデル・.git・.venv などは除外しています。",
        "  - ディレクトリツリーにはファイルごとの最終更新日時 (updated: ...) を含みます。",
        "",
        "========================================",
        "=== DIRECTORY TREE =====================",
        "========================================",
        "",
    ]

    tree_text = make_tree_text(project_root)

    # 2) ファイル内容
    files = collect_files_for_dump(project_root)

    content_lines: List[str] = []
    content_lines.append("")
    content_lines.append("")
    content_lines.append("========================================")
    content_lines.append("=== FILE CONTENTS ======================")
    content_lines.append("========================================")
    content_lines.append("")

    for fpath in files:
        rel = fpath.relative_to(project_root)
        content_lines.append("")
        content_lines.append(f"=== file: {rel.as_posix()} ===")
        content_lines.append("")
        txt = read_text_safely(fpath)
        content_lines.append(txt)
        content_lines.append("")  # 区切り

    # 3) スナップショットを書き出し
    all_text = "\n".join(header_lines) + "\n" + tree_text + "\n" + "\n".join(content_lines)

    try:
        snapshot_path.write_text(all_text, encoding="utf-8")
    except Exception as e:  # noqa: BLE001
        print(f"[ERROR] failed to write snapshot: {e}")
        return 1

    print(f"[OK] snapshot written: {snapshot_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/make_toy_model.py ===

from __future__ import annotations

import json
import os
from pathlib import Path

import joblib
import numpy as np
import pandas as pd

USE_LGB = True
try:
    from lightgbm import LGBMClassifier
except Exception:
    USE_LGB = False
    from sklearn.linear_model import LogisticRegression

MODELS_DIR = Path("models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)

FEATURES = [
    "ema_5",
    "ema_20",
    "rsi_14",
    "atr_14",
    "adx_14",
    "bbp",
    "vol_chg",
    "wick_ratio",
]


def make_synthetic(n: int = 5000, seed: int = 42) -> tuple[pd.DataFrame, np.ndarray]:
    rng = np.random.default_rng(seed)
    X = pd.DataFrame(
        {
            "ema_5": rng.normal(0.0, 0.2, n),
            "ema_20": rng.normal(0.0, 0.2, n),
            "rsi_14": rng.uniform(0, 100, n),
            "atr_14": rng.uniform(0, 1, n),
            "adx_14": rng.uniform(5, 40, n),
            "bbp": rng.uniform(-0.5, 1.5, n),
            "vol_chg": rng.normal(0.0, 0.05, n),
            "wick_ratio": rng.uniform(0, 1, n),
        }
    )
    score = (
        0.8 * X["ema_5"]
        - 0.5 * X["ema_20"]
        + 0.01 * (X["rsi_14"] - 50)
        + 0.4 * (X["bbp"] - 0.5)
        - 0.2 * X["vol_chg"]
        + 0.15 * (X["adx_14"] > 20).astype(float)
    )
    p = 1 / (1 + np.exp(-score))
    y = (rng.uniform(0, 1, n) < p).astype(int)
    return X[FEATURES], y


def main() -> None:
    X, y = make_synthetic()

    if USE_LGB:
        model = LGBMClassifier(
            n_estimators=200,
            max_depth=-1,
            num_leaves=31,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=123,
            n_jobs=1,
        )
    else:
        model = LogisticRegression(max_iter=1000, n_jobs=1)

    model.fit(X, y)

    joblib.dump(model, MODELS_DIR / "LightGBM_clf.pkl")
    with open(MODELS_DIR / "LightGBM_clf.features.json", "w", encoding="utf-8") as f:
        json.dump(list(X.columns), f, ensure_ascii=False, indent=2)

    classes = getattr(model, "classes_", None)
    if classes is not None:
        with open(MODELS_DIR / "LightGBM_clf.classes.json", "w", encoding="utf-8") as f:
            json.dump([str(c) for c in classes], f, ensure_ascii=False, indent=2)

    print("[OK] Exported:")
    print(" - models/LightGBM_clf.pkl")
    print(" - models/LightGBM_clf.features.json")
    print(" - models/LightGBM_clf.classes.json (optional)")


if __name__ == "__main__":
    main()



=== file: scripts/mt5_export_csv.py ===

# scripts/mt5_export_csv.py
# MT5のローカルヒストリから rates を取得→ data/{SYMBOL}_{TF}.csv に保存
from __future__ import annotations
import argparse
from datetime import datetime, timedelta, timezone
from pathlib import Path
import pandas as pd

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--symbol", required=True)
    ap.add_argument("--timeframe", default="M5", help="M1/M5/M15/M30/H1/H4/D1")
    ap.add_argument("--days", type=int, default=365*2, help="過去n日を取得")
    args = ap.parse_args()

    import MetaTrader5 as mt5  # pip install MetaTrader5

    if not mt5.initialize():
        raise SystemExit("MT5 initialize failed")

    tf_map = {
        "M1": mt5.TIMEFRAME_M1, "M5": mt5.TIMEFRAME_M5, "M15": mt5.TIMEFRAME_M15,
        "M30": mt5.TIMEFRAME_M30, "H1": mt5.TIMEFRAME_H1, "H4": mt5.TIMEFRAME_H4,
        "D1": mt5.TIMEFRAME_D1
    }
    tf = tf_map.get(args.timeframe.upper(), mt5.TIMEFRAME_M5)

    utc_to = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(days=args.days)

    rates = mt5.copy_rates_range(args.symbol, tf, utc_from, utc_to)
    mt5.shutdown()
    if rates is None or len(rates) == 0:
        raise SystemExit("no rates from MT5")

    df = pd.DataFrame(rates)
    # MT5の time はunix秒
    df["time"] = pd.to_datetime(df["time"], unit="s")
    df.rename(columns={"real_volume":"tick_volume"}, inplace=True)

    out_dir = Path(__file__).resolve().parents[1] / "data"
    out_dir.mkdir(parents=True, exist_ok=True)
    out = out_dir / f"{args.symbol.upper()}_{args.timeframe.upper()}.csv"
    df[["time","open","high","low","close","tick_volume"]].to_csv(out, index=False)
    print(f"wrote: {out} rows={len(df)}")

if __name__ == "__main__":
    main()



=== file: scripts/mt5_smoke.py ===

# scripts/mt5_smoke.py
#
# 目的:
#   - MetaTrader5 の初期化が成功するか
#   - 現在ログインしている口座情報が取れるか
# を確認するスモークテスト。

import MetaTrader5 as mt5


def main() -> None:
    print("[mt5_smoke] initialize() ...")
    if not mt5.initialize():
        print(f"[mt5_smoke] initialize() FAILED: last_error={mt5.last_error()}")
        print("  -> MT5 が起動しているか、ログイン状態かを確認してください。")
        return

    print("[mt5_smoke] initialize() OK")

    info = mt5.account_info()
    if info is None:
        print("[mt5_smoke] account_info() is None.")
        print("  -> MT5 が起動しているか、デモ口座などにログインしているか確認してください。")
    else:
        print("[mt5_smoke] account_info():")
        print(f"  login   = {info.login}")
        print(f"  name    = {info.name}")
        print(f"  balance = {info.balance}")
        print(f"  equity  = {info.equity}")

    mt5.shutdown()
    print("[mt5_smoke] shutdown() done.")


if __name__ == "__main__":
    main()



=== file: scripts/print_runtime.py ===

from __future__ import annotations

from core.config import cfg
from core.utils.runtime import is_live

print("is_live:", is_live())
print("filters:", cfg.get("filters"))
print("entry:", cfg.get("entry"))
print("session:", cfg.get("session"))



=== file: scripts/promote_model.py ===

# scripts/promote_model.py
import os, shutil, json, sys
from core.config import cfg
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)


def main() -> None:
    tr = cfg.get("training", {})
    staging = tr.get("staging_dir", "models/_staging")
    prod = tr.get("model_out_dir", "models")
    if not os.path.isdir(staging):
        raise SystemExit("staging not found")
    prom = os.path.join(staging, "PROMOTE.json")
    if os.path.exists(prom):
        st = json.load(open(prom, "r", encoding="utf-8"))
        print("PROMOTE.json:", st)
    # 上書き昇格
    os.makedirs(prod, exist_ok=True)
    ts = __import__("datetime").datetime.now().strftime("%Y%m%d_%H%M%S")
    bk = os.path.join(prod, f"_backup_{ts}")
    os.makedirs(bk, exist_ok=True)
    for fn in os.listdir(prod):
        if fn.startswith("_backup_"): continue
        shutil.copy2(os.path.join(prod, fn), os.path.join(bk, fn))
    for fn in os.listdir(staging):
        shutil.copy2(os.path.join(staging, fn), os.path.join(prod, fn))
    print("PROMOTED. backup:", bk)


if __name__ == "__main__":
    main()



=== file: scripts/rollback_model.py ===

# scripts/rollback_model.py
import os, shutil, sys
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)


def main() -> None:
    prod = "models"
    bks = sorted([d for d in os.listdir(prod) if d.startswith("_backup_")])
    if not bks:
        raise SystemExit("no backups found")
    last = os.path.join(prod, bks[-1])
    for fn in os.listdir(last):
        shutil.copy2(os.path.join(last, fn), os.path.join(prod, fn))
    print("ROLLED BACK to:", last)


if __name__ == "__main__":
    main()



=== file: scripts/selftest_mt5.py ===

from __future__ import annotations

import sys
import traceback
from pathlib import Path
from typing import Any

# ============================================
# プロジェクトルートを sys.path に追加
# （scripts/ の 1 個上が fxbot ルート）
# ============================================
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from app.core import mt5_client  # noqa: E402


def _get_attr(obj: Any, name: str, default: Any = "(n/a)") -> Any:
    """dict / MT5 の AccountInfo のどちらでも安全に属性を取り出すヘルパー。"""
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def main() -> int:
    print("=== MT5 self test ===")
    print("このスクリプトは、現在の設定タブで選択されている口座プロファイルを使って接続確認を行います。")
    print(f"PROJECT_ROOT: {PROJECT_ROOT}")
    print()

    try:
        # 1) initialize
        print("[1] mt5_client.initialize() ...")
        ok = mt5_client.initialize()
        print(f"    -> initialize() returned: {ok!r}")
        if not ok:
            print("ERROR: MT5 の初期化に失敗しました。")
            print(" - MT5 ターミナルが起動しているか？")
            print(" - 設定タブで選択した口座ID / サーバー / パスワードは正しいか？")
            return 1

        # 2) account_info
        print()
        print("[2] mt5_client.get_account_info() ...")
        info = mt5_client.get_account_info()
        if not info:
            print("ERROR: get_account_info() が None / False を返しました。")
            print("      ログイン情報やサーバー設定を確認してください。")
            return 2

        login = _get_attr(info, "login")
        name = _get_attr(info, "name")
        server = _get_attr(info, "server")
        balance = _get_attr(info, "balance")
        equity = _get_attr(info, "equity")
        trade_mode = _get_attr(info, "trade_mode")

        print("  --- Account Info ---")
        print(f"  login      : {login}")
        print(f"  name       : {name}")
        print(f"  server     : {server}")
        print(f"  balance    : {balance}")
        print(f"  equity     : {equity}")
        print(f"  trade_mode : {trade_mode}")
        print("  --------------------")

        # 3) positions (raw)
        print()
        print("[3] mt5_client.get_positions() ...")
        positions = mt5_client.get_positions()
        n_pos = len(positions) if positions is not None else 0
        print(f"    -> open positions: {n_pos}")
        if positions:
            # 先頭数件だけざっくり表示
            print("    sample positions (up to 5):")
            for i, pos in enumerate(positions[:5]):
                print(f"      [{i}] {pos!r}")

        # 4) positions_df (DataFrame)
        print()
        print("[4] mt5_client.get_positions_df() ...")
        try:
            df = mt5_client.get_positions_df()
        except Exception as e:  # noqa: BLE001
            print(f"    WARN: get_positions_df() で例外が発生しました: {e!r}")
        else:
            if df is None:
                print("    -> DataFrame: None （ポジション無しか、未対応の可能性）")
            else:
                try:
                    print("    -> DataFrame 形式のポジション一覧:")
                    print(df)
                except Exception:
                    print("    -> DataFrame の print 中に例外が出たため簡易表示に切替えます。")
                    print(repr(df))

        print()
        print("=== MT5 self test finished: SUCCESS ===")
        return 0

    except Exception:
        print()
        print("=== MT5 self test crashed ===")
        traceback.print_exc()
        return 99

    finally:
        # shutdown は念のため例外握りつぶしで
        try:
            print()
            print("[*] mt5_client.shutdown() ...")
            mt5_client.shutdown()
            print("    -> shutdown() 完了")
        except Exception as e:  # noqa: BLE001
            print(f"    WARN: shutdown() 中に例外が発生しました: {e!r}")


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/selftest_order_flow.py ===

from __future__ import annotations

import os
import sys
import time
from typing import Any

import MetaTrader5 as MT5  # type: ignore[import]
from core.config import cfg  # noqa: F401  # ensure configuration loads
from app.core.mt5_client import MT5Client


def _get_env(name: str) -> str:
    v = os.getenv(name, "")
    if not v:
        raise SystemExit(
            f"環境変数 {name} が設定されていません。\n"
            "GUI の設定タブで口座プロファイルを選択してから、再度このスクリプトを実行してください。"
        )
    return v


def _get_attr(obj: Any, name: str, default: Any = "(n/a)") -> Any:
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(name, default)
    return getattr(obj, name, default)


def _wait_for_position(ticket: int, timeout_sec: float = 10.0, interval: float = 0.5) -> bool:
    """指定 ticket のポジションが MT5.positions_get で見えるようになるまで待つ。"""
    deadline = time.time() + timeout_sec
    while time.time() < deadline:
        pos = MT5.positions_get(ticket=ticket)
        if pos:
            return True
        time.sleep(interval)
    return False


def main() -> int:
    print("=== MT5 order flow self test ===")
    print("このスクリプトは DEMO 口座で、")
    print("  1) 成行 BUY で 0.01 lot エントリー")
    print("  2) 約定確認")
    print("  3) すぐ成行クローズ")
    print("を行います。必ずデモ口座で実行してください。")
    print()

    # --- 認証情報 & テストパラメータ ---
    login = int(_get_env("MT5_LOGIN"))
    password = _get_env("MT5_PASSWORD")
    server = _get_env("MT5_SERVER")

    symbol = os.getenv("FXBOT_TEST_SYMBOL", "USDJPY-")
    lot = float(os.getenv("FXBOT_TEST_LOT", "0.01"))

    print(f"[config] login={login} server={server} symbol={symbol} lot={lot}")
    print()

    client = MT5Client(login=login, password=password, server=server)

    try:
        # 1) initialize
        print("[1] client.initialize()")
        if not client.initialize():
            print("    -> initialize() 失敗")
            return 1
        print("    -> OK")

        # 2) login_account
        print("[2] client.login_account()")
        if not client.login_account():
            print("    -> login_account() 失敗")
            return 2
        print("    -> OK")

        # 3) account_info 表示
        info = MT5.account_info()
        print()
        print("[3] account_info()")
        if info is None:
            print("    -> account_info() が None")
        else:
            print(f"    login   : {_get_attr(info, 'login')}")
            print(f"    name    : {_get_attr(info, 'name')}")
            print(f"    balance : {_get_attr(info, 'balance')}")
            print(f"    equity  : {_get_attr(info, 'equity')}")

        # 4) 成行 BUY エントリー
        print()
        print(f"[4] order_send() で BUY エントリー: symbol={symbol}, lot={lot}")
        ticket = client.order_send(symbol=symbol, order_type="BUY", lot=lot)
        if not ticket:
            print("    -> order_send() が ticket を返しませんでした。")
            return 3
        print(f"    -> 発注成功: ticket={ticket}")

        # 5) ポジション出現待ち
        print()
        print("[5] positions_get(ticket) でポジション出現を待機中...")
        if not _wait_for_position(ticket):
            print("    -> タイムアウト: ポジションが見つかりません。")
            return 4
        print("    -> ポジション検出 OK")

        # 6) 成行クローズ
        print()
        print("[6] close_position() で即クローズ")
        ok = client.close_position(ticket=ticket, symbol=symbol)
        print(f"    -> close_position() returned {ok!r}")
        if not ok:
            print("    -> クローズ失敗")
            return 5

        # 7) クローズ後の確認
        time.sleep(1.0)
        remaining = MT5.positions_get(ticket=ticket)
        print()
        print(f"[7] close 後の positions_get(ticket={ticket}) = {remaining}")
        if remaining:
            print("    WARN: クローズ後もポジションが残っています。")
            return 6

        print()
        print("=== MT5 order flow self test finished: SUCCESS ===")
        return 0

    finally:
        print()
        print("[*] client.shutdown()")
        try:
            client.shutdown()
            print("    -> shutdown() 完了")
        except Exception as e:  # noqa: BLE001
            print(f"    WARN: shutdown() 中に例外発生: {e!r}")


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/sim_trailing.py ===

import argparse

from app.services.trailing import AtrTrailer, TrailConfig, TrailState


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--side", choices=["BUY", "SELL"], default="BUY")
    parser.add_argument("--entry", type=float, required=True)
    parser.add_argument(
        "--atr",
        type=float,
        required=True,
        help="ATR in price units, e.g., 0.12 for 12 pips on USDJPY",
    )
    parser.add_argument("--pip", type=float, default=0.01, help="pip size, USDJPY=0.01")
    parser.add_argument("--point", type=float, default=0.001, help="point size, USDJPY=0.001")
    parser.add_argument("--activate", type=float, default=0.5)
    parser.add_argument("--step", type=float, default=0.25)
    parser.add_argument("--lockbe", type=float, default=0.3)
    parser.add_argument("--floor", type=float, default=5.0)
    args = parser.parse_args()

    cfg = TrailConfig(
        pip_size=args.pip,
        point=args.point,
        atr=args.atr,
        activate_mult=args.activate,
        step_mult=args.step,
        lock_be_mult=args.lockbe,
        hard_floor_pips=args.floor,
        only_in_profit=True,
        max_layers=20,
    )
    state = TrailState(side=args.side, entry=args.entry)
    trailer = AtrTrailer(cfg, state)

    steps = []
    for i in range(0, 31):
        delta = cfg.atr * 0.1 * i
        if args.side == "BUY":
            steps.append(args.entry + delta)
        else:
            steps.append(args.entry - delta)

    print(f"# side={args.side} entry={args.entry} atr={args.atr}")
    print("# price, activated, be_locked, layers, current_sl, new_sl")
    for px in steps:
        new_sl = trailer.suggest_sl(px)
        print(f"{px:.5f}, {state.activated}, {state.be_locked}, {state.layers}, {state.current_sl}, {new_sl}")


if __name__ == "__main__":
    main()



=== file: scripts/swap_model.py ===

from __future__ import annotations

import glob
import hashlib
import json
import os
import shutil
import sys
import time
from pathlib import Path

ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

MODELS_DIR = "models"
LIVE_LINK = os.path.join(MODELS_DIR, "live")


def _is_ready(bundle_dir: str) -> bool:
    ready = os.path.exists(os.path.join(bundle_dir, "READY"))
    manifest = os.path.exists(os.path.join(bundle_dir, "manifest.json"))
    return ready and manifest


def _latest_ready() -> str:
    candidates: list[tuple[float, str]] = []
    for bundle in glob.glob(os.path.join(MODELS_DIR, "*")):
        if (
            os.path.isdir(bundle)
            and _is_ready(bundle)
            and os.path.basename(bundle) not in {"live", "prev"}
        ):
            candidates.append((os.path.getmtime(bundle), bundle))
    if not candidates:
        raise SystemExit("no READY bundles found.")
    candidates.sort(reverse=True)
    return candidates[0][1]


def _sha256(path: Path) -> str:
    digest = hashlib.sha256()
    with open(path, "rb") as fh:
        for chunk in iter(lambda: fh.read(1024 * 1024), b""):
            digest.update(chunk)
    return digest.hexdigest()


def _load_latest_best_threshold() -> tuple[float | None, str | None]:
    try:
        report_dir = os.path.join("logs", "retrain")
        pattern = os.path.join(report_dir, "report_*.json")
        reports = sorted(glob.glob(pattern))
        if not reports:
            return None, None
        latest = reports[-1]
        with open(latest, "r", encoding="utf-8") as fh:
            data = json.load(fh)
        metrics = data.get("metrics_test") or {}
        best_t = metrics.get("best_threshold")
        if isinstance(best_t, (int, float)):
            return float(best_t), latest
        return None, latest
    except Exception:
        return None, None


def activate_model_from_pkl(
    pkl_path: Path, models_dir: Path, target_name: str = "LightGBM_clf.pkl"
) -> int:
    models_dir.mkdir(parents=True, exist_ok=True)
    active_pkl = models_dir / target_name
    tmp = active_pkl.with_suffix(".tmp.pkl")
    shutil.copy2(pkl_path, tmp)
    if active_pkl.exists():
        backup = models_dir / f"backup_{int(time.time())}.pkl"
        shutil.copy2(active_pkl, backup)
    os.replace(tmp, active_pkl)

    best_t, source_report = _load_latest_best_threshold()

    meta = {
        "activated_at_jst": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
        "source": "direct-pkl",
        "source_path": str(pkl_path.resolve()),
        "target_path": str(active_pkl.resolve()),
        "sha256": _sha256(active_pkl),
        "version": str(time.time()),
        "features_hash": None,
    }
    meta["best_threshold"] = None if best_t is None else f"{float(best_t):.6f}"
    if source_report:
        meta["best_threshold_source_report"] = source_report
    with open(models_dir / "active_model.json", "w", encoding="utf-8") as fh:
        json.dump(meta, fh, ensure_ascii=False, indent=2)
    print(f"[swap_model] ACTIVATED direct pkl -> {active_pkl}")
    return 0


def _fallback_main(args: list[str]) -> int:
    # READY探索に失敗した場合のフォールバック。単独実行にも対応。
    if args:
        candidate = Path(args[0])
        if candidate.suffix.lower() == ".pkl" and candidate.exists():
            return activate_model_from_pkl(candidate, Path("models"))
    print("no READY bundles found and no valid .pkl path provided.")
    return 1


def main(args: list[str] | None = None) -> int:
    if args is None:
        args = sys.argv[1:]
    try:
        target = _latest_ready()
    except SystemExit as exc:
        if exc.code:
            print(exc.code)
        return _fallback_main(args)

    prev = os.path.join(MODELS_DIR, "prev")
    if os.path.exists(prev):
        shutil.rmtree(prev)
    if os.path.exists(LIVE_LINK):
        shutil.move(LIVE_LINK, prev)
    shutil.copytree(target, LIVE_LINK)
    print(f"[SWAP] live -> {os.path.basename(target)} (prev saved)")
    return 0


if __name__ == "__main__":
    # 既存のREADY切替ロジックがsys.exitする前に、フォールバックを呼べるようにする保険。
    sys.exit(main())



=== file: scripts/train_calibrator.py ===

# scripts/train_calibrator.py
from __future__ import annotations
import numpy as np, joblib
from pathlib import Path
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression

import sys
if not (Path("logs/val_p_buy_raw.npy").exists() and Path("logs/val_y_true.npy").exists()):
    sys.exit("val_p_buy_raw.npy / val_y_true.npy が見つかりません。先に検証推論を実行して保存してください。")
    
# 検証データの「生BUY確率」と「正解ラベル」を用意して保存しておく
# 例: logs/val_p_buy_raw.npy (shape (N,)), logs/val_y_true.npy (0/1)
p_raw = np.load("logs/val_p_buy_raw.npy").astype(float).ravel()
y_true = np.load("logs/val_y_true.npy").astype(int).ravel()

models = Path("models"); models.mkdir(exist_ok=True, parents=True)

# Platt
lr = LogisticRegression(max_iter=1000)
lr.fit(p_raw.reshape(-1,1), y_true)
joblib.dump(lr, models / "calib_platt.pkl")

# Isotonic
iso = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds="clip")
iso.fit(p_raw, y_true)
joblib.dump(iso, models / "calib_isotonic.pkl")

print("wrote: models/calib_platt.pkl, models/calib_isotonic.pkl")



=== file: scripts/walkforward_retrain.py ===

from __future__ import annotations

import argparse
import json
import os
import sys
import warnings
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path

import lightgbm as lgbm
import numpy as np
import pandas as pd
from joblib import dump
from sklearn.metrics import log_loss, precision_recall_curve, roc_auc_score
from sklearn.model_selection import train_test_split

# ------------------------------------------------------------
# 基本設定（フォルダなど）
# ------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"
MODELS_DIR = PROJECT_ROOT / "models"
LOGS_DIR = PROJECT_ROOT / "logs"
MODELS_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)


def resolve_data_root(cli_data_dir: str | None) -> Path:
    """
    データのルート候補を複数試して、最初に存在したディレクトリを採用する。
    優先順位:
      1) --data-dir 引数
      2) 環境変数 FXBOT_DATA
      3) このスクリプトのプロジェクトルート配下の data/
      4) カレントディレクトリ配下の data/
    """
    candidates: list[Path] = []

    # 1) CLI 引数
    if cli_data_dir:
        candidates.append(Path(cli_data_dir))

    # 2) 環境変数
    env_dir = os.getenv("FXBOT_DATA")
    if env_dir:
        candidates.append(Path(env_dir))

    # 3) プロジェクトルートの data (C:\Users\...\fxbot\data / D:\...\fxbot\data / C:\fxbot\data)
    candidates.append(DATA_DIR)

    # 4) 念のためカレントディレクトリの data
    candidates.append(Path.cwd() / "data")

    existing = [p for p in candidates if p.is_dir()]
    if existing:
        return existing[0].resolve()

    # どれもなければ最後に DATA_DIR を返す（存在しなくてもエラー時のメッセージ用）
    return DATA_DIR.resolve()


RNG = np.random.default_rng(42)
pd.options.display.width = 200
warnings.filterwarnings("ignore", category=UserWarning)


# ------------------------------------------------------------
# ユーティリティ
# ------------------------------------------------------------
def jst_now_str() -> str:
    return datetime.now(UTC).astimezone().isoformat(timespec="seconds")


def safe_log(msg: str):
    ts = jst_now_str()
    print(f"{ts} | {msg}", flush=True)


def find_csv(symbol: str, timeframe: str, data_dir: str | None = None) -> Path:
    """
    CSVレイアウト両対応:
      - flat:       data/USDJPY_M5.csv
      - per-symbol: data/USDJPY/ohlcv/  内の  {symbol}_{tf}.csv もしくは  {tf}.csv
    優先順: 明示一致 → タイムスタンプが新しいもの
    """
    # ルート決定（--data-dir / FXBOT_DATA / PROJECT_ROOT/data / ./data の順で存在を確認）
    root = resolve_data_root(data_dir)

    symU = symbol.upper()
    symL = symbol.lower()
    tf = timeframe.upper()

    # 記号付きシンボル（USDJPY- 等）から英字だけのバージョンも作る
    symU_clean = "".join(ch for ch in symU if ch.isalpha())
    symL_clean = symU_clean.lower()

    candidates: list[Path] = []

    # --- flat layout (data/直下)
    candidates += list(root.glob(f"{symU}_{tf}.csv"))
    candidates += list(root.glob(f"{symL}_{tf}.csv"))
    candidates += list(root.glob(f"{symU_clean}_{tf}.csv"))
    candidates += list(root.glob(f"{symL_clean}_{tf}.csv"))
    candidates += list(root.glob(f"*_{tf}.csv"))  # 例: ANYTHING_M5.csv

    # --- per-symbol layout（推奨: data/USDJPY/ohlcv/）
    base_dirs = [
        root / symU / "ohlcv",
        root / symL / "ohlcv",
        root / symU_clean / "ohlcv",
        root / symL_clean / "ohlcv",
        root / symU,
        root / symL,
        root / symU_clean,
        root / symL_clean,
    ]

    for b in base_dirs:
        candidates += list(b.glob(f"{symU}_{tf}.csv"))
        candidates += list(b.glob(f"{symL}_{tf}.csv"))
        candidates += list(b.glob(f"{symU_clean}_{tf}.csv"))
        candidates += list(b.glob(f"{symL_clean}_{tf}.csv"))
        candidates += list(b.glob(f"*_{tf}.csv"))  # 例: anyprefix_M5.csv
        candidates += list(b.glob(f"{tf}.csv"))    # 例: M5.csv

    # 実在ファイルだけ、重複除去
    uniq: list[Path] = []
    seen = set()
    for p in candidates:
        if p.is_file():
            try:
                key = p.resolve()
            except Exception:
                key = p
            if key not in seen:
                seen.add(key)
                uniq.append(p)

    if not uniq:
        tried = [
            root / f"{symU}_{tf}.csv",
            root / f"{symU_clean}_{tf}.csv",
            root / symU / "ohlcv" / f"{symU}_{tf}.csv",
            root / symU_clean / "ohlcv" / f"{symU_clean}_{tf}.csv",
        ]
        msg = (
            "CSVが見つかりません。\\n"
            f"  symbol={symbol} timeframe={timeframe}\\n"
            f"  data_dir={root}\\n"
            "  試した場所の例:\\n    - " + "\\n    - ".join(str(p) for p in tried)
        )
        raise FileNotFoundError(msg)

    # 明示一致（{symbol}_{tf}.csv / clean版）があれば最優先
    exact = [
        p
        for p in uniq
        if p.name.lower()
        in {
            f"{symL}_{tf.lower()}.csv",
            f"{symL_clean}_{tf.lower()}.csv",
        }
    ]
    if exact:
        return exact[0]

    # それ以外は最終更新が新しいもの
    return max(uniq, key=lambda p: p.stat().st_mtime)


# ------------------------------------------------------------
# 特徴量生成
# ------------------------------------------------------------
def rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = np.clip(delta, 0, None)
    down = -np.clip(delta, None, 0)
    ma_up = up.rolling(period, min_periods=period).mean()
    ma_down = down.rolling(period, min_periods=period).mean()
    rs = ma_up / (ma_down + 1e-12)
    return 100 - (100 / (1 + rs))


def atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    prev_close = close.shift(1)
    tr = pd.concat(
        [
            (high - low),
            (high - prev_close).abs(),
            (low - prev_close).abs(),
        ],
        axis=1,
    ).max(axis=1)
    return tr.rolling(period, min_periods=period).mean()


def build_features(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    入力: time, open, high, low, close, tick_volume などのOHLCVを想定
    出力: 特徴量 DataFrame（欠損除去済み）
    """
    df = df_raw.copy()

    # 必須列チェック（ここで止まる場合はCSV修正が必要）
    need = {"time", "open", "high", "low", "close"}
    miss = need - set(df.columns)
    if miss:
        safe_log(f"[WFO][error] CSV missing columns: {sorted(miss)}")
        return pd.DataFrame()

    # 時刻整備
    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time", kind="stable").drop_duplicates(subset=["time"])

    n = len(df)

    # --- ミニ特徴量モード（行数が少ないときの救済） ---
    # 60行未満なら、ロール系は使わずに最低限の特徴量だけで返す
    if n < 60:
        safe_log(
            f"[WFO][warn] tiny dataset detected ({n} rows). Using mini feature set."
        )
        rng = (df["high"] - df["low"]).replace(0, np.nan)
        mini = pd.DataFrame(
            {
                "time": df["time"],
                "open": df["open"],
                "high": df["high"],
                "low": df["low"],
                "close": df["close"],
                # 最低限：1本リターン、レンジ内位置
                "ret1": df["close"].pct_change().fillna(0.0),
                "pos_in_range": ((df["close"] - df["low"]) / rng).fillna(0.5),
            }
        )
        # 数学的におかしい値を除去
        mini = mini.replace([np.inf, -np.inf], np.nan).dropna()
        return mini

    # --- 通常のフル特徴量モード ---
    # 基本の戻りとボラ
    df["ret1"] = df["close"].pct_change()
    df["ret3"] = df["close"].pct_change(3)
    df["ret5"] = df["close"].pct_change(5)
    df["vol20"] = df["close"].pct_change().rolling(20, min_periods=10).std()

    # 移動平均・バンド（min_periodsで消滅を抑制）
    for w in (5, 10, 20, 50):
        df[f"sma{w}"] = df["close"].rolling(w, min_periods=max(2, w // 2)).mean()
        df[f"ema{w}"] = df["close"].ewm(span=w, adjust=False).mean()
    df["bb_mid"] = df["close"].rolling(20, min_periods=10).mean()
    df["bb_std"] = df["close"].rolling(20, min_periods=10).std()
    df["bb_p"] = (df["close"] - df["bb_mid"]) / (df["bb_std"] + 1e-12)

    # RSI / ATR
    def _rsi(series: pd.Series, period: int = 14) -> pd.Series:
        delta = series.diff()
        up = np.clip(delta, 0, None)
        down = -np.clip(delta, None, 0)
        ma_up = up.rolling(period, min_periods=period // 2).mean()
        ma_down = down.rolling(period, min_periods=period // 2).mean()
        rs = ma_up / (ma_down + 1e-12)
        return 100 - (100 / (1 + rs))

    def _atr(df_: pd.DataFrame, period: int = 14) -> pd.Series:
        high, low, close = df_["high"], df_["low"], df_["close"]
        prev_close = close.shift(1)
        tr = pd.concat(
            [(high - low), (high - prev_close).abs(), (low - prev_close).abs()],
            axis=1,
        ).max(axis=1)
        return tr.rolling(period, min_periods=period // 2).mean()

    df["rsi14"] = _rsi(df["close"], 14)
    df["atr14"] = _atr(df, 14)

    # ヒゲ比率（レンジのどこで引けたか）
    rng = (df["high"] - df["low"]).replace(0, np.nan)
    df["pos_in_range"] = (df["close"] - df["low"]) / rng

    # 出来高代理（あれば）
    if "tick_volume" in df.columns:
        df["vol_sma20"] = df["tick_volume"].rolling(20, min_periods=10).mean()
        df["vol_chg"] = df["tick_volume"].pct_change()

    feature_cols = [
        "ret1",
        "ret3",
        "ret5",
        "vol20",
        "sma5",
        "sma10",
        "sma20",
        "sma50",
        "ema5",
        "ema10",
        "ema20",
        "ema50",
        "bb_p",
        "rsi14",
        "atr14",
        "pos_in_range",
        "vol_sma20",
        "vol_chg",
    ]
    feature_cols = [c for c in feature_cols if c in df.columns]

    keep_cols = ["time", "open", "high", "low", "close"] + feature_cols
    df = df[keep_cols].copy()

    # 先頭のNaNを一括でトリム（最大ウィンドウ50に合わせる）
    trim = 50
    if len(df) > trim:
        df = df.iloc[trim:].copy()

    # それでも残るNaN/infは除去
    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    return df


def make_label(df: pd.DataFrame, horizon: int = 10, pips: float = 0.0) -> pd.Series:
    """
    horizon 後の方向ラベル:
      close_{t+h} - close_t > 0 なら 1, それ以外 0
    pips を与えた場合は閾値として使う（pipsは価格差 0.01=1pips 相当の口座もあるので注意）
    """
    future = df["close"].shift(-horizon)
    diff = future - df["close"]
    if pips and pips > 0:
        y = (diff > pips).astype(int)
    else:
        y = (diff > 0).astype(int)
    return y


# ------------------------------------------------------------
# WFO スキーム
# ------------------------------------------------------------
@dataclass
class WFOMetrics:
    fold: int
    train_start: str
    train_end: str
    test_start: str
    test_end: str
    n_train: int
    n_test: int
    auc: float
    logloss: float
    f1_at_thr: float
    thr: float


def iter_wfo_slices(df: pd.DataFrame, train_bars: int, test_bars: int, step_bars: int):
    """
    walk-forward: 固定長学習→固定長テスト→stepで前進
    """
    n = len(df)
    start = 0
    fold = 0
    while True:
        train_start = start
        train_end = train_start + train_bars
        test_end = train_end + test_bars
        if test_end > n:
            break

        yield fold, slice(train_start, train_end), slice(train_end, test_end)
        fold += 1
        start += step_bars


# ------------------------------------------------------------
# しきい値最適化
# ------------------------------------------------------------
def pick_threshold(y_true: np.ndarray, prob: np.ndarray) -> tuple[float, float]:
    """
    PR 曲線から F1 最大点を採用。閾値を返す。
    """
    precision, recall, thresholds = precision_recall_curve(y_true, prob)
    # thresholds の長さは len(precision)-1
    f1 = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-12)
    idx = int(np.nanargmax(f1))
    best_thr = float(np.clip(thresholds[idx], 0.05, 0.95))
    return best_thr, float(f1[idx])


# ------------------------------------------------------------
# 学習（LightGBM）
# ------------------------------------------------------------
def train_lgbm(X: pd.DataFrame, y: pd.Series) -> lgbm.LGBMClassifier:
    params = dict(
        objective="binary",
        boosting_type="gbdt",
        n_estimators=400,
        learning_rate=0.05,
        num_leaves=63,
        max_depth=-1,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_alpha=0.0,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=1,  # VPS 2GB 想定で控えめ
        verbose=-1,
    )
    model = lgbm.LGBMClassifier(**params)
    # DataFrame のまま渡す（列順・名前を維持）
    model.fit(X, y)
    return model


# ------------------------------------------------------------
# メイン
# ------------------------------------------------------------
def main():
    # 引数なしでも安全に動くように、デフォルト値と説明を追加
    ap = argparse.ArgumentParser(
        description="LightGBM walk-forward retrain",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    ap.add_argument(
        "--symbol",
        default="USDJPY-",
        help="例: USDJPY- （未指定時は安全なデフォルト）",
    )
    ap.add_argument(
        "--timeframe",
        default="M5",
        help="例: M5, M15, H1",
    )
    ap.add_argument("--horizon", type=int, default=10, help="予測先 (bars)")
    ap.add_argument(
        "--train_bars",
        type=int,
        default=90_000,
        help="学習バー数（例: 90k≈数ヶ月~年）",
    )
    ap.add_argument(
        "--test_bars",
        type=int,
        default=7_000,
        help="テストバー数（例: 1週間分くらい）",
    )
    ap.add_argument(
        "--step_bars",
        type=int,
        default=7_000,
        help="前進幅（通常 test_bars と同じ）",
    )
    ap.add_argument(
        "--model_name",
        default="LightGBM_clf",
        help="保存名のベース",
    )
    ap.add_argument(
        "--data-dir",
        type=str,
        default=None,
        help="CSVのルートディレクトリ（未指定なら FXBOT_DATA / PROJECT_ROOT/data / ./data の順で探索）",
    )
    # 危険操作制御フラグ
    ap.add_argument(
        "--apply",
        action="store_true",
        help="新しいモデルとしきい値を active_model.json に反映する",
    )
    ap.add_argument(
        "--dry-run",
        action="store_true",
        help="モデル評価のみ行い、active_model.json などは一切更新しない",
    )
    args = ap.parse_args()

    safe_log(
        f"[WFO] start walkforward retrain | symbol={args.symbol} tf={args.timeframe}"
    )

    # CSV 探索 & 読み込み
    csv_path = find_csv(args.symbol, args.timeframe, data_dir=args.data_dir)
    print(f"[retrain] using CSV: {csv_path}")
    safe_log(f"[WFO] load csv: {csv_path}")
    df_raw = pd.read_csv(csv_path)

    # 最低限の列チェック
    need_cols = {"time", "open", "high", "low", "close"}
    missing = need_cols - set(df_raw.columns)
    if missing:
        raise ValueError(f"CSV に必要な列が不足しています: {missing}")

    # 特徴量
    feats = build_features(df_raw)
    if feats.empty:
        safe_log("[WFO] feature building aborted (not enough rows).")
        sys.exit(1)

    # ラベル
    y = make_label(feats, args.horizon)
    feats = feats.iloc[: -args.horizon, :].reset_index(drop=True)
    y = y.iloc[: -args.horizon].reset_index(drop=True)

    # 特徴量行数チェック
    if feats.shape[0] == 0:
        safe_log(
            "[WFO][error] no rows after feature engineering + horizon alignment. "
            "Likely because rows <= horizon. Provide a longer CSV or reduce --horizon."
        )
        sys.exit(1)

    # 説明変数
    drop_cols = ["time", "open", "high", "low", "close"]
    X = feats.drop(columns=[c for c in drop_cols if c in feats.columns])

    # 念のための欠損除去
    mask = ~X.isna().any(axis=1)
    X, y = X[mask], y[mask]
    X = X.astype(np.float32)

    n_total = len(X)
    if n_total < (args.train_bars + args.test_bars + 1):
        # データが少ない場合は 80/20 の単純スプリットで学習→保存のみ
        safe_log("[WFO] dataset is small; using simple 80/20 split instead of WFO.")
        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, shuffle=False)

        clf = train_lgbm(Xtr, ytr)
        prob = clf.predict_proba(Xte)[:, 1]  # DataFrameのまま渡している
        auc = float(roc_auc_score(yte, prob))
        ll = float(log_loss(yte, np.clip(prob, 1e-6, 1 - 1e-6)))

        thr, f1 = pick_threshold(yte.values, prob)
        safe_log(
            f"[WFO] simple-split auc={auc:.4f} logloss={ll:.4f} thr={thr:.3f} f1={f1:.3f}"
        )

        # 全データ再学習→保存
        final_clf = train_lgbm(X, y)
        model_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = MODELS_DIR / f"{args.model_name}_{model_ts}.pkl"
        dump(final_clf, model_path)
        meta = {
            "model_name": args.model_name,
            "version": model_ts,
            "features": list(X.columns),
            "horizon": args.horizon,
            "metrics": {"auc": auc, "logloss": ll, "thr": thr, "f1": f1},
            "source_csv": str(csv_path.name),
        }
        meta_path = MODELS_DIR / f"{args.model_name}_{model_ts}.meta.json"
        meta_path.write_text(
            json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
        )

        # アクティブモデル更新（--apply のときだけ）
        safe_log(f"[WFO] wrote: {model_path.name}, {meta_path.name}")
        if args.dry_run:
            safe_log(
                "[WFO] DRY-RUN のため active_model.json は更新しません。"
            )
        elif not args.apply:
            safe_log(
                "[WFO] --apply が指定されていないため active_model.json は更新しません。"
            )
        else:
            active = {
                "model_file": str(model_path.name),
                "meta_file": str(meta_path.name),
                "best_threshold": thr,
                "updated_at": jst_now_str(),
            }
            (MODELS_DIR / "active_model.json").write_text(
                json.dumps(active, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            safe_log(
                f"[WFO] active_model.json updated (best_threshold={thr:.3f})"
            )
        return

    # --- WFO ---
    safe_log(
        f"[WFO] bars: total={n_total} train={args.train_bars} "
        f"test={args.test_bars} step={args.step_bars}"
    )

    metrics: list[WFOMetrics] = []
    prob_oof = np.full(n_total, np.nan, dtype=np.float64)
    thr_list: list[float] = []

    for fold, s_tr, s_te in iter_wfo_slices(
        X, args.train_bars, args.test_bars, args.step_bars
    ):
        Xtr, ytr = X.iloc[s_tr], y.iloc[s_tr]
        Xte, yte = X.iloc[s_te], y.iloc[s_te]

        # 学習
        clf = train_lgbm(Xtr, ytr)

        # 予測（DataFrameのまま）
        proba = clf.predict_proba(Xte)[:, 1]

        # メトリクス
        try:
            auc = float(roc_auc_score(yte, proba))
        except ValueError:
            auc = float("nan")

        ll = float(log_loss(yte, np.clip(proba, 1e-6, 1 - 1e-6)))
        thr, f1 = pick_threshold(yte.values, proba)

        # OOF へ
        prob_oof[s_te] = proba
        thr_list.append(thr)

        # 期間情報
        t_idx = feats.iloc[s_tr, :]["time"]
        tr_start = str(t_idx.iloc[0]) if len(t_idx) else ""
        tr_end = str(t_idx.iloc[-1]) if len(t_idx) else ""
        t_idx2 = feats.iloc[s_te, :]["time"]
        te_start = str(t_idx2.iloc[0]) if len(t_idx2) else ""
        te_end = str(t_idx2.iloc[-1]) if len(t_idx2) else ""

        m = WFOMetrics(
            fold=fold,
            train_start=tr_start,
            train_end=tr_end,
            test_start=te_start,
            test_end=te_end,
            n_train=len(Xtr),
            n_test=len(Xte),
            auc=auc,
            logloss=ll,
            f1_at_thr=f1,
            thr=thr,
        )
        metrics.append(m)
        safe_log(
            f"[WFO][fold {fold}] auc={auc:.4f} logloss={ll:.4f} "
            f"thr={thr:.3f} f1={f1:.3f} n={len(Xtr)}/{len(Xte)}"
        )

    # WFO 全体まとめ
    valid_idx = ~np.isnan(prob_oof)
    if valid_idx.sum() == 0:
        safe_log("[WFO] no valid test predictions; abort.")
        sys.exit(1)

    y_oof = y.values[valid_idx]
    p_oof = prob_oof[valid_idx]
    auc_oof = float(roc_auc_score(y_oof, p_oof))
    ll_oof = float(log_loss(y_oof, np.clip(p_oof, 1e-6, 1 - 1e-6)))
    thr_oof, f1_oof = pick_threshold(y_oof, p_oof)

    # 少し引き気味に（過適合/ズレ対策で 0.95 を掛ける）
    best_thr = float(np.clip(thr_oof * 0.95, 0.05, 0.95))

    safe_log(
        f"[WFO][OOF] auc={auc_oof:.4f} logloss={ll_oof:.4f} "
        f"thr*={best_thr:.3f} (raw={thr_oof:.3f}) f1={f1_oof:.3f}"
    )

    # 全データで最終モデル
    final_clf = train_lgbm(X, y)
    model_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = MODELS_DIR / f"{args.model_name}_{model_ts}.pkl"
    dump(final_clf, model_path)

    meta = {
        "model_name": args.model_name,
        "version": model_ts,
        "features": list(X.columns),
        "horizon": args.horizon,
        "oof_metrics": {
            "auc": auc_oof,
            "logloss": ll_oof,
            "thr_oof": thr_oof,
            "f1_oof": f1_oof,
            "thr_final": best_thr,
        },
        "folds": [asdict(m) for m in metrics],
        "source_csv": str(csv_path.name),
        "bars": {
            "total": n_total,
            "train": args.train_bars,
            "test": args.test_bars,
            "step": args.step_bars,
        },
    }
    meta_path = MODELS_DIR / f"{args.model_name}_{model_ts}.meta.json"
    meta_path.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
    )

    safe_log(f"[WFO] wrote: {model_path.name}, {meta_path.name}")

    # active_model.json 更新（GUI/実運用が読むファイル）: --apply のときだけ
    if args.dry_run:
        safe_log(
            "[WFO] DRY-RUN のため active_model.json は更新しません。"
        )
    elif not args.apply:
        safe_log(
            "[WFO] --apply が指定されていないため active_model.json は更新しません。"
        )
    else:
        active = {
            "model_file": str(model_path.name),
            "meta_file": str(meta_path.name),
            "best_threshold": best_thr,
            "updated_at": jst_now_str(),
        }
        (MODELS_DIR / "active_model.json").write_text(
            json.dumps(active, ensure_ascii=False, indent=2), encoding="utf-8"
        )
        safe_log(
            f"[WFO] active_model.json updated (best_threshold={best_thr:.3f})"
        )
    safe_log("[WFO] done.")


if __name__ == "__main__":
    main()



=== file: scripts/walkforward_train.py ===

# scripts/walkforward_train.py
from __future__ import annotations
import argparse, os, json, time, shutil, hashlib, random
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Tuple
import numpy as np
import pandas as pd

# 依存がなければロジ回帰にフォールバック
try:
    import lightgbm as lgb
    HAVE_LGB = True
except Exception:
    from sklearn.linear_model import LogisticRegression
    HAVE_LGB = False
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.calibration import CalibratedClassifierCV
import pickle

MODELS_DIR = "models"
DATA_DIR = "data"

def _now_iso() -> str:
    return datetime.now(timezone.utc).astimezone().isoformat(timespec="seconds")

def _sha256(path: str) -> str:
    import hashlib
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for ch in iter(lambda: f.read(8192), b""):
            h.update(ch)
    return h.hexdigest()

def load_dataset(csv_glob: str) -> pd.DataFrame:
    import glob
    files = sorted(glob.glob(os.path.join(DATA_DIR, csv_glob)))
    if not files:
        raise FileNotFoundError(f"No dataset CSVs under data/ matched: {csv_glob}")
    dfs = []
    for f in files:
        df = pd.read_csv(f)
        dfs.append(df)
    df = pd.concat(dfs, axis=0, ignore_index=True)
    # 期待カラム: time, open, high, low, close, label (0/1 for BUY=1), ...features...
    if "label" not in df.columns:
        raise ValueError("dataset must contain 'label' column (0/1)")
    return df

def split_walkforward(df: pd.DataFrame, weeks_train: int, weeks_valid: int, steps: int) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
    """
    直近から遡るWF。週単位で train/valid を切って steps 回。
    """
    if "time" in df.columns:
        dt = pd.to_datetime(df["time"])
    else:
        # 疑似時系列
        base = datetime(2020,1,1)
        dt = pd.Series([base + timedelta(minutes=i) for i in range(len(df))])
    df = df.copy()
    df["__dt__"] = dt

    spans = []
    end = df["__dt__"].max()
    for k in range(steps):
        valid_end = end - timedelta(weeks=k*weeks_valid)
        valid_start = valid_end - timedelta(weeks=weeks_valid)
        train_end = valid_start
        train_start = train_end - timedelta(weeks=weeks_train)

        tr = df[(df["__dt__"]>=train_start) & (df["__dt__"]<train_end)]
        va = df[(df["__dt__"]>=valid_start) & (df["__dt__"]<valid_end)]
        if len(tr) < 100 or len(va) < 100:
            continue
        spans.append((tr, va))
    spans.reverse()  # 古い→新しい
    return spans[-1:]  # 直近の1ステップだけで十分（高速）

def _train_one(tr: pd.DataFrame, va: pd.DataFrame, features: List[str]) -> Tuple[Any, Any, Dict[str, float]]:
    Xtr, ytr = tr[features].values, tr["label"].values
    Xva, yva = va[features].values, va["label"].values

    if HAVE_LGB:
        clf = lgb.LGBMClassifier(
            n_estimators=300, learning_rate=0.05, max_depth=-1,
            subsample=0.8, colsample_bytree=0.8, random_state=42
        )
    else:
        clf = LogisticRegression(max_iter=200)

    clf.fit(Xtr, ytr)
    pva = clf.predict_proba(Xva)[:,1]
    auc = roc_auc_score(yva, pva)
    acc = accuracy_score(yva, (pva>=0.5).astype(int))

    # キャリブレータ（isotonic優先、フォールバックはsigmoid）
    try:
        cal = CalibratedClassifierCV(base_estimator=clf, method="isotonic", cv=5)
        cal.fit(Xtr, ytr)
        cal_name = "isotonic"
    except Exception:
        cal = CalibratedClassifierCV(base_estimator=clf, method="sigmoid", cv=5)
        cal.fit(Xtr, ytr)
        cal_name = "platt"

    return clf, cal, {"auc": float(auc), "acc": float(acc), "calibrator": cal_name}

def _features_from(df: pd.DataFrame) -> List[str]:
    # 最低限：OHLCや派生が入っている前提。label/time/非数値は除外。
    feats = [c for c in df.columns if c not in ("time","label") and pd.api.types.is_numeric_dtype(df[c])]
    if not feats:
        raise ValueError("no numeric features found.")
    return feats

def save_bundle(tag: str, clf: Any, cal: Any, features: List[str], classes: Dict[str, int], metrics: Dict[str, Any]) -> str:
    out = os.path.join(MODELS_DIR, tag)
    os.makedirs(out, exist_ok=True)

    with open(os.path.join(out, "LightGBM_clf.pkl"), "wb") as f:
        pickle.dump(clf, f)
    with open(os.path.join(out, "features.json"), "w", encoding="utf-8") as f:
        json.dump(features, f, ensure_ascii=False)
    with open(os.path.join(out, "classes.json"), "w", encoding="utf-8") as f:
        json.dump(classes, f, ensure_ascii=False)

    if metrics.get("calibrator") == "isotonic":
        with open(os.path.join(out, "calib_isotonic.pkl"), "wb") as f:
            pickle.dump(cal, f)
    else:
        with open(os.path.join(out, "calib_platt.pkl"), "wb") as f:
            pickle.dump(cal, f)

    # manifest
    sums = {}
    for name in ("LightGBM_clf.pkl","features.json","classes.json","calib_isotonic.pkl","calib_platt.pkl"):
        p = os.path.join(out, name)
        if os.path.exists(p):
            sums[name] = _sha256(p)

    mani = {
        "tag": tag,
        "ts": _now_iso(),
        "metrics": metrics,
        "sha256": sums,
        "features_hash": hashlib.sha256(json.dumps(features).encode()).hexdigest(),
    }
    with open(os.path.join(out, "manifest.json"), "w", encoding="utf-8") as f:
        json.dump(mani, f, ensure_ascii=False, indent=2)

    # READY は最後に
    with open(os.path.join(out, "READY"), "w") as f:
        f.write("ok\n")
    return out

def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", default="*.csv", help="data/ 以下で読むCSVのglob")
    ap.add_argument("--weeks-train", type=int, default=156, help="学習期間（週）=3年")
    ap.add_argument("--weeks-valid", type=int, default=12,  help="検証期間（週）=3ヶ月")
    ap.add_argument("--min-auc", type=float, default=0.55,  help="差し替えの最低AUC")
    ap.add_argument("--tag", default=None, help="出力タグ（デフォルトは日時）")
    args = ap.parse_args()

    df = load_dataset(args.csv)
    feats = _features_from(df)
    spans = split_walkforward(df, args.weeks_train, args.weeks_valid, steps=3)
    if not spans:
        raise SystemExit("not enough data for walk-forward.")

    tr, va = spans[-1]
    clf, cal, m = _train_one(tr, va, feats)

    print(f"[METRICS] AUC={m['auc']:.4f} ACC={m['acc']:.4f} CAL={m['calibrator']}")

    if m["auc"] < args.min_auc:
        print(f"[SKIP] AUC {m['auc']:.4f} < min_auc {args.min_auc}")
        return

    tag = args.tag or datetime.now().strftime("lgb_%Y%m%d_%H%M%S")
    bundle = save_bundle(tag, clf, cal, feats, {"BUY":1, "SELL":0}, m)
    print(f"[READY] {bundle}")

if __name__ == "__main__":
    main()



=== file: scripts/weekly_retrain.py ===

#!/usr/bin/env python
"""
scripts/weekly_retrain.py

週次自動再学習ジョブ用スクリプト。

    データ取得
    -> 特徴量作成
    -> LightGBM 学習
    -> Walk-Forward 検証
    -> しきい値最適化
    -> モデル保存 & 署名 (active_model.json 更新)

前提:
- ルート直下 (fxbot/) から実行すること
- 設定: configs/config.yaml もしくは --config で指定
- 価格CSV: data/USDJPY/ohlcv/USDJPY_M5.csv のような構造
"""

from __future__ import annotations

import argparse
import hashlib
import json
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path

import lightgbm as lgb
import numpy as np
import numpy.typing as npt
import pandas as pd
import yaml
from joblib import dump
from loguru import logger

# ---- 定数 (Ruff の magic number 対策も兼ねる) -----------------------------

MIN_WFO_SPLITS: int = 2
DEFAULT_CLASS_THRESHOLD: float = 0.5

JST = UTC  # 後で必要なら Asia/Tokyo に変更してもOK


# ------------------------
# 設定読み込みまわり
# ------------------------


@dataclass
class PathsConfig:
    data_dir: Path
    models_dir: Path
    logs_dir: Path


@dataclass
class RetrainConfig:
    symbol: str
    timeframe: str
    label_horizon: int = 10  # 何バー先をラベルにするか
    min_pips: float = 1.0  # クラス分けに使う最小pips
    n_splits: int = 4  # Walk-Forward の分割数
    threshold_grid: list[float] | None = None  # None を許容

    def __post_init__(self) -> None:
        if self.threshold_grid is None:
            # DEFAULT_CLASS_THRESHOLD を中心に、少し前後を見る
            self.threshold_grid = [
                DEFAULT_CLASS_THRESHOLD - 0.05,
                DEFAULT_CLASS_THRESHOLD,
                DEFAULT_CLASS_THRESHOLD + 0.05,
                DEFAULT_CLASS_THRESHOLD + 0.10,
                DEFAULT_CLASS_THRESHOLD + 0.15,
            ]


@dataclass
class WeeklyRetrainConfig:
    paths: PathsConfig
    retrain: RetrainConfig


def load_config(config_path: Path) -> WeeklyRetrainConfig:
    """YAML 設定のロード。"""

    if not config_path.exists():
        raise FileNotFoundError(f"config.yaml が見つかりません: {config_path}")

    with config_path.open("r", encoding="utf-8") as f:
        raw = yaml.safe_load(f)

    paths_raw = raw.get("paths", {}) or {}
    runtime_raw = raw.get("runtime", {}) or {}
    ai_raw = raw.get("ai", {}) or {}
    retrain_raw = ai_raw.get("retrain", {}) or {}

    data_dir = Path(paths_raw.get("data_dir", "./data")).expanduser()
    models_dir = Path(paths_raw.get("models_dir", "./models")).expanduser()
    logs_dir = Path(paths_raw.get("logs_dir", "./logs")).expanduser()

    symbol = runtime_raw.get("symbol", "USDJPY")
    timeframe = runtime_raw.get("timeframe_exec", "M5")

    label_horizon = int(retrain_raw.get("label_horizon_bars", 10))
    min_pips = float(retrain_raw.get("min_pips", 1.0))
    n_splits = int(retrain_raw.get("wfo_n_splits", 4))

    thr_raw = retrain_raw.get("threshold_grid")
    if thr_raw is None:
        threshold_grid: list[float] | None = None
    else:
        threshold_grid = [float(x) for x in thr_raw]

    cfg = WeeklyRetrainConfig(
        paths=PathsConfig(
            data_dir=data_dir,
            models_dir=models_dir,
            logs_dir=logs_dir,
        ),
        retrain=RetrainConfig(
            symbol=symbol,
            timeframe=timeframe,
            label_horizon=label_horizon,
            min_pips=min_pips,
            n_splits=n_splits,
            threshold_grid=threshold_grid,
        ),
    )
    return cfg


# ------------------------
# データ & 特徴量
# ------------------------


def load_price_data(csv_path: Path) -> pd.DataFrame:
    """MT5 から書き出した価格CSVを読み込む。"""

    if not csv_path.exists():
        raise FileNotFoundError(f"価格CSVが見つかりません: {csv_path}")

    df = pd.read_csv(csv_path)
    if "time" not in df.columns:
        raise ValueError("CSV に 'time' 列がありません。")

    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time").reset_index(drop=True)

    # 列名のゆらぎに対応
    vol_col: str | None = None
    for cand in ("tick_volume", "volume", "vol"):
        if cand in df.columns:
            vol_col = cand
            break
    if vol_col is None:
        df["volume"] = 0.0
        vol_col = "volume"

    for col in ("open", "high", "low", "close"):
        if col not in df.columns:
            raise ValueError(f"CSV に '{col}' 列がありません。")

    return df[["time", "open", "high", "low", "close", vol_col]].rename(
        columns={vol_col: "volume"}
    )


def compute_rsi(close: pd.Series, period: int = 14) -> pd.Series:
    diff = close.diff()
    gain = diff.clip(lower=0)
    loss = -diff.clip(upper=0)
    avg_gain = gain.ewm(alpha=1 / period, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1 / period, adjust=False).mean()
    rs = avg_gain / (avg_loss + 1e-9)
    rsi = 100 - (100 / (1 + rs))
    return rsi


def compute_atr(
    high: pd.Series,
    low: pd.Series,
    close: pd.Series,
    period: int = 14,
) -> pd.Series:
    prev_close = close.shift(1)
    tr1 = high - low
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = tr.ewm(alpha=1 / period, adjust=False).mean()
    return atr


def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    非常にシンプルな特徴量セット。
    後で core/feature_pipeline.py に差し替えてもOK。
    """

    out = pd.DataFrame(index=df.index)

    out["ret_1"] = df["close"].pct_change()
    out["ret_5"] = df["close"].pct_change(5)
    out["ema_5"] = df["close"].ewm(span=5, adjust=False).mean()
    out["ema_20"] = df["close"].ewm(span=20, adjust=False).mean()
    out["ema_ratio"] = out["ema_5"] / (out["ema_20"] + 1e-9)

    out["rsi_14"] = compute_rsi(df["close"], period=14)
    out["atr_14"] = compute_atr(df["high"], df["low"], df["close"], period=14)

    out["range"] = (df["high"] - df["low"]) / (df["close"].shift(1) + 1e-9)
    out["vol_chg"] = df["volume"].pct_change().fillna(0.0)

    out = out.replace([np.inf, -np.inf], np.nan)
    out = out.dropna()
    return out


def build_labels(
    df: pd.DataFrame,
    horizon: int = 10,
    min_pips: float = 1.0,
) -> pd.Series:
    """
    horizon 足後の方向ラベルを作る。
    - USDJPY 前提で 1pips = 0.01 として計算。
    - 上昇(min_pips超) = 1, 下降(min_pips超) = 0
      それ以外（変化が小さい）は NaN にして除外。
    """

    future = df["close"].shift(-horizon)
    delta = future - df["close"]
    pips = delta * 100.0  # USDJPY 前提
    y = pd.Series(index=df.index, dtype="float32")
    y[pips >= min_pips] = 1.0
    y[pips <= -min_pips] = 0.0
    return y


def align_features_and_labels(
    feats: pd.DataFrame,
    labels: pd.Series,
) -> tuple[pd.DataFrame, pd.Series]:
    df = feats.join(labels.rename("y"), how="left")
    df = df.dropna()
    y = df.pop("y").astype(int)
    X = df
    return X, y


# ------------------------
# Walk-Forward 検証 & 学習
# ------------------------


@dataclass
class FoldResult:
    fold: int
    train_start: str
    train_end: str
    val_start: str
    val_end: str
    logloss: float
    accuracy: float
    n_train: int
    n_val: int


@dataclass
class WFOResult:
    folds: list[FoldResult]
    mean_logloss: float
    mean_accuracy: float


def iter_walkforward_indices(
    n_samples: int,
    n_splits: int,
) -> list[tuple[np.ndarray, np.ndarray]]:
    """
    非常にシンプルな walk-forward。
    - データは既に time でソートされている前提
    - n_splits+1 個のブロックに分割し、前方累積を train、次ブロックを val にする
    """

    if n_splits < MIN_WFO_SPLITS:
        raise ValueError("n_splits は最低 2 以上を推奨します。")

    block = n_samples // (n_splits + 1)
    indices = np.arange(n_samples, dtype=int)

    splits: list[tuple[np.ndarray, np.ndarray]] = []
    for k in range(n_splits):
        train_end = block * (k + 1)
        val_end = block * (k + 2)
        if val_end <= train_end:
            break
        train_idx = indices[:train_end]
        val_idx = indices[train_end:val_end]
        splits.append((train_idx, val_idx))
    return splits


def train_lightgbm_wfo(
    X: pd.DataFrame,
    y: pd.Series,
    cfg: RetrainConfig,
) -> Tuple[WFOResult, List[lgb.Booster], npt.NDArray[np.float64]]:
    params: dict[str, object] = {
        "objective": "binary",
        "metric": ["binary_logloss"],
        "learning_rate": 0.05,
        "num_leaves": 31,
        "max_depth": -1,
        "min_data_in_leaf": 50,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "verbosity": -1,
        "force_col_wise": True,
    }

    n = len(X)
    splits = iter_walkforward_indices(n, cfg.n_splits)

    oof_pred: npt.NDArray[np.float_] = np.full(
        shape=n,
        fill_value=np.nan,
        dtype="float32",
    )
    boosters: list[lgb.Booster] = []
    fold_results: list[FoldResult] = []

    for fold_idx, (tr_idx, va_idx) in enumerate(splits):
        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]
        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]

        train_data = lgb.Dataset(X_tr, label=y_tr)
        valid_data = lgb.Dataset(X_va, label=y_va)

        logger.info(
            f"[WFO] fold={fold_idx} train={len(X_tr)} val={len(X_va)} "
            f"from={tr_idx[0]} to={va_idx[-1]}"
        )

        booster = lgb.train(
            params,
            train_data,
            num_boost_round=500,
            valid_sets=[valid_data],
            valid_names=["valid"],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
            ],
        )

        boosters.append(booster)

        y_proba: npt.NDArray[np.float_] = booster.predict(
            X_va,
            num_iteration=booster.best_iteration,
        )
        oof_pred[va_idx] = y_proba.astype("float32")

        # メトリクス
        eps = 1e-15
        y_clipped: npt.NDArray[np.float_] = np.clip(
            y_proba,
            eps,
            1 - eps,
        )
        logloss = float(
            -np.mean(y_va * np.log(y_clipped) + (1 - y_va) * np.log(1 - y_clipped))
        )

        preds_label = (y_proba >= DEFAULT_CLASS_THRESHOLD).astype(int)
        acc = float(((y_va == preds_label).sum()) / len(y_va))

        fold_results.append(
            FoldResult(
                fold=fold_idx,
                train_start=str(tr_idx[0]),
                train_end=str(tr_idx[-1]),
                val_start=str(va_idx[0]),
                val_end=str(va_idx[-1]),
                logloss=logloss,
                accuracy=acc,
                n_train=int(len(X_tr)),
                n_val=int(len(X_va)),
            )
        )

        logger.info(f"[WFO] fold={fold_idx} logloss={logloss:.5f} acc={acc:.4f}")

    valid_mask = ~np.isnan(oof_pred)
    mean_logloss = float("nan")
    mean_accuracy = float("nan")
    if valid_mask.sum() > 0:
        y_valid_arr: npt.NDArray[np.int_] = y[valid_mask].to_numpy()
        p_valid: npt.NDArray[np.float_] = oof_pred[valid_mask]

        eps = 1e-15
        p_clip = np.clip(p_valid, eps, 1 - eps)
        mean_logloss = float(
            -np.mean(
                y_valid_arr * np.log(p_clip) + (1 - y_valid_arr) * np.log(1 - p_clip)
            )
        )
        preds_valid = (p_valid >= DEFAULT_CLASS_THRESHOLD).astype(int)
        mean_accuracy = float((y_valid_arr == preds_valid).sum() / len(y_valid_arr))

    wfo_result = WFOResult(
        folds=fold_results,
        mean_logloss=mean_logloss,
        mean_accuracy=mean_accuracy,
    )

    return wfo_result, boosters, oof_pred


# ------------------------
# しきい値最適化
# ------------------------


def optimize_threshold(
    y: pd.Series,
    oof_pred: npt.NDArray[np.float_],
    threshold_grid: list[float],
) -> dict[str, float]:
    """
    非常にシンプルな「1トレード +1 / -1」の疑似損益で最適なしきい値を決める。
    """

    valid_mask = ~np.isnan(oof_pred)
    y_valid_arr: npt.NDArray[np.int_] = y[valid_mask].to_numpy()
    p_valid: npt.NDArray[np.float_] = oof_pred[valid_mask]

    best_thr = DEFAULT_CLASS_THRESHOLD
    best_score = -1e9
    results: list[tuple[float, float, float]] = []

    for thr in threshold_grid:
        trade_mask = p_valid >= thr
        if trade_mask.sum() == 0:
            continue

        y_tr = y_valid_arr[trade_mask]
        pnl = np.where(y_tr == 1, 1.0, -1.0)
        equity = pnl.cumsum()
        total = float(equity[-1])
        winrate = float((pnl > 0).sum() / len(pnl))
        results.append((thr, total, winrate))

        if total > best_score:
            best_score = total
            best_thr = thr

    logger.info(
        "[THR] grid_results="
        + ", ".join(
            f"thr={thr:.3f} total={total:.1f} win={win:.3f}"
            for thr, total, win in results
        )
    )
    logger.info(f"[THR] best_thr={best_thr:.3f} equity={best_score:.1f}")

    return {
        "best_threshold": float(best_thr),
        "best_equity": float(best_score),
    }


def save_wfo_report_and_equity(
    cfg: WeeklyRetrainConfig,
    df_prices: pd.DataFrame,
    X: pd.DataFrame,
    y: pd.Series,
    oof_pred: npt.NDArray[np.float_],
    wfo_result: WFOResult,
    thr_info: dict[str, float],
) -> str:
    """
    Walk-Forward の結果サマリ (report_*.json) と
    擬似的な train/test エクイティカーブ (equity_train_*.csv / equity_test_*.csv)
    を logs/retrain/ 以下に出力する。
    戻り値は run_id (ファイル名の *_run_id 部分)。
    """

    # 出力先ディレクトリを作成
    base_dir = cfg.paths.logs_dir / "retrain"
    base_dir.mkdir(parents=True, exist_ok=True)

    # 一意なIDをタイムスタンプから作る
    ts = datetime.now(tz=UTC)
    run_id = str(int(ts.timestamp()))

    # ---- エクイティ用の下ごしらえ ---------------------------------
    # X と y は df_prices の一部なので、その time を合わせる
    # （align_features_and_labels のあとの X.index は df_prices.index のサブセット）
    df_all = pd.DataFrame(
        {
            "time": df_prices.loc[X.index, "time"].to_numpy(),
            "y": y.to_numpy(),
            "proba": oof_pred,
        }
    ).reset_index(drop=True)

    # NaN は「トレードしない」とみなす
    best_thr = float(thr_info.get("best_threshold", DEFAULT_CLASS_THRESHOLD))

    def make_equity_curve(df: pd.DataFrame) -> tuple[pd.DataFrame, dict[str, float]]:
        """
        proba >= best_thr のときだけ「+1 / -1」のトレードとして
        疑似エクイティを作る。
        equity: 累積損益（初期 0）
        signal: +1 (勝ちトレード), -1 (負けトレード), 0 (ノートレード)
        """
        equity_list: list[float] = []
        signal_list: list[int] = []
        pnl_list: list[float] = []

        equity = 0.0
        for _, row in df.iterrows():
            p = float(row["proba"])
            sig = 0
            if not np.isnan(p) and p >= best_thr:
                pnl = 1.0 if int(row["y"]) == 1 else -1.0
                equity += pnl
                pnl_list.append(pnl)
                sig = 1 if pnl > 0 else -1

            equity_list.append(equity)
            signal_list.append(sig)

        out = pd.DataFrame(
            {
                "time": df["time"].to_numpy(),
                "equity": equity_list,
                "signal": signal_list,
            }
        )

        n_trades = len(pnl_list)
        total = float(sum(pnl_list)) if pnl_list else 0.0
        wins = float(sum(p > 0 for p in pnl_list)) if pnl_list else 0.0
        gross_profit = float(sum(p for p in pnl_list if p > 0.0))
        gross_loss = float(-sum(p for p in pnl_list if p < 0.0))
        pf = gross_profit / gross_loss if gross_loss > 0 else float("inf")
        winrate = wins / n_trades if n_trades > 0 else 0.0

        stats = {
            "n_trades": float(n_trades),
            "total_pnl": total,
            "win_rate": winrate,
            "gross_profit": gross_profit,
            "gross_loss": gross_loss,
            "profit_factor": pf,
        }
        return out, stats

    # ざっくり 70% を train、残り 30% を test として分割
    n_all = len(df_all)
    split = int(n_all * 0.7)
    df_train = df_all.iloc[:split].copy()
    df_test = df_all.iloc[split:].copy()

    eq_train_df, stats_train = make_equity_curve(df_train)
    eq_test_df, stats_test = make_equity_curve(df_test)

    # ---- CSV 出力 ----------------------------------------------------
    equity_train_path = base_dir / f"equity_train_{run_id}.csv"
    equity_test_path = base_dir / f"equity_test_{run_id}.csv"

    eq_train_df.to_csv(equity_train_path, index=False)
    eq_test_df.to_csv(equity_test_path, index=False)

    logger.info(f"[WFO] equity_train saved: {equity_train_path}")
    logger.info(f"[WFO] equity_test  saved: {equity_test_path}")

    # ---- JSON レポート出力 -------------------------------------------
    report = {
        "run_id": run_id,
        "created_at_utc": ts.isoformat(),
        "symbol": cfg.retrain.symbol,
        "timeframe": cfg.retrain.timeframe,
        "label_horizon_bars": cfg.retrain.label_horizon,
        "min_pips": cfg.retrain.min_pips,
        "n_samples": int(len(X)),
        "wfo": {
            "mean_logloss": wfo_result.mean_logloss,
            "mean_accuracy": wfo_result.mean_accuracy,
            "folds": [asdict(f) for f in wfo_result.folds],
        },
        "threshold": thr_info,
        "equity_train_stats": stats_train,
        "equity_test_stats": stats_test,
        "data_range": {
            "from": df_prices["time"].min().isoformat(),
            "to": df_prices["time"].max().isoformat(),
        },
    }

    report_path = base_dir / f"report_{run_id}.json"
    with report_path.open("w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    logger.info(f"[WFO] report saved: {report_path}")

    return run_id


# ------------------------
# モデル保存 & 署名
# ------------------------


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def save_model_and_meta(  # noqa: PLR0913  (引数多めでもここはOKとする)
    booster: lgb.Booster,
    cfg: WeeklyRetrainConfig,
    wfo_result: WFOResult,
    threshold_info: dict[str, float],
    feature_cols: list[str],
    data_info: dict[str, str],
) -> Path:
    cfg.paths.models_dir.mkdir(parents=True, exist_ok=True)

    ts = datetime.now(tz=UTC)
    ts_str = ts.strftime("%Y%m%d_%H%M%S")
    version = ts.timestamp()

    model_name = f"LightGBM_clf_{ts_str}.pkl"
    model_path = cfg.paths.models_dir / model_name

    dump(booster, model_path)

    sha = sha256_file(model_path)

    meta = {
        "model_name": "LightGBM_clf",
        "file": model_name,
        "created_at_utc": ts.isoformat(),
        "version": version,
        "symbol": cfg.retrain.symbol,
        "timeframe": cfg.retrain.timeframe,
        "label_horizon_bars": cfg.retrain.label_horizon,
        "min_pips": cfg.retrain.min_pips,
        "features": list(feature_cols),
        "wfo": {
            "mean_logloss": wfo_result.mean_logloss,
            "mean_accuracy": wfo_result.mean_accuracy,
            "folds": [asdict(f) for f in wfo_result.folds],
        },
        "threshold": threshold_info,
        "data": data_info,
        "sha256": sha,
    }

    meta_path = cfg.paths.models_dir / f"{model_name}.meta.json"
    with meta_path.open("w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    logger.info(f"[SAVE] model={model_path} sha256={sha}")
    logger.info(f"[SAVE] meta={meta_path}")

    active = {
        "model_name": "LightGBM_clf",
        "file": model_name,
        "meta_file": meta_path.name,
        "version": version,
        "best_threshold": threshold_info.get("best_threshold"),
    }
    active_path = cfg.paths.models_dir / "active_model.json"
    with active_path.open("w", encoding="utf-8") as f:
        json.dump(active, f, ensure_ascii=False, indent=2)

    logger.info(f"[SAVE] active_model={active_path}")
    return model_path


# ------------------------
# メイン処理
# ------------------------


def run_weekly_retrain(cfg: WeeklyRetrainConfig, dry_run: bool = False) -> None:
    paths = cfg.paths
    rt = cfg.retrain

    paths.logs_dir.mkdir(parents=True, exist_ok=True)
    log_file = (
        paths.logs_dir / f"weekly_retrain_{datetime.now().strftime('%Y%m%d')}.log"
    )
    logger.add(log_file, encoding="utf-8")

    logger.info(
        f"[CFG] symbol={rt.symbol} tf={rt.timeframe} label_horizon={rt.label_horizon}"
    )

    # config の symbol が "USDJPY-" でも、
    # 実データは data/USDJPY/ohlcv/USDJPY_M5.csv を読む
    symbol_dir = rt.symbol.replace("-", "")
    symbol_file = rt.symbol.replace("-", "")

    csv_path = (
        paths.data_dir / symbol_dir / "ohlcv" / f"{symbol_file}_{rt.timeframe}.csv"
    )

    logger.info(f"[STEP] load_price_data csv={csv_path}")
    df_prices = load_price_data(csv_path)
    logger.info(
        f"[STEP] loaded rows={len(df_prices)} "
        f"from={df_prices['time'].min()} to={df_prices['time'].max()}"
    )

    logger.info("[STEP] build_features")
    feats = build_features(df_prices)
    logger.info(f"[STEP] features shape={feats.shape}")

    logger.info("[STEP] build_labels")
    labels = build_labels(
        df_prices,
        horizon=rt.label_horizon,
        min_pips=rt.min_pips,
    )

    logger.info("[STEP] align_features_and_labels")
    X, y = align_features_and_labels(feats, labels)
    logger.info(
        f"[DATA] X={X.shape} y_pos={int((y == 1).sum())} y_neg={int((y == 0).sum())}"
    )

    if len(X) < 1000:
        logger.warning(
            "[WARN] 学習データが少なすぎます(1000行未満)。処理を中止します。"
        )
        return

    logger.info("[STEP] train_lightgbm_wfo")
    wfo_result, boosters, oof_pred = train_lightgbm_wfo(X, y, rt)
    logger.info(
        f"[WFO] mean_logloss={wfo_result.mean_logloss:.5f} "
        f"mean_acc={wfo_result.mean_accuracy:.4f}"
    )

    logger.info("[STEP] optimize_threshold")
    thr_info = optimize_threshold(y, oof_pred, rt.threshold_grid or [])

    logger.info("[STEP] save_wfo_report_and_equity")
    run_id = save_wfo_report_and_equity(
        cfg=cfg,
        df_prices=df_prices,
        X=X,
        y=y,
        oof_pred=oof_pred,
        wfo_result=wfo_result,
        thr_info=thr_info,
    )
    logger.info(f"[WFO] artifacts saved with run_id={run_id}")

    if dry_run:
        logger.info(
            "[DRYRUN] dry-run 指定のためモデル保存/署名は行いません。ここで終了します。"
        )
        return

    logger.info("[STEP] train final model on all data")
    params: dict[str, object] = {
        "objective": "binary",
        "metric": ["binary_logloss"],
        "learning_rate": 0.05,
        "num_leaves": 31,
        "max_depth": -1,
        "min_data_in_leaf": 50,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "verbosity": -1,
        "force_col_wise": True,
    }
    train_all = lgb.Dataset(X, label=y)
    best_iters = [b.best_iteration or 200 for b in boosters]
    num_boost_round = int(np.median(best_iters))
    booster_all = lgb.train(
        params,
        train_all,
        num_boost_round=num_boost_round,
    )

    logger.info("[STEP] save_model_and_meta")
    data_info = {
        "csv_path": str(csv_path),
        "from": df_prices["time"].min().isoformat(),
        "to": df_prices["time"].max().isoformat(),
        "n_rows_raw": int(len(df_prices)),
        "n_rows_train": int(len(X)),
    }
    model_path = save_model_and_meta(
        booster=booster_all,
        cfg=cfg,
        wfo_result=wfo_result,
        threshold_info=thr_info,
        feature_cols=list(X.columns),
        data_info=data_info,
    )

    logger.info(f"[DONE] weekly retrain completed. model={model_path}")


def main() -> None:
    parser = argparse.ArgumentParser(description="週次自動再学習 (weekly_retrain)")
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="設定ファイルへのパス (default: configs/config.yaml)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="学習だけ行い、モデル保存や active_model 更新は行わない",
    )
    args = parser.parse_args()

    # デフォルト候補: configs/config.yaml
    default_config = Path("configs/config.yaml")
    config_path = Path(args.config) if args.config else default_config

    cfg = load_config(config_path)
    run_weekly_retrain(cfg, dry_run=args.dry_run)


if __name__ == "__main__":
    main()



=== file: temp_equity_check.py ===

import pandas as pd, pathlib, numpy as np
p = pathlib.Path(r"D:\macht\OneDrive\fxbot\logs\backtest\USDJPY\M5\equity_curve.csv")
df = pd.read_csv(p)
print("[equity] cols:", df.columns.tolist())
sig = df.get("signal", pd.Series(0, index=df.index)).astype('int', errors='ignore')
chg = sig.ne(sig.shift(1)).fillna(sig.iloc[0] != 0)
print("signal unique:", sorted(pd.unique(sig)))
print("nonzero count:", int((sig != 0).sum()), "/", len(sig))
print("change points:", int(chg.sum()))
print("first rows:\n", df.head(3))



=== file: tests/test_sanity.py ===

# tests/test_sanity.py
def test_sanity() -> None:
    assert 1 + 1 == 2



=== file: tmp_view.py ===

from pathlib import Path
lines=Path('app/core/mt5_client.py').read_text('utf-8', errors='replace').splitlines()
for i in range(120, 190):
    print(f'{i+1:03}: {lines[i]}')



=== file: tools/__init__.py ===




=== file: tools/backtest_equity_curve.py ===

# tools/backtest_equity_curve.py
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))  # ←これを追加

import argparse, json, glob, os
from typing import Any
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from core.ai.loader import load_lgb_clf
from core.ai.features import build_features

def _load_active_meta() -> dict[str, Any]:
    p = "models/active_model.json"
    j = json.load(open(p, encoding="utf-8"))
    best_t = float(j.get("best_threshold", 0.2))
    lookahead = int(j.get("selected_lookahead", 15))
    return {"best_threshold": best_t, "lookahead": lookahead}

def _load_dataset(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    # 必要列: time/open/high/low/close/volume を想定
    if not set(["open","high","low","close","volume"]).issubset(df.columns):
        raise ValueError("CSVにOHLCV列が不足しています")
    df = build_features(df)  # 学習時と同じ拡張20列
    df = df.dropna().reset_index(drop=True)
    return df

def _ensure_feature_order(df: pd.DataFrame, model: Any) -> tuple[pd.DataFrame, list[str]]:
    # ラッパは model.expected_features を持たせてある想定（無ければ推定）
    feat = getattr(model, "expected_features", None)
    if feat is None:
        # モデル側に無い場合は、学習で使っていそうな20列を拾う（応急）
        candidates = ["open","high","low","close","volume",
                      "ret_1","ret_3","ret_5","ret_10",
                      "ret_std_10","ret_std_20",
                      "tr","atr_14","rsi_14","adx_14","bbp_20",
                      "upper_wick_ratio","lower_wick_ratio","body_ratio","vol_zscore_20"]
        feat = [c for c in candidates if c in df.columns]
    X = df[feat].copy()
    return X, feat

def run_backtest(csv_path: str, out_csv: str, init_equity: float = 100_000.0, show: bool = True) -> None:
    meta = _load_active_meta()
    best_t = float(meta.get("best_threshold", 0.2))
    L = int(meta.get("lookahead", 15))
    model = load_lgb_clf("models/LightGBM_clf.pkl")  # calib付きでロードされる
    df_raw = pd.read_csv(csv_path)
    df = _load_dataset(csv_path)
    X, feat = _ensure_feature_order(df, model)

    # (B) build_features → dropna 後に行数チェック
    # 窓を使う指標（RSI14/ATR14/ret_std_20 等）で冒頭がNaNになるため、
    # 短すぎるCSVだと0行になってしまう。足りない場合は早期終了して案内。
    if X.shape[0] == 0 or X.shape[0] < 40:
        print("[bt][error] too few rows after feature engineering (need ≈40+ rows). "
              "Your CSV is too short; provide more bars (100+ recommended).")
        return

    # 予測
    # (A) 列名付き DataFrame のまま渡す（学習時の列名を維持）
    proba = model.predict_proba(X)  # shape (n,2) を想定（[neg, pos]）
    # ここでは [p_sell, p_buy] ではなく [p0,p1]=[neg,pos] を buy=pos とみなす
    if proba.shape[1] == 2:
        p_buy = proba[:,1]
        p_sell = proba[:,0]
    else:
        # 2クラスでない場合の保険
        p_buy = proba.ravel()
        p_sell = 1.0 - p_buy

    closes = df["close"].to_numpy()
    n = len(df)

    equity = init_equity
    equity_curve = []
    pos = 0        # 0:ノーポジ, +1:買い, -1:売り
    entry_idx = -1
    entry_price = np.nan

    for i in range(n):
        # エグジット判定
        if pos != 0 and (i - entry_idx) >= L:
            exit_price = closes[i]
            ret = (exit_price/entry_price - 1.0) * (1 if pos>0 else -1)
            equity *= (1.0 + ret)
            pos = 0
            entry_idx = -1
            entry_price = np.nan

        # エントリー判定（ノーポジ時のみ）
        if pos == 0:
            if (p_buy[i] >= best_t) and (p_buy[i] > p_sell[i]):
                pos = +1
                entry_idx = i
                entry_price = closes[i]
            elif (p_sell[i] >= best_t) and (p_sell[i] > p_buy[i]):
                pos = -1
                entry_idx = i
                entry_price = closes[i]

        equity_curve.append(equity)

    out = pd.DataFrame({
        "time": df_raw.loc[df.index, "time"] if "time" in df_raw.columns else np.arange(n),
        "close": closes,
        "p_buy": p_buy,
        "p_sell": p_sell,
        "equity": equity_curve
    })
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    out.to_csv(out_csv, index=False, encoding="utf-8")
    print(f"[bt] wrote equity: {out_csv}  (final={equity_curve[-1]:.2f}, ret={(equity_curve[-1]/init_equity-1)*100:.2f}%)")

    if show:
        plt.figure(figsize=(9,4))
        plt.plot(out["equity"])
        plt.title(f"Equity Curve (start={init_equity:.0f} JPY, L={L}, thr={best_t})")
        plt.xlabel("bars")
        plt.ylabel("equity (JPY)")
        plt.tight_layout()
        plt.show()

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", default=None, help="バックテスト対象CSV。未指定なら data/*.csv の最新を使用")
    ap.add_argument("--out", default="logs/backtest/equity_curve.csv")
    ap.add_argument("--capital", type=float, default=100_000.0)
    ap.add_argument("--no-show", action="store_true")
    args = ap.parse_args()

    csv = args.csv or sorted(glob.glob("data/*.csv"))[-1]
    run_backtest(csv, args.out, init_equity=args.capital, show=not args.no_show)



=== file: tools/backtest_run.py ===

# tools/backtest_run.py
from __future__ import annotations

import argparse
import json
import sys
from dataclasses import dataclass
from pathlib import Path

import numpy as np

# --- プロジェクトルートを sys.path に追加してから app.* を import する ---
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from app.strategies.ai_strategy import (
    build_features,
    load_active_model,
    predict_signals,
    trades_from_signals,
)

LOG_DIR = PROJECT_ROOT / "logs" / "backtest"

# === equity utils ===

# === monthly_returns.csv を生成するユーティリティ ===
import pandas as pd


def compute_monthly_returns(equity_csv_path: str, out_path: str):
    df = pd.read_csv(equity_csv_path)

    if "timestamp" not in df.columns and "time" in df.columns:
        df = df.rename(columns={"time": "timestamp"})

    # 必須カラムチェック：timestamp, equity
    if not {"timestamp", "equity"}.issubset(df.columns):
        raise ValueError("equity_curve.csv に必要なカラムがありません (timestamp, equity)")

    # timestamp → datetime 変換
    df["timestamp"] = pd.to_datetime(df["timestamp"])

    # 月末の equity を集計
    df["year"] = df["timestamp"].dt.year
    df["month"] = df["timestamp"].dt.month

    # 月初値を取得
    first = df.groupby(["year", "month"])["equity"].first()
    last = df.groupby(["year", "month"])["equity"].last()

    # リターン計算
    monthly_return = (last - first) / first * 100

    # 同時に月次DDも計算する（peak-to-trough）
    def calc_dd(sub):
        peak = sub["equity"].cummax()
        dd = (sub["equity"] - peak) / peak * 100
        return dd.min()

    dd_rows = []
    for (y, m), sub in df.groupby(["year", "month"]):
        dd_rows.append({
            "year": y,
            "month": m,
            "dd_pct": calc_dd(sub)
        })
    dd_df = pd.DataFrame(dd_rows).set_index(["year", "month"])

    out = pd.DataFrame({
        "return_pct": monthly_return,
    }).join(dd_df)

    out = out.reset_index().sort_values(["year", "month"])
    out.to_csv(out_path, index=False)

    print(f"[bt] write monthly {out_path}")


@dataclass
class Trade:
    entry_time: pd.Timestamp
    entry_price: float
    exit_time: pd.Timestamp
    exit_price: float
    direction: int  # +1 long, -1 short
    profit_jpy: float


def equity_from_bnh(df: pd.DataFrame, capital: float) -> pd.Series:
    """
    Buy&Hold（現物1倍）相当の指数エクイティ。close/close0でスケール。
    """
    close = df["close"].astype(float)
    idx = close / close.iloc[0]
    return capital * idx


def trades_from_signal_series(
    df: pd.DataFrame,
    sig: pd.Series,
    lot: float = 0.1,
    contract_size: int = 100_000,
) -> list[Trade]:
    """
    signal（1/-1/0）からフリップ方式でトレード列を作る。
    - signal が 1→ロング保有、-1→ショート保有、0→ノーポジ
    - signal が変わった時点で前ポジをクローズ→新ポジを建てる
    - JPYペアを想定：損益[JPY] = (exit - entry) * direction * lot * contract_size
    """
    sig = sig.astype(int).reindex(df.index).fillna(0)
    close = df["close"].astype(float)
    times = pd.to_datetime(df["time"])

    cur_dir = 0
    cur_price: float | None = None
    cur_time: pd.Timestamp | None = None
    out: list[Trade] = []

    for t, px, s in zip(times, close, sig):
        s = int(s)
        if cur_dir == 0:
            if s in (1, -1):
                cur_dir = s
                cur_price = px
                cur_time = t
        else:
            if s == cur_dir:
                continue
            # 方向が変わった/0になった → クローズ
            profit = (px - cur_price) * cur_dir * lot * contract_size  # type: ignore[arg-type]
            out.append(Trade(cur_time, cur_price, t, px, cur_dir, profit))  # type: ignore[arg-type]
            cur_dir = 0
            cur_price = None
            cur_time = None
            # 新規に建て直す（0でなければ）
            if s in (1, -1):
                cur_dir = s
                cur_price = px
                cur_time = t

    # 終端でオープン中なら、最終値でクローズしてしまう
    if cur_dir != 0 and cur_price is not None and cur_time is not None:
        px = close.iloc[-1]
        t = times.iloc[-1]
        profit = (px - cur_price) * cur_dir * lot * contract_size
        out.append(Trade(cur_time, cur_price, t, px, cur_dir, profit))

    return out


def equity_from_trades(
    df: pd.DataFrame, trades: list[Trade], capital: float
) -> pd.Series:
    """
    トレード配列からエクイティ曲線を作る（逐次加算）。
    """
    eq = pd.Series(capital, index=pd.to_datetime(df["time"]))
    cum = capital
    i = 0
    for tr in trades:
        # 成立時刻で損益を反映
        while i < len(eq.index) and eq.index[i] <= tr.exit_time:
            if eq.index[i] == tr.exit_time:
                cum += tr.profit_jpy
            eq.iloc[i] = cum
            i += 1
    # 以降もラスト値で埋める
    while i < len(eq.index):
        eq.iloc[i] = cum
        i += 1
    return eq


def equity_from_trade_df(
    df_ohlcv: pd.DataFrame, trades_df: pd.DataFrame, capital: float
) -> pd.Series:
    """
    trades_df 形式（DataFrame）から全バーに展開したエクイティ曲線を作る。
    必須カラム: exit_time, pnl
    任意: entry_time, entry_price, exit_price, direction（無くてもOK）
    """
    if trades_df is None or trades_df.empty:
        # 取引なしならフラット
        idx = pd.to_datetime(df_ohlcv["time"])
        return pd.Series(capital, index=idx)

    td = trades_df.copy()

    # 時刻カラムを時系列化（存在するものだけ）
    for col in ("entry_time", "exit_time"):
        if col in td.columns:
            td[col] = pd.to_datetime(td[col], errors="coerce")

    if "exit_time" not in td.columns:
        raise ValueError("trades_df に exit_time 列が必要です。")

    # pnl は数値化
    td["pnl"] = pd.to_numeric(td.get("pnl", 0.0), errors="coerce").fillna(0.0)

    # exit_time でグルーピングして、同時決済があれば合算
    pnl_by_exit = td.groupby("exit_time")["pnl"].sum().sort_index()

    # 全バーへ展開：exit_time のバーでのみ損益を加算、以降は前値でFFill
    idx = pd.to_datetime(df_ohlcv["time"])
    eq = pd.Series(capital, index=idx)
    cum = capital
    i = 0
    exit_times = pnl_by_exit.index.to_list()
    k = 0

    while i < len(idx):
        t = idx[i]
        # このバーの exit_time に決済があればすべて加算
        while k < len(exit_times) and exit_times[k] <= t:
            cum += float(pnl_by_exit.iloc[k])
            k += 1
        eq.iloc[i] = cum
        i += 1

    return eq


def to_equity(close: pd.Series, capital: float = 100000.0) -> pd.DataFrame:
    close = close.astype(float)
    ret = close.pct_change().fillna(0.0)
    eq = (1.0 + ret).cumprod() * capital
    return pd.DataFrame({"time": close.index, "equity": eq.values})


def _max_consecutive(x: pd.Series, val: int) -> int:
    # 最大連続カウント（val=1を数える）
    c = 0
    m = 0
    for v in x:
        if v == val:
            c += 1
            m = max(m, c)
        else:
            c = 0
    return m


def _dd_duration_max(eq: pd.Series) -> int:
    """ドローダウン期間の最大日数を算出。時系列がintならスキップする。"""
    peak = -np.inf
    last_peak_time: pd.Timestamp | None = None
    max_days = 0
    for t, v in eq.items():
        # t が datetime でない場合は飛ばす
        if not hasattr(t, "to_pydatetime") and not hasattr(t, "year"):
            continue
        if v > peak:
            peak = v
            last_peak_time = pd.Timestamp(t)
        elif last_peak_time is not None:
            d = (pd.Timestamp(t) - last_peak_time).days
            max_days = max(max_days, d)
    return int(max_days)


def metrics_from_equity(eq: pd.Series) -> dict:
    ret = eq.pct_change().fillna(0.0)
    total = eq.iloc[-1] / eq.iloc[0] - 1.0
    dd = (eq / eq.cummax() - 1.0).min()
    sharpe = (ret.mean() / (ret.std() + 1e-12)) * np.sqrt(
        252 * 24 * 12
    )  # M5相当の便宜スケール
    return {
        "start_equity": float(eq.iloc[0]),
        "end_equity": float(eq.iloc[-1]),
        "total_return": float(total),
        "max_drawdown": float(dd),
        "sharpe_like": float(sharpe),
        "bars": int(len(eq)),
        "max_dd_days": _dd_duration_max(eq),
    }


def monthly_returns_from_equity(
    eq_df: pd.DataFrame,
    trades_df: pd.DataFrame | None = None,
) -> pd.DataFrame:
    """
    エクイティ曲線（eq_df）とトレード一覧（trades_df）から、
    月次のリターン・DD・トレード統計をまとめた DataFrame を返す。

    返り値カラム:
        year, month, return_pct, dd_pct, trades, win_rate, pf
    """
    if eq_df is None or eq_df.empty:
        return pd.DataFrame(
            columns=[
                "year",
                "month",
                "return_pct",
                "dd_pct",
                "trades",
                "win_rate",
                "pf",
            ]
        )

    df = eq_df.copy()
    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time")
    df = df.set_index("time")

    # --- 月次のリターン（％） ---
    monthly_last = df["equity"].resample("ME").last()
    monthly_first = df["equity"].resample("ME").first()

    # 0割り防止
    ret_raw = (monthly_last / monthly_first).replace([np.inf, -np.inf], np.nan) - 1.0
    return_pct = (ret_raw * 100.0).rename("return_pct")  # ％に変換

    # --- 月次の最大ドローダウン（％） ---
    def _month_dd(equity: pd.Series) -> float:
        if equity.empty:
            return 0.0
        dd = (equity / equity.cummax() - 1.0).min()
        return float(dd * 100.0)  # ％に変換（-5.4 なら -5.4%）

    dd_pct = df["equity"].resample("ME").apply(_month_dd).rename("dd_pct")

    # ベースとなる DataFrame（year/month 作成）
    out = pd.concat([return_pct, dd_pct], axis=1).dropna(how="all")
    if out.empty:
        return pd.DataFrame(
            columns=[
                "year",
                "month",
                "return_pct",
                "dd_pct",
                "trades",
                "win_rate",
                "pf",
            ]
        )

    out.index = pd.to_datetime(out.index)
    out["year"] = out.index.year
    out["month"] = out.index.month

    # デフォルト値（トレード統計は 0 で初期化）
    out["trades"] = 0
    out["win_rate"] = 0.0
    out["pf"] = 0.0

    # --- トレード統計（月次） ---
    if trades_df is not None and not trades_df.empty and "exit_time" in trades_df.columns:
        td = trades_df.copy()
        td["exit_time"] = pd.to_datetime(td["exit_time"])
        td = td.dropna(subset=["exit_time"])

        if "pnl" in td.columns:
            td["pnl"] = pd.to_numeric(td["pnl"], errors="coerce").fillna(0.0)

            rows = []
            for (y, m), g in td.groupby([td["exit_time"].dt.year, td["exit_time"].dt.month]):
                pnl = g["pnl"].astype(float)
                n_tr = int(len(pnl))
                if n_tr == 0:
                    continue
                win_mask = pnl > 0
                win_rate = float(win_mask.mean() * 100.0)

                sum_win = float(pnl[win_mask].sum())
                sum_loss_abs = float((-pnl[~win_mask]).clip(lower=0).sum())

                if sum_loss_abs > 0:
                    pf = float(sum_win / sum_loss_abs)
                elif sum_win > 0:
                    pf = float("inf")
                else:
                    pf = 0.0

                rows.append(
                    {
                        "year": int(y),
                        "month": int(m),
                        "trades": n_tr,
                        "win_rate": win_rate,
                        "pf": pf,
                    }
                )

            if rows:
                stats = pd.DataFrame(rows)
                out = out.merge(stats, on=["year", "month"], how="left", suffixes=("", "_t"))

                # 欠損を初期値で埋める
                out["trades"] = out["trades_t"].fillna(out["trades"]).astype(int)
                out["win_rate"] = out["win_rate_t"].fillna(out["win_rate"])
                out["pf"] = out["pf_t"].fillna(out["pf"])

                # 一時列を削除
                out = out.drop(columns=[c for c in out.columns if c.endswith("_t")])

    # カラム順を最終仕様に揃える
    out = out[["year", "month", "return_pct", "dd_pct", "trades", "win_rate", "pf"]]
    out = out.reset_index(drop=True)
    return out


def trades_from_buyhold(df: pd.DataFrame, capital: float) -> pd.DataFrame:
    # テンプレ：開始→終了の単一トレード（将来は戦略で複数トレードに差し替え）
    if df.empty:
        return pd.DataFrame(
            columns=[
                "entry_time",
                "exit_time",
                "pnl",
                "holding_bars",
                "holding_days",
                "win",
            ]
        )
    entry = df["time"].iloc[0]
    exit_ = df["time"].iloc[-1]
    close = df["close"].astype(float)
    ret = (close.iloc[-1] / close.iloc[0]) - 1.0
    pnl = capital * ret
    holding_bars = len(df)
    holding_days = (pd.Timestamp(exit_) - pd.Timestamp(entry)).days
    win = int(pnl > 0)
    return pd.DataFrame(
        [
            {
                "entry_time": entry,
                "exit_time": exit_,
                "pnl": float(pnl),
                "holding_bars": int(holding_bars),
                "holding_days": int(holding_days),
                "win": win,
            }
        ]
    )


def trade_metrics(trades: pd.DataFrame) -> dict:
    if trades.empty:
        return {
            "trades": 0,
            "win_rate": 0.0,
            "avg_pnl": 0.0,
            "profit_factor": 0.0,
            "avg_holding_bars": 0.0,
            "avg_holding_days": 0.0,
            "max_consec_win": 0,
            "max_consec_loss": 0,
        }
    wins = trades["pnl"] > 0
    sum_win = trades.loc[wins, "pnl"].sum()
    sum_loss_abs = (-trades.loc[~wins, "pnl"]).clip(lower=0).sum()
    pf = (
        float(sum_win / sum_loss_abs)
        if sum_loss_abs > 0
        else float("inf") if sum_win > 0 else 0.0
    )

    seq = wins.astype(int)
    consec_win = _max_consecutive(seq, 1)
    consec_loss = _max_consecutive(1 - seq, 1)

    return {
        "trades": int(len(trades)),
        "win_rate": float(wins.mean()) if len(trades) else 0.0,
        "avg_pnl": float(trades["pnl"].mean()) if len(trades) else 0.0,
        "profit_factor": pf,
        "avg_holding_bars": (
            float(trades["holding_bars"].mean()) if len(trades) else 0.0
        ),
        "avg_holding_days": (
            float(trades["holding_days"].mean()) if len(trades) else 0.0
        ),
        "max_consec_win": int(consec_win),
        "max_consec_loss": int(consec_loss),
    }


def slice_period(
    df: pd.DataFrame, start: str | None = None, end: str | None = None
) -> pd.DataFrame:
    """
    指定期間で DataFrame をスライスする。
    start / end のどちらか、または両方が None の場合は、その条件をスキップする。
    両方 None の場合は全期間を返す。
    """
    if start is None and end is None:
        return df.reset_index(drop=True)

    m = pd.Series(True, index=df.index)
    if start is not None:
        m &= df["time"] >= pd.Timestamp(start)
    if end is not None:
        m &= df["time"] <= pd.Timestamp(end)
    return df.loc[m].reset_index(drop=True)


def run_backtest(
    data_csv: Path,
    start: str | None,
    end: str | None,
    capital: float,
    out_dir: Path,
) -> Path:
    print("[bt] start", flush=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    print(f"[bt] read_csv {data_csv}", flush=True)
    df = pd.read_csv(data_csv, parse_dates=["time"])

    tag_start = start or "ALL"
    tag_end = end or "ALL"
    print(f"[bt] slice {tag_start} .. {tag_end}", flush=True)
    df = slice_period(df, start, end)
    if df.empty:
        raise RuntimeError("No data in the requested period.")
    close = df["close"]
    close.index = df["time"]

    print("[bt] equity compute", flush=True)
    eq_df = to_equity(close, capital)
    eq_df["signal"] = 0  # Buy&Holdなのでシグナル無し
    eq_csv = out_dir / "equity_curve.csv"
    print(f"[bt] write equity {eq_csv}", flush=True)
    eq_df.to_csv(eq_csv, index=False)
    monthly_path = out_dir / "monthly_returns.csv"
    compute_monthly_returns(eq_csv, monthly_path)

    # 仮トレード（Buy&Hold）
    trades = trades_from_buyhold(df, capital)
    trades.to_csv(out_dir / "trades.csv", index=False)

    # メトリクス
    base = metrics_from_equity(eq_df["equity"])
    tmet = trade_metrics(trades)
    base.update(tmet)
    (out_dir / "metrics.json").write_text(
        json.dumps(base, ensure_ascii=False, indent=2)
    )

    print("[bt] done", flush=True)
    return eq_csv


def run_wfo(
    data_csv: Path,
    start: str | None,
    end: str | None,
    capital: float,
    out_dir: Path,
    train_ratio: float = 0.7,
) -> Path:
    print("[wfo] start", flush=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    print(f"[wfo] read_csv {data_csv}", flush=True)
    df = pd.read_csv(data_csv, parse_dates=["time"])

    tag_start = start or "ALL"
    tag_end = end or "ALL"
    print(f"[wfo] slice {tag_start} .. {tag_end}", flush=True)
    df = slice_period(df, start, end)
    if df.empty:
        raise RuntimeError("No data in the requested period.")
    n = len(df)
    n_tr = max(10, int(n * train_ratio))
    df_tr = df.iloc[:n_tr].reset_index(drop=True)
    df_ts = df.iloc[n_tr:].reset_index(drop=True)

    def _one(d: pd.DataFrame, name: str) -> dict:
        print(f"[wfo] equity compute {name}", flush=True)
        feat = build_features(d, params={})

        # --- 必須列の補完 ---
        if "time" not in feat.columns:
            feat["time"] = pd.to_datetime(d["time"]).reset_index(drop=True)
        if "close" not in feat.columns:
            feat["close"] = pd.Series(d["close"].astype(float)).reset_index(drop=True)

        try:
            kind, payload, threshold, params = load_active_model()
            print(f"[wfo] using model: {payload} threshold={threshold}", flush=True)

            # 予測 → シグナル
            feat["signal"] = predict_signals(kind, payload, feat, threshold, params)
            signal_series = feat["signal"].astype(int).reset_index(drop=True)

            # ログで“出てるか”チェック
            nz = int((signal_series != 0).sum())
            print(f"[wfo] signals nonzero={nz} / {len(signal_series)}", flush=True)
            (out_dir / f"signals_{name}.csv").write_text(
                pd.DataFrame({"time": feat["time"], "signal": signal_series}).to_csv(
                    index=False
                )
            )

            # トレード生成
            trades = trades_from_signals(feat, capital, params)

            # エクイティ展開（DataFrame でも list[Trade] でもOKにする）
            if isinstance(trades, pd.DataFrame):
                eq_series = equity_from_trade_df(feat, trades, capital)
            else:
                eq_series = equity_from_trades(feat, trades, capital)

            eq_df = pd.DataFrame(
                {
                    "time": eq_series.index,
                    "equity": eq_series.values,
                    "signal": signal_series.values,
                }
            )
        except Exception as e:
            print(f"[wfo] AI model not used ({e}) -> fallback to buy&hold", flush=True)
            close = d["close"]
            close.index = d["time"]
            eq_df = to_equity(close, capital)
            eq_df["signal"] = 0
            trades = trades_from_buyhold(d, capital)

        p = out_dir / f"equity_{name}.csv"
        print(f"[wfo] write {p}", flush=True)
        eq_df.to_csv(p, index=False)
        trades.to_csv(out_dir / f"trades_{name}.csv", index=False)

        # 月次損益＋トレード統計
        mr = monthly_returns_from_equity(eq_df, trades_df=trades)
        mr.to_csv(out_dir / f"monthly_returns_{name}.csv", index=False)

        m = metrics_from_equity(eq_df["equity"])
        m.update(trade_metrics(trades))
        return m

    m_tr = _one(df_tr, "train")
    m_ts = _one(df_ts, "test")

    # 可視化用に test をメインへコピー
    (out_dir / "equity_curve.csv").write_text((out_dir / "equity_test.csv").read_text())
    (out_dir / "metrics_wfo.json").write_text(
        json.dumps({"train": m_tr, "test": m_ts}, ensure_ascii=False, indent=2)
    )

    print("[wfo] done", flush=True)
    return out_dir / "equity_curve.csv"


def _normalize_dates_from_args(
    args: argparse.Namespace,
    parser: argparse.ArgumentParser,
) -> tuple[str | None, str | None]:
    """
    --start-date / --end-date を優先しつつ、
    旧 --start / --end も互換用としてサポートする。

    返り値は YYYY-MM-DD 形式の文字列か None。
    """
    raw_start = getattr(args, "start_date", None) or getattr(args, "start", None)
    raw_end = getattr(args, "end_date", None) or getattr(args, "end", None)

    def _norm(x: str | None) -> str | None:
        if x is None or x == "":
            return None
        try:
            ts = pd.to_datetime(x)
        except Exception:
            parser.error(f"invalid date format: {x!r} (expected YYYY-MM-DD)")
        return ts.strftime("%Y-%m-%d")

    start_str = _norm(raw_start)
    end_str = _norm(raw_end)

    if start_str is not None and end_str is not None:
        if pd.Timestamp(start_str) > pd.Timestamp(end_str):
            parser.error("start date must be <= end date")

    return start_str, end_str


def _build_period_tag(start: str | None, end: str | None) -> str:
    """
    ログ用の期間タグを生成する。
    例:
      start=2024-07-01, end=2024-12-31 → '2024-07-01_to_2024-12-31'
      start=None,       end=2024-12-31 → 'ALL_to_2024-12-31'
      start=2024-07-01, end=None       → '2024-07-01_to_ALL'
      start=None,       end=None       → 'ALL_to_ALL'
    """
    s = start or "ALL"
    e = end or "ALL"
    return f"{s}_to_{e}"


def _mirror_latest_run(period_dir: Path, base_dir: Path) -> None:
    """
    期間付きフォルダに出力されたファイルを、ベースディレクトリ
    (logs/backtest/{symbol}/{timeframe}) にもコピーして、
    GUI や他ツール向けの「最新結果」として見えるようにする。
    """
    base_dir.mkdir(parents=True, exist_ok=True)
    if not period_dir.exists():
        return
    for f in period_dir.glob("*"):
        if f.is_file():
            target = base_dir / f.name
            target.write_bytes(f.read_bytes())


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="バックテスト対象のCSVファイルパス")
    # 旧オプション（互換用）
    ap.add_argument("--start", help="(legacy) 開始日付 YYYY-MM-DD", required=False)
    ap.add_argument("--end", help="(legacy) 終了日付 YYYY-MM-DD", required=False)
    # 新オプション（推奨）
    ap.add_argument("--start-date", help="開始日付 YYYY-MM-DD", required=False)
    ap.add_argument("--end-date", help="終了日付 YYYY-MM-DD", required=False)

    ap.add_argument("--capital", type=float, default=100000.0)
    ap.add_argument("--mode", choices=["bt", "wfo"], default="bt")
    ap.add_argument("--symbol", default="USDJPY")
    ap.add_argument("--timeframe", default="M5")
    ap.add_argument("--layout", choices=["flat", "per-symbol"], default="per-symbol")
    ap.add_argument("--train-ratio", type=float, default=0.7)
    args = ap.parse_args()

    csv = Path(args.csv).resolve()
    base_dir = LOG_DIR / args.symbol / args.timeframe
    base_dir.mkdir(parents=True, exist_ok=True)

    # 日付引数の正規化（YYYY-MM-DD or None）
    start_str, end_str = _normalize_dates_from_args(args, ap)
    period_tag = _build_period_tag(start_str, end_str)
    period_dir = base_dir / f"backtest_{period_tag}"
    period_dir.mkdir(parents=True, exist_ok=True)

    print(f"[main] mode={args.mode} csv={csv}", flush=True)
    print(f"[main] period={period_tag}", flush=True)
    if args.mode == "bt":
        p = run_backtest(csv, start_str, end_str, args.capital, period_dir)
    else:
        p = run_wfo(
            csv,
            start_str,
            end_str,
            args.capital,
            period_dir,
            train_ratio=args.train_ratio,
        )

    # 期間付きフォルダの内容を「最新結果」としてベースディレクトリへミラー
    _mirror_latest_run(period_dir, base_dir)

    print(str(p), flush=True)


if __name__ == "__main__":
    main()



=== file: tools/dump_feature_importance.py ===

# tools/dump_feature_importance.py
import csv
import glob
import json
import os
from datetime import datetime
from typing import Any, Iterable, Tuple

import joblib


def _load_latest_report() -> Tuple[str | None, dict[str, Any] | None]:
    rps = sorted(glob.glob(os.path.join("logs", "retrain", "report_*.json")))
    if not rps:
        return None, None
    rp = rps[-1]
    with open(rp, encoding="utf-8") as f:
        j = json.load(f)
    return rp, j


def _load_features_from_report(j: dict[str, Any]) -> list[str]:
    feats = j.get("features") or []
    return list(feats)


def _load_model(pkl_path: str) -> Any:
    return joblib.load(pkl_path)


def _write_feat_csv(model: Any, feat_cols: list[str], out_csv: str) -> str:
    try:
        booster = getattr(model, "booster_", None)
        if booster is None:
            split_importance = getattr(model, "feature_importances_", None)
            gain_importance = None
            names = feat_cols
        else:
            names = booster.feature_name()
            split_importance = booster.feature_importance(importance_type="split")
            gain_importance = booster.feature_importance(importance_type="gain")

        def _all_column_style(xs: Iterable[str]) -> bool:
            if not xs:
                return False
            return all(str(x).startswith("Column_") for x in xs)

        if not names or len(names) != len(feat_cols) or _all_column_style(names):
            names = feat_cols[:]

        rows: list[dict[str, float | str]] = []
        for i, name in enumerate(names):
            s = (
                float(split_importance[i])
                if (split_importance is not None and i < len(split_importance))
                else 0.0
            )
            g = (
                float(gain_importance[i])
                if (gain_importance is not None and i < len(gain_importance))
                else 0.0
            )
            rows.append({"feature": name, "gain": g, "split": s})
        rows.sort(key=lambda r: (r["gain"], r["split"]), reverse=True)

        os.makedirs(os.path.dirname(out_csv), exist_ok=True)
        with open(out_csv, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["feature", "gain", "split"])
            w.writeheader()
            for r in rows:
                w.writerow(r)
        return out_csv
    except Exception as e:
        print(f"[dump] failed: {e}")
        return ""


def main() -> None:
    # モデルは active_model.json or 既定の models/LightGBM_clf.pkl を使う
    active_meta = os.path.join("models", "active_model.json")
    if os.path.exists(active_meta):
        try:
            j = json.load(open(active_meta, encoding="utf-8"))
            model_path = (
                j.get("target_path")
                or j.get("source_path")
                or os.path.join("models", "LightGBM_clf.pkl")
            )
        except Exception:
            model_path = os.path.join("models", "LightGBM_clf.pkl")
    else:
        model_path = os.path.join("models", "LightGBM_clf.pkl")

    rp, rep = _load_latest_report()
    if rep is None:
        print("[dump] no reports found. specify features manually.")
        return
    feats = _load_features_from_report(rep)
    if not feats:
        print("[dump] features not found in report.")
        return

    model = _load_model(model_path)
    tag = "manual"
    if rep is not None:
        lh = (rep.get("lookahead") or {}).get("selected")
        tag = f"lk{lh}" if lh is not None else "manual"
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_csv = os.path.join("logs", "retrain", f"feat_importance_{tag}_{ts}.csv")
    out = _write_feat_csv(model, feats, out_csv)
    if out:
        print("[dump] wrote:", out)
        print("[dump] top5 preview:")
        with open(out, encoding="utf-8") as f:
            for i, line in enumerate(f):
                print(line.rstrip())
                if i >= 5:
                    break


if __name__ == "__main__":
    main()



=== file: tools/inspect_report.py ===

﻿import json, glob

rp = sorted(glob.glob("logs/retrain/report_*.json"))[-1]
with open(rp, encoding="utf-8") as f:
    j = json.load(f)

lk = j.get("lookahead", {})
print("report:", rp)
print("status:", j.get("status"))
print("selected_lookahead:", lk.get("selected"))
print("promote_thresholds:", j.get("promote_thresholds"))
print("metrics_test:", j.get("metrics_test"))
print("calibration:", j.get("calibration"))
print("candidates (lookahead -> f1@best, auc@cal):")
for c in lk.get("candidates", []):
    m = c["metrics_test"]
    print(f"  L={c['lookahead']}: f1@best={m.get('f1@best')}, auc@cal={m.get('auc@cal', m.get('auc'))}")



=== file: tools/list_wfo_reports.py ===

"""
tools/list_wfo_reports.py

Walkforward / 再学習レポート (logs/retrain/report_*.json) の一覧を表示するツール。

目的:
- WFO レポートファイルの「場所」と「最低限の中身」を確認する
- GUI から参照するときの前提を揃える
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

try:
    import fxbot_path
except ImportError:
    fxbot_path = None


@dataclass
class WFOReportSummary:
    path: Path
    id: str
    created_at: datetime | None
    symbol: str | None
    timeframe: str | None
    label_horizon: int | None
    pf: float | None
    max_dd: float | None
    sharpe: float | None
    win_rate: float | None

    @classmethod
    def from_json(cls, path: Path, data: dict[str, Any]) -> WFOReportSummary:
        # 1) ID
        rid = str(data.get("id") or path.stem.replace("report_", ""))

        # 2) created_at
        created_raw = data.get("created_at")
        created_at: datetime | None = None
        if isinstance(created_raw, str):
            for fmt in ("%Y-%m-%dT%H:%M:%S", "%Y-%m-%d %H:%M:%S"):
                try:
                    created_at = datetime.strptime(created_raw, fmt)
                    break
                except ValueError:
                    continue

        # 3) top-level keys
        symbol = data.get("symbol")
        timeframe = data.get("timeframe")
        label_horizon = data.get("label_horizon")

        # 4) metrics
        metrics = data.get("metrics") or {}
        pf = metrics.get("pf")
        max_dd = metrics.get("max_dd")
        sharpe = metrics.get("sharpe")
        win_rate = metrics.get("win_rate")

        return cls(
            path=path,
            id=rid,
            created_at=created_at,
            symbol=symbol,
            timeframe=timeframe,
            label_horizon=label_horizon,
            pf=pf,
            max_dd=max_dd,
            sharpe=sharpe,
            win_rate=win_rate,
        )


def get_project_root() -> Path:
    """fxbot_path があればそれを使い、なければカレントから推測。"""
    if fxbot_path is not None and hasattr(fxbot_path, "get_project_root"):
        return Path(fxbot_path.get_project_root())
    # フォールバック：このファイルの親の親をルートとみなす
    return Path(__file__).resolve().parents[1]


def find_wfo_reports(root: Path | None = None) -> list[WFOReportSummary]:
    if root is None:
        root = get_project_root()

    logs_retrain = root / "logs" / "retrain"
    if not logs_retrain.exists():
        print(f"[WARN] logs/retrain/ が見つかりません: {logs_retrain}")
        return []

    summaries: list[WFOReportSummary] = []
    for path in sorted(logs_retrain.glob("report_*.json")):
        try:
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
            summary = WFOReportSummary.from_json(path, data)
            summaries.append(summary)
        except Exception as e:  # noqa: BLE001
            print(f"[ERROR] {path} の読み込みに失敗しました: {e}")

    return summaries


def print_table(reports: list[WFOReportSummary]) -> None:
    if not reports:
        print("[INFO] WFO レポートが見つかりませんでした。")
        return

    # ヘッダ
    header = [
        "idx",
        "id",
        "created_at",
        "symbol",
        "tf",
        "horizon",
        "PF",
        "MaxDD",
        "WinRate",
        "path",
    ]
    print("\t".join(header))

    for idx, r in enumerate(reports, start=1):
        created_str = (
            r.created_at.strftime("%Y-%m-%d %H:%M:%S") if r.created_at else "-"
        )
        row = [
            str(idx),
            r.id,
            created_str,
            r.symbol or "-",
            r.timeframe or "-",
            str(r.label_horizon) if r.label_horizon is not None else "-",
            f"{r.pf:.3f}" if isinstance(r.pf, (int, float)) else "-",
            f"{r.max_dd:.3f}" if isinstance(r.max_dd, (int, float)) else "-",
            f"{r.win_rate:.3f}" if isinstance(r.win_rate, (int, float)) else "-",
            str(r.path.relative_to(get_project_root())),
        ]
        print("\t".join(row))


def main() -> None:
    root = get_project_root()
    print(f"[INFO] project_root = {root}")
    reports = find_wfo_reports(root)
    print_table(reports)


if __name__ == "__main__":
    main()



=== file: tools/train_lightgbm.py ===

# tools/train_lightgbm.py
from __future__ import annotations
from pathlib import Path
import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
from app.strategies.ai_strategy import build_features_recipe

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_ROOT / "data" / "USDJPY" / "ohlcv" / "USDJPY_M15.csv"
MODEL_DIR = PROJECT_ROOT / "models"
MODEL_DIR.mkdir(exist_ok=True, parents=True)

# =============================
# パラメータ設定
# =============================
LOOKAHEAD = 5  # 5本先の終値と比較して上昇しているかを分類
THRESH_PCT = 0.001  # 0.1%以上上昇を1とみなす

# =============================
# データ読み込み & 特徴量生成
# =============================
print(f"[train] load {DATA_PATH}")
df = pd.read_csv(DATA_PATH)
df["time"] = pd.to_datetime(df["time"])

# build_features_recipe() は ai_strategy.py にある既存関数
feat = build_features_recipe(df, "ohlcv_tech_v1")

# 目的変数：5バー後に0.1%以上上昇しているか
feat["target"] = (feat["close"].shift(-LOOKAHEAD) / feat["close"] - 1.0 > THRESH_PCT).astype(int)
feat = feat.dropna().reset_index(drop=True)

X = feat.drop(columns=["time", "target"])
y = feat["target"]

print(f"[train] samples={len(X)} features={X.shape[1]} pos_rate={y.mean():.3f}")

# =============================
# モデル学習
# =============================
params = dict(
    objective="binary",
    metric="binary_logloss",
    learning_rate=0.05,
    num_leaves=31,
    n_estimators=200,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
)

model = lgb.LGBMClassifier(**params)
model.fit(X, y)

# =============================
# モデル保存（pkl と Booster の二刀流）
# =============================
MODEL_PATH = MODEL_DIR / "LightGBM_clf.pkl"
BOOSTER_PATH = MODEL_DIR / "LightGBM_clf.txt"

# pkl（互換性重視）
joblib.dump(model, MODEL_PATH, compress=0, protocol=4)
print(f"[train] saved model (pkl) -> {MODEL_PATH}")

# booster テキスト（フォールバック用）
try:
    booster = model.booster_
    booster.save_model(str(BOOSTER_PATH))
    print(f"[train] saved model (booster txt) -> {BOOSTER_PATH}")
except Exception as e:
    print(f"[train] WARN: booster save failed: {e}")



=== file: tools/train_scaler.py ===

# tools/train_scaler.py
from pathlib import Path
import json
import pandas as pd
from sklearn.preprocessing import StandardScaler
from app.strategies.ai_strategy import build_features_recipe

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_ROOT / "data" / "USDJPY" / "ohlcv" / "USDJPY_M15.csv"
INFO_PATH = PROJECT_ROOT / "models" / "LightGBM_info.json"
OUT_DIR = PROJECT_ROOT / "models" / "scalers"
OUT_DIR.mkdir(parents=True, exist_ok=True)

print(f"[train_scaler] load {DATA_PATH}")
df = pd.read_csv(DATA_PATH)
feat = build_features_recipe(df, "ohlcv_tech_v1")

# 学習時の列順を取り出す
info = json.loads(INFO_PATH.read_text(encoding="utf-8"))
cols = info["features"]
missing = [c for c in cols if c not in feat.columns]
if missing:
    raise RuntimeError(f"[train_scaler] missing columns from build_features: {missing}")

X = feat.loc[:, cols].dropna()

scaler = StandardScaler()
scaler.fit(X.values)

import joblib
joblib.dump(scaler, OUT_DIR / "std_v1.pkl")
print(f"[train_scaler] saved: {OUT_DIR / 'std_v1.pkl'} (shape={X.shape})")

