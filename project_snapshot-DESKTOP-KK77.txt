############################################################
# project_snapshot
############################################################
generated_at: 2025-11-15 17:04:04
project_root: D:\macht\OneDrive\fxbot

NOTE:
  - このファイルは ChatGPT にプロジェクト構造と主要ファイルを伝えるためのスナップショットです。
  - ログ・データ・モデル・.git・.venv などは除外しています。
  - ディレクトリツリーにはファイルごとの最終更新日時 (updated: ...) を含みます。

========================================
=== DIRECTORY TREE =====================
========================================

fxbot/
.gitignore (updated: 2025-10-30 13:42:17)
.importlinter.ini (updated: 2025-11-08 15:23:56)
active_model.json (updated: 2025-11-13 09:14:58)
app/
  __init__.py (updated: 2025-11-07 17:20:34)
  core/
    config_loader.py (updated: 2025-11-08 16:30:44)
    data_finder.py (updated: 2025-11-08 19:07:54)
    logger.py (updated: 2025-11-08 18:42:29)
    market.py (updated: 2025-11-08 16:28:41)
    mt5_client.py (updated: 2025-11-08 17:48:29)
  gui/
    __init__.py (updated: 2025-11-01 12:57:56)
    ai_tab.py (updated: 2025-11-07 18:48:25)
    backtest_tab.py (updated: 2025-11-12 18:29:20)
    control_tab.py (updated: 2025-11-09 14:38:03)
    dashboard_tab.py (updated: 2025-11-07 18:01:58)
    dashboard_tab_qt.py (updated: 2025-11-07 18:38:07)
    history_tab.py (updated: 2025-11-08 16:13:26)
    main.py (updated: 2025-11-09 15:06:13)
    widgets/
      feature_importance.py (updated: 2025-11-08 16:37:43)
  main_tk.py (updated: 2025-11-08 17:52:21)
  services/
    __init__.py (updated: 2025-11-07 17:20:34)
    ai_service.py (updated: 2025-11-08 17:51:10)
    aisvc_loader.py (updated: 2025-11-13 09:32:15)
    circuit_breaker.py (updated: 2025-11-04 18:26:11)
    data_guard.py (updated: 2025-11-09 16:25:51)
    event_store.py (updated: 2025-11-15 11:03:00)
    execution_stub.py (updated: 2025-11-15 11:06:33)
    metrics.py (updated: 2025-11-09 16:02:03)
    mt5_service.py (updated: 2025-11-08 16:29:33)
    orderbook_stub.py (updated: 2025-11-08 16:30:57)
    trade_service.py (updated: 2025-11-08 17:50:39)
    trade_state.py (updated: 2025-11-08 16:28:37)
    trailing.py (updated: 2025-10-31 18:18:13)
    trailing_hook.py (updated: 2025-11-08 16:30:32)
  strategies/
    __init__.py (updated: 2025-11-09 19:59:41)
    ai_strategy.py (updated: 2025-11-10 17:41:41)
configs/
  .env.example (updated: 2025-10-30 13:42:31)
  config copy.yaml (updated: 2025-10-31 18:17:51)
  config.local.yaml (updated: 2025-10-30 16:40:40)
  config.yaml (updated: 2025-11-08 19:03:02)
core/
  __init__.py (updated: 2025-11-07 17:20:34)
  ai/
    __init__.py (updated: 2025-10-30 17:35:38)
    calibration.py (updated: 2025-11-08 16:39:34)
    features.py (updated: 2025-11-08 18:06:34)
    loader.py (updated: 2025-11-09 15:51:02)
    service.py (updated: 2025-11-09 15:37:46)
  config.py (updated: 2025-11-08 16:30:52)
  indicators.py (updated: 2025-10-31 17:37:37)
  metrics/
  metrics.py (updated: 2025-11-01 17:49:30)
    __init__.py (updated: 2025-11-07 18:25:03)
    fi_extractor.py (updated: 2025-11-08 16:37:25)
    registry.py (updated: 2025-11-08 16:30:26)
  position_guard.py (updated: 2025-11-04 18:25:46)
  utils/
    __init__.py (updated: 2025-10-30 17:35:43)
    clock.py (updated: 2025-10-31 17:59:26)
    hashing.py (updated: 2025-10-30 17:35:50)
    runtime.py (updated: 2025-11-04 18:04:57)
    timeutil.py (updated: 2025-10-30 17:35:56)
fxbot_path.py (updated: 2025-11-15 10:58:16)
models_store/
  LightGBM_20251113_0914.pkl (updated: 2025-11-13 09:14:58)
mypy.ini (updated: 2025-11-08 15:55:54)
notebooks/
PROJECT_MAP.md (updated: 2025-11-08 12:15:22)
project_snapshot.txt (updated: 2025-11-15 11:15:34)
project_tree.txt (updated: 2025-11-08 15:20:24)
pyproject.toml (updated: 2025-11-13 09:01:25)
pytest.ini (updated: 2025-11-08 15:25:11)
README.md (updated: 2025-10-30 13:42:10)
requirements.txt (updated: 2025-11-15 17:02:50)
runtime/
  metrics.json (updated: 2025-11-15 11:09:54)
  metrics_044ho80o.json (updated: 2025-11-13 16:03:27)
  metrics_04i6g_bm.json (updated: 2025-11-13 16:03:42)
  metrics_08lb8guj.json (updated: 2025-11-13 16:16:56)
  metrics_0fdsym9v.json (updated: 2025-11-13 16:09:10)
  metrics_0ks5iv74.json (updated: 2025-11-13 16:08:24)
  metrics_0m3pzxsa.json (updated: 2025-11-13 16:06:46)
  metrics_0s0y1mq_.json (updated: 2025-11-13 16:12:04)
  metrics_0so3t4e4.json (updated: 2025-11-13 16:08:49)
  metrics_0t6las4q.json (updated: 2025-11-13 16:17:06)
  metrics_13d1atob.json (updated: 2025-11-13 16:15:59)
  metrics_1_r7f8xe.json (updated: 2025-11-13 16:10:21)
  metrics_1ce8in0z.json (updated: 2025-11-13 16:09:05)
  metrics_1f8wcifb.json (updated: 2025-11-13 16:10:11)
  metrics_1nqq6c9g.json (updated: 2025-11-13 16:13:41)
  metrics_1y_j88c6.json (updated: 2025-11-13 16:05:40)
  metrics_2b5irl55.json (updated: 2025-11-13 16:16:20)
  metrics_2llsmgld.json (updated: 2025-11-13 16:07:12)
  metrics_2wh_r543.json (updated: 2025-11-13 16:07:27)
  metrics_32ej_3qs.json (updated: 2025-11-13 16:06:41)
  metrics_384xt0qj.json (updated: 2025-11-13 16:08:03)
  metrics_3_z7j5dq.json (updated: 2025-11-13 16:11:13)
  metrics_3op0smhm.json (updated: 2025-11-13 16:13:31)
  metrics_3pck01qi.json (updated: 2025-11-09 15:59:06)
  metrics_3pw7kppo.json (updated: 2025-11-09 15:59:00)
  metrics_3w4e28bl.json (updated: 2025-11-13 16:15:44)
  metrics_3wwdl7sf.json (updated: 2025-11-13 16:16:46)
  metrics_3xjcntun.json (updated: 2025-11-09 15:59:03)
  metrics_4fx6u97k.json (updated: 2025-11-13 16:14:53)
  metrics_4jkcw838.json (updated: 2025-11-13 16:16:51)
  metrics_4jwlaw79.json (updated: 2025-11-13 16:06:11)
  metrics_4sqd0ccq.json (updated: 2025-11-13 16:11:02)
  metrics_4u83ovxn.json (updated: 2025-11-15 11:09:54)
  metrics_51fzibz0.json (updated: 2025-11-15 11:09:34)
  metrics_56tbpp98.json (updated: 2025-11-13 16:11:08)
  metrics_5_uus6ny.json (updated: 2025-11-13 16:08:39)
  metrics_5c1keku_.json (updated: 2025-11-13 16:09:30)
  metrics_681i8dz0.json (updated: 2025-11-15 11:09:39)
  metrics_6g2ygykl.json (updated: 2025-11-13 16:08:13)
  metrics_6jx0f61y.json (updated: 2025-11-13 16:17:01)
  metrics_6mv45wka.json (updated: 2025-11-13 16:08:44)
  metrics_6n99dteb.json (updated: 2025-11-13 16:06:31)
  metrics_6oizo1ba.json (updated: 2025-11-13 16:12:19)
  metrics_6u8fjy4e.json (updated: 2025-11-13 16:11:59)
  metrics_751tqi7x.json (updated: 2025-11-15 11:09:29)
  metrics_78ji6p4u.json (updated: 2025-11-13 16:09:56)
  metrics_7bbmsyvw.json (updated: 2025-11-13 16:04:13)
  metrics_7caerxlw.json (updated: 2025-11-13 16:14:22)
  metrics_7pcppci4.json (updated: 2025-11-13 16:17:11)
  metrics_7upf8_is.json (updated: 2025-11-13 16:07:17)
  metrics_7wtig7zq.json (updated: 2025-11-13 16:15:29)
  metrics_7yx4f9p_.json (updated: 2025-11-10 08:03:17)
  metrics_8i0ku3__.json (updated: 2025-11-15 11:09:24)
  metrics_8iecfllz.json (updated: 2025-11-07 19:20:22)
  metrics_8s427o_h.json (updated: 2025-11-13 16:07:02)
  metrics_8t5wcv0v.json (updated: 2025-11-13 16:14:27)
  metrics_8wbixgeh.json (updated: 2025-11-13 16:09:25)
  metrics_9clf0tik.json (updated: 2025-11-13 16:13:00)
  metrics_9wqc4wjf.json (updated: 2025-11-13 16:15:13)
  metrics_9y2xjt1a.json (updated: 2025-11-10 08:03:47)
  metrics__7n81rbm.json (updated: 2025-11-10 08:03:23)
  metrics__9tk6o8d.json (updated: 2025-11-13 16:14:17)
  metrics___1o6ki2.json (updated: 2025-11-13 16:06:16)
  metrics__cx1yvq5.json (updated: 2025-11-13 16:14:38)
  metrics__g0tzwco.json (updated: 2025-11-13 16:05:24)
  metrics__g6j76yc.json (updated: 2025-11-13 16:08:19)
  metrics__l9umv1h.json (updated: 2025-11-13 16:17:16)
  metrics__nrd_y1k.json (updated: 2025-11-13 16:13:51)
  metrics__rjbfcpd.json (updated: 2025-11-13 16:10:16)
  metrics__s4hfi2b.json (updated: 2025-11-13 16:14:48)
  metrics__tv_40i0.json (updated: 2025-11-13 16:08:54)
  metrics__vtagxy_.json (updated: 2025-11-13 16:06:21)
  metrics_a4qsce4c.json (updated: 2025-11-13 16:04:38)
  metrics_a_jw5ekj.json (updated: 2025-11-13 16:06:26)
  metrics_abcuwh_8.json (updated: 2025-11-10 08:03:35)
  metrics_aen0sw0t.json (updated: 2025-11-13 16:13:05)
  metrics_avlwot5z.json (updated: 2025-11-13 16:11:23)
  metrics_b_e39daa.json (updated: 2025-11-13 16:13:21)
  metrics_beh8fq83.json (updated: 2025-11-13 16:04:49)
  metrics_bkm4j_2_.json (updated: 2025-11-13 16:09:00)
  metrics_bmocgusp.json (updated: 2025-11-13 16:13:57)
  metrics_bsejl6ul.json (updated: 2025-11-13 16:10:27)
  metrics_c19f7jhr.json (updated: 2025-11-13 16:03:57)
  metrics_c6dlu7j3.json (updated: 2025-11-13 16:08:34)
  metrics_c7srgc6l.json (updated: 2025-11-13 16:03:52)
  metrics_cfvm2vuv.json (updated: 2025-11-13 16:12:14)
  metrics_cs6cyrr4.json (updated: 2025-11-13 16:13:16)
  metrics_ctdk6hr_.json (updated: 2025-11-13 16:13:36)
  metrics_d0zio68n.json (updated: 2025-11-13 16:15:18)
  metrics_d2fneb6c.json (updated: 2025-11-13 16:16:30)
  metrics_d83v0qrg.json (updated: 2025-11-13 16:10:57)
  metrics_d_7cu4co.json (updated: 2025-11-13 16:09:51)
  metrics_e55ccqf4.json (updated: 2025-11-13 16:11:28)
  metrics_efewdoso.json (updated: 2025-11-13 16:12:40)
  metrics_es10kbkj.json (updated: 2025-11-13 16:14:12)
  metrics_exyc2aoi.json (updated: 2025-11-13 16:12:09)
  metrics_f4ba8yj0.json (updated: 2025-11-13 16:07:38)
  metrics_fusrtchw.json (updated: 2025-11-13 16:12:35)
  metrics_fvkjx3i3.json (updated: 2025-11-13 16:14:02)
  metrics_fwgrd7ek.json (updated: 2025-11-13 16:13:26)
  metrics_fx59gkw6.json (updated: 2025-11-13 16:14:07)
  metrics_gctnecas.json (updated: 2025-11-13 16:12:24)
  metrics_gdrsfom2.json (updated: 2025-11-13 16:12:55)
  metrics_gh0exqfy.json (updated: 2025-11-13 16:04:28)
  metrics_gn3r368k.json (updated: 2025-11-10 08:03:29)
  metrics_gre0l2vv.json (updated: 2025-11-13 16:03:22)
  metrics_gu2jj4x6.json (updated: 2025-11-09 15:53:13)
  metrics_gy7yzni_.json (updated: 2025-11-15 11:09:49)
  metrics_h2ke1que.json (updated: 2025-11-13 16:11:38)
  metrics_h2zbzsi6.json (updated: 2025-11-13 16:15:34)
  metrics_i5pwbdy8.json (updated: 2025-11-13 16:05:04)
  metrics_i9ixee1p.json (updated: 2025-11-13 16:05:30)
  metrics_iegm05ma.json (updated: 2025-11-13 16:07:07)
  metrics_jeul60z4.json (updated: 2025-11-13 16:10:42)
  metrics_jldkp6oj.json (updated: 2025-11-10 08:03:53)
  metrics_joyqy1yz.json (updated: 2025-11-13 16:09:41)
  metrics_k452r_bd.json (updated: 2025-11-09 15:58:57)
  metrics_keyuq093.json (updated: 2025-11-13 16:04:43)
  metrics_kijozz54.json (updated: 2025-11-13 16:10:52)
  metrics_l0nrt8ax.json (updated: 2025-11-13 16:05:14)
  metrics_l4qkqhvl.json (updated: 2025-11-13 16:11:18)
  metrics_l9a3g7f_.json (updated: 2025-11-10 08:03:11)
  metrics_lg3z_ojj.json (updated: 2025-11-13 16:04:08)
  metrics_lij48idf.json (updated: 2025-11-13 16:16:35)
  metrics_mfx0u92s.json (updated: 2025-11-13 16:06:00)
  metrics_ml129ig1.json (updated: 2025-11-13 16:07:48)
  metrics_mziwmuji.json (updated: 2025-11-13 16:05:55)
  metrics_mzkihnhl.json (updated: 2025-11-13 16:06:05)
  metrics_n92kltdx.json (updated: 2025-11-13 16:11:43)
  metrics_naa4pb8_.json (updated: 2025-11-13 16:09:20)
  metrics_o0bwp99t.json (updated: 2025-11-13 16:07:58)
  metrics_oe0gx0ow.json (updated: 2025-11-13 16:05:50)
  metrics_og6_x57t.json (updated: 2025-11-13 16:07:53)
  metrics_okatezba.json (updated: 2025-11-13 16:09:35)
  metrics_olwp427j.json (updated: 2025-11-13 16:15:54)
  metrics_oy5jty8q.json (updated: 2025-11-13 16:03:32)
  metrics_pbfm5_s8.json (updated: 2025-11-13 16:12:29)
  metrics_pguw1jt8.json (updated: 2025-11-13 16:06:36)
  metrics_q1mfycvq.json (updated: 2025-11-13 16:09:15)
  metrics_q32vexb9.json (updated: 2025-11-13 16:03:37)
  metrics_qd9sdimy.json (updated: 2025-11-13 16:11:49)
  metrics_qmjxkoxo.json (updated: 2025-11-13 16:05:35)
  metrics_qygbv_em.json (updated: 2025-11-13 16:04:02)
  metrics_r1diayz3.json (updated: 2025-11-09 16:02:59)
  metrics_r1oyukcw.json (updated: 2025-11-13 16:14:32)
  metrics_r2iljktq.json (updated: 2025-11-09 16:03:05)
  metrics_r4cxpbus.json (updated: 2025-11-13 16:10:06)
  metrics_ri5kzye8.json (updated: 2025-11-13 16:08:29)
  metrics_s_yeu75a.json (updated: 2025-11-13 16:04:33)
  metrics_smrtxq4q.json (updated: 2025-11-13 16:16:15)
  metrics_sunrm50l.json (updated: 2025-11-13 16:12:45)
  metrics_szni51cl.json (updated: 2025-11-13 16:08:08)
  metrics_szy5o819.json (updated: 2025-11-13 16:06:57)
  metrics_t09rkk69.json (updated: 2025-11-13 16:16:25)
  metrics_tnyuui1p.json (updated: 2025-11-13 16:16:05)
  metrics_tvk5w34y.json (updated: 2025-11-13 16:07:32)
  metrics_u85k0bty.json (updated: 2025-11-13 16:04:18)
  metrics_ue987r_m.json (updated: 2025-11-13 16:15:49)
  metrics_ugsv8j1h.json (updated: 2025-11-13 16:05:19)
  metrics_uhoep47z.json (updated: 2025-11-13 16:04:54)
  metrics_uiqkfigp.json (updated: 2025-11-13 16:10:47)
  metrics_ujl1qfgc.json (updated: 2025-11-13 16:05:45)
  metrics_v30su220.json (updated: 2025-11-13 16:15:39)
  metrics_v372j6bp.json (updated: 2025-11-13 16:10:01)
  metrics_vd8t6g2k.json (updated: 2025-11-13 16:11:33)
  metrics_vi859rsu.json (updated: 2025-11-13 16:15:24)
  metrics_vlx06rwe.json (updated: 2025-11-13 16:04:59)
  metrics_vm46jhw3.json (updated: 2025-11-10 08:03:41)
  metrics_vzimyuwy.json (updated: 2025-11-13 16:12:50)
  metrics_w88v4g45.json (updated: 2025-11-13 16:10:37)
  metrics_wcsasy33.json (updated: 2025-11-13 16:16:10)
  metrics_x7rwrpb_.json (updated: 2025-11-09 15:53:10)
  metrics_xbjh179p.json (updated: 2025-11-15 11:09:44)
  metrics_xcr4m91b.json (updated: 2025-11-13 16:09:46)
  metrics_xcyd97h5.json (updated: 2025-11-13 16:07:43)
  metrics_xe7x_sje.json (updated: 2025-11-13 16:04:23)
  metrics_xjeelk50.json (updated: 2025-11-13 16:15:08)
  metrics_xqnkdh58.json (updated: 2025-11-13 16:10:32)
  metrics_xuw93ulk.json (updated: 2025-11-13 16:05:09)
  metrics_xxx7__b9.json (updated: 2025-11-13 16:06:51)
  metrics_y1mt7hlm.json (updated: 2025-11-13 16:14:43)
  metrics_y5_m62os.json (updated: 2025-11-13 16:15:03)
  metrics_yehnyu4c.json (updated: 2025-11-13 16:07:22)
  metrics_yuncl873.json (updated: 2025-11-13 16:14:58)
  metrics_yv04iu3_.json (updated: 2025-11-13 16:11:54)
  metrics_z4d75mrh.json (updated: 2025-11-13 16:16:40)
  metrics_zn9wzer_.json (updated: 2025-11-13 16:03:47)
  metrics_zrz0v00o.json (updated: 2025-11-13 16:13:46)
  metrics_zskuik5i.json (updated: 2025-11-13 16:13:10)
  trade_state.json (updated: 2025-11-09 15:27:01)
scripts/
  cb_smoke.py (updated: 2025-11-08 18:04:19)
  diagnose_symbol.py (updated: 2025-11-08 16:28:25)
  dryrun_smoke.py (updated: 2025-11-08 17:52:10)
  export_mt5_history.py (updated: 2025-11-08 16:33:31)
  export_val_probs.py (updated: 2025-11-08 17:49:06)
  make_csv_from_mt5.py (updated: 2025-11-09 11:55:06)
  make_project_snapshot.py (updated: 2025-11-15 11:15:03)
  make_toy_model.py (updated: 2025-11-08 18:06:59)
  mt5_export_csv.py (updated: 2025-11-08 19:08:33)
  mt5_smoke.py (updated: 2025-11-15 13:27:41)
  print_runtime.py (updated: 2025-11-04 18:05:28)
  promote_model.py (updated: 2025-11-08 16:28:33)
  register_weekly_task.ps1 (updated: 2025-11-05 16:02:04)
  rollback_model.py (updated: 2025-11-08 16:28:29)
  selftest_order_flow.py (updated: 2025-11-08 16:12:40)
  sim_trailing.py (updated: 2025-10-31 18:45:58)
  swap_model.py (updated: 2025-11-08 16:13:21)
  train_calibrator.py (updated: 2025-10-31 10:58:42)
  verify_smoke.ps1 (updated: 2025-11-04 18:09:12)
  walkforward_retrain.py (updated: 2025-11-14 06:44:09)
  walkforward_train.py (updated: 2025-11-08 18:07:05)
  weekly_retrain.py (updated: 2025-11-13 10:53:00)
  weekly_wf.ps1 (updated: 2025-11-04 19:33:48)
setup_weekly_job.ps1 (updated: 2025-11-13 09:16:34)
temp_equity_check.py (updated: 2025-11-12 17:23:39)
tests/
  test_sanity.py (updated: 2025-11-08 16:28:20)
To Doリスト.txt (updated: 2025-10-30 12:27:55)
To Doリスト2.txt (updated: 2025-10-30 12:27:55)
tools/
  __init__.py (updated: 2025-11-09 20:00:29)
  backtest_equity_curve.py (updated: 2025-11-08 17:48:36)
  backtest_run.py (updated: 2025-11-12 10:57:30)
  dump_feature_importance.py (updated: 2025-11-08 16:29:47)
  export_tree_clean.ps1 (updated: 2025-11-08 15:20:10)
  inspect_report.py (updated: 2025-11-06 17:29:33)
  train_lightgbm.py (updated: 2025-11-10 17:19:21)
  train_scaler.py (updated: 2025-11-10 16:21:05)
やるべきこと.txt (updated: 2025-11-05 16:24:05)
フェーズA~Hまでの結果.txt (updated: 2025-10-31 17:46:59)
仕様の統合まとめ.docx (updated: 2025-11-10 16:03:06)
仮想環境.txt (updated: 2025-11-13 17:30:21)
作業6～8.txt (updated: 2025-10-31 18:09:22)
作業9.txt (updated: 2025-10-31 19:02:14)
質問.txt (updated: 2025-11-13 17:30:23)


========================================
=== FILE CONTENTS ======================
========================================


=== file: .importlinter.ini ===

[importlinter]
# プロジェクトのルートパッケージ（＝ import 解析の起点）
root_package = app
include_external_packages = False

# レイヤー順序：gui -> services -> core
[contract:layers_gui_services_core]
name = GUI/Service/Core のレイヤー厳守
type = layers
layers =
    app.gui
    app.services
    app.core

# GUI → core への“直リンク”を禁止（必ず services 経由）
[contract:forbid_gui_to_core_direct]
name = GUI から core 直輸入の禁止
type = forbidden
source_modules =
    app.gui
forbidden_modules =
    app.core

# batch（存在する場合）→ gui 依存を禁止（バッチはUIに依らない）
[contract:batch_isolation]
name = batch は GUI に依存しない
type = forbidden
source_modules =
    app.batch
forbidden_modules =
    app.gui

# 重要：ルート直下の core/ へは依存させない（混線防止）
[contract:forbid_top_core]
name = app.* から ルート直下 core への依存を禁止
type = forbidden
source_modules =
    app
forbidden_modules =
    core



=== file: active_model.json ===

{
  "model_name": "LightGBM_20251113_0914.pkl",
  "version": "20251113_0914",
  "features_hash": "placeholder_hash",
  "trained_at_jst": "2025-11-13 09:14:58"
}


=== file: app/__init__.py ===




=== file: app/core/config_loader.py ===

from pathlib import Path
from typing import Any, Dict

import yaml


def _deep_merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    for k, v in (b or {}).items():
        if isinstance(v, dict) and isinstance(a.get(k), dict):
            a[k] = _deep_merge(a[k], v)
        else:
            a[k] = v
    return a


def load_config() -> Dict[str, Any]:
    base: Dict[str, Any] = {}
    base_path = Path("configs/config.yaml")
    if base_path.exists():
        base = yaml.safe_load(base_path.read_text(encoding="utf-8")) or {}
    local_path = Path("configs/config.local.yaml")
    if local_path.exists():
        local: Dict[str, Any] = yaml.safe_load(local_path.read_text(encoding="utf-8")) or {}
        base = _deep_merge(base, local)
    return base



=== file: app/core/data_finder.py ===

# app/core/data_finder.py
from __future__ import annotations
import os, glob
from pathlib import Path
from typing import Iterable, Optional, Tuple
import pandas as pd

def _expand_path(p: str) -> Iterable[Path]:
    # 環境変数・%APPDATA% を解決し、glob を展開
    p = os.path.expandvars(p)
    p = os.path.expanduser(p)
    for hit in glob.glob(p, recursive=True):
        yield Path(hit)

def _load_csv(path: Path) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        low = {c.lower(): c for c in df.columns}
        need = ["time","open","high","low","close"]
        if not all(k in low for k in need):
            return None
        return df
    except Exception:
        return None

def resolve_csv(symbol: str, timeframe: str, search_paths: Iterable[str]) -> Tuple[Optional[Path], Optional[pd.DataFrame]]:
    """search_paths から {SYMBOL}_{TF}.csv を探して返す。最初に見つかったものを採用。"""
    target_name = f"{symbol.upper()}_{timeframe.upper()}.csv"
    for base in search_paths:
        for base_path in _expand_path(base):
            if not base_path.exists():
                continue
            # 1) 直下にある場合
            p1 = base_path / target_name
            if p1.exists():
                df = _load_csv(p1)
                if df is not None:
                    return p1, df
            # 2) サブフォルダも探索
            for hit in base_path.rglob(target_name):
                df = _load_csv(hit)
                if df is not None:
                    return hit, df
    return None, None



=== file: app/core/logger.py ===

# app/core/logger.py
from loguru import logger
import sys
from pathlib import Path

LOG_DIR = Path(__file__).resolve().parents[2] / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)

def setup():
    """Loguru ロガーの共通設定"""
    logger.remove()
    logger.add(sys.stdout, level="INFO", format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}")
    logger.add(
        LOG_DIR / "app.log",
        rotation="10 MB",
        retention="10 days",
        compression="zip",
        level="INFO",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    )
    return logger

# すぐ利用できるように初期化
setup()



=== file: app/core/market.py ===

import MetaTrader5 as mt5
from typing import Any, Optional, Tuple


def _pip_from_symbol_info(si: Any) -> float:
    # 例: USDJPY なら point=0.001 → pip=0.01（= point*10）
    return float(si.point) * 10.0 if si and si.point else 0.0

def select_symbol(symbol: str) -> bool:
    si = mt5.symbol_info(symbol)
    if si is None:
        return False
    if not si.visible:
        if not mt5.symbol_select(symbol, True):
            return False
    return True

def spread_pips(symbol: str) -> Optional[float]:
    if not select_symbol(symbol):
        return None
    si = mt5.symbol_info(symbol)
    tick = mt5.symbol_info_tick(symbol)
    pip = _pip_from_symbol_info(si)
    if tick and pip > 0:
        return (float(tick.ask) - float(tick.bid)) / pip
    if si and pip > 0 and si.ask and si.bid:
        return (float(si.ask) - float(si.bid)) / pip
    if si and pip > 0 and si.spread:
        try:
            return (float(si.spread) * float(si.point)) / pip
        except Exception:
            pass
    return None

def tick(symbol: str) -> Optional[Tuple[float, float]]:
    """(bid, ask) を返す。取得失敗で None。"""
    if not select_symbol(symbol):
        return None
    t = mt5.symbol_info_tick(symbol)
    if t is None:
        return None
    return float(t.bid), float(t.ask)

def pips_to_price(symbol: str, pips: float) -> Optional[float]:
    if not select_symbol(symbol):
        return None
    si = mt5.symbol_info(symbol)
    pip = _pip_from_symbol_info(si)
    return pips * pip if pip > 0 else None



=== file: app/core/mt5_client.py ===

from typing import Any

import MetaTrader5 as mt5
import pandas as pd
from loguru import logger

# 使い方メモ：
# initialize() → get_account_info()/get_positions() → shutdown()
# GUI側で毎回初期化・終了してもOKだが、将来的には使い回しも検討（接続維持監視を足す）

def initialize() -> bool:
    if not mt5.initialize():
        logger.error(f"MT5接続失敗: {mt5.last_error()}")
        return False
    return True

def shutdown() -> None:
    try:
        mt5.shutdown()
    except Exception as e:
        logger.warning(f"MT5 shutdown 警告: {e}")

def get_account_info() -> dict[str, Any] | None:
    info = mt5.account_info()
    if info is None:
        return None
    return {
        "balance": float(info.balance),
        "equity": float(info.equity),
        "margin": float(info.margin),
        "free_margin": float(info.margin_free),
        "profit": float(info.profit),
    }

def get_positions() -> list[dict[str, Any]]:
    """
    ポジション一覧を辞書のリストで返す。取得に失敗した場合は空リスト。
    """
    known_cols = ["ticket", "symbol", "type", "volume", "price_open", "sl", "tp", "profit"]
    positions = mt5.positions_get()

    if positions is None:
        logger.warning(f"positions_get が None: {mt5.last_error()}")
        return []

    if len(positions) == 0:
        return []

    rows: list[dict[str, Any]] = []
    for p in positions:
        data = {col: getattr(p, col, None) for col in known_cols}
        rows.append(data)
    return rows



=== file: app/gui/__init__.py ===




=== file: app/gui/ai_tab.py ===

# app/gui/ai_tab.py
from __future__ import annotations
from pathlib import Path
from PyQt6 import QtWidgets
import joblib

from app.services.ai_service import AISvc
from app.gui.widgets.feature_importance import FeatureImportanceWidget

class AITab(QtWidgets.QWidget):
    def __init__(self, parent=None, ai_service: AISvc | None = None):
        super().__init__(parent)
        self.ai_service = ai_service or AISvc()

        # 可能ならLightGBMモデルを自動ロード
        try:
            p = Path("models/LightGBM_clf.pkl")
            if p.exists():
                obj = joblib.load(p)
                model = obj.get("model", obj) if isinstance(obj, dict) else obj
                self.ai_service.models.setdefault("lgbm_cls", model)
        except Exception as e:
            # 失敗してもGUIは表示だけ先に
            print(f"[AITab] model autoload skipped: {e}")

        lay = QtWidgets.QVBoxLayout(self)
        lay.addWidget(FeatureImportanceWidget(self.ai_service, self), 1)



=== file: app/gui/backtest_tab.py ===

# app/gui/backtest_tab.py
from __future__ import annotations
from PyQt6 import QtWidgets, QtCore
from PyQt6.QtCore import Qt, QProcess
import sys, json, re
from pathlib import Path
import pandas as pd
import numpy as np
from matplotlib.figure import Figure
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as Canvas
from matplotlib.backends.backend_qtagg import NavigationToolbar2QT as Toolbar
from pandas.api.types import is_datetime64_any_dtype
from app.services.data_guard import ensure_data
from matplotlib.ticker import FuncFormatter, AutoLocator
from matplotlib.dates import AutoDateLocator, ConciseDateFormatter
from matplotlib.widgets import SpanSelector
import matplotlib.gridspec as gridspec
from functools import partial
from datetime import datetime, timedelta

def _thousands(x, pos):
    try:
        return f"{int(x):,}"
    except Exception:
        return str(x)

def _find_trades_csv(equity_csv: Path):
    """equity_curve.csv と同じフォルダにある trades*.csv を探す（優先: test -> train -> trades）"""
    cand = [
        equity_csv.with_name("trades_test.csv"),
        equity_csv.with_name("trades_train.csv"),
        equity_csv.with_name("trades.csv"),
    ]
    for c in cand:
        if c.exists():
            return c
    return None

def plot_equity_with_markers_to_figure(fig: Figure, csv_path: str, note: str = ""):
    """equity_curve.csv を描画。signal変化点でマーク。変化が無ければ trades*.csv の entry_time でマーク。"""
    p = Path(csv_path)
    try:
        df = pd.read_csv(p)
    except Exception as e:
        print(f"[gui] plot error: failed to read {csv_path}: {e}")
        return

    if "equity" not in df.columns:
        print(f"[gui] plot error: CSVに 'equity' 列がありません。columns={df.columns.tolist()}")
        return

    # 日時化
    if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
        df["time"] = pd.to_datetime(df["time"], errors="coerce")

    # ベース線
    fig.clear()
    ax = fig.add_subplot(111)
    ax.set_title("Equity Curve & Trade Markers" + (f" — {p.name}" if p.name else ""))
    ax.set_xlabel("time")
    ax.set_ylabel("equity (JPY)")
    ax.plot(df["time"], df["equity"], lw=1.4, antialiased=True, label="Equity", zorder=2)
    ax.margins(x=0.01, y=0.06)
    ax.grid(True, which="major", alpha=0.25)
    ax.yaxis.set_major_formatter(FuncFormatter(_thousands))
    ax.yaxis.set_major_locator(AutoLocator())

    # --- マーカー①: signal の変化点 ---
    buys = sells = pd.DataFrame()
    if "signal" in df.columns:
        sig = pd.to_numeric(df["signal"], errors="coerce").fillna(0).astype(int)
        chg = sig.ne(sig.shift(1)).fillna(sig.iloc[0] != 0)
        if len(sig) > 0 and sig.iloc[0] != 0:
            chg.iloc[0] = True
        buys  = df[(sig ==  1) & chg]
        sells = df[(sig == -1) & chg]

    # --- マーカー②: trades*.csv の entry_time でフォールバック ---
    if (buys.empty and sells.empty):
        tcsv = _find_trades_csv(p)
        if tcsv is not None:
            try:
                tdf = pd.read_csv(tcsv)
                # 必須列チェック
                if "entry_time" in tdf.columns:
                    tdf["entry_time"] = pd.to_datetime(tdf["entry_time"], errors="coerce")
                    # 方向の推定：directionが無ければ pnl の符号で代用
                    if "direction" in tdf.columns:
                        dirv = pd.to_numeric(tdf["direction"], errors="coerce").fillna(0).astype(int)
                    else:
                        dirv = np.sign(pd.to_numeric(tdf.get("pnl", 0.0), errors="coerce").fillna(0.0)).astype(int)
                    # エクイティ上の Y 座標は近い時刻の equity を使う
                    if "time" in df.columns:
                        eq = df[["time", "equity"]].dropna().copy()
                        eq["time"] = pd.to_datetime(eq["time"], errors="coerce")
                        eq = eq.set_index("time").sort_index()

                        # entry の最近傍 equity を拾う
                        def _y_at(t):
                            try:
                                idx = eq.index.searchsorted(t, side="left")
                                if idx == len(eq):
                                    idx -= 1
                                return float(eq.iloc[idx, 0])
                            except Exception:
                                return np.nan

                        tdf["y"] = tdf["entry_time"].map(_y_at)
                        tb = tdf[(dirv ==  1)].copy()
                        ts = tdf[(dirv == -1)].copy()
                        tb = tb.dropna(subset=["y"])
                        ts = ts.dropna(subset=["y"])
                        buys  = pd.DataFrame({"time": tb["entry_time"], "equity": tb["y"]})
                        sells = pd.DataFrame({"time": ts["entry_time"], "equity": ts["y"]})
                        print(f"[gui] fallback markers from trades: buys={len(buys)} sells={len(sells)} ({tcsv.name})")
            except Exception as e:
                print(f"[gui] trades fallback error: {e}")

    # マーカー描画
    if not buys.empty:
        ax.scatter(buys["time"], buys["equity"],
                   marker="o", s=48, facecolors="tab:blue", edgecolors="black",
                   linewidths=0.7, label="Buy", zorder=5)
    if not sells.empty:
        ax.scatter(sells["time"], sells["equity"],
                   marker="x", s=64, c="tab:orange", linewidths=1.4,
                   label="Sell", zorder=6)

    # 日付目盛り
    if "time" in df.columns:
        loc = AutoDateLocator()
        ax.xaxis.set_major_locator(loc)
        ax.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        fig.autofmt_xdate()

    ax.legend(loc="upper left")

PROJECT_ROOT = Path(__file__).resolve().parents[2]

class PlotWindow(QtWidgets.QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("FXBot — Chart")
        self.resize(1000, 650)

        self.figure = Figure(figsize=(10, 6), tight_layout=True)
        self.canvas = Canvas(self.figure)
        self.toolbar = Toolbar(self.canvas, self)
        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.toolbar)
        layout.addWidget(self.canvas)

        # 2段のaxes（上：拡大、下：全体ナビ）
        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.08, figure=self.figure)
        self.ax_main = self.figure.add_subplot(gs[0])
        self.ax_nav  = self.figure.add_subplot(gs[1], sharex=self.ax_main)
        self.span = None
        self._last_kind: str | None = None
        self._last_csv: str | None = None

    def plot_equity_csv(self, csv_path: str):
        # 上段をクリアして本描画（既存の描画関数を再利用）
        self.figure.clear()
        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.08, figure=self.figure)
        ax_main = self.figure.add_subplot(gs[0])
        ax_nav  = self.figure.add_subplot(gs[1], sharex=ax_main)

        # 読み込み
        df = pd.read_csv(csv_path)
        if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
            df["time"] = pd.to_datetime(df["time"], errors="coerce")

        # 上段：本描画（マーカー付き）
        ax_main.plot(df["time"], df["equity"], lw=1.4, label="Equity")
        ax_main.grid(True, alpha=0.25)
        sig = df.get("signal", pd.Series(0, index=df.index)).astype(int)
        chg = sig.ne(sig.shift(1)).fillna(False)
        if len(sig) > 0 and sig.iloc[0] != 0:
            chg.iloc[0] = True
        buys  = df[(sig == 1)  & chg]
        sells = df[(sig == -1) & chg]
        if not buys.empty:
            ax_main.scatter(buys["time"], buys["equity"], marker="o", s=48,
                            facecolors="tab:blue", edgecolors="black", linewidths=0.7, label="Buy", zorder=5)
        if not sells.empty:
            ax_main.scatter(sells["time"], sells["equity"], marker="x", s=64,
                            c="tab:orange", linewidths=1.4, label="Sell", zorder=6)
        ax_main.legend(loc="upper left")
        ax_main.margins(x=0.01, y=0.05)

        # 目盛り・タイトル
        from matplotlib.ticker import AutoLocator, FuncFormatter
        ax_main.set_title("Equity Curve & Trade Markers — " + Path(csv_path).name)
        ax_main.set_ylabel("equity (JPY)")
        ax_main.yaxis.set_major_locator(AutoLocator())
        ax_main.yaxis.set_major_formatter(FuncFormatter(_thousands))

        # 下段：全体ナビ（薄い線）
        ax_nav.plot(df["time"], df["equity"], lw=1.0, alpha=0.5)
        ax_nav.grid(True, alpha=0.2)

        # SpanSelector で範囲選択 → 上段の xlim を同期
        def onselect(xmin, xmax):
            ax_main.set_xlim(xmin, xmax)
            self.canvas.draw_idle()

        self.span = SpanSelector(ax_nav, onselect, "horizontal", useblit=True,
                                 interactive=True, props=dict(alpha=0.15))

        # 目盛り体裁
        loc = AutoDateLocator()
        ax_nav.xaxis.set_major_locator(loc)
        ax_nav.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        ax_main.xaxis.set_major_locator(loc)
        ax_main.xaxis.set_major_formatter(ConciseDateFormatter(loc))
        self.figure.autofmt_xdate()
        self.canvas.draw_idle()

        # 保存（期間ジャンプ用に参照）
        self.ax_main = ax_main
        self.ax_nav  = ax_nav
        self._last_kind = "equity"
        self._last_csv = str(csv_path)

    def plot_price_preview(self, csv_path: str, note: str = ""):
        import pandas as pd
        from matplotlib.dates import AutoDateLocator, ConciseDateFormatter
        df = pd.read_csv(csv_path)
        if "time" in df.columns and not is_datetime64_any_dtype(df["time"]):
            df["time"] = pd.to_datetime(df["time"], errors="coerce")
        if "close" not in df.columns:
            raise ValueError("CSVに 'close' 列がありません。")

        self.figure.clear()
        ax = self.figure.add_subplot(111)
        price = pd.to_numeric(df["close"], errors="coerce").ffill()
        norm = price / price.iloc[0] * 100.0
        ax.plot(df["time"] if "time" in df.columns else range(len(norm)),
                norm.values, lw=1.4, label="Price (close, =100@start)")
        ax.set_title("Price Preview (from OHLCV)" + (f" — {note}" if note else ""))
        ax.set_ylabel("index (=100@start)")
        ax.set_xlabel("time")
        ax.grid(True, alpha=0.25)
        if "time" in df.columns:
            loc = AutoDateLocator()
            ax.xaxis.set_major_locator(loc)
            ax.xaxis.set_major_formatter(ConciseDateFormatter(loc))
            self.figure.autofmt_xdate()
        ax.legend(loc="best")
        self.canvas.draw_idle()
        self.ax_main = ax
        self.ax_nav = None
        self._last_kind = "price"
        self._last_csv = str(csv_path)

    def plot_heatmap(self, monthly_returns_df, note: str = ""):
        """monthly_returns_df: columns=['time','return']（%ではなく小数）を想定"""
        import pandas as pd
        import numpy as np
        self.figure.clear()
        ax = self.figure.add_subplot(111)

        df = monthly_returns_df.copy()
        if "time" in df.columns:
            df["time"] = pd.to_datetime(df["time"], errors="coerce")
            df["year"] = df["time"].dt.year
            df["month"] = df["time"].dt.month
        else:
            raise ValueError("heatmapには 'time' 列が必要です。")

        pivot = df.pivot_table(index="year", columns="month", values="return", aggfunc="sum")
        # 0〜11月で欠損は0に
        pivot = pivot.reindex(columns=range(1,13)).fillna(0.0)

        im = ax.imshow(pivot.values, aspect="auto", interpolation="nearest")
        ax.set_xticks(range(12)); ax.set_xticklabels([f"{m:02d}" for m in range(1,13)])
        ax.set_yticks(range(len(pivot.index))); ax.set_yticklabels(pivot.index.astype(str))
        ax.set_title("Monthly Return Heatmap" + (f" — {note}" if note else ""))
        ax.set_xlabel("month"); ax.set_ylabel("year")
        # カラーバー（%表示）
        from matplotlib.ticker import PercentFormatter
        cbar = self.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.ax.yaxis.set_major_formatter(PercentFormatter(1.0))
        self.canvas.draw_idle()

    # === ���ԃW�����vAPI�i1W / 1M / ALL�j ===
    def jump_range(self, mode: str) -> None:
        """
        ポップアウト表示で 1W / 1M / ALL の X 範囲を切り替える。
        1) CSV の time 列 → 2) 描画中 xdata → 3) ファイル名のTF推定でバー数ズーム。
        """
        valid_modes = {"1W", "1M", "ALL"}
        mode = (mode or "").upper()
        if mode not in valid_modes:
            raise ValueError(f"Unknown mode: {mode}")

        if self._last_kind not in {"equity", "price"} or not self._last_csv:
            raise RuntimeError("直近に価格/エクイティを表示してから操作してください。")

        csv_path = Path(self._last_csv)
        if not csv_path.exists():
            raise RuntimeError(f"直近のCSVが見つかりません: {csv_path}")

        def _target_axes():
            axes = []
            seen: set[int] = set()
            for candidate in (getattr(self, "ax_main", None), getattr(self, "ax_nav", None)):
                if candidate is None or candidate not in self.figure.axes:
                    continue
                ident = id(candidate)
                if ident in seen:
                    continue
                seen.add(ident)
                axes.append(candidate)
            if not axes:
                for ax in self.figure.axes:
                    ident = id(ax)
                    if ident in seen:
                        continue
                    seen.add(ident)
                    axes.append(ax)
            return axes

        def _apply_xlim(xmin, xmax):
            axes = _target_axes()
            if not axes:
                raise RuntimeError("描画済みのAxesが無いためジャンプできません。")
            for ax in axes:
                ax.set_xlim(xmin, xmax)
            self.canvas.draw_idle()

        def _extract_xdata():
            for ax in (getattr(self, "ax_nav", None), getattr(self, "ax_main", None)):
                if ax is None:
                    continue
                lines = ax.get_lines()
                if lines:
                    data = lines[0].get_xdata()
                    if data is not None and len(data) >= 2:
                        return data
            for ax in self.figure.axes:
                lines = ax.get_lines()
                if lines:
                    data = lines[0].get_xdata()
                    if data is not None and len(data) >= 2:
                        return data
            return None

        def _apply_time_window(series: pd.Series) -> bool:
            series = pd.Series(series).dropna()
            if len(series) < 2:
                return False
            series = series.sort_values()
            latest = pd.Timestamp(series.iloc[-1])
            if mode == "ALL":
                earliest = pd.Timestamp(series.iloc[0])
            elif mode == "1W":
                earliest = latest - pd.Timedelta(days=7)
            else:
                earliest = latest - pd.Timedelta(days=31)
            first = pd.Timestamp(series.iloc[0])
            earliest = max(first, earliest)
            _apply_xlim(earliest.to_pydatetime(), latest.to_pydatetime())
            return True

        tf_keywords = {
            "M1": 1, "M3": 3, "M5": 5, "M10": 10, "M15": 15, "M30": 30,
            "H1": 60, "H2": 120, "H3": 180, "H4": 240, "H6": 360, "H8": 480, "H12": 720,
            "D1": 1440, "D2": 2880, "W1": 10080
        }

        def _infer_tf_minutes(path: Path) -> int | None:
            chunks = [path.stem.upper(), *(part.upper() for part in path.parts)]
            for name in chunks:
                for token, minutes in tf_keywords.items():
                    if re.search(rf"(?<![A-Z0-9]){token}(?![A-Z0-9])", name):
                        return minutes
                for part in re.split(r"[^A-Z0-9]+", name):
                    if not part:
                        continue
                    if part in tf_keywords:
                        return tf_keywords[part]
                    m = re.fullmatch(r"(M|H|D|W)(\d{1,3})", part)
                    if m:
                        unit = m.group(1)
                        value = int(m.group(2))
                        if value == 0:
                            continue
                        if unit == "M":
                            return value
                        if unit == "H":
                            return value * 60
                        if unit == "D":
                            return value * 1440
                        if unit == "W":
                            return value * 10080
            return None

        def _bars_for_mode(tf_minutes: int, total: int) -> int:
            if mode == "ALL":
                return total
            days = 7 if mode == "1W" else 31
            bars = int(days * 24 * 60 / max(tf_minutes, 1))
            return max(1, bars)

        try:
            df_time = pd.read_csv(csv_path, usecols=["time"])
        except ValueError:
            df_time = None
        except Exception as e:
            print(f"[pop] jump_range warn: failed to read time column from {csv_path}: {e}")
            df_time = None

        if df_time is not None and "time" in df_time.columns:
            times = pd.to_datetime(df_time["time"], errors="coerce")
            if _apply_time_window(times):
                return

        xdata = _extract_xdata()
        if xdata is None:
            raise RuntimeError("描画済みのXデータが無いためジャンプできません。")

        arr = np.asarray(xdata)
        if arr.size < 2:
            raise RuntimeError("描画済みデータが少なすぎます。")

        is_datetime = np.issubdtype(arr.dtype, np.datetime64)
        if not is_datetime and arr.dtype == object:
            sample = next((v for v in arr if v is not None), None)
            if isinstance(sample, (pd.Timestamp, datetime, np.datetime64)):
                is_datetime = True

        if is_datetime:
            if _apply_time_window(pd.Series(arr)):
                return

        tf_minutes = _infer_tf_minutes(csv_path)
        if tf_minutes is None:
            raise RuntimeError("ファイル名からタイムフレームを推定できません。例: *_M5_*.csv")

        total = arr.size
        bars = _bars_for_mode(tf_minutes, total)
        start_idx = 0 if mode == "ALL" else max(0, total - bars)
        xmin = arr[start_idx]
        xmax = arr[-1]
        if isinstance(xmin, np.generic):
            xmin = xmin.item()
        if isinstance(xmax, np.generic):
            xmax = xmax.item()
        _apply_xlim(xmin, xmax)

class BacktestTab(QtWidgets.QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)

        # === 入力フォーム ===
        self.symbol_edit = QtWidgets.QLineEdit("USDJPY")
        self.tf_combo = QtWidgets.QComboBox(); self.tf_combo.addItems(["M5", "M15", "H1"])
        self.mode_combo = QtWidgets.QComboBox(); self.mode_combo.addItems(["Backtest", "Walk-Forward"])
        self.start_edit = QtWidgets.QLineEdit("2024-01-01")
        self.end_edit = QtWidgets.QLineEdit(pd.Timestamp.now().strftime("%Y-%m-%d"))
        self.capital_edit = QtWidgets.QLineEdit("100000")
        self.layout_combo = QtWidgets.QComboBox(); self.layout_combo.addItems(["per-symbol", "flat"])
        self.train_ratio_edit = QtWidgets.QLineEdit("0.7")

        form = QtWidgets.QGridLayout()
        r = 0
        form.addWidget(QtWidgets.QLabel("Symbol"), r, 0); form.addWidget(self.symbol_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("Timeframe"), r, 2); form.addWidget(self.tf_combo, r, 3); r+=1
        form.addWidget(QtWidgets.QLabel("Mode"), r, 0); form.addWidget(self.mode_combo, r, 1)
        form.addWidget(QtWidgets.QLabel("Layout"), r, 2); form.addWidget(self.layout_combo, r, 3); r+=1
        form.addWidget(QtWidgets.QLabel("Start"), r, 0); form.addWidget(self.start_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("End"), r, 2); form.addWidget(self.end_edit, r, 3); r+=1
        form.addWidget(QtWidgets.QLabel("Initial Capital (JPY)"), r, 0); form.addWidget(self.capital_edit, r, 1)
        form.addWidget(QtWidgets.QLabel("Train Ratio (WFO)"), r, 2); form.addWidget(self.train_ratio_edit, r, 3); r+=1

        # === ボタン ===
        self.btn_update = QtWidgets.QPushButton("データ確認＆更新")
        self.btn_run    = QtWidgets.QPushButton("テスト実行")
        self.btn_popout = QtWidgets.QPushButton("別ウインドウで表示")
        self.btn_export = QtWidgets.QPushButton("結果をエクスポート(JSON)")
        self.btn_savepng= QtWidgets.QPushButton("グラフをPNG保存")
        self.btn_heatmap= QtWidgets.QPushButton("月次ヒートマップ")

        btns = QtWidgets.QHBoxLayout()
        btns.addWidget(self.btn_update); btns.addWidget(self.btn_run)
        btns.addWidget(self.btn_popout); btns.addWidget(self.btn_heatmap)
        btns.addStretch(1)
        btns.addWidget(self.btn_savepng); btns.addWidget(self.btn_export)

        # === 期間ジャンプ（1W / 1M / ALL） ===
        range_box = QtWidgets.QHBoxLayout()
        range_box.setContentsMargins(0, 0, 0, 0)
        range_box.setSpacing(6)
        range_box.addWidget(QtWidgets.QLabel("期間:"))

        self.btn_1w  = QtWidgets.QPushButton("1W")
        self.btn_1m  = QtWidgets.QPushButton("1M")
        self.btn_all = QtWidgets.QPushButton("ALL")
        for b in (self.btn_1w, self.btn_1m, self.btn_all):
            b.setFixedHeight(26)

        self.btn_1w.clicked.connect(partial(self._on_range_jump, "1W"))
        self.btn_1m.clicked.connect(partial(self._on_range_jump, "1M"))
        self.btn_all.clicked.connect(partial(self._on_range_jump, "ALL"))

        range_box.addWidget(self.btn_1w)
        range_box.addWidget(self.btn_1m)
        range_box.addWidget(self.btn_all)
        range_box.addStretch(1)

        # === プロット・メトリクス ===
        self.fig = Figure(figsize=(6,3), constrained_layout=True)
        self.canvas = Canvas(self.fig)
        self.label_meta = QtWidgets.QLabel("未実行"); self.label_meta.setWordWrap(True)

        # モデル情報（active_model.json）
        self.model_info = QtWidgets.QLabel("model: -")
        self.model_info.setStyleSheet("color: #666;")

        # 出力パス
        self.path_edit = QtWidgets.QLineEdit(""); self.path_edit.setReadOnly(True)
        self.btn_open  = QtWidgets.QPushButton("CSVを開く…")
        path_line = QtWidgets.QHBoxLayout()
        path_line.addWidget(QtWidgets.QLabel("Equity CSV:")); path_line.addWidget(self.path_edit, 1); path_line.addWidget(self.btn_open)

        # 進捗ログ
        self.progress_box = QtWidgets.QPlainTextEdit(); self.progress_box.setReadOnly(True); self.progress_box.setMaximumBlockCount(1000)

        # メトリクス表
        self.table = QtWidgets.QTableWidget(0, 2)
        self.table.setHorizontalHeaderLabels(["Metric", "Value"])
        self.table.horizontalHeader().setStretchLastSection(True)
        self.table.setMinimumHeight(140)

        # 全体配置
        lay = QtWidgets.QVBoxLayout(self)
        lay.addLayout(form)
        lay.addLayout(btns)
        lay.addLayout(range_box)          # ← 期間ボタン列を追加
        lay.addWidget(self.canvas, 1)
        lay.addWidget(self.label_meta)
        lay.addLayout(path_line)
        lay.addWidget(QtWidgets.QLabel("モデル情報:")); lay.addWidget(self.model_info)
        lay.addWidget(QtWidgets.QLabel("メトリクス:")); lay.addWidget(self.table)
        lay.addWidget(QtWidgets.QLabel("進捗ログ:")); lay.addWidget(self.progress_box, 2)

        # 内部状態
        self.proc: QProcess | None = None
        self._last_plot_kind = None   # "equity" or "price"
        self._last_plot_data = None
        self._last_plot_note = ""
        self._pop = None              # PlotWindow
        self._last_monthly_returns: Path | None = None

        # シグナル
        self.btn_update.clicked.connect(self._update_data)
        self.btn_run.clicked.connect(self._run_test)
        self.btn_open.clicked.connect(self._pick_file)
        self.btn_popout.clicked.connect(self._pop_out)
        self.btn_export.clicked.connect(self._export_result_json)
        self.btn_savepng.clicked.connect(self._save_png)
        self.btn_heatmap.clicked.connect(self._show_heatmap)
        self.mode_combo.currentTextChanged.connect(self._toggle_wfo_controls)

        # ダブルクリックでポップアウト
        def _on_canvas_dbl(ev):
            if ev.dblclick: self._pop_out()
        self.canvas.mpl_connect("button_press_event", _on_canvas_dbl)

        # 初回モデル情報表示
        self._load_model_info()
        self._toggle_wfo_controls(self.mode_combo.currentText())

    # ------------------ ヘルパ ------------------
    def _append_progress(self, text: str):
        self.progress_box.appendPlainText(text.rstrip())

    def _toggle_wfo_controls(self, mode_text: str):
        is_wfo = (mode_text == "Walk-Forward")
        self.train_ratio_edit.setEnabled(is_wfo)

    def _load_model_info(self):
        p = PROJECT_ROOT / "active_model.json"
        if p.exists():
            try:
                j = json.loads(p.read_text(encoding="utf-8"))
                name = j.get("model_name") or j.get("name") or "-"
                th   = j.get("best_threshold") or j.get("threshold") or "-"
                fh   = j.get("features_hash") or "-"
                self.model_info.setText(f"name={name}  best_threshold={th}  features_hash={fh}")
            except Exception as e:
                self.model_info.setText(f"(model info error: {e})")
        else:
            self.model_info.setText("(active_model.json が見つかりません)")

    # ------------------ データ更新 ------------------
    def _update_data(self):
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()
        start = self.start_edit.text().strip()
        end   = self.end_edit.text().strip()
        layout= self.layout_combo.currentText()

        try:
            csv = ensure_data(sym, tf, start, end, env="laptop", layout=layout)
            self._load_plot(csv)  # OHLCVプレビュー
            self.label_meta.setText(f"データOK: {csv}")
        except Exception as e:
            self.label_meta.setText(f"データ更新失敗: {e}")
            self._append_progress(f"[update] error: {e}")

    # ------------------ 実行（QProcess） ------------------
    def _run_test(self):
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()
        start = self.start_edit.text().strip()
        end   = self.end_edit.text().strip()
        capital = self.capital_edit.text().strip()
        layout  = self.layout_combo.currentText()
        mode = "bt" if self.mode_combo.currentText()=="Backtest" else "wfo"
        train_ratio = self.train_ratio_edit.text().strip() or "0.7"

        # 実行前に保証
        try:
            csv = ensure_data(sym, tf, start, end, env="laptop", layout=layout)
        except Exception as e:
            self.label_meta.setText(f"データ不足: {e}")
            return

        if self.proc:
            self.proc.kill(); self.proc = None

        # “-m tools.backtest_run” でモジュール実行に切り替え
        args = [
            "-m", "tools.backtest_run",
            "--csv", str(csv),
            "--start", start, "--end", end,
            "--capital", capital,
            "--mode", mode,
            "--symbol", sym,
            "--timeframe", tf,
            "--layout", layout,
        ]
        if mode == "wfo":
            args += ["--train-ratio", train_ratio]

        self._append_progress("[gui] run: python " + " ".join(args))

        self.proc = QProcess(self)
        self.proc.setProgram(sys.executable)
        self.proc.setArguments(args)
        self.proc.setWorkingDirectory(str(PROJECT_ROOT))  # 重要：プロジェクト直下をCWDに

        self.proc.setProcessChannelMode(QProcess.ProcessChannelMode.MergedChannels)
        self.proc.readyReadStandardOutput.connect(self._on_proc_output)
        self.proc.finished.connect(lambda code, status: self._on_proc_finished(code, status, sym, tf, mode))
        self.label_meta.setText("実行中…")
        self.proc.start()

    def _on_proc_output(self):
        if not self.proc: return
        out = bytes(self.proc.readAllStandardOutput()).decode("utf-8", errors="replace")
        if out:
            for line in out.splitlines():
                self._append_progress(line)

    def _on_proc_finished(self, code: int, status: QtCore.QProcess.ExitStatus, sym: str, tf: str, mode: str):
        if code != 0:
            self.label_meta.setText(f"失敗(code={code})")
            self._append_progress(f"[gui] process failed code={code}")
            return

        out_dir = PROJECT_ROOT / "logs" / "backtest" / sym / tf
        out_csv = out_dir / "equity_curve.csv"
        self.path_edit.setText(str(out_csv))
        if not out_csv.exists():
            self.label_meta.setText(f"出力CSVが見つかりません: {out_csv}")
            self._append_progress(f"[gui] missing file: {out_csv}")
            return

        self._load_plot(out_csv)  # Equity描画

        # メトリクス読込
        metrics_path = out_dir / ("metrics.json" if mode=="bt" else "metrics_wfo.json")
        self._load_metrics(metrics_path)

        # 月次リターン保存パスを控える
        self._last_monthly_returns = out_dir / ("monthly_returns.csv" if mode=="bt" else "monthly_returns_test.csv")

        self.label_meta.setText("完了")

    # ------------------ ユーティリティUI ------------------
    def _pick_file(self):
        p, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Equity CSV", str(PROJECT_ROOT), "CSV (*.csv)")
        if p:
            self.path_edit.setText(p)
            self._load_plot(p)

    def _save_png(self):
        p, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Save PNG", str(PROJECT_ROOT / "logs"), "PNG (*.png)")
        if not p: return
        try:
            self.fig.savefig(p, dpi=150)
            self._append_progress(f"[gui] saved png: {p}")
        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "保存失敗", str(e))

    def _export_result_json(self):
        # 画面パラメータ＋メトリクス＋モデル情報をまとめて保存
        sym = self.symbol_edit.text().strip().upper()
        tf  = self.tf_combo.currentText()
        mode = self.mode_combo.currentText()
        payload = {
            "params": {
                "symbol": sym, "timeframe": tf,
                "mode": mode,
                "start": self.start_edit.text().strip(),
                "end": self.end_edit.text().strip(),
                "capital": float(self.capital_edit.text().strip() or 0),
                "layout": self.layout_combo.currentText(),
                "train_ratio": float(self.train_ratio_edit.text().strip() or 0.7)
            },
            "model": self.model_info.text(),
        }
        # metrics 表からも収集
        for r in range(self.table.rowCount()):
            k = self.table.item(r,0).text()
            v = self.table.item(r,1).text()
            payload.setdefault("metrics", {})[k] = v

        p, _ = QtWidgets.QFileDialog.getSaveFileName(self, "Export JSON", str(PROJECT_ROOT / "logs"), "JSON (*.json)")
        if not p: return
        Path(p).write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        self._append_progress(f"[gui] exported: {p}")

    def _show_heatmap(self):
        if not self._last_monthly_returns or not self._last_monthly_returns.exists():
            QtWidgets.QMessageBox.information(self, "情報", "月次リターンがまだありません。先にテスト実行してください。")
            return
        df = pd.read_csv(self._last_monthly_returns)
        if self._pop is None:
            self._pop = PlotWindow(self)
        self._pop.show(); self._pop.raise_(); self._pop.activateWindow()
        note = self._last_monthly_returns.name
        self._pop.plot_heatmap(df, note)

    def _pop_out(self):
        if self._last_plot_kind is None or self._last_plot_data is None:
            self._append_progress("[gui] popout: 直近の描画データがありません")
            QtWidgets.QMessageBox.information(self, "情報", "まだ描画されていません。先に「データ確認＆更新」または「テスト実行」をしてください。")
            return
        if self._pop is None:
            self._pop = PlotWindow(self)
        self._pop.show(); self._pop.raise_(); self._pop.activateWindow()
        if self._last_plot_kind == "equity":
            csvp = self._last_plot_data
            self._pop.plot_equity_csv(csvp)
        else:
            self._pop.plot_price_preview(self._last_plot_data, self._last_plot_note)

    # ------------------ 描画・メトリクス ------------------
    def _load_plot(self, path_or_csv):
        p = Path(path_or_csv)
        csv_path = str(p)
        ##
        if "equity_curve.csv" in csv_path:
            try:
                plot_equity_with_markers_to_figure(self.fig, csv_path, note=p.name)
                self.canvas.draw_idle()
                self._append_progress(f"[gui] plotted EQUITY (markers) from {p.name}")
                self._last_plot_kind = "equity"
                self._last_plot_data = str(p)  # CSVパスを記憶（期間ジャンプで利用）
                self._last_plot_note = p.name
            except Exception as e:
                self.label_meta.setText(f"描画失敗: {e}")
                self._append_progress(f"[gui] plot error: {e}")
            return

        ##
        try:
            df = pd.read_csv(p)
            self.fig.clear()
            ax = self.fig.add_subplot(111)

            if "close" in df.columns:
                price = pd.to_numeric(df["close"], errors="coerce").ffill()
                if len(price) == 0 or price.iloc[0] == 0:
                    raise ValueError("プレビュー用 'close' 列が空です。")
                norm = price / price.iloc[0] * 100.0
                ax.plot(norm.values, label="Price (close, =100@start)")
                ax.set_title("Price Preview (from OHLCV)")
                ax.set_ylabel("index (=100@start)")
                self._append_progress(f"[gui] plotted PRICE {len(df)} rows from {p.name}")
                self._last_plot_kind = "price"
                self._last_plot_data = str(p)
                self._last_plot_note = p.name
            else:
                raise ValueError(f"CSVに 'close' 列が含まれていません。columns={list(df.columns)}")

            ax.set_xlabel("bars")
            ax.legend()
            #self.fig.tight_layout()
            self.canvas.draw_idle()

        except Exception as e:
            self.label_meta.setText(f"描画失敗: {e}")
            self._append_progress(f"[gui] plot error: {e}")

    def _load_metrics(self, metrics_path: Path):
        self.table.setRowCount(0)
        try:
            txt = Path(metrics_path).read_text(encoding="utf-8")
            m = json.loads(txt)
            # WFO の場合は {train:{}, test:{}} 形式
            if "train" in m and "test" in m:
                # test 側を表に出す、train はログに
                ts = m["test"]; tr = m["train"]
                msg = (f"[WFO] Train ret={tr['total_return']*100:.2f}% mdd={tr['max_drawdown']*100:.2f}% sharpe≈{tr['sharpe_like']:.2f} "
                       f"| Test ret={ts['total_return']*100:.2f}% mdd={ts['max_drawdown']*100:.2f}% sharpe≈{ts['sharpe_like']:.2f}")
                self.label_meta.setText(msg)
                rows = ts
            else:
                rows = m
                self.label_meta.setText(
                    f"[BT] ret={m['total_return']*100:.2f}% mdd={m['max_drawdown']*100:.2f}% sharpe≈{m['sharpe_like']:.2f} bars={m['bars']}"
                )

            # 表へ
            for k in [
                "start_equity","end_equity","total_return","max_drawdown","max_dd_days","sharpe_like","bars",
                "trades","win_rate","avg_pnl","profit_factor","avg_holding_bars","avg_holding_days",
                "max_consec_win","max_consec_loss"
            ]:
                if k in rows:
                    r = self.table.rowCount()
                    self.table.insertRow(r)
                    self.table.setItem(r, 0, QtWidgets.QTableWidgetItem(k))
                    val = rows[k]
                    if isinstance(val, float):
                        if "return" in k or "drawdown" in k or k=="win_rate":
                            val = f"{val*100:.2f}%"
                        else:
                            val = f"{val:.4g}"
                    self.table.setItem(r, 1, QtWidgets.QTableWidgetItem(str(val)))

            self._append_progress("[gui] metrics loaded")
        except Exception as e:
            self.label_meta.setText(f"メトリクス読込失敗: {e}")
            self._append_progress(f"[gui] metrics error: {e}")

    # ------------------ 期間ジャンプ（本体） ------------------
    def _on_range_jump(self, mode: str) -> None:
        """
        Backtestタブのインライン描画と、ポップアウト済みウィンドウの両方に期間ジャンプを適用する。
        - インライン: equity だけでなく price プレビューにも対応。
        - ポップアウト: PlotWindow.jump_range() に委譲。
        """
        try:
            # 1) ポップアウト側（起動済みなら）
            if self._pop is not None:
                try:
                    self._pop.jump_range(mode)
                except Exception as e:
                    self._append_progress(f"[pop] jump_range warn: {e}")

            # 2) インライン側：equity
            if self._last_plot_kind == "equity" and isinstance(self._last_plot_data, str):
                csvp = Path(self._last_plot_data)
                if csvp.exists():
                    df = pd.read_csv(csvp)
                    if "time" in df.columns:
                        t = pd.to_datetime(df["time"], errors="coerce").dropna()
                        if len(t) >= 2:
                            tmax = t.iloc[-1]
                            if mode == "ALL":
                                tmin = t.iloc[0]
                            elif mode == "1W":
                                tmin = tmax - pd.Timedelta(days=7)
                            elif mode == "1M":
                                tmin = tmax - pd.Timedelta(days=31)
                            else:
                                raise ValueError(f"Unknown mode: {mode}")
                            tmin = max(t.iloc[0], tmin)
                            if self.fig.axes:
                                ax = self.fig.axes[0]
                                ax.set_xlim(tmin.to_pydatetime(), tmax.to_pydatetime())
                                self.canvas.draw_idle()
                        else:
                            raise RuntimeError("エクイティの時系列が短すぎます。")
                    else:
                        raise RuntimeError("equity_curve.csv に 'time' 列がありません。")
                else:
                    raise RuntimeError("最後に描画したCSVが見つかりません。")

            # 3) インライン側：price（time列ありなら日付、なければバー数でズーム）
            elif self._last_plot_kind == "price" and isinstance(self._last_plot_data, str):
                csvp = Path(self._last_plot_data)
                if not csvp.exists():
                    raise RuntimeError("最後に描画した価格CSVが見つかりません。")
                df = pd.read_csv(csvp)

                if "time" in df.columns:
                    t = pd.to_datetime(df["time"], errors="coerce").dropna()
                    if len(t) < 2:
                        raise RuntimeError("価格プレビューの時系列が短すぎます。")
                    tmax = t.iloc[-1]
                    if mode == "ALL":
                        tmin = t.iloc[0]
                    elif mode == "1W":
                        tmin = tmax - pd.Timedelta(days=7)
                    elif mode == "1M":
                        tmin = tmax - pd.Timedelta(days=31)
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    tmin = max(t.iloc[0], tmin)
                    if self.fig.axes:
                        ax = self.fig.axes[0]
                        ax.set_xlim(tmin.to_pydatetime(), tmax.to_pydatetime())
                        self.canvas.draw_idle()
                else:
                    # x軸がインデックス（0..n-1）の場合は「バー数」で近似
                    n = len(df)
                    if n < 2:
                        raise RuntimeError("価格プレビューの行数が不足しています。")
                    tf = (self.tf_combo.currentText() or "M5").upper()
                    tf_min = {"M1":1, "M5":5, "M15":15, "M30":30, "H1":60, "H4":240, "D1":1440}.get(tf, 5)
                    def bars_for_days(days: int) -> int:
                        return int(days * 24 * 60 / tf_min)
                    if mode == "ALL":
                        start_idx = 0
                    elif mode == "1W":
                        start_idx = max(0, n - bars_for_days(7))
                    elif mode == "1M":
                        start_idx = max(0, n - bars_for_days(31))
                    else:
                        raise ValueError(f"Unknown mode: {mode}")
                    if self.fig.axes:
                        ax = self.fig.axes[0]
                        ax.set_xlim(start_idx, n - 1)
                        self.canvas.draw_idle()

            else:
                # それ以外（未描画など）
                self._append_progress("[gui] 期間ジャンプは直近の描画対象に対してのみ動作します。")

            # ステータス表示
            self.label_meta.setText(f"表示期間を {mode} に切り替えました")

        except Exception as e:
            QtWidgets.QMessageBox.warning(self, "期間ジャンプエラー", str(e))
            self._append_progress(f"[range] error: {e}")
    



=== file: app/gui/control_tab.py ===

from typing import Optional
from PyQt6.QtCore import Qt
from PyQt6.QtWidgets import (
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QMessageBox,
    QPushButton,
    QSlider,
    QSpinBox,
    QVBoxLayout,
    QWidget,
)

from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_state
from app.services.orderbook_stub import orderbook


class ControlTab(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setLayout(QVBoxLayout())
        root_layout: Optional[QVBoxLayout] = self.layout()  # guard for static analyzers
        if root_layout is None:
            root_layout = QVBoxLayout()
            self.setLayout(root_layout)

        # 運転
        box_run = QGroupBox("運転")
        lay_run = QHBoxLayout()
        self.btn_toggle = QPushButton("取引：停止中（クリックで開始）")
        self.btn_toggle.setCheckable(True)
        self.btn_toggle.clicked.connect(self._toggle_trading)

        self.btn_close_all = QPushButton("全クローズ（ドライラン）")
        self.btn_close_all.clicked.connect(self._close_all_mock)

        self.btn_cb_reset = QPushButton("サーキット解除")
        self.btn_cb_reset.clicked.connect(self._cb_reset)

        lay_run.addWidget(self.btn_toggle)
        lay_run.addWidget(self.btn_close_all)
        lay_run.addWidget(self.btn_cb_reset)
        box_run.setLayout(lay_run)
        root_layout.addWidget(box_run)

        # しきい値
        box_thr = QGroupBox("エントリーしきい値（確信度）")
        lay_thr = QHBoxLayout()
        self.lbl_buy = QLabel("買い: 0.60")
        self.sld_buy = QSlider(Qt.Orientation.Horizontal)
        self.sld_buy.setRange(50, 80)
        self.sld_buy.setValue(60)
        self.sld_buy.valueChanged.connect(self._on_thr_changed)

        self.lbl_sell = QLabel("売り: 0.60")
        self.sld_sell = QSlider(Qt.Orientation.Horizontal)
        self.sld_sell.setRange(50, 80)
        self.sld_sell.setValue(60)
        self.sld_sell.valueChanged.connect(self._on_thr_changed)

        lay_thr.addWidget(self.lbl_buy)
        lay_thr.addWidget(self.sld_buy)
        lay_thr.addSpacing(12)
        lay_thr.addWidget(self.lbl_sell)
        lay_thr.addWidget(self.sld_sell)
        box_thr.setLayout(lay_thr)
        root_layout.addWidget(box_thr)

        # 決済
        box_exit = QGroupBox("決済（固定pips）")
        lay_exit = QHBoxLayout()
        self.sp_sl = QSpinBox()
        self.sp_sl.setRange(1, 200)
        self.sp_sl.setValue(10)
        self.sp_sl.valueChanged.connect(self._on_exit_changed)

        self.sp_tp = QSpinBox()
        self.sp_tp.setRange(1, 300)
        self.sp_tp.setValue(15)
        self.sp_tp.valueChanged.connect(self._on_exit_changed)

        lay_exit.addWidget(QLabel("SL"))
        lay_exit.addWidget(self.sp_sl)
        lay_exit.addSpacing(16)
        lay_exit.addWidget(QLabel("TP"))
        lay_exit.addWidget(self.sp_tp)
        box_exit.setLayout(lay_exit)
        root_layout.addWidget(box_exit)

        # 状態表示
        self.lbl_status = QLabel("")
        root_layout.addWidget(self.lbl_status)

        self._sync_from_state()

    def _sync_from_state(self):
        s = trade_state.get_settings()
        self.btn_toggle.setChecked(s.trading_enabled)
        self.btn_toggle.setText(
            "取引：稼働中（クリックで停止）" if s.trading_enabled else "取引：停止中（クリックで開始）"
        )
        self.sld_buy.setValue(int(s.threshold_buy * 100))
        self.sld_sell.setValue(int(s.threshold_sell * 100))
        self.sp_sl.setValue(int(s.sl_pips))
        self.sp_tp.setValue(int(s.tp_pips))
        self._refresh_status()

    def _refresh_status(self):
        s = trade_state.as_dict()
        state_txt = "稼働中" if s["trading_enabled"] else "停止中"
        self.lbl_status.setText(
            f"状態: {state_txt} / 買い閾値: {s['threshold_buy']:.2f} / 売り閾値: {s['threshold_sell']:.2f} / "
            f"SL: {s['sl_pips']} / TP: {s['tp_pips']}"
        )

    def _toggle_trading(self):
        try:
            enabled = self.btn_toggle.isChecked()
            trade_state.update(trading_enabled=enabled)
            self.btn_toggle.setText(
                "取引：稼働中（クリックで停止）" if enabled else "取引：停止中（クリックで開始）"
            )
            self._refresh_status()
        except Exception as e:
            self.btn_toggle.setChecked(not self.btn_toggle.isChecked())
            QMessageBox.critical(self, "Trading switch error", str(e))
            print("[control_tab] toggle failed:", e)

    def _on_thr_changed(self, *_):
        buy = self.sld_buy.value() / 100.0
        sell = self.sld_sell.value() / 100.0
        trade_state.update(threshold_buy=buy, threshold_sell=sell)
        self.lbl_buy.setText(f"買い: {buy:.2f}")
        self.lbl_sell.setText(f"売り: {sell:.2f}")
        self._refresh_status()

    def _on_exit_changed(self, *_):
        trade_state.update(sl_pips=int(self.sp_sl.value()), tp_pips=int(self.sp_tp.value()))
        self._refresh_status()

    def _close_all_mock(self):
        cfg = load_config()
        symbol = cfg.get("runtime", {}).get("symbol", "USDJPY")
        orderbook().close_all(symbol)

    def _cb_reset(self):
        circuit_breaker.reset()
        circuit_breaker.scan_and_update()
        self._refresh_status()



=== file: app/gui/dashboard_tab.py ===

# app/gui/dashboard_tab.py
import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parents[2]  # D:\macht\OneDrive\fxbot
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))
import json, os
from core.metrics import METRICS_JSON

import time
try:
    import tkinter as tk
    from tkinter import ttk
except ImportError:
    # Tkinterが無い環境用メッセージ（Windows公式Pythonなら入ってます）
    raise

from app.services import trade_service
from core.metrics import METRICS


class DashboardTab(ttk.Frame):
    """
    Realtime Metrics (ATR / Grace / Trail) を表示するだけの軽量パネル。
    別スレッドでMETRICSが更新される前提。self.afterで1秒ごとにpull。
    """
    def __init__(self, master, *args, **kwargs):
        super().__init__(master, *args, **kwargs)

        self._vars = {}

        box = ttk.LabelFrame(self, text="Realtime Metrics (ATR / Grace / Trail)")
        box.pack(fill="x", padx=8, pady=6)

        grid = ttk.Frame(box)
        grid.pack(fill="x", padx=8, pady=8)

        rows = [
            ("Last decision", "last_decision"),
            ("Reason", "last_reason"),
            ("ATR ref", "atr_ref"),
            ("ATR gate", "atr_gate_state"),
            ("Post-fill grace", "post_fill_grace"),
            ("Spread", "spread"),
            ("ADX / Min", "adx_min"),
            ("Prob threshold", "prob_threshold"),
            ("Min ATR %", "min_atr_pct"),
            ("Trail: activated", "trail_activated"),
            ("Trail: BE locked", "trail_be_locked"),
            ("Trail: layers", "trail_layers"),
            ("Trail: current SL", "trail_current_sl"),
            ("Guard/Open", "guard_open"),
            ("Guard/Inflight", "guard_inflight"),
            ("Guard/LastFix", "guard_last_fix"),
            ("CB/Tripped", "cb_tripped"),
            ("CB/Reason", "cb_reason"),
            ("CB/ConsecLoss", "cb_consec"),
            ("CB/DailyLossJPY", "cb_daily_loss"),
            ("Counts ENTRY/SKIP/BLOCK", "counts"),
            ("Updated (local)", "ts"),
        ]

        for i, (label, key) in enumerate(rows):
            ttk.Label(grid, text=label, width=22).grid(row=i, column=0, sticky="w", padx=4, pady=2)
            var = tk.StringVar(value="-")
            ttk.Label(grid, textvariable=var, width=28).grid(row=i, column=1, sticky="w", padx=4, pady=2)
            self._vars[key] = var

        # 最初の更新をセット
        self.after(500, self._refresh_metrics)

    def _refresh_metrics(self):
        # まずはファイルから読む（別プロセス更新に対応）
        kv = {}
        try:
            with open(METRICS_JSON, "r", encoding="utf-8") as f:
                kv = json.load(f)
        except Exception:
            # ファイルがまだ無い／壊れている場合はローカルKVSを参照
            from core.metrics import METRICS
            kv = METRICS.get()

        # 以降は同じ（値の反映処理）
        self._vars["last_decision"].set(str(kv.get("last_decision", "-")))
        self._vars["last_reason"].set(str(kv.get("last_reason", "-")))
        self._vars["atr_ref"].set(f"{float(kv.get('atr_ref', 0) or 0):.6f}")
        self._vars["atr_gate_state"].set(str(kv.get("atr_gate_state", "-")))
        self._vars["post_fill_grace"].set("ON" if kv.get("post_fill_grace") else "OFF")
        self._vars["spread"].set(str(kv.get("spread", "-")))
        self._vars["prob_threshold"].set(str(kv.get("prob_threshold", "-")))
        self._vars["min_atr_pct"].set(str(kv.get("min_atr_pct", "-")))
        adx = kv.get("adx"); m = kv.get("min_adx")
        self._vars["adx_min"].set(f"{adx} / {m}")
        self._vars["trail_activated"].set("ON" if kv.get("trail_activated") else "OFF")
        self._vars["trail_be_locked"].set("ON" if kv.get("trail_be_locked") else "OFF")
        self._vars["trail_layers"].set(str(kv.get("trail_layers", 0)))
        self._vars["trail_current_sl"].set(str(kv.get("trail_current_sl", "-")))
        cE = int(kv.get("count_entry", 0)); cS = int(kv.get("count_skip", 0)); cB = int(kv.get("count_blocked", 0))
        self._vars["counts"].set(f"{cE} / {cS} / {cB}")
        ts = kv.get("ts")
        local = "-" if not ts else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ts))
        self._vars["ts"].set(local)

        svc = getattr(trade_service, "SERVICE", None)
        guard_state = getattr(svc, "pos_guard", None)
        if guard_state:
            self._vars["guard_open"].set(str(guard_state.state.open_count))
            self._vars["guard_inflight"].set(str(len(guard_state.state.inflight_orders)))
            self._vars["guard_last_fix"].set(guard_state.state.last_fix_reason or "-")
        else:
            self._vars["guard_open"].set("-")
            self._vars["guard_inflight"].set("-")
            self._vars["guard_last_fix"].set("-")

        cb = getattr(svc, "cb", None) if svc else None
        cb_status = cb.status() if cb else {}
        self._vars["cb_tripped"].set(str(cb_status.get("tripped", False)))
        self._vars["cb_reason"].set(str(cb_status.get("reason", "-")))
        self._vars["cb_consec"].set(str(cb_status.get("consecutive_losses", "-")))
        self._vars["cb_daily_loss"].set(f"{float(cb_status.get('daily_loss_accum_jpy', 0.0)):.0f}")

        self.after(1000, self._refresh_metrics)



=== file: app/gui/dashboard_tab_qt.py ===

# app/gui/dashboard_tab_qt.py
from __future__ import annotations
from typing import Dict, Any
import json
import time

from PyQt6 import QtCore, QtWidgets
from core.metrics import METRICS_JSON, METRICS

class DashboardTab(QtWidgets.QWidget):
    """
    PyQt6版 Dashboard。runtime/metrics.json を1秒ごとに再読込し、
    値が無ければ core.metrics.METRICS(KVS) をフォールバック参照。
    """
    def __init__(self, parent: QtWidgets.QWidget | None = None) -> None:
        super().__init__(parent)
        self._labels: Dict[str, QtWidgets.QLabel] = {}

        group = QtWidgets.QGroupBox("Realtime Metrics (ATR / Grace / Trail)")
        grid = QtWidgets.QGridLayout()
        group.setLayout(grid)

        rows = [
            ("Last decision", "last_decision"),
            ("Reason", "last_reason"),
            ("ATR ref", "atr_ref"),
            ("ATR gate", "atr_gate_state"),
            ("Post-fill grace", "post_fill_grace"),
            ("Spread", "spread"),
            ("ADX / Min", "adx_min"),
            ("Prob threshold", "prob_threshold"),
            ("Min ATR %", "min_atr_pct"),
            ("Trail: activated", "trail_activated"),
            ("Trail: BE locked", "trail_be_locked"),
            ("Trail: layers", "trail_layers"),
            ("Trail: current SL", "trail_current_sl"),
            ("Guard/Open", "guard_open"),
            ("Guard/Inflight", "guard_inflight"),
            ("Guard/LastFix", "guard_last_fix"),
            ("CB/Tripped", "cb_tripped"),
            ("CB/Reason", "cb_reason"),
            ("CB/ConsecLoss", "cb_consec"),
            ("CB/DailyLossJPY", "cb_daily_loss"),
            ("Counts ENTRY/SKIP/BLOCK", "counts"),
            ("Updated (local)", "ts"),
        ]

        for r, (label, key) in enumerate(rows):
            grid.addWidget(QtWidgets.QLabel(label), r, 0, alignment=QtCore.Qt.AlignmentFlag.AlignLeft)
            val = QtWidgets.QLabel("-")
            val.setMinimumWidth(220)
            grid.addWidget(val, r, 1, alignment=QtCore.Qt.AlignmentFlag.AlignLeft)
            self._labels[key] = val

        lay = QtWidgets.QVBoxLayout(self)
        lay.addWidget(group)

        # タイマーで定期更新
        self._timer = QtCore.QTimer(self)
        self._timer.setInterval(1000)
        self._timer.timeout.connect(self._refresh_metrics)
        self._timer.start()

        self._refresh_metrics()

    def _refresh_metrics(self) -> None:
        kv: Dict[str, Any] = {}
        try:
            with open(METRICS_JSON, "r", encoding="utf-8") as f:
                kv = json.load(f)
        except Exception:
            kv = METRICS.get()  # 同一プロセスKVSのフォールバック

        # 値の整形と描画
        self._set("last_decision", str(kv.get("last_decision", "-")))
        self._set("last_reason", str(kv.get("last_reason", "-")))
        self._set("atr_ref", f"{float(kv.get('atr_ref', 0) or 0):.6f}")
        self._set("atr_gate_state", str(kv.get("atr_gate_state", "-")))
        self._set("post_fill_grace", "ON" if kv.get("post_fill_grace") else "OFF")
        self._set("spread", str(kv.get("spread", "-")))
        self._set("prob_threshold", str(kv.get("prob_threshold", "-")))
        self._set("min_atr_pct", str(kv.get("min_atr_pct", "-")))
        adx = kv.get("adx"); m = kv.get("min_adx")
        self._set("adx_min", f"{adx} / {m}")
        self._set("trail_activated", "ON" if kv.get("trail_activated") else "OFF")
        self._set("trail_be_locked", "ON" if kv.get("trail_be_locked") else "OFF")
        self._set("trail_layers", str(kv.get("trail_layers", 0)))
        self._set("trail_current_sl", str(kv.get("trail_current_sl", "-")))
        cE = int(kv.get("count_entry", 0)); cS = int(kv.get("count_skip", 0)); cB = int(kv.get("count_blocked", 0))
        self._set("counts", f"{cE} / {cS} / {cB}")
        ts = kv.get("ts")
        local = "-" if not ts else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(ts))
        self._set("ts", local)

    def _set(self, key: str, val: str) -> None:
        lab = self._labels.get(key)
        if lab is not None:
            lab.setText(val)



=== file: app/gui/history_tab.py ===

from __future__ import annotations

from typing import List, Optional

from PyQt6 import QtCore, QtWidgets
from PyQt6.QtWidgets import QHeaderView

from app.services.event_store import EVENT_STORE, UiEvent

_COLUMNS = ["ts", "kind", "symbol", "side", "price", "sl", "tp", "profit_jpy", "reason", "notes"]


class HistoryTab(QtWidgets.QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.table = QtWidgets.QTableWidget(self)
        self.table.setColumnCount(len(_COLUMNS))
        self.table.setHorizontalHeaderLabels(_COLUMNS)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)
        h: Optional[QHeaderView] = self.table.horizontalHeader()
        if h is not None:
            h.setStretchLastSection(True)

        v: Optional[QHeaderView] = self.table.verticalHeader()
        if v is not None:
            v.setVisible(False)

        self.btnExport = QtWidgets.QPushButton("Export CSV")
        self.btnExport.clicked.connect(self._export_csv)

        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.table)
        layout.addWidget(self.btnExport)

        self._timer = QtCore.QTimer(self)
        self._timer.setInterval(1000)
        self._timer.timeout.connect(self.refresh)
        self._timer.start()

        self.refresh()

    def refresh(self) -> None:
        events: List[UiEvent] = EVENT_STORE.recent(300)
        self.table.setRowCount(len(events))
        for r, ev in enumerate(events):
            row = [getattr(ev, col) for col in _COLUMNS]
            for c, val in enumerate(row):
                item = QtWidgets.QTableWidgetItem("" if val is None else str(val))
                self.table.setItem(r, c, item)

    def _export_csv(self) -> None:
        path, _ = QtWidgets.QFileDialog.getSaveFileName(
            self, "Export history to CSV", "history.csv", "CSV Files (*.csv)"
        )
        if not path:
            return
        import csv

        events: List[UiEvent] = EVENT_STORE.recent(1000)
        with open(path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(_COLUMNS)
            for ev in events:
                writer.writerow([getattr(ev, col) for col in _COLUMNS])



=== file: app/gui/main.py ===

import sys
import traceback

from PyQt6.QtCore import QTimer
from PyQt6.QtWidgets import QApplication, QMainWindow, QTabWidget

from app.core import logger as app_logger
from app.gui.control_tab import ControlTab
from app.gui.dashboard_tab_qt import DashboardTab
from app.gui.history_tab import HistoryTab
from app.services.execution_stub import evaluate_and_log_once
from app.gui.ai_tab import AITab
from app.gui.backtest_tab import BacktestTab

class MainWindow(QMainWindow):
    def __init__(self) -> None:
        super().__init__()
        self.setWindowTitle("FX AI Bot Control Panel")
        self.resize(980, 640)

        tabs = QTabWidget()
        tabs.addTab(DashboardTab(), "Dashboard")
        tabs.addTab(ControlTab(), "Control")
        tabs.addTab(HistoryTab(), "History")
        tabs.addTab(AITab(), "AI")
        tabs.addTab(BacktestTab(), "Backtest")
        self.setCentralWidget(tabs)

        app_logger.setup()

        self.timer = QTimer(self)

        def _tick_safe():
            try:
                evaluate_and_log_once()
            except Exception:
                print("[gui.timer] evaluate failed:\n" + traceback.format_exc())

        self.timer.timeout.connect(_tick_safe)
        self.timer.start(3000)
        _tick_safe()


def main() -> None:
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()



=== file: app/gui/widgets/feature_importance.py ===

# app/gui/widgets/feature_importance.py
from __future__ import annotations
from typing import Any, Dict, Optional, cast
import json
from pathlib import Path

from PyQt6 import QtWidgets
from PyQt6.QtWidgets import QHeaderView
import pandas as pd
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure

class FeatureImportanceWidget(QtWidgets.QWidget):
    def __init__(self, ai_service, parent: Optional[QtWidgets.QWidget] = None):
        super().__init__(parent)
        self.ai_service = ai_service
        self._df_cache: Optional[pd.DataFrame] = None
        self._alias: Dict[str, str] = self._load_alias()

        self.modelCombo = QtWidgets.QComboBox()
        self.methodCombo = QtWidgets.QComboBox()
        self.methodCombo.addItems(["gain", "split"])
        self.topSpin = QtWidgets.QSpinBox()
        self.topSpin.setRange(3, 100)
        self.topSpin.setValue(20)
        self.refreshBtn = QtWidgets.QPushButton("更新")

        ctrl = QtWidgets.QHBoxLayout()
        ctrl.addWidget(QtWidgets.QLabel("Model"))
        ctrl.addWidget(self.modelCombo, 1)
        ctrl.addWidget(QtWidgets.QLabel("Method"))
        ctrl.addWidget(self.methodCombo)
        ctrl.addWidget(QtWidgets.QLabel("TopN"))
        ctrl.addWidget(self.topSpin)
        ctrl.addWidget(self.refreshBtn)

        self.fig = Figure(figsize=(6, 4))
        self.canvas = FigureCanvas(self.fig)

        self.table = QtWidgets.QTableWidget()
        self.table.setColumnCount(3)
        self.table.setHorizontalHeaderLabels(["feature", "importance(%)", "model"])
        header: Optional[QHeaderView] = self.table.horizontalHeader()
        if header is not None:
            header.setStretchLastSection(True)
        self.table.setEditTriggers(QtWidgets.QAbstractItemView.EditTrigger.NoEditTriggers)
        self.table.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectionBehavior.SelectRows)

        lay = QtWidgets.QVBoxLayout(self)
        lay.addLayout(ctrl)
        lay.addWidget(self.canvas, 2)
        lay.addWidget(self.table, 1)

        self.refreshBtn.clicked.connect(self.refresh)
        self.methodCombo.currentTextChanged.connect(self.refresh)
        self.topSpin.valueChanged.connect(self.refresh)
        self.modelCombo.currentTextChanged.connect(self._plot_current)

        self.refresh()

    def refresh(self):
        method = self.methodCombo.currentText()
        top_n = int(self.topSpin.value())
        try:
            df = self.ai_service.get_feature_importance(method=method, top_n=top_n)
        except Exception as e:
            QtWidgets.QMessageBox.critical(self, "FI取得エラー", str(e))
            return

        if df is None or df.empty:
            self._df_cache = None
            self.modelCombo.clear()
            self._render_empty()
            return

        df = df.copy()
        if "feature" in df.columns and self._alias:
            df["feature"] = df["feature"].map(lambda x: self._alias.get(str(x), str(x)))

        self._df_cache = df
        models = sorted(df["model"].unique().tolist())
        prev = self.modelCombo.currentText()
        self.modelCombo.blockSignals(True)
        self.modelCombo.clear()
        self.modelCombo.addItems(models)
        self.modelCombo.blockSignals(False)
        if prev in models:
            self.modelCombo.setCurrentIndex(models.index(prev))
        self._plot_current()
        self._fill_table(df)

    def _plot_current(self):
        df = self._df_cache
        if df is None or df.empty:
            self._render_empty()
            return
        model = self.modelCombo.currentText()
        sub = df[df["model"] == model].sort_values("importance", ascending=True)
        self.fig.clear()
        ax = self.fig.add_subplot(111)
        ax.barh(sub["feature"], sub["importance"])
        ax.set_xlabel("importance (%)")
        ax.set_title(f"Feature Importance - {model} ({self.methodCombo.currentText()})")
        self.fig.tight_layout()
        self.canvas.draw_idle()

    def _fill_table(self, df: pd.DataFrame):
        rows = list(df.reset_index(drop=True).itertuples(index=False))
        self.table.setRowCount(len(rows))
        for r, row in enumerate(rows):
            importance_val = float(cast(Any, row.importance))
            self.table.setItem(r, 0, QtWidgets.QTableWidgetItem(str(row.feature)))
            self.table.setItem(r, 1, QtWidgets.QTableWidgetItem(f"{importance_val:.2f}"))
            self.table.setItem(r, 2, QtWidgets.QTableWidgetItem(str(row.model)))
        self.table.resizeColumnsToContents()

    def _render_empty(self):
        self.fig.clear()
        ax = self.fig.add_subplot(111)
        ax.text(0.5, 0.5, "No data", ha="center", va="center", transform=ax.transAxes)
        ax.axis("off")
        self.canvas.draw_idle()
        self.table.setRowCount(0)

    def _load_alias(self) -> Dict[str, str]:
        try:
            root = Path(__file__).resolve().parents[3]
            path = root / "config" / "feature_alias.json"
            if path.exists():
                raw = json.loads(path.read_text(encoding="utf-8"))
                if isinstance(raw, dict):
                    return {str(k): str(v) for k, v in raw.items()}
        except Exception:
            pass
        return {}



=== file: app/main_tk.py ===

# app/main.py
import tkinter as tk
from tkinter import ttk
from app.gui.dashboard_tab import DashboardTab

def main() -> None:
    root = tk.Tk()
    root.title("FXBot Dashboard")
    root.geometry("520x540")

    nb = ttk.Notebook(root)
    nb.pack(fill="both", expand=True)

    dash = DashboardTab(nb)
    nb.add(dash, text="Dashboard")

    root.mainloop()

if __name__ == "__main__":
    main()



=== file: app/services/__init__.py ===




=== file: app/services/ai_service.py ===

from __future__ import annotations
from typing import Dict, Any, Optional
import time
import pandas as pd

try:
    from core.metrics.fi_extractor import extract_feature_importance
except Exception:
    # パス問題の応急処置
    import sys, os
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
    from core.metrics.fi_extractor import extract_feature_importance


class AISvc:
    """
    既存の推論サービス想定。モデル群は self.models に格納されている想定。
    例: self.models = {"lgbm_cls": lgb_model, "xgb_cls": xgb_model}
    """

    def __init__(self) -> None:
        self.models: Dict[str, Any] = {}
        self._fi_cache: Optional[pd.DataFrame] = None
        self._fi_cache_key: Optional[str] = None
        self._fi_cache_ts: float = 0.0
        # ... （既存の初期化）

    # ... （既存のメソッド： load_models(), predict(), など）

    def get_feature_importance(self, method: str = "gain", top_n: int = 20, cache_sec: int = 300) -> pd.DataFrame:
        """
        GUIから呼び出してFIを取得するAPI。
        method: "gain" | "split"
        top_n : モデルごとの上位N
        cache_sec: 何秒間キャッシュするか（SHAPと違い軽量だが、連打対策）
        """
        # キャッシュキー：モデルID（オブジェクトidの並び）＋method＋top_n
        model_key = ",".join(f"{k}:{id(v)}" for k, v in sorted(self.models.items()))
        key = f"{model_key}|{method}|{top_n}"
        now = time.time()
        if self._fi_cache is not None and self._fi_cache_key == key and (now - self._fi_cache_ts) < cache_sec:
            return self._fi_cache.copy()

        df = extract_feature_importance(self.models, method=method, top_n=top_n)
        self._fi_cache = df.copy()
        self._fi_cache_key = key
        self._fi_cache_ts = now
        return df



=== file: app/services/aisvc_loader.py ===

# app/services/aisvc_loader.py
import json
from pathlib import Path

ROOT = Path(r"C:\fxbot")  # 運用固定
ACTIVE = ROOT / "active_model.json"
MODELS = ROOT / "models_store"


class ActiveModelInfo(dict):
    @property
    def model_path(self) -> Path:
        return MODELS / self["model_name"]


def load_active_model_meta() -> ActiveModelInfo | None:
    if not ACTIVE.exists():
        return None
    meta = json.loads(ACTIVE.read_text(encoding="utf-8"))
    return ActiveModelInfo(meta)


def resolve_model_path() -> Path | None:
    meta = load_active_model_meta()
    if not meta:
        return None
    p = meta.model_path
    return p if p.exists() else None


# 例：GUI起動時
def load_model_for_inference():
    p = resolve_model_path()
    if not p:
        print("[AISvc] no active model; fallback to bundled default")
        # ここで同梱のデフォルトをロードするなど
        return None
    print(f"[AISvc] loading: {p.name}")
    # 実際は joblib/pickle/onnxruntime 等でロード
    # return joblib.load(p)
    return None



=== file: app/services/circuit_breaker.py ===

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Optional


JST = timezone(timedelta(hours=9))


@dataclass
class CBState:
    tripped: bool = False
    reason: Optional[str] = None
    consecutive_losses: int = 0
    last_trip_ts: Optional[float] = None
    daily_loss_accum_jpy: float = 0.0
    day_key: str = ""


class CircuitBreaker:
    """
    Resettable circuit breaker that combines consecutive-loss and daily-loss budgets
    with a cool-down period.
    """

    def __init__(
        self,
        max_consecutive_losses: int = 5,
        daily_loss_limit_jpy: float = 0.0,
        cooldown_min: int = 30,
    ):
        self.max_consecutive_losses = int(max_consecutive_losses)
        self.daily_loss_limit_jpy = float(daily_loss_limit_jpy)
        self.cooldown_min = int(cooldown_min)
        self.state = CBState()

    # ------------------------------------------------------------------ #
    # Public API
    # ------------------------------------------------------------------ #
    def on_trade_result(self, profit_jpy: float) -> None:
        """Record a trade result and trip if thresholds are violated."""
        self._rollover_if_new_day()
        if profit_jpy <= 0:
            self.state.consecutive_losses += 1
        else:
            self.state.consecutive_losses = 0

        self.state.daily_loss_accum_jpy += float(profit_jpy)

        if self.max_consecutive_losses > 0 and self.state.consecutive_losses >= self.max_consecutive_losses:
            self._trip("consecutive_losses")

        if (
            self.daily_loss_limit_jpy
            and self.state.daily_loss_accum_jpy <= -abs(self.daily_loss_limit_jpy)
        ):
            self._trip("daily_loss_limit")

    def can_trade(self) -> bool:
        """Return True if trading is allowed (not tripped or cool-down finished)."""
        if self.state.tripped and self.state.last_trip_ts:
            elapsed = datetime.now(tz=timezone.utc).timestamp() - self.state.last_trip_ts
            if elapsed < self.cooldown_min * 60:
                return False
            self.reset()
        return True

    def reset(self) -> None:
        """Reset trip status (but keep daily accumulator)."""
        self.state.tripped = False
        self.state.reason = None
        self.state.consecutive_losses = 0
        self.state.last_trip_ts = None

    def status(self) -> dict:
        """Return a serialisable snapshot of the breaker state."""
        return {
            "tripped": self.state.tripped,
            "reason": self.state.reason,
            "consecutive_losses": self.state.consecutive_losses,
            "daily_loss_accum_jpy": self.state.daily_loss_accum_jpy,
            "day_key": self.state.day_key,
            "cooldown_min": self.cooldown_min,
        }

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #
    def _trip(self, reason: str) -> None:
        self.state.tripped = True
        self.state.reason = reason
        self.state.last_trip_ts = datetime.now(tz=timezone.utc).timestamp()

    def _rollover_if_new_day(self) -> None:
        now = datetime.now(JST)
        key = now.strftime("%Y-%m-%d")
        if key != self.state.day_key:
            self.state.day_key = key
            self.state.daily_loss_accum_jpy = 0.0



=== file: app/services/data_guard.py ===

# app/services/data_guard.py
from __future__ import annotations
import subprocess
from pathlib import Path
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parents[2]  # app/services/ → app → プロジェクトルート
DATA_DIR = PROJECT_ROOT / "data"

def csv_path(symbol_tag: str, timeframe: str, layout: str="per-symbol") -> Path:
    """
    symbol_tag は接尾辞なし（例: USDJPY）
    layout: "flat" or "per-symbol"
    """
    if layout == "per-symbol":
        return DATA_DIR / symbol_tag / "ohlcv" / f"{symbol_tag}_{timeframe}.csv"
    return DATA_DIR / f"{symbol_tag}_{timeframe}.csv"

def ensure_data(symbol_tag: str, timeframe: str, start_date: str, end_date: str,
                env: str="laptop", layout: str="per-symbol") -> Path:
    """
    指定の [start_date, end_date] を満たすCSVが存在するか確認し、足りなければ scripts.make_csv_from_mt5 を呼んで追記する。
    戻り値: CSVのフルパス
    """
    out_csv = csv_path(symbol_tag, timeframe, layout)
    need_fetch = True

    if out_csv.exists():
        try:
            df = pd.read_csv(out_csv, parse_dates=["time"])
            if not df.empty:
                has_start = (df["time"].min() <= pd.Timestamp(start_date))
                has_end   = (df["time"].max() >= pd.Timestamp(end_date))
                need_fetch = not (has_start and has_end)
        except Exception:
            need_fetch = True

    if need_fetch:
        # make_csv_from_mt5 を呼ぶ（不足分は自動追記）
        cmd = [
            str((PROJECT_ROOT / "scripts" / "make_csv_from_mt5.py").resolve()),
            "--symbol", symbol_tag,
            "--timeframes", timeframe,
            "--start", start_date,
            "--layout", layout,
            "--env", env,
        ]
        # Windows では python 経由で実行
        subprocess.check_call(["python", *cmd], cwd=str(PROJECT_ROOT))

    # 最終チェック
    if not out_csv.exists():
        raise FileNotFoundError(f"CSV not found after update: {out_csv}")
    return out_csv



=== file: app/services/event_store.py ===

from __future__ import annotations

import json
import os
import threading
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from typing import Any, Deque, List, Optional
from pathlib import Path

# プロジェクトルート = app/services/ から 2 つ上
_PROJECT_ROOT = Path(__file__).resolve().parents[2]

_LOG_DIR = _PROJECT_ROOT / "logs"
_LOG_FILE = _LOG_DIR / "ui_events.jsonl"
_LOG_DIR.mkdir(parents=True, exist_ok=True)

_lock = threading.Lock()


@dataclass
class UiEvent:
    ts: str
    kind: str
    symbol: str
    side: Optional[str] = None
    price: Optional[float] = None
    sl: Optional[float] = None
    tp: Optional[float] = None
    profit_jpy: Optional[float] = None
    reason: Optional[str] = None
    notes: Optional[str] = None


def _now() -> str:
    return datetime.now(timezone.utc).astimezone().isoformat(timespec="seconds")


class _EventStore:
    def __init__(self, maxlen: int = 1000):
        self._buf: Deque[UiEvent] = deque(maxlen=maxlen)

    def append(self, ev: UiEvent) -> None:
        with _lock:
            self._buf.appendleft(ev)
            with open(_LOG_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(ev), ensure_ascii=False) + "\n")

    def add(self, **kwargs: Any) -> None:
        kwargs.setdefault("ts", _now())
        self.append(UiEvent(**kwargs))

    def recent(self, n: int = 200) -> List[UiEvent]:
        with _lock:
            return list(list(self._buf)[:n])


EVENT_STORE = _EventStore()



=== file: app/services/execution_stub.py ===

﻿from __future__ import annotations

import json
import os
import re
import statistics
from collections import deque, defaultdict
from datetime import datetime
from dataclasses import dataclass
from typing import Any, DefaultDict, Dict, Optional, Tuple
from zoneinfo import ZoneInfo
from pathlib import Path

from loguru import logger

from app.core import market, mt5_client
from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_service, trade_state
from app.services.orderbook_stub import orderbook
from app.services.trailing import AtrTrailer, TrailConfig, TrailState
from app.services.trailing_hook import apply_trailing_update
from core import position_guard
from core.ai.service import AISvc, ProbOut
from core.metrics import METRICS
from core.utils.timeutil import now_jst_iso
from app.services.event_store import EVENT_STORE
from app.services.metrics import publish_metrics

# プロジェクトルート = app/services/ から 2 つ上
_PROJECT_ROOT = Path(__file__).resolve().parents[2]

LOG_DIR = _PROJECT_ROOT / "logs" / "decisions"
LOG_DIR.mkdir(parents=True, exist_ok=True)

_ATR_MED: deque[float] = deque(maxlen=128)
_ATR_LAST_PASS: bool = False
_ATR_LAST_REF: Optional[float] = None
_ATR_LAST_ENABLE: Optional[float] = None
_ATR_LAST_DISABLE: Optional[float] = None

# Trailing state store shared across dryrun/production per symbol
runtime_trail_states: DefaultDict[str, Dict[str, Any]] = defaultdict(dict)


def _load_runtime_threshold(default: float = 0.5) -> float:
    try:
        meta_path = _PROJECT_ROOT / "models" / "active_model.json"
        if meta_path.exists():
            with open(meta_path, "r", encoding="utf-8") as fh:
                meta = json.load(fh)
            threshold = meta.get("best_threshold")
            if isinstance(threshold, (int, float)) and 0.0 < threshold < 1.0:
                print(f"[exec] using best_threshold from active_model.json: {threshold}")
                return float(threshold)
    except Exception as exc:
        print(f"[exec][warn] failed to load best_threshold: {exc}")
    print(f"[exec] using default threshold: {default}")
    return float(default)


BEST_THRESHOLD = _load_runtime_threshold(0.5)
print(f"[exec] active BEST_THRESHOLD={BEST_THRESHOLD}", flush=True)

def reset_atr_gate_state() -> None:
    """???/????????ATR????????????"""
    global _ATR_MED, _ATR_LAST_PASS
    _ATR_MED.clear()
    _ATR_LAST_PASS = False


def _atr_gate_ok(atr_pct_now: float, runtime_cfg: Dict[str, Any]) -> bool:
    """Hysteresis-enabled ATR gate to avoid rapid flip-flops around thresholds."""
    global _ATR_LAST_PASS, _ATR_LAST_REF, _ATR_LAST_ENABLE, _ATR_LAST_DISABLE

    filters_cfg: Dict[str, Any] = {}
    if isinstance(runtime_cfg, dict):
        filters_cfg = (runtime_cfg.get("filters") or {})

    if not filters_cfg:
        try:
            cfg = load_config()
            filters_cfg = cfg.get("filters", {})
        except Exception:
            filters_cfg = {}

    hy = (filters_cfg.get("atr_hysteresis") or {}) if isinstance(filters_cfg, dict) else {}

    default_min = 0.00055
    if isinstance(runtime_cfg, dict):
        default_min = float(runtime_cfg.get("min_atr_pct", default_min))

    en = float(hy.get("enable_min_pct", default_min))
    de = float(hy.get("disable_min_pct", min(en, 0.00045)))
    _ATR_LAST_ENABLE = en
    _ATR_LAST_DISABLE = de
    lb = int(hy.get("lookback", 12)) or 1
    if lb <= 0:
        lb = 1

    _ATR_MED.append(float(atr_pct_now))
    window = list(_ATR_MED)[-lb:] or [atr_pct_now]
    try:
        ref = statistics.median(window)
    except Exception:
        ref = float(window[-1])
    _ATR_LAST_REF = ref

    if _ATR_LAST_PASS:
        if ref < de:
            _ATR_LAST_PASS = False
        return _ATR_LAST_PASS or ref >= de
    else:
        if ref >= en:
            _ATR_LAST_PASS = True
        return _ATR_LAST_PASS or ref >= en


def _tick_to_dict(tick: Any) -> Optional[Dict[str, float]]:
    if tick is None:
        return None

    if isinstance(tick, dict):
        bid = tick.get("bid")
        ask = tick.get("ask")
    else:
        try:
            bid, ask = tick
        except (TypeError, ValueError):
            return None
    try:
        bid_f = float(bid) if bid is not None else 0.0
        ask_f = float(ask) if ask is not None else 0.0
    except (TypeError, ValueError):
        return None
    return {"bid": bid_f, "ask": ask_f, "mid": (bid_f + ask_f) / 2.0}

def _pip_size_for(symbol: str) -> float:
    return 0.01 if symbol.endswith("JPY") else 0.0001

def _point_for(symbol: str) -> float:
    return 0.001 if symbol.endswith("JPY") else 0.0001

def _mid_price(tick_dict: Optional[Dict[str, float]]) -> Optional[float]:
    if tick_dict is None:
        return None
    return tick_dict.get("mid")

def _current_price_for_side(tick_dict: Optional[Dict[str, float]], side: str, price_source: str) -> Optional[float]:
    if tick_dict is None:
        return None
    ps = (price_source or "mid").lower()
    if ps == "bid":
        return tick_dict.get("bid") if side == "BUY" else tick_dict.get("ask")
    if ps == "ask":
        return tick_dict.get("ask") if side == "BUY" else tick_dict.get("bid")
    return tick_dict.get("mid")

def _register_trailing_state(symbol: str, signal: Dict[str, Any], tick_dict: Optional[Dict[str, float]]) -> None:
    xp = signal.get("exit_plan") or {}
    if xp.get("mode") != "atr":
        runtime_trail_states.pop(symbol, None)
        return

    trailing = xp.get("trailing") or {}
    if not trailing.get("enabled", True):
        runtime_trail_states.pop(symbol, None)
        return

    atr_val = float(xp.get("atr") or 0.0)
    if atr_val <= 0.0:
        return

    side = signal.get("side")
    if not side:
        return

    pip_size = float(_pip_size_for(symbol))
    point = float(_point_for(symbol))
    price_source = (trailing.get("price_source") or "mid").lower()

    entry_price = signal.get("entry_price")
    if entry_price is None and tick_dict is not None:
        entry_price = _current_price_for_side(tick_dict, side, price_source)
    if entry_price is None:
        entry_price = _mid_price(tick_dict) if tick_dict else None
    if entry_price is None:
        return

    state = {
        "mode": "atr",
        "side": side,
        "symbol": symbol,
        "entry": float(entry_price),
        "atr": atr_val,
        "pip_size": pip_size,
        "point": point,
        "activate_atr_mult": float(trailing.get("activate_atr_mult", 0.5)),
        "step_atr_mult": float(trailing.get("step_atr_mult", 0.25)),
        "lock_be_atr_mult": float(trailing.get("lock_be_atr_mult", 0.3)),
        "hard_floor_pips": float(trailing.get("hard_floor_pips", 5.0)),
        "only_in_profit": bool(trailing.get("only_in_profit", True)),
        "max_layers": int(trailing.get("max_layers", 20)),
        "price_source": price_source,
        "activated": False,
        "be_locked": False,
        "layers": 0,
        "current_sl": None,
    }

    trail = runtime_trail_states.setdefault(symbol, {})
    trail.clear()
    trail.update(state)
    signal["trail_state"] = {
        "activated": False,
        "be_locked": False,
        "layers": 0,
        "current_sl": None,
        "atr": atr_val,
        "activate_atr_mult": state["activate_atr_mult"],
        "step_atr_mult": state["step_atr_mult"],
        "lock_be_atr_mult": state["lock_be_atr_mult"],
        "hard_floor_pips": state["hard_floor_pips"],
        "price_source": price_source,
        "max_layers": state["max_layers"],
        "only_in_profit": state["only_in_profit"],
        "side": side,
        "symbol": symbol,
        "entry": float(entry_price),
    }
    publish_metrics({
        "trail_activated": False,
        "trail_be_locked": False,
        "trail_layers":    0,
        "trail_current_sl": None,
    })
    signal["entry_price"] = float(entry_price)

def _update_trailing_state(symbol: str, tick_dict: Optional[Dict[str, float]]) -> Optional[Dict[str, Any]]:
    if tick_dict is None:
        return None

    state = runtime_trail_states.setdefault(symbol, {})
    if not state or state.get("mode") != "atr":
        return None

    side = state.get("side")
    entry = state.get("entry")
    atr_val = float(state.get("atr") or 0.0)
    if not side or entry is None or atr_val <= 0.0:
        return None

    price_source = (state.get("price_source") or "mid").lower()
    current_price = _current_price_for_side(tick_dict, side, price_source)
    if current_price is None:
        return None

    cfg = TrailConfig(
        pip_size=float(state.get("pip_size", _pip_size_for(symbol))),
        point=float(state.get("point", _point_for(symbol))),
        atr=atr_val,
        activate_mult=float(state.get("activate_atr_mult", 0.5)),
        step_mult=float(state.get("step_atr_mult", 0.25)),
        lock_be_mult=float(state.get("lock_be_atr_mult", 0.3)),
        hard_floor_pips=float(state.get("hard_floor_pips", 5.0)),
        only_in_profit=bool(state.get("only_in_profit", True)),
        max_layers=int(state.get("max_layers", 20)),
    )
    trail_state = TrailState(
        side=side,
        entry=float(entry),
        activated=bool(state.get("activated", False)),
        be_locked=bool(state.get("be_locked", False)),
        layers=int(state.get("layers", 0)),
        current_sl=state.get("current_sl"),
    )

    trailer = AtrTrailer(cfg, trail_state)
    new_sl = trailer.suggest_sl(float(current_price))

    state.update(
        {
            "activated": trail_state.activated,
            "be_locked": trail_state.be_locked,
            "layers": trail_state.layers,
            "current_sl": trail_state.current_sl,
        }
    )
    runtime_trail_states[symbol] = state

    if new_sl is None:
        return None

    return {
        "new_sl": new_sl,
        "price": current_price,
        "state": {
            "activated": trail_state.activated,
            "be_locked": trail_state.be_locked,
            "layers": trail_state.layers,
            "current_sl": trail_state.current_sl,
            "price_source": price_source,
            "atr": atr_val,
            "max_layers": int(state.get("max_layers", 20)),
            "only_in_profit": bool(state.get("only_in_profit", True)),
            "side": side,
            "symbol": state.get("symbol", symbol),
        },
    }

def _session_hour_allowed() -> bool:
    """
    config.session.allow_hours_jst ??????????????????????
    ????????/???/???????????
    """
    try:
        from core.config import cfg as _cfg
    except Exception:
        _cfg = {}

    session_cfg = {}
    if isinstance(_cfg, dict):
        raw = _cfg.get("session")
        session_cfg = raw if isinstance(raw, dict) else {}

    allow = session_cfg.get("allow_hours_jst", [])
    if not isinstance(allow, (list, tuple, set)) or len(allow) == 0:
        return True

    try:
        import pytz
        from datetime import datetime
        jst = pytz.timezone("Asia/Tokyo")
        hour = datetime.now(jst).hour
    except Exception:
        return True

    return hour in set(allow)

def _symbol_to_filename(symbol: str) -> str:
    safe = re.sub(r"[^A-Za-z0-9_]+", "_", symbol)
    return safe.strip("_") or "UNKNOWN"


def _write_decision_log(symbol: str, record: Dict[str, Any]) -> None:
    fname = LOG_DIR / f"decisions_{_symbol_to_filename(symbol)}.jsonl"
    with open(fname, "a", encoding="utf-8") as fp:
        fp.write(json.dumps(record, ensure_ascii=False) + "\n")


def _build_decision_trace(
    *,
    ts_jst: str,
    symbol: str,
    ai_out: "ProbOut",
    cb_status: Dict[str, Any],
    filters_ctx: Dict[str, Any],
    decision: Dict[str, Any],
    prob_threshold: float,
    calibrator_name: str,
) -> Dict[str, Any]:
    """Assemble a structured trace record for downstream analysis."""
    if isinstance(decision, dict):
        action = str(decision.get("action") or "").upper()
        if action in {"BUY", "SELL", "LONG", "SHORT"}:
            decision_label = "ENTRY"
        else:
            decision_label = str(decision.get("action") or "")
    else:
        decision_label = str(decision)

    trace = {
        "ts_jst": ts_jst,
        "type": "decision",
        "symbol": symbol,
        "filters": filters_ctx,
        "probs": {
            "buy": round(ai_out.p_buy, 6),
            "sell": round(ai_out.p_sell, 6),
            "skip": round(ai_out.p_skip, 6),
        },
        "calibrator": calibrator_name,
        "meta": ai_out.meta,
        "threshold": float(prob_threshold),
        "decision": decision_label,
        "ai": ai_out.model_dump(),
        "cb": cb_status,
        "features_hash": ai_out.features_hash,
        "model": ai_out.model_name,
    }
    if isinstance(decision, dict):
        trace["decision_detail"] = decision
    exit_plan = decision.get("signal", {}).get("exit_plan") if isinstance(decision, dict) else None
    trace["exit_plan"] = exit_plan or {"mode": "none"}
    return trace

def _collect_features(
    symbol: str,
    base_features: Tuple[str, ...],
    tick: Optional[Tuple[float, float]],
    spread_pips: Optional[float],
    open_positions: int,
) -> Dict[str, float]:
    bid, ask = tick if tick else (None, None)
    mid = (float(bid) + float(ask)) / 2 if bid is not None and ask is not None else 0.0
    spr = float(spread_pips) if spread_pips is not None else 0.0

    features: Dict[str, float] = {}
    if not base_features:
        features["bias"] = 1.0
        return features

    for name in base_features:
        if name == "ema_5":
            features[name] = mid
        elif name == "ema_20":
            features[name] = mid
        elif name == "rsi_14":
            features[name] = 50.0
        elif name == "atr_14":
            features[name] = spr
        elif name == "adx_14":
            features[name] = 20.0 + min(20.0, spr * 5.0)
        elif name == "bbp":
            features[name] = 0.5 if spr == 0 else max(0.0, min(1.0, spr / 5.0))
        elif name == "vol_chg":
            features[name] = float(open_positions)
        elif name == "wick_ratio":
            features[name] = 0.5
        else:
            features[name] = 0.0
    return features


@dataclass
class ExecutionStub:
    """
    ドライラン用の実行スタブ：
    - AI確率（AISvc.predict）を呼び出して意思決定だけ行い、約定はしない
    - サーキットブレーカー（self.cb）発動中は BLOCKED を記録
    """
    cb: circuit_breaker.CircuitBreaker
    ai: AISvc

    def __post_init__(self) -> None:
        try:
            self.ai.threshold = float(BEST_THRESHOLD)
        except Exception:
            pass
        try:
            sell_threshold = max(min(1.0 - BEST_THRESHOLD, 1.0), 0.0)
            trade_state.update(
                prob_threshold=float(BEST_THRESHOLD),
                threshold_buy=float(BEST_THRESHOLD),
                threshold_sell=float(sell_threshold),
            )
        except Exception:
            pass


    def on_tick(
        self,
        symbol: str,
        features: Dict[str, float],
        runtime_cfg: Dict[str, Any],
    ) -> Dict[str, Any]:
        ts = now_jst_iso()

        cb_status = self.cb.status()
        ai_out = self.ai.predict(features)

        tick_dict = _tick_to_dict(runtime_cfg.get("tick"))

        spread_limit = float(runtime_cfg.get("spread_limit_pips", 1.5))
        min_adx = float(runtime_cfg.get("min_adx", 15.0))
        min_atr_pct = float(runtime_cfg.get("min_atr_pct", 0.0003))
        disable_adx_gate = bool(runtime_cfg.get("disable_adx_gate", False))
        prob_threshold = float(BEST_THRESHOLD)
        runtime_cfg["prob_threshold"] = prob_threshold
        side_bias = runtime_cfg.get("side_bias")

        raw_spread = runtime_cfg.get("spread_pips", 0.0)
        try:
            cur_spread = float(raw_spread)
        except (TypeError, ValueError):
            cur_spread = 0.0
        cur_spread = round(cur_spread, 5)

        cur_adx = round(float(features.get("adx_14", 0.0)), 5)
        cur_atr_pct = round(float(features.get("atr_14", 0.0)), 8)

        base_filters: Dict[str, Any] = {
            "spread": cur_spread,
            "spread_limit": spread_limit,
            "adx": cur_adx,
            "min_adx": min_adx,
            "adx_disabled": disable_adx_gate,
            "atr_pct": cur_atr_pct,
            "min_atr_pct": min_atr_pct,
            "prob_threshold": prob_threshold,
        }
        if side_bias is not None:
            base_filters["side_bias"] = side_bias

        atr_gate_ok = _atr_gate_ok(cur_atr_pct, runtime_cfg)
        if _ATR_LAST_REF is not None:
            base_filters["atr_ref"] = round(float(_ATR_LAST_REF), 8)
        base_filters["atr_gate_state"] = "open" if _ATR_LAST_PASS else "closed"
        if _ATR_LAST_ENABLE is not None:
            base_filters["atr_enable_min"] = float(_ATR_LAST_ENABLE)
        if _ATR_LAST_DISABLE is not None:
            base_filters["atr_disable_min"] = float(_ATR_LAST_DISABLE)

        grace_active = trade_service.post_fill_grace_active()
        base_filters["post_fill_grace"] = grace_active

        def _emit(decision: Dict[str, Any], filters_ctx: Dict[str, Any], level: str = "info") -> None:
            action = decision.get("action")
            reason = decision.get("reason")

            gate_state = filters_ctx.get("atr_gate_state")
            atr_ref = float(filters_ctx.get("atr_ref", filters_ctx.get("atr_pct", 0.0)) or 0.0)
            post_grace = bool(filters_ctx.get("post_fill_grace", False))

            # --- カウンタは KVS から安全に読み出して加算 ---
            cur = METRICS.get()  # dictコピーが返る想定
            ce = int(cur.get("count_entry", 0))
            cs = int(cur.get("count_skip", 0))
            cb = int(cur.get("count_blocked", 0))
            if action == "ENTRY":
                ce += 1
            elif action == "SKIP":
                cs += 1
            elif action == "BLOCKED":
                cb += 1

            # --- まとめて publish（KVS更新＋runtime/metrics.json原子的書き換え） ---
            publish_metrics({
                "last_decision": action,
                "last_reason":   reason,
                "atr_ref":       float(atr_ref),
                "atr_gate_state": gate_state,
                "post_fill_grace": bool(post_grace),
                "spread":          filters_ctx.get("spread"),
                "adx":             filters_ctx.get("adx"),
                "min_adx":         filters_ctx.get("min_adx"),
                "prob_threshold":  filters_ctx.get("prob_threshold"),
                "min_atr_pct":     filters_ctx.get("min_atr_pct"),
                "count_entry":     ce,
                "count_skip":      cs,
                "count_blocked":   cb,
                # ts は publish_metrics 側でも自動付与するが、ここで入れても良い
            })


            trail_signal = decision.get("signal") if isinstance(decision, dict) else None
            if isinstance(trail_signal, dict) and "trail_state" in trail_signal:
                trail_state = trail_signal.get("trail_state") or {}
                new_sl_val = trail_state.get("current_sl")
                trail_side = trail_state.get("side") or trail_signal.get("side") or decision.get("side")
                trail_symbol = trail_state.get("symbol") or trail_signal.get("symbol") or symbol
                ticket = trail_state.get("ticket") if isinstance(trail_state, dict) else None
                if new_sl_val is not None and trail_side and trail_symbol:
                    try:
                        apply_trailing_update(
                            ticket=ticket if isinstance(ticket, int) else None,
                            side=str(trail_side),
                            symbol=str(trail_symbol),
                            new_sl=float(new_sl_val),
                            reason=str(action or "trail"),
                        )
                    except Exception as exc:
                        logger.debug(f"[TRAIL][HOOK][ERR] {exc}")

            trace = _build_decision_trace(
                ts_jst=ts,
                symbol=symbol,
                ai_out=ai_out,
                cb_status=cb_status,
                filters_ctx=filters_ctx,
                decision=decision,
                prob_threshold=prob_threshold,
                calibrator_name=self.ai.calibrator_name,
            )
            trace["runtime"] = runtime_cfg
            _write_decision_log(symbol, trace)

            ai_payload = ai_out.model_dump()
            ai_payload["best_threshold"] = BEST_THRESHOLD
            ai_payload.setdefault("threshold", getattr(self.ai, "threshold", prob_threshold))
            payload = {
                "mode": "dryrun",
                "symbol": symbol,
                "decision": decision.get("action"),
                "reason": decision.get("reason"),
                "ai": ai_payload,
                "filters": filters_ctx,
                "cb": cb_status,
            }
            log = logger.bind(event="dryrun", ts=ts)
            if level == "warning":
                log.warning(payload)
            elif level == "error":
                log.error(payload)
            else:
                log.info(payload)

        trail_info = _update_trailing_state(symbol, tick_dict)
        if trail_info:
            filters_ctx = dict(base_filters)
            filters_ctx["trail_state"] = trail_info["state"]
            filters_ctx["trail_new_sl"] = trail_info["new_sl"]
            filters_ctx["trail_price"] = trail_info["price"]
            publish_metrics({
                "trail_activated": bool(trail_info["state"].get("activated")),
                "trail_be_locked": bool(trail_info["state"].get("be_locked")),
                "trail_layers":    int(trail_info["state"].get("layers") or 0),
                "trail_current_sl": trail_info["state"].get("current_sl"),
            })

            decision_payload = {
                "action": "TRAIL_UPDATE",
                "reason": None,
                "signal": {
                    "trail_state": trail_info["state"],
                    "trail_new_sl": trail_info["new_sl"],
                    "trail_price": trail_info["price"],
                    "side": trail_info["state"].get("side"),
                    "symbol": trail_info["state"].get("symbol", symbol),
                },
            }
            _emit(decision_payload, filters_ctx, level="info")

        if not _session_hour_allowed():
            filters_ctx = dict(base_filters)
            filters_ctx["session"] = "closed"
            decision_payload = {"action": "SKIP", "reason": "session_closed"}
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        if not grace_active and cur_spread and cur_spread > spread_limit:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "spread"
            decision_payload = {"action": "BLOCKED", "reason": "spread"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not grace_active and not disable_adx_gate and cur_adx < min_adx:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "adx_low"
            decision_payload = {"action": "BLOCKED", "reason": "adx_low"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not grace_active and not atr_gate_ok:
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "atr_low"
            decision_payload = {"action": "BLOCKED", "reason": "atr_low"}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        if not self.cb.can_trade():
            cb_status = self.cb.status()
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "circuit_breaker"
            decision_payload = {"action": "BLOCKED", "reason": cb_status.get("reason", "circuit_breaker")}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        cb_status = self.cb.status()

        if cb_status.get("tripped"):
            filters_ctx = dict(base_filters)
            filters_ctx["blocked"] = "circuit_breaker"
            decision_payload = {"action": "BLOCKED", "reason": cb_status.get("reason", "circuit_breaker")}
            _emit(decision_payload, filters_ctx, level="warning")
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        config = load_config()
        entry_cfg = config.get("entry", {}) if isinstance(config, dict) else {}
        edge = float(entry_cfg.get("entry_min_edge", entry_cfg.get("min_edge", 0.0)))
        buy_threshold = prob_threshold
        sell_threshold = max(min(1.0 - prob_threshold, 1.0), 0.0)

        base_filters["threshold_buy"] = buy_threshold
        base_filters["threshold_sell"] = sell_threshold
        base_filters["edge"] = edge

        filters_ctx = dict(base_filters)
        filters_ctx["blocked"] = None

        if side_bias is None:
            side_bias = (entry_cfg.get("side_bias") or "auto").lower()
        else:
            side_bias = str(side_bias).lower()
        filters_ctx["side_bias"] = side_bias

        p_buy = float(ai_out.p_buy)
        p_sell = float(ai_out.p_sell)

        buy_ok = p_buy >= buy_threshold
        sell_ok = p_sell >= sell_threshold

        chosen_side = None
        chosen_prob = 0.0
        other_prob = 0.0

        if buy_ok and (not sell_ok or p_buy >= p_sell):
            chosen_side = "BUY"
            chosen_prob = p_buy
            other_prob = p_sell
        elif sell_ok:
            chosen_side = "SELL"
            chosen_prob = p_sell
            other_prob = p_buy

        if chosen_side is not None and p_buy == p_sell:
            if side_bias == "buy":
                chosen_side = "BUY"
                chosen_prob = p_buy
                other_prob = p_sell
            elif side_bias == "sell":
                chosen_side = "SELL"
                chosen_prob = p_sell
                other_prob = p_buy

        decision_info: Dict[str, Any] = {
            "threshold_buy": buy_threshold,
            "threshold_sell": sell_threshold,
            "edge": edge,
            "prob_buy": p_buy,
            "prob_sell": p_sell,
        }

        if chosen_side is None:
            decision_info.update({"decision": "SKIP", "reason": "ai_threshold"})
            decision_payload = {
                "action": "SKIP",
                "reason": "ai_threshold",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        if (chosen_prob - other_prob) < edge:
            decision_info.update({"decision": "SKIP", "reason": "ai_low_edge"})
            decision_payload = {
                "action": "SKIP",
                "reason": "ai_low_edge",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, filters_ctx, level="info")
            return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}

        decision_info.update(
            {
                "decision": "ENTRY",
                "side": chosen_side,
                "prob": chosen_prob,
                "edge_delta": chosen_prob - other_prob,
            }
        )

        if not trade_service.can_open_new_position(symbol):
            blocked_filters = dict(base_filters)
            blocked_filters["blocked"] = "pos_guard"
            decision_payload = {
                "action": "BLOCKED",
                "reason": "pos_guard",
                "ai_meta": ai_out.meta,
                "dec": decision_info,
            }
            _emit(decision_payload, blocked_filters, level="warning")
            position_guard.on_order_rejected_or_canceled(symbol)
            return {"blocked": True, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": None}

        signal = {
            "side": chosen_side,
            "prob": chosen_prob,
            "meta": chosen_side,
            "best_threshold": buy_threshold,
        }

        recent_ohlc = globals().get("get_recent_ohlc")
        ohlc_tail = None
        if callable(recent_ohlc):
            try:
                ohlc_tail = recent_ohlc(symbol, bars=64)
            except Exception:
                ohlc_tail = None

        exit_plan = None
        try:
            exit_plan = trade_service.build_exit_plan(symbol, ohlc_tail)
        except Exception:
            exit_plan = None

        if not exit_plan:
            exit_builder = globals().get("_build_exit_plan")
            decision_exit_builder = globals().get("_build_decision_exit_plan")
            if callable(exit_builder) and ohlc_tail is not None:
                if callable(decision_exit_builder):
                    exit_plan = decision_exit_builder(symbol, ohlc_tail)
                else:
                    exit_plan = exit_builder(symbol, ohlc_tail)

        signal["exit_plan"] = exit_plan or {"mode": "none"}

        _register_trailing_state(symbol, signal, tick_dict)

        trade_service.mark_filled_now()
        filters_ctx = dict(base_filters)
        filters_ctx["blocked"] = None
        decision_payload = {
            "action": "ENTRY",
            "reason": decision_info.get("reason","entry_ok"),
            "ai_meta": ai_out.meta,
            "signal": signal,
            "dec": decision_info,
        }
        _emit(decision_payload, filters_ctx, level="info")
        return {"blocked": False, "ai": ai_out, "cb": cb_status, "ts": ts, "decision": decision_payload}


def evaluate_and_log_once() -> None:
    """Dry-run evaluation that mirrors the live decision path."""
    cfg = load_config()
    runtime_cfg = cfg.get("runtime", {})
    ai_cfg = cfg.get("ai", {})
    entry_cfg = cfg.get("entry", {})
    filters_cfg = cfg.get("filters", {})

    best_threshold = float(BEST_THRESHOLD)
    sell_threshold = max(min(1.0 - best_threshold, 1.0), 0.0)

    trade_state.update(
        threshold_buy=best_threshold,
        threshold_sell=sell_threshold,
        prob_threshold=best_threshold,
        side_bias=str(entry_cfg.get("side_bias", "auto") or "auto"),
    )
    settings = trade_state.get_settings()

    symbol = runtime_cfg.get("symbol", "USDJPY")
    spread_limit_pips = float(runtime_cfg.get("spread_limit_pips", runtime_cfg.get("spread_limit", 1.5)))
    max_pos = int(runtime_cfg.get("max_positions", 1))
    min_adx = float(filters_cfg.get("adx_min", 15.0))
    disable_adx_gate = bool(filters_cfg.get("adx_disable", False))
    min_atr_pct = float(filters_cfg.get("min_atr_pct", 0.0003))

    if not settings.trading_enabled:
        logger.bind(event="dryrun", ts=now_jst_iso()).info(
            {"mode": "dryrun", "enabled": False, "reason": "trading_disabled"}
        )
        return

    if not mt5_client.initialize():
        logger.bind(event="dryrun", ts=now_jst_iso()).warning(
            {"mode": "dryrun", "enabled": True, "error": "mt5_init_failed"}
        )
        return

    try:
        spr_callable = getattr(market, "spread", None)
        spr = spr_callable(symbol) if callable(spr_callable) else 0.0

        ob_obj = orderbook() if callable(orderbook) else orderbook
        get_maybe = getattr(ob_obj, "get", None)
        ob = get_maybe(symbol) if callable(get_maybe) else None

        open_cnt = 0
        if ob is not None:
            updater = getattr(ob, "update_with_market_and_close_if_hit", None)
            if callable(updater):
                updater(symbol)
            cnt_getter = getattr(ob, "count_open", None)
            if callable(cnt_getter):
                try:
                    open_cnt = int(cnt_getter(symbol))
                except Exception:
                    open_cnt = 0

        tick = market.tick(symbol)
        tick_dict = None
        if tick:
            try:
                bid, ask = tick
                tick_dict = {"bid": float(bid), "ask": float(ask)}
            except (TypeError, ValueError):
                tick_dict = None

        base_features = tuple(ai_cfg.get("features", {}).get("base", []))
        features = _collect_features(symbol, base_features, tick, spr, open_cnt)

        cb_cfg = cfg.get("circuit_breaker", {}) if isinstance(cfg, dict) else {}
        cb = circuit_breaker.CircuitBreaker(
            max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", 5)),
            daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
            cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
        )
        ai = AISvc(threshold=best_threshold)
        print(f"[exec] AISvc model: {getattr(ai, 'model_name', 'unknown')} (threshold={best_threshold})")
        stub = ExecutionStub(cb=cb, ai=ai)

        runtime_payload = {
            "threshold_buy": best_threshold,
            "threshold_sell": sell_threshold,
            "prob_threshold": best_threshold,
            "spread_limit_pips": spread_limit_pips,
            "max_positions": max_pos,
            "spread_pips": spr,
            "open_positions": open_cnt,
            "ai_threshold": stub.ai.threshold,
            "min_adx": min_adx,
            "disable_adx_gate": disable_adx_gate,
            "min_atr_pct": min_atr_pct,
            "tick": tick,
            "side_bias": settings.side_bias,
        }

        result = stub.on_tick(symbol, features, runtime_payload)
        _ = result
    finally:
        mt5_client.shutdown()




=== file: app/services/metrics.py ===

# app/services/metrics.py
from __future__ import annotations
from pathlib import Path
from typing import Dict, Any
import json, os, time, tempfile
import shutil,time
from core.metrics import METRICS_JSON, METRICS  # METRICS_JSON はファイルパス、METRICS はKVS

def publish_metrics(kv: Dict[str, Any]) -> None:
    """
    Dashboardが読むランタイム指標を KVS と JSON(atomic write) に出力する。
    必要なキー例は下記の通り（全部でなくてOK）:
      last_decision, last_reason, atr_ref, atr_gate_state, post_fill_grace,
      spread, prob_threshold, min_atr_pct, adx, min_adx,
      trail_activated, trail_be_locked, trail_layers, trail_current_sl,
      count_entry, count_skip, count_blocked, cb_tripped, cb_reason, ts
    """
    # KVS（同一プロセス向けフォールバック）
    METRICS.update(**kv)

    # JSON（別プロセス連携／Dashboard標準入力）
    path = Path(METRICS_JSON)
    path.parent.mkdir(parents=True, exist_ok=True)
    data = dict(kv)
    # ts（ローカル更新時刻）はここで保証
    data.setdefault("ts", int(time.time()))

    txt = json.dumps(data, ensure_ascii=False, separators=(",", ":"))
    tmp_path = Path(tempfile.mkstemp(prefix="metrics_", suffix=".json", dir=str(path.parent))[1])
    tmp_path.write_text(txt, encoding="utf-8")

    # --- safe replace with retry ---
    for i in range(10):
        try:
            shutil.move(tmp_path, path)
            break
        except PermissionError:
            time.sleep(0.5)
    else:
        print(f"[metrics][warn] could not update {path} (still locked). skipped.")
    


=== file: app/services/mt5_service.py ===

# app/services/mt5_service.py
from __future__ import annotations
import time
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple

import MetaTrader5 as mt5

@dataclass
class BrokerConstraints:
    # 価格刻み
    digits: int
    point: float
    tick_size: float
    # 制約
    stop_level_points: int     # 最小SL/TP距離（ポイント）
    freeze_level_points: int   # 凍結レベル（ポイント）
    trade_stops_level: int     # 旧称/互換（ポイント）
    # 価格制約（スリッページ等は発注側で取り回し）
    min_sl_step_points: int = 1

def _symbol_props(symbol: str) -> BrokerConstraints:
    info = mt5.symbol_info(symbol)
    if info is None:
        raise RuntimeError(f"symbol_info({symbol}) failed: {mt5.last_error()}")
    # 一部ブローカは stop_level と trade_stops_level どちらかしか意味がないことがある
    stop_level = getattr(info, "stop_level", 0) or getattr(info, "trade_stops_level", 0) or 0
    trade_stops = getattr(info, "trade_stops_level", 0) or stop_level
    freeze = getattr(info, "freeze_level", 0) or 0
    return BrokerConstraints(
        digits=info.digits,
        point=info.point,
        tick_size=getattr(info, "trade_tick_size", info.point),
        stop_level_points=int(stop_level),
        freeze_level_points=int(freeze),
        trade_stops_level=int(trade_stops),
        min_sl_step_points=1
    )

def _round_to_point(price: float, point: float) -> float:
    # MT5に合わせた丸め（ポイント単位）
    return round(price / point) * point

def _sl_min_distance_ok(side: str, price_now: float, sl_price: float, min_points: int, point: float) -> bool:
    # side=BUY → SLは price_now より下、side=SELL → SLは上
    dist_points = abs(price_now - sl_price) / point
    if side == "BUY":
        return sl_price < price_now and dist_points >= min_points
    else:
        return sl_price > price_now and dist_points >= min_points

def _freeze_level_ok(side: str, price_now: float, sl_price: float, freeze_points: int, point: float) -> bool:
    # FreezeLevel以内だと変更不可
    dist_points = abs(price_now - sl_price) / point
    return dist_points > freeze_points

def _snap_sl_to_rules(side: str, price_now: float, desired_sl: float, bc: BrokerConstraints) -> Optional[float]:
    """
    望ましいSLを、StopLevel/FreezeLevel/丸めに収まるよう調整。
    条件を満たせない場合は None（＝更新スキップ）。
    """
    sl = _round_to_point(desired_sl, bc.point)

    # StopLevelを満たすよう少し離す
    if not _sl_min_distance_ok(side, price_now, sl, bc.stop_level_points, bc.point):
        # 何ポイントずらせば良いか計算して、min_sl_step_points単位で押し出す
        need = bc.stop_level_points - abs(price_now - sl) / bc.point
        # 端数切り上げ
        steps = int(max(0, need)) + 1
        delta = steps * bc.min_sl_step_points * bc.point
        if side == "BUY":
            sl = price_now - delta
        else:
            sl = price_now + delta
        sl = _round_to_point(sl, bc.point)

    # FreezeLevelチェック（満たせなければ諦める）
    if not _freeze_level_ok(side, price_now, sl, bc.freeze_level_points, bc.point):
        return None

    # 方向性の安全（BUYなら下、SELLなら上）
    if side == "BUY" and sl >= price_now:
        return None
    if side == "SELL" and sl <= price_now:
        return None

    return sl

def _price_for_side(tick: Dict[str, float], side: str) -> float:
    # 更新判定に使う現在レート（SLは逆サイドに付くので BUY→bid、SELL→ask を基準に）
    if side == "BUY":
        return tick["bid"]
    else:
        return tick["ask"]

def _position_of(ticket: int) -> Any:
    # ticketから現在のポジション情報を取る（なければNone）
    pos = mt5.positions_get(ticket=ticket)
    if pos is None:
        raise RuntimeError(f"positions_get failed: {mt5.last_error()}")
    return pos[0] if len(pos) > 0 else None

def _current_tick(symbol: str) -> Dict[str, float]:
    t = mt5.symbol_info_tick(symbol)
    if t is None:
        raise RuntimeError(f"symbol_info_tick({symbol}) failed: {mt5.last_error()}")
    return {"bid": t.bid, "ask": t.ask, "last": getattr(t, "last", (t.bid + t.ask)/2)}

class MT5Service:
    """
    本線：安全なSL更新（OrderModify）
    """
    def __init__(self, max_retries: int = 3, backoff_sec: float = 0.3, min_change_points: int = 2):
        self.max_retries = max_retries
        self.backoff_sec = backoff_sec
        self.min_change_points = min_change_points  # 現SLからこのポイント以上ずれた時のみ更新

    def safe_order_modify_sl(self, ticket: int, side: str, symbol: str, desired_sl: float, reason: str = "") -> Tuple[bool, Optional[float], str]:
        """
        返り値: (成功/失敗, 実際に送ったSL, 詳細メッセージ)
        """
        pos = _position_of(ticket)
        if pos is None:
            return (False, None, f"no-position ticket={ticket}")

        bc = _symbol_props(symbol)
        tick = _current_tick(symbol)
        price_now = _price_for_side(tick, side)

        # 既存SLとの差分が小さすぎるなら何もしない（チラつき抑制）
        current_sl = float(getattr(pos, "sl", 0.0) or 0.0)
        if current_sl > 0:
            dpoints = abs(current_sl - desired_sl) / bc.point
            if dpoints < self.min_change_points:
                return (True, None, f"skip: delta<{self.min_change_points}pt (current_sl={current_sl}, desired={desired_sl})")

        # 規約に沿ってSLをスナップ
        snapped = _snap_sl_to_rules(side, price_now, desired_sl, bc)
        if snapped is None:
            return (False, None, f"reject: violates stop/freeze/side rules (desired={desired_sl}, price_now={price_now})")

        # TRADE_ACTION_SLTP で変更
        request = {
            "action": mt5.TRADE_ACTION_SLTP,
            "position": ticket,
            "symbol": symbol,
            "sl": round(snapped, bc.digits),
            "tp": float(getattr(pos, "tp", 0.0) or 0.0),  # 既存TPは維持
            "deviation": 10,  # 形式上必要。SLTP変更では通常影響しない
            "comment": f"trail:{reason}"[:28],
            "type_time": mt5.ORDER_TIME_GTC,
            "type_filling": mt5.ORDER_FILLING_RETURN,
        }

        last_err = ""
        for i in range(self.max_retries + 1):
            res = mt5.order_send(request)
            if res is None:
                last_err = f"order_send None: {mt5.last_error()}"
            else:
                if res.retcode == mt5.TRADE_RETCODE_DONE or res.retcode == mt5.TRADE_RETCODE_DONE_PARTIAL:
                    return (True, request["sl"], f"OK retcode={res.retcode}")
                last_err = f"retcode={res.retcode}, comment={getattr(res, 'comment', '')}"

            time.sleep(self.backoff_sec)

            # 価格更新して再スナップ（Freeze/StopLevel変化に追随）
            tick = _current_tick(symbol)
            price_now = _price_for_side(tick, side)
            snapped = _snap_sl_to_rules(side, price_now, desired_sl, bc)
            if snapped is None:
                break
            request["sl"] = round(snapped, bc.digits)

        return (False, None, f"fail: {last_err}")



=== file: app/services/orderbook_stub.py ===

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Optional
from datetime import datetime, timezone, timedelta
from loguru import logger
from app.core import market

JST = timezone(timedelta(hours=9))
TIMEOUT_SECONDS = 0  # 本番寄せ：タイムアウトによる強制クローズを無効化

@dataclass
class MockPosition:
    id: int
    symbol: str
    side: str        # "BUY" or "SELL"
    lot: float
    entry: float
    sl: float
    tp: float
    open_time: datetime = field(default_factory=lambda: datetime.now(JST))
    close_time: Optional[datetime] = None
    closed: bool = False
    close_price: Optional[float] = None
    profit_pips: Optional[float] = None

class OrderBook:
    def __init__(self) -> None:
        self._next_id = 1
        self._positions: List[MockPosition] = []

    def count_open(self, symbol: Optional[str] = None) -> int:
        return sum(1 for p in self._positions if not p.closed and (symbol is None or p.symbol == symbol))

    def open(self, symbol: str, side: str, lot: float, entry: float, sl: float, tp: float) -> MockPosition:
        pos = MockPosition(
            id=self._next_id, symbol=symbol, side=side, lot=lot, entry=entry, sl=sl, tp=tp
        )
        self._next_id += 1
        self._positions.append(pos)
        logger.bind(event="dryrun_open").info({
            "mode":"dryrun", "action":"open", "id": pos.id, "symbol": symbol, "side": side,
            "lot": lot, "entry": entry, "sl": sl, "tp": tp, "ts": pos.open_time.isoformat(timespec="seconds")
        })
        return pos

    def _close(self, p: MockPosition, price: float, reason: str) -> None:
        if p.closed:
            return
        p.closed = True
        p.close_time = datetime.now(JST)
        p.close_price = price
        pip_delta = market.pips_to_price(p.symbol, 1.0)
        if pip_delta and pip_delta > 0:
            p.profit_pips = (p.close_price - p.entry)/pip_delta if p.side == "BUY" else (p.entry - p.close_price)/pip_delta
        else:
            p.profit_pips = None
        logger.bind(event="dryrun_close").info({
            "mode":"dryrun", "action":"close", "id": p.id, "symbol": p.symbol, "side": p.side,
            "entry": p.entry, "close": p.close_price, "profit_pips": p.profit_pips,
            "reason": reason, "ts": p.close_time.isoformat(timespec="seconds")
        })

    def update_with_market_and_close_if_hit(self, symbol: str) -> None:
        """現在の価格で SL/TP 到達、またはTIMEOUTでクローズ。"""
        tk = market.tick(symbol)
        for p in list(self._positions):
            if p.closed or p.symbol != symbol:
                continue
            # TIMEOUT
            if TIMEOUT_SECONDS and (datetime.now(JST) - p.open_time).total_seconds() >= TIMEOUT_SECONDS:
                price = (tk[1] if p.side == "BUY" else tk[0]) if tk else p.entry
                self._close(p, price, "TIMEOUT")
                continue
            if not tk:
                continue
            bid, ask = tk
            price = ask if p.side == "BUY" else bid
            hit_tp = (price >= p.tp) if p.side == "BUY" else (price <= p.tp)
            hit_sl = (price <= p.sl) if p.side == "BUY" else (price >= p.sl)
            if hit_tp:
                self._close(p, price, "TP")
            elif hit_sl:
                self._close(p, price, "SL")

    def close_all(self, symbol: Optional[str] = None) -> None:
        """現在値で全クローズ（ドライラン）。"""
        tk = None
        if symbol:
            tk = market.tick(symbol)
        for p in list(self._positions):
            if p.closed: 
                continue
            if symbol and p.symbol != symbol: 
                continue
            price = None
            if tk:
                bid, ask = tk
                price = ask if p.side == "BUY" else bid
            self._close(p, price if price is not None else p.entry, "FORCE_CLOSE")

# シングルトン
_orderbook = OrderBook()
def orderbook() -> OrderBook:
    return _orderbook



=== file: app/services/trade_service.py ===

from __future__ import annotations

import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, Iterable, Optional

from app.core import mt5_client
from app.core.config_loader import load_config
from app.services import trade_state
from app.services.circuit_breaker import CircuitBreaker
from app.services.event_store import EVENT_STORE
from core.config import cfg
from core.indicators import atr as _atr
from core.position_guard import PositionGuard
from core.utils.clock import now_jst


@dataclass
class LotRule:
    base_equity_per_0p01: int = 10_000
    min_lot: float = 0.01
    max_lot: float = 1.00
    step: float = 0.01


def round_to_step(x: float, step: float) -> float:
    return (int(x / step)) * step


def calc_lot(equity: float, rule: LotRule = LotRule()) -> float:
    raw = (equity / rule.base_equity_per_0p01) * 0.01
    lot = max(rule.min_lot, min(rule.max_lot, round_to_step(raw, rule.step)))
    return float(f"{lot:.2f}")


def snapshot_account() -> Optional[dict]:
    if not mt5_client.initialize():
        return None
    try:
        return mt5_client.get_account_info()
    finally:
        mt5_client.shutdown()


class TradeService:
    """Facade that coordinates guards, circuit breaker, and decision helpers."""

    def __init__(self) -> None:
        self.pos_guard = PositionGuard()
        self.cb = CircuitBreaker()
        self._reconcile_interval = 15
        self._desync_fix = True
        self._last_reconcile = 0.0
        self.state = trade_state.get_runtime()
        self.reload()

    # ------------------------------------------------------------------ #
    # Configuration & helpers
    # ------------------------------------------------------------------ #
    def reload(self) -> None:
        conf = cfg
        g = conf.get("guard", {}) or {}
        cb_cfg = conf.get("circuit_breaker", {}) or {}

        max_positions = int(g.get("max_positions", conf.get("runtime", {}).get("max_positions", 1)))
        inflight_timeout = int(g.get("inflight_timeout_sec", 20))
        self.pos_guard = PositionGuard(max_positions=max_positions, inflight_timeout_sec=inflight_timeout)

        self.cb = CircuitBreaker(
            max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", conf.get("risk", {}).get("max_consecutive_losses", 5))),
            daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
            cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
        )
        self._reconcile_interval = int(g.get("reconcile_interval_sec", 15))
        self._desync_fix = bool(g.get("desync_fix", True))
        self._last_reconcile = 0.0
        self.state = trade_state.get_runtime()

    def _periodic_reconcile(self, symbol: str) -> None:
        now = time.time()
        if now - self._last_reconcile >= self._reconcile_interval:
            self._last_reconcile = now
            self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=self._desync_fix)

    # ------------------------------------------------------------------ #
    # Decisions & guards
    # ------------------------------------------------------------------ #
    def can_open(self, symbol: Optional[str]) -> bool:
        if symbol:
            self._periodic_reconcile(symbol)
        return self.pos_guard.can_open()

    def decide_entry_from_probs(self, p_buy: float, p_sell: float) -> Dict:
        conf = load_config()
        entry_cfg = conf.get("entry", {}) if isinstance(conf, dict) else {}
        th = float(entry_cfg.get("prob_threshold", entry_cfg.get("threshold_buy", 0.60)))
        edge = float(entry_cfg.get("entry_min_edge", entry_cfg.get("min_edge", 0.0)))
        bias = (entry_cfg.get("side_bias") or "auto").lower()

        pmax = p_buy if p_buy >= p_sell else p_sell
        p2nd = p_sell if p_buy >= p_sell else p_buy
        side = "BUY" if p_buy >= p_sell else "SELL"

        if pmax < th:
            return {"decision": "SKIP", "meta": "SKIP", "side": None, "reason": "ai_skip", "threshold": th}

        if (pmax - p2nd) < edge:
            return {
                "decision": "SKIP",
                "meta": "SKIP",
                "side": None,
                "reason": "ai_low_edge",
                "threshold": th,
                "edge": edge,
            }

        if p_buy == p_sell:
            if bias == "buy":
                side = "BUY"
            elif bias == "sell":
                side = "SELL"

        return {"decision": "ENTRY", "meta": side, "side": side, "threshold": th, "edge": edge}

    def decide_entry(self, p_buy: float, p_sell: float) -> Optional[str]:
        result = self.decide_entry_from_probs(p_buy, p_sell)
        return result["side"] if result.get("decision") == "ENTRY" else None

    def can_trade(self) -> bool:
        return self.cb.can_trade()

    def mark_order_inflight(self, order_id: str) -> None:
        self.pos_guard.mark_inflight(order_id)

    def on_order_result(self, *, order_id: str, ok: bool, symbol: str) -> None:
        self.pos_guard.clear_inflight(order_id)
        if ok:
            self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=True)

    def on_order_success(self, *, ticket: Optional[int], side: str, symbol: str, price: Optional[float] = None) -> None:
        self.pos_guard.reconcile_with_broker(symbol=symbol, desync_fix=True)
        runtime = self.state
        runtime.last_ticket = ticket
        runtime.last_side = side
        runtime.last_symbol = symbol
        EVENT_STORE.add(kind="ENTRY", symbol=symbol, side=side, price=price, sl=None, notes=f"ticket={ticket}")

    def on_broker_sync(self, symbol: Optional[str], fix: bool = True) -> None:
        self.pos_guard.reconcile_with_broker(symbol, desync_fix=fix)

    def record_trade_result(
        self,
        *,
        symbol: str,
        side: str,
        profit_jpy: float,
        info: Optional[dict[str, Any]] = None,
    ) -> None:
        resolved_symbol = symbol or self.state.last_symbol or "-"
        resolved_side = side or self.state.last_side
        notes = "settled"
        if info:
            if "notes" in info:
                notes = str(info["notes"])
            else:
                notes = str(info)
        EVENT_STORE.add(
            kind="CLOSE",
            symbol=resolved_symbol,
            side=resolved_side,
            profit_jpy=float(profit_jpy),
            notes=notes,
        )
        self.cb.on_trade_result(profit_jpy)


# ------------------------------------------------------------------ #
# Module-level helpers (backwards compatibility)
# ------------------------------------------------------------------ #
SERVICE = TradeService()


def can_open_new_position(symbol: Optional[str] = None) -> bool:
    settings = trade_state.get_settings()
    if not settings.trading_enabled:
        return False
    sym = symbol or load_config().get("runtime", {}).get("symbol")
    return SERVICE.can_open(sym)


def decide_entry(p_buy: float, p_sell: float) -> Optional[str]:
    return SERVICE.decide_entry(p_buy, p_sell)


def decide_entry_from_probs(p_buy: float, p_sell: float) -> dict:
    return SERVICE.decide_entry_from_probs(p_buy, p_sell)


def get_account_summary() -> dict[str, Any] | None:
    return mt5_client.get_account_info()


def build_exit_plan(symbol: str, ohlc_tail: Optional[Iterable[dict[str, Any]]]) -> dict[str, Any]:
    conf = load_config()
    ex_cfg = conf.get("exits", {}) if isinstance(conf, dict) else {}
    mode = (ex_cfg.get("mode") or "fixed").lower()

    if mode == "none":
        return {"mode": "none"}

    if mode == "fixed":
        fx = ex_cfg.get("fixed", {}) or {}
        return {
            "mode": "fixed",
            "tp_pips": float(fx.get("tp_pips", 10)),
            "sl_pips": float(fx.get("sl_pips", 10)),
        }

    if mode == "atr":
        ax = ex_cfg.get("atr", {}) or {}
        period = int(ax.get("period", 14))
        tp_mult = float(ax.get("tp_mult", 1.2))
        sl_mult = float(ax.get("sl_mult", 1.0))
        trailing = ax.get("trailing", {}) or {}

        highs: list[float] = []
        lows: list[float] = []
        closes: list[float] = []
        for row in ohlc_tail or []:
            h = row.get("h") or row.get("high")
            l = row.get("l") or row.get("low")
            c = row.get("c") or row.get("close")
            if h is not None:
                highs.append(float(h))
            if l is not None:
                lows.append(float(l))
            if c is not None:
                closes.append(float(c))

        atr_value = _atr(highs, lows, closes, period)
        return {
            "mode": "atr",
            "atr": atr_value,
            "tp_mult": tp_mult,
            "sl_mult": sl_mult,
            "trailing": {
                "enabled": bool(trailing.get("enabled", True)),
                "activate_atr_mult": float(trailing.get("activate_atr_mult", 0.5)),
                "step_atr_mult": float(trailing.get("step_atr_mult", 0.25)),
                "lock_be_atr_mult": float(trailing.get("lock_be_atr_mult", 0.3)),
                "hard_floor_pips": float(trailing.get("hard_floor_pips", 5)),
                "only_in_profit": bool(trailing.get("only_in_profit", True)),
                "max_layers": int(trailing.get("max_layers", 20)),
                "price_source": (trailing.get("price_source") or "mid").lower(),
            },
        }

    return {"mode": "fixed", "tp_pips": 10, "sl_pips": 10}


_trade_last_fill_ts: Optional[datetime] = None


def mark_filled_now() -> None:
    """Record the timestamp of the latest successful fill."""
    global _trade_last_fill_ts
    _trade_last_fill_ts = now_jst()


def post_fill_grace_active() -> bool:
    """Return True when the post-fill grace window is active."""
    if _trade_last_fill_ts is None:
        return False

    conf = load_config()
    runtime_cfg = conf.get("runtime", {}) if isinstance(conf, dict) else {}
    grace_sec = int((runtime_cfg or {}).get("post_fill_grace_sec", 0) or 0)
    if grace_sec <= 0:
        return False

    return (now_jst() - _trade_last_fill_ts) <= timedelta(seconds=grace_sec)


def mark_order_inflight(order_id: str) -> None:
    SERVICE.mark_order_inflight(order_id)


def on_order_result(order_id: str, ok: bool, symbol: str) -> None:
    SERVICE.on_order_result(order_id=order_id, ok=ok, symbol=symbol)


def reconcile_positions(symbol: Optional[str] = None, desync_fix: bool = True) -> None:
    SERVICE.on_broker_sync(symbol, fix=desync_fix)


def on_order_success(ticket: Optional[int], side: str, symbol: str, price: Optional[float] = None) -> None:
    SERVICE.on_order_success(ticket=ticket, side=side, symbol=symbol, price=price)


def record_trade_result(
    *,
    symbol: str,
    side: str,
    profit_jpy: float,
    info: Optional[dict[str, Any]] = None,
) -> None:
    SERVICE.record_trade_result(symbol=symbol, side=side, profit_jpy=profit_jpy, info=info)


def circuit_breaker_can_trade() -> bool:
    return SERVICE.can_trade()



=== file: app/services/trade_state.py ===

from dataclasses import dataclass, asdict
from typing import Any, Optional

@dataclass
class TradeSettings:
    trading_enabled: bool = False
    threshold_buy: float = 0.60
    threshold_sell: float = 0.60
    prob_threshold: float = 0.60
    side_bias: str = "auto"
    tp_pips: int = 15
    sl_pips: int = 10

# シングルトン的にプロセス内共有
_settings = TradeSettings()



@dataclass
class TradeRuntime:
    last_ticket: Optional[int] = None
    last_side: Optional[str] = None
    last_symbol: Optional[str] = None


_runtime_state = TradeRuntime()


def get_runtime() -> TradeRuntime:
    return _runtime_state


def update_runtime(**kwargs: Any) -> None:
    for k, v in kwargs.items():
        if hasattr(_runtime_state, k):
            setattr(_runtime_state, k, v)

def get_settings() -> TradeSettings:
    return _settings

def update(**kwargs: Any) -> None:
    for k, v in kwargs.items():
        if hasattr(_settings, k):
            setattr(_settings, k, v)

def as_dict() -> dict[str, Any]:
    return asdict(_settings)



=== file: app/services/trailing.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional


@dataclass
class TrailConfig:
    pip_size: float
    point: float
    atr: float
    activate_mult: float
    step_mult: float
    lock_be_mult: float
    hard_floor_pips: float
    only_in_profit: bool
    max_layers: int


@dataclass
class TrailState:
    side: str  # "BUY" or "SELL"
    entry: float
    activated: bool = False
    be_locked: bool = False
    layers: int = 0
    current_sl: Optional[float] = None


def _round_to_point(price: float, point: float) -> float:
    k = round(price / point)
    return k * point


def _pips(price_diff: float, pip_size: float) -> float:
    return price_diff / pip_size


def _price_from_pips(pips: float, pip_size: float) -> float:
    return pips * pip_size


def _profit_side(side: str, entry: float, price: float) -> float:
    return (price - entry) if side == "BUY" else (entry - price)


class AtrTrailer:
    def __init__(self, cfg: TrailConfig, state: TrailState):
        self.cfg = cfg
        self.st = state

    def activation_threshold(self) -> float:
        return self.cfg.atr * self.cfg.activate_mult

    def step_size(self) -> float:
        return self.cfg.atr * self.cfg.step_mult

    def be_threshold(self) -> float:
        return self.cfg.atr * self.cfg.lock_be_mult

    def suggest_sl(self, current_price: float) -> Optional[float]:
        profit = _profit_side(self.st.side, self.st.entry, current_price)
        if profit <= 0:
            return None

        if not self.st.activated and profit >= self.activation_threshold():
            self.st.activated = True
            sl = self._hard_floor_sl()
            applied = self._apply_if_better(sl)
            if applied is not None:
                return applied

        if not self.st.activated:
            return None

        if (not self.st.be_locked) and (profit >= self.be_threshold()):
            self.st.be_locked = True
            be_sl = self._breakeven_sl()
            hf_sl = self._hard_floor_sl()
            if self.st.side == "BUY":
                new_sl = max(be_sl, hf_sl)
            else:
                new_sl = min(be_sl, hf_sl)
            return self._apply_if_better(new_sl)

        step = self.step_size()
        if step <= 0:
            return None

        layers_should = int(profit // step)
        layers_should = min(layers_should, self.cfg.max_layers)
        if layers_should <= self.st.layers:
            return None

        move_layers = layers_should - self.st.layers
        new_sl = self._layer_sl(move_layers, current_price)
        self.st.layers = layers_should
        return self._apply_if_better(new_sl)

    def _hard_floor_sl(self) -> float:
        delta = _price_from_pips(self.cfg.hard_floor_pips, self.cfg.pip_size)
        if self.st.side == "BUY":
            sl = self.st.entry + delta
        else:
            sl = self.st.entry - delta
        return _round_to_point(sl, self.cfg.point)

    def _breakeven_sl(self) -> float:
        return _round_to_point(self.st.entry, self.cfg.point)

    def _layer_sl(self, move_layers: int, current_price: float) -> float:
        step = self.step_size() * move_layers
        if self.st.side == "BUY":
            sl = current_price - step
        else:
            sl = current_price + step
        sl = self._ensure_profit_side(sl)
        return _round_to_point(sl, self.cfg.point)

    def _ensure_profit_side(self, sl: float) -> float:
        hf = self._hard_floor_sl()
        current = self.st.current_sl
        if self.st.side == "BUY":
            sl = max(sl, hf)
            if self.cfg.only_in_profit and current is not None:
                sl = max(sl, current)
        else:
            sl = min(sl, hf)
            if self.cfg.only_in_profit and current is not None:
                sl = min(sl, current)
        return sl

    def _apply_if_better(self, new_sl: float) -> Optional[float]:
        cur = self.st.current_sl
        if cur is None:
            self.st.current_sl = new_sl
            return new_sl
        if self.st.side == "BUY" and new_sl > cur:
            self.st.current_sl = new_sl
            return new_sl
        if self.st.side == "SELL" and new_sl < cur:
            self.st.current_sl = new_sl
            return new_sl
        return None



=== file: app/services/trailing_hook.py ===

from __future__ import annotations

import logging
from typing import Any, Optional

from app.services.event_store import EVENT_STORE
from core.metrics import METRICS
from core.utils.runtime import is_live

logger = logging.getLogger(__name__)

_mt5svc: Optional[MT5Service] = None

try:
    from app.services.mt5_service import MT5Service
except Exception:
    _mt5svc = None
else:
    _mt5svc = MT5Service(max_retries=3, backoff_sec=0.3, min_change_points=2)


def apply_trailing_update(
    *,
    ticket: Optional[int],
    side: str,
    symbol: str,
    new_sl: float,
    reason: str = "trail",
) -> bool:
    """
    Apply trailing-stop loss updates (dry-run logs or live MT5 OrderModify).
    """

    METRICS.set({"trail_proposed_sl": new_sl, "trail_reason": reason})

    if not is_live():
        EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="DRYRUN")
        logger.info(f"[TRAIL][DRYRUN] side={side} symbol={symbol} new_sl={new_sl} reason={reason}")
        return True

    if _mt5svc is None or ticket is None:
        EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="SKIP")
        logger.warning(
            f"[TRAIL][LIVE][SKIP] ticket={ticket} svc={_mt5svc} side={side} symbol={symbol} new_sl={new_sl}"
        )
        return False

    ok, sent_sl, msg = _mt5svc.safe_order_modify_sl(
        ticket=ticket,
        side=side,
        symbol=symbol,
        desired_sl=new_sl,
        reason=reason,
    )
    sl_val = float(sent_sl) if sent_sl is not None else None

    if ok:
        EVENT_STORE.add(
            kind="TRAIL",
            symbol=symbol,
            side=side,
            sl=sl_val,
            reason=reason,
            notes="OK",
        )
        logger.info(f"[TRAIL][OK] ticket={ticket} side={side} sl={sent_sl} reason={reason} {msg}")
        if sl_val is not None:
            METRICS.set({"trail_current_sl": sl_val})
        METRICS.set({"trail_last_ok": True})
        return True

    if sl_val is not None:
        METRICS.set({"trail_current_sl": sl_val})

    EVENT_STORE.add(kind="TRAIL", symbol=symbol, side=side, sl=float(new_sl), reason=reason, notes="NG")
    logger.warning(f"[TRAIL][NG] ticket={ticket} side={side} desired={new_sl} reason={reason} {msg}")
    METRICS.set({"trail_last_ok": False})
    return False



=== file: app/strategies/__init__.py ===




=== file: app/strategies/ai_strategy.py ===

# app/strategies/ai_strategy.py
from __future__ import annotations
from pathlib import Path
import json
import pandas as pd
import numpy as np
import math
import joblib
import pickle
import binascii
from typing import Tuple, Dict, Any

PROJECT_ROOT = Path(__file__).resolve().parents[2]

def _load_model_generic(path_str: str):
    """
    1) joblib.load()
    2) pickle.load()
    3) LightGBM Booster（.txtやバイナリ）
       └ pklが失敗したら「同名.txt」にも自動フォールバック
    全滅時は先頭バイトをdumpして原因特定メッセージを返す。
    """
    p = Path(path_str)

    # 1) joblib
    try:
        import joblib
        m = joblib.load(p)
        print(f"[wfo] model loaded via joblib: {p.name}", flush=True)
        return m
    except Exception as e1:
        e1_msg = str(e1)

    # 2) pickle
    try:
        with open(p, "rb") as f:
            m = pickle.load(f)
        print(f"[wfo] model loaded via pickle: {p.name}", flush=True)
        return m
    except Exception as e2:
        e2_msg = str(e2)

    # 3) Booster
    def _try_booster(mp: Path):
        import lightgbm as lgb
        booster = lgb.Booster(model_file=str(mp))
        class _BoosterWrapper:
            def __init__(self, bst): self.bst = bst
            def predict_proba(self, X):
                prob1 = self.bst.predict(X)
                prob1 = np.asarray(prob1).reshape(-1)
                prob0 = 1.0 - prob1
                return np.vstack([prob0, prob1]).T
        print(f"[wfo] model loaded via booster: {mp.name}", flush=True)
        return _BoosterWrapper(booster)

    e3_msg = ""
    try:
        return _try_booster(p)
    except Exception as e3:
        e3_msg = str(e3)

    # pkl→txt フォールバック
    alt_txt = p.with_suffix(".txt")
    if alt_txt.exists():
        try:
            return _try_booster(alt_txt)
        except Exception as e4:
            e3_msg += f" | alt_txt='{alt_txt.name}': {e4}"

    # 先頭バイトを表示
    try:
        head = p.read_bytes()[:16]
        head_hex = binascii.hexlify(head).decode("ascii")
    except Exception:
        head_hex = "unreadable"

    raise RuntimeError(
        f"model load failed: joblib='{e1_msg}' | pickle='{e2_msg}' | booster='{e3_msg}' | head={head_hex}"
    )

    
# =====================================================
# active_model.json の読み込み
# =====================================================
def load_active_model() -> Tuple[str, str, float, Dict[str, Any]]:
    meta = PROJECT_ROOT / "active_model.json"
    if not meta.exists():
        raise FileNotFoundError(f"{meta} not found.")

    j = json.loads(meta.read_text(encoding="utf-8"))
    model_name = j.get("model_name", "").strip()
    threshold = float(j.get("best_threshold", 0.5))
    params = j.get("params", {}) or {}

    if model_name.startswith("builtin_"):
        return ("builtin", model_name, threshold, params)

    # 外部モデル
    pkl = PROJECT_ROOT / "models" / f"{model_name}.pkl"
    txt = PROJECT_ROOT / "models" / f"{model_name}.txt"

    # ここを txt 優先にする
    if txt.exists():
        return ("pickle", str(txt), threshold, params)
    if pkl.exists():
        return ("pickle", str(pkl), threshold, params)

    raise FileNotFoundError(f"Model file not found: {pkl} nor {txt}")


# =====================================================
# 特徴量レシピ
# =====================================================
def _rsi(x: pd.Series, period: int = 14) -> pd.Series:
    delta = x.diff()
    up = (delta.clip(lower=0)).rolling(period).mean()
    down = (-delta.clip(upper=0)).rolling(period).mean()
    rs = up / (down + 1e-12)
    return 100 - (100 / (1 + rs))

def _ema(x: pd.Series, span: int) -> pd.Series:
    return x.ewm(span=span, adjust=False).mean()

def _bbands(x: pd.Series, window: int = 20, n_sigma: float = 2.0):
    ma = x.rolling(window).mean()
    sd = x.rolling(window).std(ddof=0)
    upper = ma + n_sigma * sd
    lower = ma - n_sigma * sd
    return upper, lower

def _stoch(high: pd.Series, low: pd.Series, close: pd.Series, k_win: int = 14, d_win: int = 3):
    ll = low.rolling(k_win).min()
    hh = high.rolling(k_win).max()
    k = (close - ll) / (hh - ll + 1e-12) * 100
    d = k.rolling(d_win).mean()
    return k, d

def build_features_recipe(df: pd.DataFrame, name: str) -> pd.DataFrame:
    """
    内蔵レシピで特徴量を作成。time列は残します。
    name:
      - "ohlcv_tech_v1": 代表的なテクニカル群
    """
    out = df.copy()
    close = out["close"].astype(float)
    high = out["high"].astype(float)
    low  = out["low"].astype(float)

    if name == "ohlcv_tech_v1":
        out["ret1"]  = close.pct_change()
        out["ret5"]  = close.pct_change(5)
        out["ret20"] = close.pct_change(20)

        out["sma_10"] = close.rolling(10, min_periods=1).mean()
        out["sma_50"] = close.rolling(50, min_periods=1).mean()
        out["ema_20"] = _ema(close, 20)

        out["rsi_14"] = _rsi(close, 14)
        u,l = _bbands(close, 20, 2.0)
        out["bb_high_20_2"] = u
        out["bb_low_20_2"]  = l

        k,d = _stoch(high, low, close, 14, 3)
        out["stoch_k_14_3"] = k
        out["stoch_d_14_3"] = d

        # シンプルなATR風（高低差のEMA）
        tr = (high - low).abs()
        out["atr_14"] = tr.rolling(14).mean()

        # ボラ指標
        out["vol_pct_20"] = close.pct_change().rolling(20).std() * math.sqrt(20)

    else:
        raise ValueError(f"unknown feature recipe: {name}")

    out = out.dropna().reset_index(drop=True)
    return out

def build_features(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
    """外部モデル・内蔵双方で使う特徴量ビルドの統一入口"""
    recipe = (params or {}).get("feature_recipe", "ohlcv_tech_v1")
    feat = build_features_recipe(df, recipe)
    return feat

# =====================================================
# 予測とシグナル
# =====================================================
def _load_scaler_if_any(params: Dict[str, Any]):
    name = params.get("scaler_name")
    if not name:
        return None
    p = PROJECT_ROOT / "models" / "scalers" / f"{name}.pkl"
    if not p.exists():
        raise FileNotFoundError(f"Scaler not found: {p}")

    # まずヘッダを見て形式推定
    head_hex = None
    try:
        b = p.read_bytes()
        head_hex = binascii.hexlify(b[:8]).decode("ascii")
        # NumPy .npy の典型ヘッダは \x93NUMPY (= 93 4e 55 4d 50 59)
        is_npy = b[:6] == b"\x93NUMPY"
    except Exception:
        is_npy = False

    # 1) 標準pickle
    if not is_npy:
        try:
            with open(p, "rb") as f:
                scaler = pickle.load(f)
            typename = type(scaler).__name__
            print(f"[wfo] using scaler: {name} ({typename})", flush=True)
            return scaler
        except Exception as e1:
            # 2) joblib
            try:
                import joblib
                scaler = joblib.load(p)
                typename = type(scaler).__name__
                print(f"[wfo] using scaler: {name} ({typename}) via joblib", flush=True)
                return scaler
            except Exception as e2:
                # 3) 最後に NumPy ローダ
                try:
                    arr = np.load(str(p), allow_pickle=True)
                    # .npz の場合は最初のキーを取り出す
                    if hasattr(arr, "files"):
                        key0 = arr.files[0]
                        arr = arr[key0]
                    print(f"[wfo] using scaler: {name} (ndarray via np.load)", flush=True)
                    return arr
                except Exception as e3:
                    raise RuntimeError(
                        f"Scaler load failed: pickle='{e1}' | joblib='{e2}' | numpy='{e3}' | head={head_hex}"
                    )

    # ヘッダで .npy と判定された場合は最初から NumPy
    try:
        arr = np.load(str(p), allow_pickle=True)
        if hasattr(arr, "files"):
            key0 = arr.files[0]
            arr = arr[key0]
        print(f"[wfo] using scaler: {name} (ndarray via np.load)", flush=True)
        return arr
    except Exception as e:
        raise RuntimeError(f"Scaler load failed (npy fast-path): {e} | head={head_hex}")


def _ensure_feature_order(feat_df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
    cols = params.get("feature_cols")
    if cols:
        # 明示された列順に合わせ、不足はエラー、余分は削除
        missing = [c for c in cols if c not in feat_df.columns]
        if missing:
            raise ValueError(f"Missing features for model: {missing}")
        return feat_df.loc[:, cols]
    # 明示なしなら、price系を除いた派生列をアルファベット順で安定化
    skip = {"time","open","high","low","close","tick_volume","real_volume","spread"}
    cols = [c for c in feat_df.columns if c not in skip]
    return feat_df.loc[:, sorted(cols)]

def _sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def _predict_proba_generic(model, X: np.ndarray) -> np.ndarray:
    """
    LightGBM / XGBoost / Sklearn いずれでも陽性確率を返す汎用ハンドラー
    """
    import lightgbm as lgb

    # Xは常にndarray化して安全側に倒す
    if isinstance(X, pd.DataFrame):
        X = X.values
    elif not isinstance(X, np.ndarray):
        X = np.asarray(X)

    # LightGBM Booster（生boosterまたはラッパー）
    if isinstance(model, lgb.Booster):
        try:
            prob1 = model.predict(X)
        except Exception as e:
            print(f"[wfo] warn: Booster.predict failed: {e}  -> fallback sigmoid(raw score)", flush=True)
            raw = model.predict(X, raw_score=True)
            prob1 = 1.0 / (1.0 + np.exp(-raw))
        prob1 = np.asarray(prob1).reshape(-1)
        prob0 = 1.0 - prob1
        return np.vstack([prob0, prob1]).T

    # ラッパー（_BoosterWrapper）なら predict_proba を直接呼ぶ
    if hasattr(model, "bst") and hasattr(model.bst, "predict"):
        try:
            prob1 = model.bst.predict(X)
            prob1 = np.asarray(prob1).reshape(-1)
            prob0 = 1.0 - prob1
            return np.vstack([prob0, prob1]).T
        except Exception as e:
            print(f"[wfo] warn: BoosterWrapper predict failed: {e}", flush=True)

    # Sklearn系：predict_proba
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X)
        if proba.ndim == 1:
            return proba
        if proba.shape[1] == 2:
            return proba[:, 1]
        return proba[:, -1]

    # decision_function（SVMなど）
    if hasattr(model, "decision_function"):
        score = model.decision_function(X)
        return _sigmoid(score)

    # それ以外は predict() の結果を確率扱い
    pred = model.predict(X)
    return np.asarray(pred).astype(float)


def predict_signals(kind: str, payload, df_feat: pd.DataFrame, threshold: float = 0.0, params=None) -> pd.Series:
    """
    - builtin_sma: fast/slow のクロスで +1/-1 を返す
    - pickle: 外部モデルの proba > threshold → +1、それ以外 → -1（long_short）等、params['mode'] で制御
    """
    params = params or {}
    mode = params.get("mode", "long_short")  # long_only / short_only / long_flat / long_short

    if kind == "builtin":
        name = str(payload)
        if name == "builtin_sma":
            fast = int(params.get("fast", 10))
            slow = int(params.get("slow", 50))
            sma_fast = df_feat["close"].rolling(fast, min_periods=1).mean()
            sma_slow = df_feat["close"].rolling(slow, min_periods=1).mean()
            raw = np.where(sma_fast > sma_slow, 1, -1)
        else:
            raise ValueError(f"unknown builtin model: {name}")

    else:
        # 外部モデル推論
        model = _load_model_generic(payload)
        X = _ensure_feature_order(df_feat, params)
        scaler = _load_scaler_if_any(params)
        if scaler is not None:
            Xv = X.values
            try:
                # 標準のsklearn系（StandardScaler など）
                Xv = scaler.transform(Xv)
            except AttributeError:
                # dict / (mean, scale) / ndarray を許容
                if isinstance(scaler, dict) and ("mean" in scaler or "scale" in scaler):
                    mean = np.asarray(scaler.get("mean", np.zeros(Xv.shape[1])))
                    scale = np.asarray(scaler.get("scale", np.ones(Xv.shape[1])))
                    Xv = (Xv - mean) / (scale + 1e-12)
                elif isinstance(scaler, (tuple, list)) and len(scaler) >= 2:
                    mean = np.asarray(scaler[0])
                    scale = np.asarray(scaler[1])
                    Xv = (Xv - mean) / (scale + 1e-12)
                elif isinstance(scaler, np.ndarray):
                    # 平均だけが保存されているケース
                    mean = scaler
                    Xv = (Xv - mean)
                else:
                    # 想定外の型は未スケールで続行
                    print(f"[wfo] warn: unknown scaler type -> skip scaling ({type(scaler).__name__})", flush=True)
            # DataFrameに戻す（列名は維持）
            X = pd.DataFrame(Xv, index=X.index, columns=X.columns)
        ##
        proba = _predict_proba_generic(model, X.values)

        # 2次元(=確率2列)なら陽性側だけを採用
        if proba.ndim == 2 and proba.shape[1] == 2:
            proba = proba[:, 1]

        raw = (proba > float(threshold)).astype(int)  # 1 or 0
        ##
        # raw(1/0) → モードごとの最終signal
        if mode == "long_only" or mode == "long_flat":
            # 1=long, 0=flat
            sig = np.where(raw==1, 1, 0)
            return pd.Series(sig, index=df_feat.index, name="signal")
        elif mode == "short_only":
            # 1=flat, 0=short
            sig = np.where(raw==1, 0, -1)
            return pd.Series(sig, index=df_feat.index, name="signal")
        else:
            # long_short: 1=long, 0=short
            sig = np.where(raw==1, 1, -1)
            return pd.Series(sig, index=df_feat.index, name="signal")

    # builtinのモード切替
    if mode == "long_only" or mode == "long_flat":
        sig = np.where(raw==1, 1, 0)
    elif mode == "short_only":
        sig = np.where(raw==1, 0, -1)
    else:
        sig = raw
    return pd.Series(sig, index=df_feat.index, name="signal")

# =====================================================
# トレード生成（動的サイズ・ロング/ショート対応）
# =====================================================
def trades_from_signals(df_feat: pd.DataFrame, initial_capital: float, params=None) -> pd.DataFrame:
    """
    signal列（+1/-1/0）に基づいて IN/OUT/反転。
    ポジションサイズは equity * risk_pct / price を lot_step で丸め。
    PnL = (exit - entry) * dir * units - (spread+commission) * units
    """
    if "signal" not in df_feat.columns:
        raise ValueError("signal列が必要です。")

    p = params or {}
    spread_pips = float(p.get("spread_pips", 0.2))
    risk_pct = float(p.get("risk_pct", 0.01))
    lot_step = int(p.get("lot_step", 1000))
    min_units = int(p.get("min_units", lot_step))
    max_units = int(p.get("max_units", 200000))
    commission_per_unit = float(p.get("commission_per_unit", 0.0))

    spread_yen_per_unit = spread_pips * 0.01
    fee_yen_per_unit = commission_per_unit
    cost_yen_per_unit = spread_yen_per_unit + fee_yen_per_unit

    position = 0           # +1 / -1 / 0
    entry_price = None
    entry_time = None
    units = 0
    equity = float(initial_capital)

    trades = []
    idx_time = df_feat["time"].tolist()
    prices  = df_feat["close"].astype(float).tolist()
    sigs    = df_feat["signal"].astype(int).tolist()

    def _round_units(u: float) -> int:
        if u <= 0:
            return 0
        u = int(u // lot_step * lot_step)
        return max(min_units, min(u, max_units)) if u > 0 else 0

    # entry_time の行番号検索を高速化するための辞書
    time_to_index = {t:i for i,t in enumerate(idx_time)}

    for i in range(len(df_feat)):
        sig = sigs[i]
        price = prices[i]
        t = idx_time[i]

        if sig != position:
            if position != 0 and entry_price is not None and units > 0:
                pnl = (price - entry_price) * position * units - cost_yen_per_unit * units
                equity += pnl
                et_idx = time_to_index.get(entry_time, i)
                trades.append({
                    "entry_time": entry_time,
                    "exit_time": t,
                    "pnl": float(pnl),
                    "direction": "LONG" if position > 0 else "SHORT",
                    "units": int(units),
                    "entry_price": float(entry_price),
                    "exit_price": float(price),
                    "holding_bars": i - et_idx,
                    "holding_days": (pd.Timestamp(t) - pd.Timestamp(entry_time)).days,
                    "win": int(pnl > 0),
                    "equity_after": float(equity)
                })

            position = sig
            entry_price = price
            entry_time = t
            if position != 0:
                raw_units = (equity * risk_pct) / max(price, 1e-9)
                units = _round_units(raw_units)
            else:
                units = 0

    if position != 0 and entry_price is not None and units > 0:
        price = prices[-1]
        t = idx_time[-1]
        pnl = (price - entry_price) * position * units - cost_yen_per_unit * units
        equity += pnl
        et_idx = time_to_index.get(entry_time, len(df_feat)-1)
        trades.append({
            "entry_time": entry_time,
            "exit_time": t,
            "pnl": float(pnl),
            "direction": "LONG" if position > 0 else "SHORT",
            "units": int(units),
            "entry_price": float(entry_price),
            "exit_price": float(price),
            "holding_bars": (len(df_feat)-1) - et_idx,
            "holding_days": (pd.Timestamp(t) - pd.Timestamp(entry_time)).days,
            "win": int(pnl > 0),
            "equity_after": float(equity)
        })

    cols = [
        "entry_time","exit_time","pnl","direction","units",
        "entry_price","exit_price","holding_bars","holding_days","win","equity_after"
    ]
    return pd.DataFrame(trades, columns=cols)




=== file: configs/config copy.yaml ===

account:
  profile: demo
  server:
    demo: OANDA-Demo
    live: OANDA-Live
  login_env_key: MT5_LOGIN
  password_env_key: MT5_PASSWORD
runtime:
  timezone: Asia/Tokyo
  symbol: USDJPY-
  timeframe_exec: M5
  max_positions: 1
  spread_limit: 0.015
  slip_limit: 0.015
  spread_limit_pips: 0.2
  post_fill_grace_sec: 60
lot:
  base_equity_per_0p01: 10000
  min_lot: 0.01
  max_lot: 1.0
  lot_step: 0.01
risk:
  daily_loss_stop: 0.05
  dd_stop: 0.1
  max_consecutive_losses: 2
entry:
  prob_threshold: 0.56     # step back to production range gradually
  threshold_buy: 0.6
  threshold_sell: 0.6
  side_bias: "auto"
  min_edge: 0.01
exit:
  mode: fixed
  tp_pips: 15
  sl_pips: 10
exits:
  mode: "fixed"        # fixed | atr | none
  fixed:
    tp_pips: 12
    sl_pips: 10
  atr:
    period: 14
    tp_mult: 1.2
    sl_mult: 1.0
    trailing:
      enabled: true
      activate_atr_mult: 0.50
      step_atr_mult: 0.25
      lock_be_atr_mult: 0.30
      hard_floor_pips: 5
      only_in_profit: true
      max_layers: 20
      price_source: "mid"
ai:
  stacking: false
  models:
  - name: lgbm_cls
  features:
    base:
    - ema_5
    - ema_20
    - rsi_14
    - atr_14
    - adx_14
    - bbp
    - vol_chg
    - wick_ratio
  retrain:
    schedule_cron: sat 03:00
filters:
  adx_min: 15.0
  adx_disable: false
  min_atr_pct: 0.0005      # ATR floor 0.05%
  atr_hysteresis:
    enable_min_pct: 0.00055
    disable_min_pct: 0.00045
    lookback: 12
session:
  allow_hours_jst:
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    - 20
    - 21
    - 22
    - 23





=== file: configs/config.local.yaml ===

runtime:
  symbol: "USDJPY-"
  spread_limit_pips: 1.5
  max_positions: 1

risk:
  max_consecutive_losses: 5



=== file: configs/config.yaml ===

account:
  profile: demo
  server:
    demo: OANDA-Demo
    live: OANDA-Live
  login_env_key: MT5_LOGIN
  password_env_key: MT5_PASSWORD
runtime:
  mode: "dryrun"   # "live" にすると実売買（MT5 OrderModify）が有効になる
  enabled: true     #trueで本番
  timezone: Asia/Tokyo
  symbol: USDJPY-
  timeframe_exec: M5
  max_positions: 1
  spread_limit: 0.015
  slip_limit: 0.015
  spread_limit_pips: 0.2
  post_fill_grace_sec: 60
guard:
  reconcile_interval_sec: 15
  desync_fix: true
  inflight_timeout_sec: 20
  max_positions: 1
circuit_breaker:
  enable: true
  max_consecutive_losses: 5
  daily_loss_limit_jpy: 30000
  cooldown_min: 30
lot:
  base_equity_per_0p01: 10000
  min_lot: 0.01
  max_lot: 1.0
  lot_step: 0.01
risk:
  daily_loss_stop: 0.05
  dd_stop: 0.1
  max_consecutive_losses: 2
entry:
  prob_threshold: 0.52     # step back to production range gradually
  threshold_buy: 0.6
  threshold_sell: 0.6
  side_bias: "auto"
  min_edge: 0.01
exit:
  mode: fixed
  tp_pips: 15
  sl_pips: 10
exits:
  mode: "fixed"        # fixed | atr | none
  fixed:
    tp_pips: 12
    sl_pips: 10
  atr:
    period: 14
    tp_mult: 1.2
    sl_mult: 1.0
    trailing:
      enabled: true
      activate_atr_mult: 0.50
      step_atr_mult: 0.25
      lock_be_atr_mult: 0.30
      hard_floor_pips: 5
      only_in_profit: true
      max_layers: 20
      price_source: "mid"
ai:
  stacking: false
  models:
  - name: lgbm_cls
  features:
    base:
    - ema_5
    - ema_20
    - rsi_14
    - atr_14
    - adx_14
    - bbp
    - vol_chg
    - wick_ratio
  retrain:
    schedule_cron: sat 03:00
filters:
  adx_min: 15.0
  adx_disable: false
  min_atr_pct: 0.0002      # ATR floor 0.05%
  atr_hysteresis:
    enable_min_pct: 0.00025
    disable_min_pct: 0.00020
    lookback: 12
session:
  allow_hours_jst: []
    
training:
  data_dir: "data/usdjpy"         # 学習データCSV置き場（例）
  file_glob: "USDJPY_M5_*.csv"    # 取り込むCSVのパターン
  model_out_dir: "models"         # 本番モデル置き場
  staging_dir: "models/_staging"  # ステージング置き場（昇格待ち）
  reports_dir: "logs/retrain"     # 学習レポートの出力先
  algo: "lightgbm"                # "lightgbm" / "logreg"
  wf:
    train_days: 5*365             # 学習窓：過去5年
    test_days: 30                 # 直近30日のホールドアウト
    step_days: 7                  # 1週間ごとに繰り返す（今回は固定で1回分）
  metrics:
    min_auc: 0.54                 # 昇格の最低条件（例）
    min_f1: 0.50                  # 〃
  calibrators: ["platt", "isotonic"]
  random_seed: 42

data:
  # 複数PCで違うフォルダでもOK：上から順に探します（環境変数も展開）
  search_paths:
    - "${FXBOT_DATA}"                      # 環境変数で指定（推奨）
    - "data"                               # プロジェクト内CSV
    # 代表例：MT5のヒストリ格納（端末ハッシュ・サーバ名は * でワイルドカード）
    - "%APPDATA%/MetaQuotes/Terminal/*/bases/*/history"
    - "%LOCALAPPDATA%/Programs/MetaTrader 5/bases/*/history"



=== file: core/__init__.py ===




=== file: core/ai/__init__.py ===

# Core AI package initializer.



=== file: core/ai/calibration.py ===

from __future__ import annotations

import pickle
from dataclasses import dataclass
from typing import Any, Literal, Optional, Sequence, Tuple, cast

import numpy as np
from numpy.typing import NDArray

FloatArray = NDArray[np.float64]

from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import log_loss

CalibMethod = Literal["platt", "isotonic"]

# 互換API: 既存コード（core.ai.service 等）から呼ばれることを想定
def load_calibrator(path: str) -> Any:
    """
    互換ローダー。昔のコードが期待しているシグネチャに合わせる。
    core.ai.service から import される想定。
    """
    with open(path, "rb") as f:
        return pickle.load(f)

def apply_calibration(p: Sequence[float] | FloatArray, calib: Calibrator | None) -> FloatArray:
    """
    �݊��K�p�w���p�B�m���z�� p �ɑ΂��āAcalibrator ������� transform ��K�p�B
    calibrator �� None �Ȃ� p �����̂܂ܕԂ��B
    """
    data = cast(FloatArray, np.asarray(p, dtype=float))
    if calib is None:
        return data
    # Calibrator.transform �� 1�����m���x�N�g�����󂯎���� 1������Ԃ��݌v
    return calib.transform(data)


@dataclass
class Calibrator:
    method: CalibMethod
    model: LogisticRegression | IsotonicRegression

    def transform(self, p: FloatArray) -> FloatArray:
        # LogisticRegression は入力 x=logit(p)
        if self.method == "platt":
            eps = 1e-12
            x = np.clip(p, eps, 1 - eps)
            logit = np.log(x / (1 - x)).reshape(-1, 1)
            probs = self.model.predict_proba(logit)[:, 1]
            return cast(FloatArray, np.asarray(probs, dtype=float))
        elif self.method == "isotonic":
            transformed = self.model.transform(p)
            return cast(FloatArray, np.asarray(transformed, dtype=float))
        else:
            raise ValueError(f"unknown method: {self.method}")


def fit_platt(y_valid: np.ndarray, p_valid: np.ndarray) -> Calibrator:
    eps = 1e-12
    x = np.clip(p_valid, eps, 1 - eps)
    logit = np.log(x / (1 - x)).reshape(-1, 1)
    lr = LogisticRegression(solver="liblinear")
    lr.fit(logit, y_valid.astype(int))
    return Calibrator(method="platt", model=lr)


def fit_isotonic(y_valid: np.ndarray, p_valid: np.ndarray) -> Calibrator:
    ir = IsotonicRegression(out_of_bounds="clip")
    ir.fit(p_valid, y_valid.astype(float))
    return Calibrator(method="isotonic", model=ir)


def choose_best_calibrator(
    y_valid: np.ndarray, p_valid: np.ndarray
) -> Tuple[Optional[Calibrator], dict[str, Any]]:
    """
    Compare logloss across available calibration strategies and return the best.
    """
    scores: dict[str, float] = {}
    y_arr = np.asarray(y_valid, dtype=float)
    p_arr = cast(FloatArray, np.asarray(p_valid, dtype=float))
    base_ll = log_loss(y_arr, p_arr, labels=[0, 1])
    scores["none"] = base_ll

    # platt
    try:
        platt = fit_platt(y_arr, p_arr)
        p_platt = platt.transform(p_arr)
        scores["platt"] = log_loss(y_arr, p_platt, labels=[0, 1])
    except Exception:
        scores["platt"] = np.inf
        platt = None

    # isotonic
    try:
        iso = fit_isotonic(y_arr, p_arr)
        p_iso = iso.transform(p_arr)
        scores["isotonic"] = log_loss(y_arr, p_iso, labels=[0, 1])
    except Exception:
        scores["isotonic"] = np.inf
        iso = None

    best = min(scores, key=lambda k: scores[k])
    meta: dict[str, Any] = {"valid_logloss": scores, "baseline": base_ll, "selected": best}

    if best == "none":
        return None, meta
    if best == "platt" and platt is not None and scores["platt"] < base_ll:
        return platt, meta
    if best == "isotonic" and iso is not None and scores["isotonic"] < base_ll:
        return iso, meta
    return None, meta


def save_calibrator(path: str, calib: Calibrator) -> None:
    with open(path, "wb") as f:
        pickle.dump(calib, f)



=== file: core/ai/features.py ===

import numpy as np
import pandas as pd


def _rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = delta.clip(lower=0)
    down = -delta.clip(upper=0)
    roll_up = up.ewm(alpha=1 / period, adjust=False).mean()
    roll_down = down.ewm(alpha=1 / period, adjust=False).mean()
    rs = roll_up / (roll_down + 1e-12)
    rsi = 100 - (100 / (1 + rs))
    return rsi


def _true_range(high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:
    prev_close = close.shift(1)
    tr1 = high - low
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)


def _adx(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    up_arr = np.asarray(high.diff(), dtype=float)
    down_arr = np.asarray(-low.diff(), dtype=float)
    plus_dm = np.where((up_arr > down_arr) & (up_arr > 0), up_arr, 0.0)
    minus_dm = np.where((down_arr > up_arr) & (down_arr > 0), down_arr, 0.0)

    tr = _true_range(high, low, close).astype(float)
    tr_smooth = tr.rolling(period).sum()
    plus_series = pd.Series(plus_dm, index=df.index)
    minus_series = pd.Series(minus_dm, index=df.index)
    plus_di = 100 * plus_series.rolling(period).sum() / (tr_smooth + 1e-12)
    minus_di = 100 * minus_series.rolling(period).sum() / (tr_smooth + 1e-12)

    dx = ((plus_di - minus_di).abs() / ((plus_di + minus_di) + 1e-12)) * 100
    adx = dx.rolling(period).mean()
    return adx


def _bb_percent_b(close: pd.Series, period: int = 20, k: float = 2.0) -> pd.Series:
    ma = close.rolling(period).mean()
    sd = close.rolling(period).std(ddof=0)
    upper = ma + k * sd
    lower = ma - k * sd
    bbp = (close - lower) / ((upper - lower) + 1e-12)
    return bbp.clip(0, 1)


def _wick_body_ratios(df: pd.DataFrame) -> pd.DataFrame:
    open_, high, low, close = df["open"], df["high"], df["low"], df["close"]
    body = (close - open_).abs()
    upper_wick = (high - np.maximum(open_, close)).clip(lower=0)
    lower_wick = (np.minimum(open_, close) - low).clip(lower=0)
    total = (high - low).replace(0, np.nan)
    return pd.DataFrame(
        {
            "upper_wick_ratio": (upper_wick / total).fillna(0),
            "lower_wick_ratio": (lower_wick / total).fillna(0),
            "body_ratio": (body / total).fillna(0),
        }
    )


def _zscore(series: pd.Series, win: int = 20) -> pd.Series:
    mean = series.rolling(win).mean()
    sd = series.rolling(win).std(ddof=0)
    return (series - mean) / (sd + 1e-12)


def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    入力: df には ["open","high","low","close","volume"] が必須
    出力: 元のOHLCV + 追加特徴列（NaNはdropnaで最終的に落としてください）
    """
    out = df.copy()

    for p in (1, 3, 5, 10):
        out[f"ret_{p}"] = df["close"].pct_change(p)

    out["ret_std_10"] = out["ret_1"].rolling(10).std(ddof=0)
    out["ret_std_20"] = out["ret_1"].rolling(20).std(ddof=0)

    out["tr"] = _true_range(df["high"], df["low"], df["close"])
    out["atr_14"] = out["tr"].rolling(14).mean()

    out["rsi_14"] = _rsi(df["close"], 14)
    out["adx_14"] = _adx(df, 14)
    out["bbp_20"] = _bb_percent_b(df["close"], 20, 2.0)

    wick = _wick_body_ratios(df)
    out = pd.concat([out, wick], axis=1)

    out["vol_zscore_20"] = _zscore(df["volume"], 20)

    return out

# --- ここから追記（任意） ---
def build_Xy(df_raw: pd.DataFrame, label_col: str = "label") -> tuple[pd.DataFrame, pd.Series, list[str]]:
    """
    build_features() で作った特徴群から、学習用の X(DF) と y(Series) を返す。
    - label_col はあなたの既存ラベル列名に合わせて変更
    """
    feat_df = build_features(df_raw)

    # 学習に使う列をここで明示化（あなたの実際の特徴列に合わせて調整）
    feature_cols = [
        "open","high","low","close","volume",
        "ret_1","ret_3","ret_5","ret_10",
        "ret_std_10","ret_std_20",
        "tr","atr_14",
        "rsi_14","adx_14","bbp_20",
        "upper_wick_ratio","lower_wick_ratio","body_ratio",
        "vol_zscore_20",
    ]

    # ラベルがまだ無い場合は外で作ってから渡す想定
    if label_col not in feat_df.columns:
        raise ValueError(f"label col '{label_col}' not in DataFrame. 先にラベル作成を行ってください。")

    # 欠損落とし＆インデックス同期
    X_df = feat_df[feature_cols].copy()
    y_ser = feat_df[label_col].copy()
    mask = ~X_df.isna().any(axis=1)
    X_df = X_df.loc[mask]
    y_ser = y_ser.loc[X_df.index]

    return X_df, y_ser, feature_cols
# --- 追記ここまで ---



=== file: core/ai/loader.py ===

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, cast, Sequence, Union

import json
import joblib
import numpy as np
import pandas as pd
from numpy.typing import NDArray

FloatArray = NDArray[np.float64]

ArrayLike = Union[np.ndarray, Sequence[float]]

class ModelWrapper:
    """dict/ラッパーの多段ネストを再帰で“ほどき”、predict_proba/predictを安定提供する薄いラッパー。"""

    def __init__(self, obj_or_path: Union[str, Path, Any]) -> None:
        if isinstance(obj_or_path, (str, Path)):
            loaded = joblib.load(str(obj_or_path))
        else:
            loaded = obj_or_path

        self.base_model = self._unwrap(loaded, depth=0)
        self.classes_ = getattr(self.base_model, "classes_", None)
        self.model_name = getattr(self.base_model, "model_name", None) or \
                          getattr(self.base_model, "__class__", type("X",(object,),{})).__name__
        if isinstance(self.base_model, dict):
            try:
                print(f"[ModelWrapper][warn] still dict after unwrap. keys={list(self.base_model.keys())[:10]}")
            except Exception:
                print("[ModelWrapper][warn] still dict after unwrap (keys unavailable).")

    def _unwrap(self, obj: Any, depth: int = 0) -> Any:
        if depth > 5:
            return obj
        if hasattr(obj, "predict_proba") or hasattr(obj, "predict") or hasattr(obj, "decision_function"):
            return obj
        if isinstance(obj, dict):
            for key in ("model", "estimator", "clf", "base_model", "wrapped", "inner", "object"):
                if key in obj and obj[key] is not None:
                    out = self._unwrap(obj[key], depth + 1)
                    if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                        return out
            for v in obj.values():
                out = self._unwrap(v, depth + 1)
                if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                    return out
        if isinstance(obj, (list, tuple)):
            for v in obj:
                out = self._unwrap(v, depth + 1)
                if hasattr(out, "predict_proba") or hasattr(out, "predict") or hasattr(out, "decision_function"):
                    return out
        return obj

    def predict_proba(self, X: ArrayLike) -> np.ndarray:
        X_arr = np.asarray(X)
        return self.base_model.predict_proba(X_arr)

    def predict(self, X: ArrayLike) -> np.ndarray:
        X_arr = np.asarray(X)
        if hasattr(self.base_model, "predict"):
            return self.base_model.predict(X_arr)
        if hasattr(self.base_model, "decision_function"):
            scores = np.asarray(self.base_model.decision_function(X_arr), dtype=float)
            probs = 1.0 / (1.0 + np.exp(-scores))
            return (probs >= 0.5).astype(int)
        raise AttributeError("The underlying model has neither predict nor decision_function.")

def _load_pickle_or_joblib(path: str) -> Any:
    return joblib.load(path)

def _apply_calibration(calibrator: Any, p1: FloatArray) -> FloatArray:
    data = np.asarray(p1, dtype=float)
    if calibrator is None:
        return data
    if hasattr(calibrator, "transform"):
        transformed = calibrator.transform(data)
        return np.asarray(transformed, dtype=float)
    if hasattr(calibrator, "predict_proba"):
        proba = calibrator.predict_proba(data)
        return np.asarray(proba, dtype=float)
    return data

# ------------------------------------------------------------
# モデルバンドル（必要なら使う。未使用なら残しても害なし）
# ------------------------------------------------------------
@dataclass
class LGBBundle:
    name: str
    version: str
    clf: object            # predict_proba を持つ推論器
    feature_order: List[str]
    ready: bool = True

# ------------------------------------------------------------
# 校正付きラッパ（※単一定義・predict_probaあり）
# ------------------------------------------------------------
class _CalibratedWrapper:
    """Thin wrapper that applies optional calibration to predict_proba outputs."""

    def __init__(self, base_model: Any, calibrator: Any, model_name: str = "(unknown)") -> None:
        self.base_model = base_model
        self.calibrator = calibrator
        self.model_name = model_name
        self.calibrator_name = getattr(calibrator, "method", "none") if calibrator else "none"
        self.expected_features: Optional[list[str]] = None

    def __getattr__(self, item: str) -> Any:
        return getattr(self.base_model, item)

    def predict_proba(self, X: Any) -> FloatArray:
        X_arr = np.asarray(X, dtype=float)
        raw = self.base_model.predict_proba(X_arr)
        raw_arr = np.asarray(raw, dtype=float)
        calibrated = _apply_calibration(self.calibrator, raw_arr)
        return np.asarray(calibrated, dtype=float)

    @property
    def classes_(self) -> NDArray[np.int_]:
        base_classes = getattr(self.base_model, "classes_", None)
        if base_classes is None:
            return np.asarray([0, 1], dtype=int)
        return np.asarray(base_classes, dtype=int)

def _read_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as fh:
        data = json.load(fh)
    if isinstance(data, dict):
        return cast(Dict[str, Any], data)
    raise ValueError(f"JSON at {path} is not an object")

# ------------------------------------------------------------
# active_model.json / 校正ファイルの解決
# ------------------------------------------------------------
def _load_active_meta() -> Dict[str, Any]:
    meta_path = Path("models") / "active_model.json"
    if meta_path.exists():
        try:
            return _read_json(str(meta_path))
        except Exception as exc:
            print(f"[core.ai.loader][warn] active_model.json read failed: {exc}")
    return {}

def _resolve_calib_path(calib_path: str | None) -> Optional[str]:
    return calib_path if calib_path else None

def _maybe_load_calibrator_from_meta(meta: Dict[str, Any]) -> Any:
    cpath = _resolve_calib_path(meta.get("calibrator_path"))
    if cpath:
        try:
            return _load_pickle_or_joblib(cpath)
        except Exception as exc:
            print(f"[core.ai.loader][warn] calibrator load failed: {exc}")
    return None

# ------------------------------------------------------------
# パブリックAPI：モデルローダ
# ------------------------------------------------------------
def load_lgb_clf(model_path: str | None = None, *, meta_path: str | None = None) -> Any:
    model_file = model_path or "models/LightGBM_clf.pkl"
    base_model = _load_pickle_or_joblib(model_file)
    # ★ここを追加：保存物が dict/ラッパでも推定器本体を取り出す
    try:
        # ModelWrapper を一時的に使って「中身の推定器」を取り出す
        _tmp = ModelWrapper(base_model)        # ← 既に同ファイル内で定義済みクラス
        base_model = _tmp.base_model           # ← predict_proba を持つはず
    except Exception:
        # 失敗してもそのまま進める（あとで _CalibratedWrapper でまた拾う）
        pass
    meta: Dict[str, Any] = {}
    if meta_path and Path(meta_path).is_file():
        try:
            meta = _read_json(meta_path)
        except Exception as exc:
            print(f"[core.ai.loader][warn] meta read failed: {exc}")
    else:
        try:
            meta = _load_active_meta()
        except Exception as exc:
            print(f"[core.ai.loader][warn] active meta read failed: {exc}")
            meta = {}

    if "calibration" in meta and "calibrator_path" not in meta:
        calib_meta = meta.get("calibration") or {}
        meta["calibrator_path"] = calib_meta.get("path")

    calibrator = _maybe_load_calibrator_from_meta(meta or {})
    wrapper = _CalibratedWrapper(base_model, calibrator, model_name=Path(model_file).name)

    expected = meta.get("feature_order") or meta.get("features")
    if expected:
        wrapper.expected_features = list(expected)

    return wrapper

# ------------------------------------------------------------
# ユーティリティ
# ------------------------------------------------------------
def build_feature_vector(features: dict, order: list[str]) -> pd.DataFrame:
    row = [features.get(k, 0.0) for k in order]
    return pd.DataFrame([row], columns=order)



=== file: core/ai/service.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional

import glob
import json
import os
from pathlib import Path

import numpy as np
import pandas as pd
from loguru import logger

from core.ai.loader import ModelWrapper, load_lgb_clf


def _read_active_model_path(default: str = "models/LightGBM_clf.pkl") -> str:
    """Resolve model path from active_model.json (target_path -> source_path -> default)."""
    meta_path = Path("models") / "active_model.json"
    if meta_path.exists():
        try:
            with meta_path.open("r", encoding="utf-8") as fh:
                meta = json.load(fh)
            return meta.get("target_path") or meta.get("source_path") or default
        except Exception as exc:
            logger.warning(f"failed to read active_model.json: {exc}")
    return default


def _as_2d_frame(X: Any) -> pd.DataFrame:
    """任意XをLightGBMに渡せる2D DataFrameに整形する。"""
    if isinstance(X, pd.DataFrame):
        return X.copy()
    if isinstance(X, dict):
        return pd.DataFrame([X])
    if isinstance(X, (list, tuple, np.ndarray)):
        arr = np.asarray(X, dtype=float)
        if arr.ndim == 1:
            arr = arr.reshape(1, -1)
        return pd.DataFrame(arr)
    return pd.DataFrame([[X]])

class _ProbOut:
    __slots__ = ("p_buy", "p_sell", "p_skip", "meta", "model_name", "version", "features_hash")

    def __init__(
        self,
        p_buy: float,
        p_sell: float,
        *,
        model_name: str = "(unknown)",
        version: str = "na",
        features_hash: str = "",
    ) -> None:
        buy_val = float(p_buy)
        sell_val = float(p_sell)
        self.p_buy = buy_val
        self.p_sell = sell_val
        self.p_skip = float(min(buy_val, sell_val))
        self.meta = "BUY" if buy_val >= sell_val else "SELL"
        self.model_name = model_name
        self.version = version
        self.features_hash = features_hash

    def model_dump(self) -> Dict[str, float | str]:
        return {
            "model_name": self.model_name,
            "version": self.version,
            "features_hash": self.features_hash,
            "p_buy": self.p_buy,
            "p_sell": self.p_sell,
            "p_skip": self.p_skip,
            "meta": self.meta,
        }


ProbOut = _ProbOut  # backward compatibility for external imports

__all__ = ["AISvc", "ProbOut", "_as_2d_frame", "_read_active_model_path"]


@dataclass
class AISvc:
    threshold: float = 0.52
    model: Optional[object] = None
    model_name: str = "unknown"
    calibrator_name: str = "none"
    is_dummy: bool = False
    expected_features: Optional[list[str]] = None

    def __post_init__(self) -> None:
        self._initialize_model()

    def _resolve_model_path(self) -> str:
        """
        優先順：
          1) self.model_path（あれば）
          2) runtime/active_model.json の "path"
          3) 既定: models/LightGBM_clf.pkl
        """
        if getattr(self, "model_path", None):
            return str(self.model_path)

        try:
            p = Path("runtime/active_model.json")
            if p.exists():
                d = json.loads(p.read_text(encoding="utf-8"))
                m = d.get("path")
                if m:
                    return str(m)
        except Exception:
            pass

        return "models/LightGBM_clf.pkl"

    def _initialize_model(self, model_path: str | None = None):
        from core.ai.loader import ModelWrapper
        from pathlib import Path

        model_path = model_path or self._resolve_model_path()

        # 何が返っても最終的に推定器へ到達できるよう ModelWrapper で統一
        bundle = load_lgb_clf(model_path)
        self.model = ModelWrapper(bundle)

        # 表示名
        self.model_name = getattr(self.model, "model_name", None) or Path(model_path).name or "(unknown)"
        print(f"[AISvc] loaded model: {self.model_name}")

        # 期待特徴量のロード（既存メソッド）
        self._load_expected_features()

    def _load_expected_features(self) -> None:
        """最新レポートの features を expected_features として保持"""
        try:
            meta_path = os.path.join("models", "active_model.json")
            report = None
            if os.path.isfile(meta_path):
                with open(meta_path, encoding="utf-8") as f:
                    meta = json.load(f)
                report = meta.get("best_threshold_source_report")
            if not report or not os.path.isfile(report or ""):
                candidates = sorted(glob.glob(os.path.join("logs", "retrain", "report_*.json")))
                if candidates:
                    report = candidates[-1]
            if report and os.path.isfile(report):
                with open(report, encoding="utf-8") as f:
                    data = json.load(f)
                feats = data.get("features")
                if isinstance(feats, list) and feats:
                    self.expected_features = feats
                    print(f"[AISvc] expected_features loaded ({len(feats)} cols) from {report}")
                    return
        except Exception as exc:
            print(f"[AISvc][warn] expected_features load failed: {exc}")
        self.expected_features = None

    def predict(self, X: Any) -> _ProbOut:
        if self.model is None:
            return _ProbOut(0.5, 0.5, model_name=self.model_name, version="na", features_hash="")

        df = _as_2d_frame(X)
        if self.expected_features:
            for col in self.expected_features:
                if col not in df.columns:
                    df[col] = 0.0
            df = df[self.expected_features]

        values = df.to_numpy(dtype=float, copy=False)
        if hasattr(self.model, "predict_proba"):
            probs = np.asarray(self.model.predict_proba(values), dtype=float)
        elif hasattr(self.model, "predict"):
            preds = np.asarray(self.model.predict(values), dtype=float).reshape(-1, 1)
            probs = np.column_stack([1.0 - preds, preds])
        else:
            probs = np.zeros((len(values), 2), dtype=float)

        features_hash = ""
        try:
            if not df.empty:
                features_hash = str(hash(tuple(df.iloc[0].astype(float).values.tolist())))
        except Exception:
            features_hash = ""

        if probs.ndim == 2:
            p_buy = probs[0, 1]
            p_sell = probs[0, 0]
        else:
            p_buy = float(probs)
            p_sell = 1.0 - p_buy

        version = getattr(getattr(self.model, "clf", self.model), "version", "na")
        return _ProbOut(
            p_buy,
            p_sell,
            model_name=self.model_name,
            version=str(version),
            features_hash=features_hash,
        )



=== file: core/config.py ===

from __future__ import annotations

from functools import lru_cache
from typing import Any, Dict

from app.core.config_loader import load_config


@lru_cache(maxsize=1)
def _load() -> Dict[str, Any]:
    return load_config()


cfg: Dict[str, Any] = _load()


def reload() -> Dict[str, Any]:
    """
    Reload configuration from disk and update cached reference.
    """
    global cfg
    cfg = load_config()
    _load.cache_clear()
    return cfg



=== file: core/indicators.py ===

import math
from typing import Sequence


def true_range(h: float, l: float, prev_close: float) -> float:
    """Return Wilder's true range for a single bar."""
    return max(h - l, abs(h - prev_close), abs(prev_close - l))


def atr(highs: Sequence[float], lows: Sequence[float], closes: Sequence[float], period: int) -> float:
    """Compute a simple average true range over the trailing window."""
    n = len(closes)
    if n < period + 1:
        return math.nan

    trs = []
    for i in range(n - period, n):
        trs.append(true_range(highs[i], lows[i], closes[i - 1]))
    return sum(trs) / len(trs)



=== file: core/metrics.py ===

# core/metrics.py
from __future__ import annotations
from dataclasses import dataclass, field
from threading import Lock
from typing import Any, Dict
import time, os, json, tempfile

RUNTIME_DIR = os.path.join(os.getcwd(), "runtime")
METRICS_JSON = os.path.join(RUNTIME_DIR, "metrics.json")

def _atomic_write_json(path: str, obj: dict):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    fd, tmp = tempfile.mkstemp(prefix="metrics_", suffix=".json", dir=os.path.dirname(path))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, separators=(",", ":"))
        os.replace(tmp, path)
    except Exception:
        try:
            os.remove(tmp)
        except Exception:
            pass

@dataclass
class _MetricsStore:
    _lock: Lock = field(default_factory=Lock)
    _kv: Dict[str, Any] = field(default_factory=dict)

    def set(self, **kwargs):
        with self._lock:
            self._kv.update(kwargs)
            self._kv["ts"] = time.time()
            _atomic_write_json(METRICS_JSON, self._kv)

    def inc(self, key: str, by: int = 1):
        with self._lock:
            self._kv[key] = int(self._kv.get(key, 0)) + by
            self._kv["ts"] = time.time()
            _atomic_write_json(METRICS_JSON, self._kv)

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._kv)

METRICS = _MetricsStore()



=== file: core/metrics/__init__.py ===

# core/metrics/__init__.py
from .registry import METRICS_JSON, METRICS



=== file: core/metrics/fi_extractor.py ===

from __future__ import annotations
from typing import Any, Dict, Iterable, List, Optional, cast

import numpy as np
import pandas as pd
from numpy.typing import NDArray


def _norm_importance(vals: Iterable[float]) -> List[float]:
    arr: NDArray[np.float64] = np.asarray(list(vals), dtype=float)
    if arr.size == 0:
        return []
    total = float(arr.sum())
    if total == 0.0:
        zero_dist = (np.ones_like(arr, dtype=float) / len(arr) * 100.0).astype(float).tolist()
        return cast(List[float], zero_dist)
    normalized = (arr / total * 100.0).astype(float).tolist()
    return cast(List[float], normalized)


def _lgbm_importance(model: Any, method: str = "gain") -> Optional[pd.DataFrame]:
    # LightGBM sklearn API or Booster を想定
    # method: "gain" or "split"
    booster = None
    feature_names = None
    if hasattr(model, "booster_"):
        booster = model.booster_
    elif hasattr(model, "booster"):
        booster = model.booster()
    elif hasattr(model, "model"):
        booster = getattr(model.model, "booster_", None)

    if booster is None:
        # fallback: sklearn APIの feature_importances_（=split相当が多い）
        if hasattr(model, "feature_importances_") and hasattr(model, "feature_name_"):
            vals = list(map(float, getattr(model, "feature_importances_")))
            feature_names = list(getattr(model, "feature_name_"))
            imp = _norm_importance(vals)
            return pd.DataFrame({"feature": feature_names, "importance": imp})
        return None

    # Booster から
    try:
        vals = booster.feature_importance(importance_type=method)
        feature_names = booster.feature_name()
        imp = _norm_importance(vals)
        return pd.DataFrame({"feature": feature_names, "importance": imp})
    except Exception:
        return None


def _xgb_importance(model: Any, method: str = "gain") -> Optional[pd.DataFrame]:
    # XGBoost：booster.get_score(importance_type=method) -> dict {feat: score}
    booster = None
    if hasattr(model, "get_booster"):
        booster = model.get_booster()
    elif hasattr(model, "booster"):
        booster = model.booster
    if booster is None:
        # fallback: sklearn APIの feature_importances_（仕様上gain相当ではない時もある）
        if hasattr(model, "feature_importances_"):
            vals = list(map(float, getattr(model, "feature_importances_")))
            # XGBのsklearnラッパは feature_names_in_ を持つ
            feats = getattr(model, "feature_names_in_", None)
            if feats is None:
                feats = [f"f{i}" for i in range(len(vals))]
            imp = _norm_importance(vals)
            return pd.DataFrame({"feature": feats, "importance": imp})
        return None

    try:
        score: Dict[str, float] = booster.get_score(importance_type=method)
        if not score:
            return None
        feats = list(score.keys())
        vals = list(score.values())
        imp = _norm_importance(vals)
        return pd.DataFrame({"feature": feats, "importance": imp})
    except Exception:
        return None


def extract_feature_importance(
    models: Dict[str, Any],
    method: str = "gain",
    top_n: int = 30,
) -> pd.DataFrame:
    """
    models: {"lgbm_cls": model_obj, "xgb_cls": model_obj, ...}
    method: "gain" | "split"（LightGBM/XGBoost両対応。XGBは"weight"=split相当）
    top_n : 上位Nのみ返す（モデルごとにtop_nを抽出→縦結合）

    return columns: ["feature","importance","model","method"]
    importanceは各モデル内で正規化（合計=100）後、top_n抽出。
    """
    frames: List[pd.DataFrame] = []
    for name, m in models.items():
        df = None
        # 判別ざっくり：文字列に"lightgbm" or "xgboost"が含まれるか、属性で判定
        module_name = type(m).__module__.lower()
        if "lightgbm" in module_name:
            df = _lgbm_importance(m, method=method)
        elif "xgboost" in module_name:
            # XGBoostの"split"相当はimportance_type="weight"
            xgb_method = method
            if method == "split":
                xgb_method = "weight"
            df = _xgb_importance(m, method=xgb_method)
        else:
            # 最後の手段：feature_importances_があれば使う
            if hasattr(m, "feature_importances_"):
                vals = list(map(float, getattr(m, "feature_importances_")))
                feats = getattr(m, "feature_names_in_", None)
                if feats is None:
                    feats = [f"f{i}" for i in range(len(vals))]
                imp = _norm_importance(vals)
                df = pd.DataFrame({"feature": feats, "importance": imp})

        if df is None or df.empty:
            continue

        df = df.sort_values("importance", ascending=False).head(top_n).copy()
        df["model"] = name
        df["method"] = method
        frames.append(df)

    if not frames:
        return pd.DataFrame(columns=["feature", "importance", "model", "method"])

    out = pd.concat(frames, axis=0, ignore_index=True)
    # 表示安定化のためimportanceを丸める
    out["importance"] = out["importance"].round(2)
    return out.sort_values(["model", "importance"], ascending=[True, False]).reset_index(drop=True)



=== file: core/metrics/registry.py ===

# core/metrics/registry.py
from __future__ import annotations
from pathlib import Path
import json
import threading
from typing import Any, Dict

# プロジェクトルート .../fxbot
ROOT = Path(__file__).resolve().parents[2]

# ランタイム出力: .../fxbot/runtime/metrics.json
_RUNTIME_DIR = ROOT / "runtime"
_RUNTIME_DIR.mkdir(parents=True, exist_ok=True)
METRICS_JSON = str(_RUNTIME_DIR / "metrics.json")  # GUI側が open(METRICS_JSON) する前提

class _MetricsKV:
    """
    簡易KVS: GUI側がファイル読めなかったときのフォールバック。
    trade_service 等が同プロセス内で set()/update() を呼べば get() で返る。
    """
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self._kv: Dict[str, Any] = {}

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._kv)

    def set(self, kv: Dict[str, Any]) -> None:
        with self._lock:
            self._kv.update(kv)

    def update(self, **kwargs: Any) -> None:
        with self._lock:
            self._kv.update(kwargs)

# 外部公開
METRICS = _MetricsKV()



=== file: core/position_guard.py ===

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import Dict, Optional

try:
    import MetaTrader5 as mt5
except Exception:  # pragma: no cover - MT5 unavailable in dryrun/tests
    mt5 = None


@dataclass
class PositionGuardState:
    inflight_orders: Dict[str, float] = field(default_factory=dict)
    last_reconcile_ts: float = 0.0
    open_count: int = 0
    last_fix_reason: Optional[str] = None


class PositionGuard:
    """
    Track live positions/in-flight orders and reconcile with broker state.
    The guard focuses on aggregate count (per account) to keep logic simple.
    """

    def __init__(self, max_positions: int = 1, inflight_timeout_sec: int = 20):
        self.max_positions = max_positions
        self.inflight_timeout_sec = inflight_timeout_sec
        self.state = PositionGuardState()

    # === public API ===

    def mark_inflight(self, order_id: str) -> None:
        """Mark an order as in-flight (before send)."""
        self.state.inflight_orders[str(order_id)] = time.time()

    def clear_inflight(self, order_id: str) -> None:
        """Clear an order from in-flight tracking."""
        self.state.inflight_orders.pop(str(order_id), None)

    def can_open(self) -> bool:
        """Return True when within allowed max positions (after GC)."""
        self._gc_inflight()
        return self.state.open_count < self.max_positions

    def reset(self) -> None:
        """Reset guard state."""
        self.state = PositionGuardState()

    def reconcile_with_broker(self, symbol: Optional[str], desync_fix: bool = True) -> None:
        """
        Sync the local open count with broker positions.
        In dryrun (no MT5), it sticks with current open_count.
        """
        now = time.time()
        self.state.last_reconcile_ts = now
        count = self.state.open_count
        try:
            if mt5 is not None:
                if symbol:
                    poss = mt5.positions_get(symbol=symbol) or []
                else:
                    poss = mt5.positions_get() or []
                count = len(poss)
        except Exception:
            return

        if count != self.state.open_count:
            reason = f"desync(open_count={self.state.open_count} -> {count})"
            self.state.last_fix_reason = reason
            if desync_fix:
                self.state.open_count = count
            self._gc_inflight()

    # === helpers ===

    def _gc_inflight(self) -> None:
        now = time.time()
        dead = [k for k, ts in list(self.state.inflight_orders.items()) if now - ts > self.inflight_timeout_sec]
        for k in dead:
            self.state.inflight_orders.pop(k, None)


# ----------------------------------------------------------------------
# Backwards-compatible procedural helpers (legacy call sites still expect these)
# ----------------------------------------------------------------------
_DEFAULT_GUARD = PositionGuard()


def get_default_guard() -> PositionGuard:
    return _DEFAULT_GUARD


def can_open_new(symbol: Optional[str], max_positions: int) -> bool:
    guard = get_default_guard()
    if guard.max_positions != max_positions:
        guard.max_positions = max_positions
    guard._gc_inflight()
    return guard.can_open()


def mark_inflight(symbol: Optional[str], flag: bool) -> None:
    guard = get_default_guard()
    key = symbol or "GLOBAL"
    if flag:
        guard.mark_inflight(key)
    else:
        guard.clear_inflight(key)


def reset() -> None:
    get_default_guard().reset()


def on_order_rejected_or_canceled(symbol: Optional[str] = None, ticket: Optional[int] = None) -> None:
    mark_inflight(symbol, False)



=== file: core/utils/__init__.py ===

# Core utils package initializer.



=== file: core/utils/clock.py ===

from __future__ import annotations

from datetime import datetime, timedelta, timezone

JST = timezone(timedelta(hours=9), name="Asia/Tokyo")


def now_jst() -> datetime:
    """Return current datetime in JST (timezone-aware)."""
    return datetime.now(JST)



=== file: core/utils/hashing.py ===

# core/utils/hashing.py
from __future__ import annotations

import hashlib
import json
from typing import Any, Mapping, Sequence


def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def hash_features(features: Mapping[str, Any], order: Sequence[str] | None = None) -> str:
    """
    特徴量辞書を安定ハッシュ化する。
    order が与えられたらその順、無ければキーでソート。
    値は JSON にしてから sha256。
    """
    if order is None:
        order = sorted(features.keys())
    snapshot = {key: features.get(key, None) for key in order}
    payload = json.dumps(snapshot, ensure_ascii=False, sort_keys=True, separators=(",", ":"))
    return sha256_text(payload)



=== file: core/utils/runtime.py ===

from __future__ import annotations

import os
from functools import lru_cache


@lru_cache(maxsize=1)
def is_live() -> bool:
    """
    Returns True when the environment variable FXBOT_RUNTIME indicates live mode.
    """
    return os.environ.get("FXBOT_RUNTIME", "").strip().lower() in {"live", "prod", "production"}



=== file: core/utils/timeutil.py ===

# core/utils/timeutil.py
from __future__ import annotations

from datetime import datetime, timedelta, timezone

JST = timezone(timedelta(hours=9), name="Asia/Tokyo")


def now_jst_iso() -> str:
    return datetime.now(JST).isoformat(timespec="seconds")



=== file: fxbot_path.py ===

from __future__ import annotations

import os
from pathlib import Path


def get_project_root() -> Path:
    """
    fxbot プロジェクトのルートパスを返す。
    このファイル自身（fxbot_path.py）が置かれているディレクトリをルートとみなす。
    例:
      C:\\Users\\macht\\OneDrive\\fxbot
      D:\\macht\\OneDrive\\fxbot
      C:\\fxbot
    """
    return Path(__file__).resolve().parent


def get_data_root(cli_data_dir: str | os.PathLike | None = None) -> Path:
    """
    データディレクトリの候補を複数試して、最初に存在したディレクトリを採用する。
    優先順位:
      1) --data-dir 引数で明示されたパス
      2) 環境変数 FXBOT_DATA
      3) プロジェクトルート配下の data/
      4) カレントディレクトリ配下の data/
    どれも存在しない場合は、最後に project_root/data を返す（存在チェックには使える）。
    """
    candidates: list[Path] = []

    # 1) CLI 引数
    if cli_data_dir:
        candidates.append(Path(cli_data_dir))

    # 2) 環境変数
    env_dir = os.getenv("FXBOT_DATA")
    if env_dir:
        candidates.append(Path(env_dir))

    # 3) プロジェクトルートの data
    root = get_project_root()
    candidates.append(root / "data")

    # 4) カレントディレクトリの data
    candidates.append(Path.cwd() / "data")

    # 実在するもののうち先頭
    for p in candidates:
        if p.is_dir():
            return p.resolve()

    # 全滅なら project_root/data をとりあえず返す
    return (root / "data").resolve()



=== file: mypy.ini ===

[mypy]
python_version = 3.13
ignore_missing_imports = True
disallow_untyped_defs = True
check_untyped_defs = True
no_implicit_optional = True
warn_unused_ignores = True
warn_return_any = True
pretty = True
show_error_codes = True

[mypy-scripts.*]
disallow_untyped_defs = False
check_untyped_defs = False

[mypy-tools.*]
disallow_untyped_defs = False
check_untyped_defs = False

[mypy-app.gui.*]
disallow_untyped_defs = False
check_untyped_defs = False

[mypy-MetaTrader5.*]
ignore_missing_imports = True

[mypy-pandas.*]
ignore_missing_imports = True

[mypy-yaml.*]
ignore_missing_imports = True

[mypy-joblib.*]
ignore_missing_imports = True

[mypy-sklearn.*]
ignore_missing_imports = True

[mypy-PyQt6.*]
ignore_missing_imports = True



=== file: project_tree.txt ===

.gitignore
app
app/__init__.py
app/core
app/core/config_loader.py
app/core/logger.py
app/core/market.py
app/core/mt5_client.py
app/data
app/gui
app/gui/__init__.py
app/gui/ai_tab.py
app/gui/backtest_tab.py
app/gui/control_tab.py
app/gui/dashboard_tab.py
app/gui/dashboard_tab_qt.py
app/gui/history_tab.py
app/gui/main.py
app/gui/widgets
app/gui/widgets/feature_importance.py
app/main_tk.py
app/models
app/services
app/services/__init__.py
app/services/ai_service.py
app/services/circuit_breaker.py
app/services/event_store.py
app/services/execution_stub.py
app/services/metrics.py
app/services/mt5_service.py
app/services/orderbook_stub.py
app/services/trade_service.py
app/services/trade_state.py
app/services/trailing.py
app/services/trailing_hook.py
configs
configs/.env.example
configs/config copy.yaml
configs/config.local.yaml
configs/config.yaml
core
core/__init__.py
core/ai
core/ai/__init__.py
core/ai/calibration.py
core/ai/features.py
core/ai/loader.py
core/ai/service.py
core/config.py
core/indicators.py
core/metrics
core/metrics.py
core/metrics/__init__.py
core/metrics/fi_extractor.py
core/metrics/registry.py
core/position_guard.py
core/utils
core/utils/__init__.py
core/utils/clock.py
core/utils/hashing.py
core/utils/runtime.py
core/utils/timeutil.py
data
data/usdjpy
data/usdjpy/USDJPY_M5_mt5.csv
data/USDJPY_M5.csv
data/USDJPY_M5_big.csv
logs
logs/app.log
logs/backtest
logs/backtest/equity_curve.csv
logs/decisions
logs/decisions/decisions_USDJPY.jsonl
logs/events.jsonl
logs/retrain
logs/retrain/feat_importance_lk10_20251107_175557.csv
logs/retrain/feat_importance_lk15_20251107_162055.csv
logs/retrain/feat_importance_lk15_20251107_162243.csv
logs/retrain/feat_importance_lk15_20251107_163007.csv
logs/retrain/feat_importance_lk15_20251107_163044.csv
logs/retrain/job_20251105_173052_251f5624.log
logs/retrain/job_20251105_173113_df2776b1.log
logs/retrain/job_20251105_174004_2dd5540f.log
logs/retrain/job_20251105_174018_d4741638.log
logs/retrain/job_20251105_175145_673a797f.log
logs/retrain/job_20251105_180545_a53ced47.log
logs/retrain/job_20251106_075546_f4036ef5.log
logs/retrain/job_20251106_171821_5f3f029d.log
logs/retrain/job_20251107_124035_a081c214.log
logs/retrain/job_20251107_162053_b7252f19.log
logs/retrain/job_20251107_163005_f87f9cc2.log
logs/retrain/job_20251107_175555_4ab13e4d.log
logs/retrain/report_1762327557.2895734.json
logs/retrain/report_1762328795.2841146.json
logs/retrain/report_1762329166.0924582.json
logs/retrain/report_1762330372.625119.json
logs/retrain/report_1762330422.048195.json
logs/retrain/report_1762330462.918168.json
logs/retrain/report_17623314611667528.json
logs/retrain/report_17623314813159773.json
logs/retrain/report_17623320128869925.json
logs/retrain/report_17623320267762287.json
logs/retrain/report_17623327167915437.json
logs/retrain/report_17623335580532284.json
logs/retrain/report_17623833702376008.json
logs/retrain/report_17624171072965233.json
logs/retrain/report_17624869548320115.json
logs/retrain/report_17625000562076344.json
logs/retrain/report_17625006073549628.json
logs/retrain/report_1762505757473982.json
logs/ui_events.jsonl
logs/val_export_meta.json
logs/val_p_buy_raw.npy
logs/val_y_true.npy
models
models/_backup_20251105_174012
models/_backup_20251105_174012/active_model.json
models/_backup_20251105_174012/backup_1762331199.pkl
models/_backup_20251105_174012/calib_isotonic.pkl
models/_backup_20251105_174012/calib_platt.pkl
models/_backup_20251105_174012/LightGBM_clf.classes.json
models/_backup_20251105_174012/LightGBM_clf.features.json
models/_backup_20251105_174012/LightGBM_clf.pkl
models/_backup_20251105_174012/LightGBM_clf.version.json
models/_backup_20251105_174026
models/_backup_20251105_174026/active_model.json
models/_backup_20251105_174026/backup_1762331199.pkl
models/_backup_20251105_174026/calib_isotonic.pkl
models/_backup_20251105_174026/calib_platt.pkl
models/_backup_20251105_174026/LightGBM_clf.classes.json
models/_backup_20251105_174026/LightGBM_clf.features.json
models/_backup_20251105_174026/LightGBM_clf.pkl
models/_backup_20251105_174026/LightGBM_clf.version.json
models/_backup_20251105_174026/PROMOTE.json
models/_backup_20251105_174026/VERSION.txt
models/_backup_20251105_180558
models/_backup_20251105_180558/active_model.json
models/_backup_20251105_180558/backup_1762331199.pkl
models/_backup_20251105_180558/calib_isotonic.pkl
models/_backup_20251105_180558/calib_platt.pkl
models/_backup_20251105_180558/LightGBM_clf.classes.json
models/_backup_20251105_180558/LightGBM_clf.features.json
models/_backup_20251105_180558/LightGBM_clf.pkl
models/_backup_20251105_180558/LightGBM_clf.version.json
models/_backup_20251105_180558/PROMOTE.json
models/_backup_20251105_180558/VERSION.txt
models/_backup_20251106_075609
models/_backup_20251106_075609/active_model.json
models/_backup_20251106_075609/backup_1762331199.pkl
models/_backup_20251106_075609/backup_1762333581.pkl
models/_backup_20251106_075609/calib_isotonic.pkl
models/_backup_20251106_075609/calib_platt.pkl
models/_backup_20251106_075609/LightGBM_clf.classes.json
models/_backup_20251106_075609/LightGBM_clf.features.json
models/_backup_20251106_075609/LightGBM_clf.pkl
models/_backup_20251106_075609/LightGBM_clf.version.json
models/_backup_20251106_075609/PROMOTE.json
models/_backup_20251106_075609/VERSION.txt
models/_backup_20251106_171826
models/_backup_20251106_171826/active_model.json
models/_backup_20251106_171826/backup_1762331199.pkl
models/_backup_20251106_171826/backup_1762333581.pkl
models/_backup_20251106_171826/backup_1762383513.pkl
models/_backup_20251106_171826/calib_isotonic.pkl
models/_backup_20251106_171826/calib_platt.pkl
models/_backup_20251106_171826/LightGBM_clf.classes.json
models/_backup_20251106_171826/LightGBM_clf.features.json
models/_backup_20251106_171826/LightGBM_clf.pkl
models/_backup_20251106_171826/LightGBM_clf.version.json
models/_backup_20251106_171826/PROMOTE.json
models/_backup_20251106_171826/VERSION.txt
models/_backup_20251107_124234
models/_backup_20251107_124234/active_model.json
models/_backup_20251107_124234/backup_1762331199.pkl
models/_backup_20251107_124234/backup_1762333581.pkl
models/_backup_20251107_124234/backup_1762383513.pkl
models/_backup_20251107_124234/backup_1762417953.pkl
models/_backup_20251107_124234/calib_isotonic.pkl
models/_backup_20251107_124234/calib_platt.pkl
models/_backup_20251107_124234/LightGBM_clf.classes.json
models/_backup_20251107_124234/LightGBM_clf.features.json
models/_backup_20251107_124234/LightGBM_clf.pkl
models/_backup_20251107_124234/LightGBM_clf.version.json
models/_backup_20251107_124234/PROMOTE.json
models/_backup_20251107_124234/VERSION.txt
models/_backup_20251107_162055
models/_backup_20251107_162055/active_model.json
models/_backup_20251107_162055/backup_1762331199.pkl
models/_backup_20251107_162055/backup_1762333581.pkl
models/_backup_20251107_162055/backup_1762383513.pkl
models/_backup_20251107_162055/backup_1762417953.pkl
models/_backup_20251107_162055/calib_isotonic.pkl
models/_backup_20251107_162055/calib_platt.pkl
models/_backup_20251107_162055/LightGBM_clf.classes.json
models/_backup_20251107_162055/LightGBM_clf.features.json
models/_backup_20251107_162055/LightGBM_clf.pkl
models/_backup_20251107_162055/LightGBM_clf.version.json
models/_backup_20251107_162055/PROMOTE.json
models/_backup_20251107_162055/VERSION.txt
models/_backup_20251107_163007
models/_backup_20251107_163007/active_model.json
models/_backup_20251107_163007/backup_1762331199.pkl
models/_backup_20251107_163007/backup_1762333581.pkl
models/_backup_20251107_163007/backup_1762383513.pkl
models/_backup_20251107_163007/backup_1762417953.pkl
models/_backup_20251107_163007/calib_isotonic.pkl
models/_backup_20251107_163007/calib_platt.pkl
models/_backup_20251107_163007/LightGBM_clf.classes.json
models/_backup_20251107_163007/LightGBM_clf.features.json
models/_backup_20251107_163007/LightGBM_clf.pkl
models/_backup_20251107_163007/LightGBM_clf.version.json
models/_backup_20251107_163007/PROMOTE.json
models/_backup_20251107_163007/VERSION.txt
models/_staging
models/_staging/calib_isotonic.pkl
models/_staging/calib_platt.pkl
models/_staging/LightGBM_clf.classes.json
models/_staging/LightGBM_clf.features.json
models/_staging/LightGBM_clf.pkl
models/_staging/PROMOTE.json
models/_staging/VERSION.txt
models/active_model.json
models/backup_1762331199.pkl
models/backup_1762333581.pkl
models/backup_1762383513.pkl
models/backup_1762417953.pkl
models/calib_isotonic.pkl
models/calib_platt.pkl
models/LightGBM_clf.classes.json
models/LightGBM_clf.features.json
models/LightGBM_clf.pkl
models/LightGBM_clf.version.json
models/PROMOTE.json
models/VERSION.txt
models_store
notebooks
PROJECT_MAP.md
project_tree.txt
README.md
requirements.txt
runtime
runtime/metrics.json
runtime/metrics_8iecfllz.json
scripts
scripts/cb_smoke.py
scripts/diagnose_symbol.py
scripts/dryrun_smoke.py
scripts/export_mt5_history.py
scripts/export_val_probs.py
scripts/make_toy_model.py
scripts/print_runtime.py
scripts/promote_model.py
scripts/register_weekly_task.ps1
scripts/rollback_model.py
scripts/selftest_order_flow.py
scripts/sim_trailing.py
scripts/swap_model.py
scripts/train_calibrator.py
scripts/verify_smoke.ps1
scripts/walkforward_retrain.py
scripts/walkforward_train.py
scripts/weekly_wf.ps1
tests
To Doリスト.txt
tools
tools/backtest_equity_curve.py
tools/dump_feature_importance.py
tools/export_tree_clean.ps1
tools/inspect_report.py
フェーズA~Hまでの結果.txt
やるべきこと.txt
作業10.txt
作業6～8.txt
作業9.txt
仕様の統合まとめ.docx



=== file: pyproject.toml ===

[tool.ruff]
target-version = "py313"
line-length = 88

[tool.ruff.lint]
select = ["E", "F", "I", "UP", "ARG", "PT", "PL"]
ignore = ["E501"]



=== file: pytest.ini ===

[pytest]
minversion = 8.0
addopts = -q -ra --maxfail=1 --durations=10
testpaths =
    tests



=== file: README.md ===

﻿# FX AI Bot (Python + MT5 + PyQt6)
Python 3.13 / PyQt6 / MetaTrader5



=== file: requirements.txt ===

﻿MetaTrader5
PyQt6
pyqtgraph
matplotlib
numpy
pandas
scikit-learn
joblib
lightgbm
xgboost
shap
pydantic
pyyaml
python-dotenv
loguru



=== file: runtime/metrics.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100885,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.70719,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":5,"count_skip":0,"count_blocked":2,"ts":1763172594}


=== file: runtime/metrics_044ho80o.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0002159,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":24.25421,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1763017407}


=== file: runtime/metrics_04i6g_bm.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00070439,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.05378,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":3,"count_skip":0,"count_blocked":2,"ts":1763017422}


=== file: runtime/metrics_08lb8guj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.23212,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":158,"count_skip":0,"count_blocked":2,"ts":1763018216}


=== file: runtime/metrics_0fdsym9v.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00171831,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.10776,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":67,"count_skip":0,"count_blocked":2,"ts":1763017750}


=== file: runtime/metrics_0ks5iv74.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00101968,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.34686,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":58,"count_skip":0,"count_blocked":2,"ts":1763017704}


=== file: runtime/metrics_0m3pzxsa.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00106303,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.68062,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":39,"count_skip":0,"count_blocked":2,"ts":1763017606}


=== file: runtime/metrics_0s0y1mq_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.51373,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":101,"count_skip":0,"count_blocked":2,"ts":1763017924}


=== file: runtime/metrics_0so3t4e4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.0778,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":63,"count_skip":0,"count_blocked":2,"ts":1763017729}


=== file: runtime/metrics_0t6las4q.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00145723,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.05805,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":160,"count_skip":0,"count_blocked":2,"ts":1763018226}


=== file: runtime/metrics_13d1atob.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.15756,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":147,"count_skip":0,"count_blocked":2,"ts":1763018159}


=== file: runtime/metrics_1_r7f8xe.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.87851,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":81,"count_skip":0,"count_blocked":2,"ts":1763017822}


=== file: runtime/metrics_1ce8in0z.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00151429,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.66906,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":66,"count_skip":0,"count_blocked":2,"ts":1763017745}


=== file: runtime/metrics_1f8wcifb.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.96455,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":79,"count_skip":0,"count_blocked":2,"ts":1763017811}


=== file: runtime/metrics_1nqq6c9g.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.14061,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":120,"count_skip":0,"count_blocked":2,"ts":1763018021}


=== file: runtime/metrics_1y_j88c6.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.74186,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":26,"count_skip":0,"count_blocked":2,"ts":1763017540}


=== file: runtime/metrics_2b5irl55.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00135831,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.30994,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":151,"count_skip":0,"count_blocked":2,"ts":1763018180}


=== file: runtime/metrics_2llsmgld.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.56017,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":44,"count_skip":0,"count_blocked":2,"ts":1763017632}


=== file: runtime/metrics_2wh_r543.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00147993,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.35101,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":47,"count_skip":0,"count_blocked":2,"ts":1763017647}


=== file: runtime/metrics_32ej_3qs.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00133374,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.39766,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":38,"count_skip":0,"count_blocked":2,"ts":1763017601}


=== file: runtime/metrics_384xt0qj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.69095,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":54,"count_skip":0,"count_blocked":2,"ts":1763017683}


=== file: runtime/metrics_3_z7j5dq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.5398,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":91,"count_skip":0,"count_blocked":2,"ts":1763017873}


=== file: runtime/metrics_3op0smhm.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.10852,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":118,"count_skip":0,"count_blocked":2,"ts":1763018011}


=== file: runtime/metrics_3pck01qi.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":4,"ts":1762671546}


=== file: runtime/metrics_3pw7kppo.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762671540}


=== file: runtime/metrics_3w4e28bl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.34986,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":144,"count_skip":0,"count_blocked":2,"ts":1763018144}


=== file: runtime/metrics_3wwdl7sf.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.40584,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":156,"count_skip":0,"count_blocked":2,"ts":1763018206}


=== file: runtime/metrics_3xjcntun.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":3,"ts":1762671543}


=== file: runtime/metrics_4fx6u97k.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.37357,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":134,"count_skip":0,"count_blocked":2,"ts":1763018093}


=== file: runtime/metrics_4jkcw838.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.65645,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":157,"count_skip":0,"count_blocked":2,"ts":1763018211}


=== file: runtime/metrics_4jwlaw79.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.99009,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":32,"count_skip":0,"count_blocked":2,"ts":1763017571}


=== file: runtime/metrics_4sqd0ccq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.57861,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":89,"count_skip":0,"count_blocked":2,"ts":1763017862}


=== file: runtime/metrics_4u83ovxn.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100885,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.70719,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":5,"count_skip":0,"count_blocked":2,"ts":1763172594}


=== file: runtime/metrics_51fzibz0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00038179,"atr_gate_state":"open","post_fill_grace":false,"spread":0.5,"adx":27.35757,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":1,"count_skip":0,"count_blocked":2,"ts":1763172574}


=== file: runtime/metrics_56tbpp98.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.37376,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":90,"count_skip":0,"count_blocked":2,"ts":1763017868}


=== file: runtime/metrics_5_uus6ny.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.42191,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":61,"count_skip":0,"count_blocked":2,"ts":1763017719}


=== file: runtime/metrics_5c1keku_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.10247,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":71,"count_skip":0,"count_blocked":2,"ts":1763017770}


=== file: runtime/metrics_681i8dz0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00054309,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.10884,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":2,"count_skip":0,"count_blocked":2,"ts":1763172579}


=== file: runtime/metrics_6g2ygykl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00153604,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.61205,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":56,"count_skip":0,"count_blocked":2,"ts":1763017693}


=== file: runtime/metrics_6jx0f61y.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00120359,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.95706,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":159,"count_skip":0,"count_blocked":2,"ts":1763018221}


=== file: runtime/metrics_6mv45wka.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.8547,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":62,"count_skip":0,"count_blocked":2,"ts":1763017724}


=== file: runtime/metrics_6n99dteb.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00176191,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.41532,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":36,"count_skip":0,"count_blocked":2,"ts":1763017591}


=== file: runtime/metrics_6oizo1ba.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00158023,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.98339,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":104,"count_skip":0,"count_blocked":2,"ts":1763017939}


=== file: runtime/metrics_6u8fjy4e.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.31557,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":100,"count_skip":0,"count_blocked":2,"ts":1763017919}


=== file: runtime/metrics_751tqi7x.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0002159,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":24.25421,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1763172569}


=== file: runtime/metrics_78ji6p4u.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00125571,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.0775,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":76,"count_skip":0,"count_blocked":2,"ts":1763017796}


=== file: runtime/metrics_7bbmsyvw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00153035,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.10197,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":9,"count_skip":0,"count_blocked":2,"ts":1763017453}


=== file: runtime/metrics_7caerxlw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.55136,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":128,"count_skip":0,"count_blocked":2,"ts":1763018062}


=== file: runtime/metrics_7pcppci4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00167187,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.75221,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":161,"count_skip":0,"count_blocked":2,"ts":1763018231}


=== file: runtime/metrics_7upf8_is.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.05466,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":45,"count_skip":0,"count_blocked":2,"ts":1763017637}


=== file: runtime/metrics_7wtig7zq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0014214,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":38.18061,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":141,"count_skip":0,"count_blocked":2,"ts":1763018129}


=== file: runtime/metrics_7yx4f9p_.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762729397}


=== file: runtime/metrics_8i0ku3__.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":5e-05,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":21.32509,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1763172564}


=== file: runtime/metrics_8iecfllz.json ===

{"last_decision":"TEST","adx":12.3,"min_adx":20,"ts":1762510822}


=== file: runtime/metrics_8s427o_h.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.73688,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":42,"count_skip":0,"count_blocked":2,"ts":1763017622}


=== file: runtime/metrics_8t5wcv0v.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.43208,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":129,"count_skip":0,"count_blocked":2,"ts":1763018067}


=== file: runtime/metrics_8wbixgeh.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.78984,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":70,"count_skip":0,"count_blocked":2,"ts":1763017765}


=== file: runtime/metrics_9clf0tik.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00165111,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.72461,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":112,"count_skip":0,"count_blocked":2,"ts":1763017980}


=== file: runtime/metrics_9wqc4wjf.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.30769,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":138,"count_skip":0,"count_blocked":2,"ts":1763018113}


=== file: runtime/metrics_9y2xjt1a.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":7,"ts":1762729427}


=== file: runtime/metrics__7n81rbm.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":3,"ts":1762729403}


=== file: runtime/metrics__9tk6o8d.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.49746,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":127,"count_skip":0,"count_blocked":2,"ts":1763018057}


=== file: runtime/metrics___1o6ki2.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.37609,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":33,"count_skip":0,"count_blocked":2,"ts":1763017576}


=== file: runtime/metrics__cx1yvq5.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00162085,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.46151,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":131,"count_skip":0,"count_blocked":2,"ts":1763018078}


=== file: runtime/metrics__g0tzwco.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.46993,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":23,"count_skip":0,"count_blocked":2,"ts":1763017524}


=== file: runtime/metrics__g6j76yc.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00129511,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.0365,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":57,"count_skip":0,"count_blocked":2,"ts":1763017699}


=== file: runtime/metrics__l9umv1h.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180397,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.21146,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":162,"count_skip":0,"count_blocked":2,"ts":1763018236}


=== file: runtime/metrics__nrd_y1k.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00138471,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.46285,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":122,"count_skip":0,"count_blocked":2,"ts":1763018031}


=== file: runtime/metrics__rjbfcpd.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.19516,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":80,"count_skip":0,"count_blocked":2,"ts":1763017816}


=== file: runtime/metrics__s4hfi2b.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00113303,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.14893,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":133,"count_skip":0,"count_blocked":2,"ts":1763018088}


=== file: runtime/metrics__tv_40i0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.20587,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":64,"count_skip":0,"count_blocked":2,"ts":1763017734}


=== file: runtime/metrics__vtagxy_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.38751,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":34,"count_skip":0,"count_blocked":2,"ts":1763017581}


=== file: runtime/metrics_a4qsce4c.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.85332,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":14,"count_skip":0,"count_blocked":2,"ts":1763017478}


=== file: runtime/metrics_a_jw5ekj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.15562,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":35,"count_skip":0,"count_blocked":2,"ts":1763017586}


=== file: runtime/metrics_abcuwh_8.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":5,"ts":1762729415}


=== file: runtime/metrics_aen0sw0t.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00143202,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.52959,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":113,"count_skip":0,"count_blocked":2,"ts":1763017985}


=== file: runtime/metrics_avlwot5z.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00168037,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.43589,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":93,"count_skip":0,"count_blocked":2,"ts":1763017883}


=== file: runtime/metrics_b_e39daa.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.08087,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":116,"count_skip":0,"count_blocked":2,"ts":1763018001}


=== file: runtime/metrics_beh8fq83.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.85148,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":16,"count_skip":0,"count_blocked":2,"ts":1763017489}


=== file: runtime/metrics_bkm4j_2_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0012697,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.46397,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":65,"count_skip":0,"count_blocked":2,"ts":1763017740}


=== file: runtime/metrics_bmocgusp.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00161176,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.48796,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":123,"count_skip":0,"count_blocked":2,"ts":1763018037}


=== file: runtime/metrics_bsejl6ul.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.67972,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":82,"count_skip":0,"count_blocked":2,"ts":1763017827}


=== file: runtime/metrics_c19f7jhr.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0011478,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.53105,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":6,"count_skip":0,"count_blocked":2,"ts":1763017437}


=== file: runtime/metrics_c6dlu7j3.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.6403,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":60,"count_skip":0,"count_blocked":2,"ts":1763017714}


=== file: runtime/metrics_c7srgc6l.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100885,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.70719,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":5,"count_skip":0,"count_blocked":2,"ts":1763017432}


=== file: runtime/metrics_cfvm2vuv.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00134718,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.03312,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":103,"count_skip":0,"count_blocked":2,"ts":1763017934}


=== file: runtime/metrics_cs6cyrr4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.75231,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":115,"count_skip":0,"count_blocked":2,"ts":1763017996}


=== file: runtime/metrics_ctdk6hr_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.70205,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":119,"count_skip":0,"count_blocked":2,"ts":1763018016}


=== file: runtime/metrics_d0zio68n.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.05924,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":139,"count_skip":0,"count_blocked":2,"ts":1763018118}


=== file: runtime/metrics_d2fneb6c.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.45621,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":153,"count_skip":0,"count_blocked":2,"ts":1763018190}


=== file: runtime/metrics_d83v0qrg.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.51296,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":88,"count_skip":0,"count_blocked":2,"ts":1763017857}


=== file: runtime/metrics_d_7cu4co.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00150227,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":23.88217,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":75,"count_skip":0,"count_blocked":2,"ts":1763017791}


=== file: runtime/metrics_e55ccqf4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00146759,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.57701,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":94,"count_skip":0,"count_blocked":2,"ts":1763017888}


=== file: runtime/metrics_efewdoso.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.1878,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":108,"count_skip":0,"count_blocked":2,"ts":1763017960}


=== file: runtime/metrics_es10kbkj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.1606,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":126,"count_skip":0,"count_blocked":2,"ts":1763018052}


=== file: runtime/metrics_exyc2aoi.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00107818,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.16007,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":102,"count_skip":0,"count_blocked":2,"ts":1763017929}


=== file: runtime/metrics_f4ba8yj0.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.3132,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":49,"count_skip":0,"count_blocked":2,"ts":1763017658}


=== file: runtime/metrics_fusrtchw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.3179,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":107,"count_skip":0,"count_blocked":2,"ts":1763017955}


=== file: runtime/metrics_fvkjx3i3.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00179554,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.45145,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":124,"count_skip":0,"count_blocked":2,"ts":1763018042}


=== file: runtime/metrics_fwgrd7ek.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100873,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.50361,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":117,"count_skip":0,"count_blocked":2,"ts":1763018006}


=== file: runtime/metrics_fx59gkw6.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.61258,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":125,"count_skip":0,"count_blocked":2,"ts":1763018047}


=== file: runtime/metrics_gctnecas.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00177087,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.98795,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":105,"count_skip":0,"count_blocked":2,"ts":1763017944}


=== file: runtime/metrics_gdrsfom2.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.09115,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":111,"count_skip":0,"count_blocked":2,"ts":1763017975}


=== file: runtime/metrics_gh0exqfy.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.59901,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":12,"count_skip":0,"count_blocked":2,"ts":1763017468}


=== file: runtime/metrics_gn3r368k.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":4,"ts":1762729409}


=== file: runtime/metrics_gre0l2vv.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":5e-05,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.5,"adx":21.32509,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1763017402}


=== file: runtime/metrics_gu2jj4x6.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762671193}


=== file: runtime/metrics_gy7yzni_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00085662,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.21134,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":4,"count_skip":0,"count_blocked":2,"ts":1763172589}


=== file: runtime/metrics_h2ke1que.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":29.78668,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":96,"count_skip":0,"count_blocked":2,"ts":1763017898}


=== file: runtime/metrics_h2zbzsi6.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00164232,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.56873,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":142,"count_skip":0,"count_blocked":2,"ts":1763018134}


=== file: runtime/metrics_i5pwbdy8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00137155,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.6944,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":19,"count_skip":0,"count_blocked":2,"ts":1763017504}


=== file: runtime/metrics_i9ixee1p.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.63559,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":24,"count_skip":0,"count_blocked":2,"ts":1763017530}


=== file: runtime/metrics_iegm05ma.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.34749,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":43,"count_skip":0,"count_blocked":2,"ts":1763017627}


=== file: runtime/metrics_jeul60z4.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00154773,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.27046,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":85,"count_skip":0,"count_blocked":2,"ts":1763017842}


=== file: runtime/metrics_jldkp6oj.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":8,"ts":1762729433}


=== file: runtime/metrics_joyqy1yz.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.12183,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":73,"count_skip":0,"count_blocked":2,"ts":1763017781}


=== file: runtime/metrics_k452r_bd.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762671537}


=== file: runtime/metrics_keyuq093.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.27232,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":15,"count_skip":0,"count_blocked":2,"ts":1763017483}


=== file: runtime/metrics_kijozz54.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.82456,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":87,"count_skip":0,"count_blocked":2,"ts":1763017852}


=== file: runtime/metrics_l0nrt8ax.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.27612,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":21,"count_skip":0,"count_blocked":2,"ts":1763017514}


=== file: runtime/metrics_l4qkqhvl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018044,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.88215,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":92,"count_skip":0,"count_blocked":2,"ts":1763017878}


=== file: runtime/metrics_l9a3g7f_.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762729391}


=== file: runtime/metrics_lg3z_ojj.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00140854,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.97584,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":8,"count_skip":0,"count_blocked":2,"ts":1763017448}


=== file: runtime/metrics_lij48idf.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.69698,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":154,"count_skip":0,"count_blocked":2,"ts":1763018195}


=== file: runtime/metrics_mfx0u92s.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.5493,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":30,"count_skip":0,"count_blocked":2,"ts":1763017560}


=== file: runtime/metrics_ml129ig1.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.15362,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":51,"count_skip":0,"count_blocked":2,"ts":1763017668}


=== file: runtime/metrics_mziwmuji.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00166155,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.83636,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":29,"count_skip":0,"count_blocked":2,"ts":1763017555}


=== file: runtime/metrics_mzkihnhl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180334,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.02933,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":31,"count_skip":0,"count_blocked":2,"ts":1763017565}


=== file: runtime/metrics_n92kltdx.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.83382,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":97,"count_skip":0,"count_blocked":2,"ts":1763017903}


=== file: runtime/metrics_naa4pb8_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.83045,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":69,"count_skip":0,"count_blocked":2,"ts":1763017760}


=== file: runtime/metrics_o0bwp99t.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.67833,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":53,"count_skip":0,"count_blocked":2,"ts":1763017678}


=== file: runtime/metrics_oe0gx0ow.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00144468,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.68122,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":28,"count_skip":0,"count_blocked":2,"ts":1763017550}


=== file: runtime/metrics_og6_x57t.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.45723,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":52,"count_skip":0,"count_blocked":2,"ts":1763017673}


=== file: runtime/metrics_okatezba.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.33992,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":72,"count_skip":0,"count_blocked":2,"ts":1763017775}


=== file: runtime/metrics_olwp427j.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.07399,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":146,"count_skip":0,"count_blocked":2,"ts":1763018154}


=== file: runtime/metrics_oy5jty8q.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00038179,"atr_gate_state":"open","post_fill_grace":false,"spread":0.5,"adx":27.35757,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":1,"count_skip":0,"count_blocked":2,"ts":1763017412}


=== file: runtime/metrics_pbfm5_s8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.85726,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":106,"count_skip":0,"count_blocked":2,"ts":1763017949}


=== file: runtime/metrics_pguw1jt8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00156887,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.49224,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":37,"count_skip":0,"count_blocked":2,"ts":1763017596}


=== file: runtime/metrics_q1mfycvq.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180514,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.36631,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":68,"count_skip":0,"count_blocked":2,"ts":1763017755}


=== file: runtime/metrics_q32vexb9.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00054309,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.10884,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":2,"count_skip":0,"count_blocked":2,"ts":1763017417}


=== file: runtime/metrics_qd9sdimy.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.28033,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":98,"count_skip":0,"count_blocked":2,"ts":1763017909}


=== file: runtime/metrics_qmjxkoxo.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.12556,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":25,"count_skip":0,"count_blocked":2,"ts":1763017535}


=== file: runtime/metrics_qygbv_em.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00128674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.18961,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":7,"count_skip":0,"count_blocked":2,"ts":1763017442}


=== file: runtime/metrics_r1diayz3.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762671779}


=== file: runtime/metrics_r1oyukcw.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017996,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":24.51121,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":130,"count_skip":0,"count_blocked":2,"ts":1763018072}


=== file: runtime/metrics_r2iljktq.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":2,"ts":1762671785}


=== file: runtime/metrics_r4cxpbus.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.72554,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":78,"count_skip":0,"count_blocked":2,"ts":1763017806}


=== file: runtime/metrics_ri5kzye8.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100614,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.47894,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":59,"count_skip":0,"count_blocked":2,"ts":1763017709}


=== file: runtime/metrics_s_yeu75a.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.3389,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":13,"count_skip":0,"count_blocked":2,"ts":1763017473}


=== file: runtime/metrics_smrtxq4q.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00158961,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.15592,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":150,"count_skip":0,"count_blocked":2,"ts":1763018175}


=== file: runtime/metrics_sunrm50l.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.88573,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":109,"count_skip":0,"count_blocked":2,"ts":1763017965}


=== file: runtime/metrics_szni51cl.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00173578,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":23.25067,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":55,"count_skip":0,"count_blocked":2,"ts":1763017688}


=== file: runtime/metrics_szy5o819.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.35016,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":41,"count_skip":0,"count_blocked":2,"ts":1763017617}


=== file: runtime/metrics_t09rkk69.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00109075,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.86342,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":152,"count_skip":0,"count_blocked":2,"ts":1763018185}


=== file: runtime/metrics_tnyuui1p.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.37394,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":148,"count_skip":0,"count_blocked":2,"ts":1763018165}


=== file: runtime/metrics_tvk5w34y.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00169045,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.05573,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":48,"count_skip":0,"count_blocked":2,"ts":1763017652}


=== file: runtime/metrics_u85k0bty.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00163165,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.93238,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":10,"count_skip":0,"count_blocked":2,"ts":1763017458}


=== file: runtime/metrics_ue987r_m.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.6835,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":145,"count_skip":0,"count_blocked":2,"ts":1763018149}


=== file: runtime/metrics_ugsv8j1h.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100855,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":31.49403,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":22,"count_skip":0,"count_blocked":2,"ts":1763017519}


=== file: runtime/metrics_uhoep47z.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00178696,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.24436,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":17,"count_skip":0,"count_blocked":2,"ts":1763017494}


=== file: runtime/metrics_uiqkfigp.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00174512,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.47869,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":86,"count_skip":0,"count_blocked":2,"ts":1763017847}


=== file: runtime/metrics_ujl1qfgc.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00118915,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":38.29087,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":27,"count_skip":0,"count_blocked":2,"ts":1763017545}


=== file: runtime/metrics_v30su220.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0018018,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.96378,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":143,"count_skip":0,"count_blocked":2,"ts":1763018139}


=== file: runtime/metrics_v372j6bp.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100674,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.0253,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":77,"count_skip":0,"count_blocked":2,"ts":1763017801}


=== file: runtime/metrics_vd8t6g2k.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00121554,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":26.12527,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":95,"count_skip":0,"count_blocked":2,"ts":1763017893}


=== file: runtime/metrics_vi859rsu.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00116248,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.62738,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":140,"count_skip":0,"count_blocked":2,"ts":1763018124}


=== file: runtime/metrics_vlx06rwe.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00160074,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":21.77141,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":18,"count_skip":0,"count_blocked":2,"ts":1763017499}


=== file: runtime/metrics_vm46jhw3.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":6,"ts":1762729421}


=== file: runtime/metrics_vzimyuwy.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180255,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.20493,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":110,"count_skip":0,"count_blocked":2,"ts":1763017970}


=== file: runtime/metrics_w88v4g45.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00130883,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.77716,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":84,"count_skip":0,"count_blocked":2,"ts":1763017837}


=== file: runtime/metrics_wcsasy33.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00177824,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":25.41911,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":149,"count_skip":0,"count_blocked":2,"ts":1763018170}


=== file: runtime/metrics_x7rwrpb_.json ===

{"last_decision":"BLOCKED","last_reason":"atr_low","atr_ref":0.0,"atr_gate_state":"closed","post_fill_grace":false,"spread":0.0,"adx":20.0,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":0,"count_skip":0,"count_blocked":1,"ts":1762671190}


=== file: runtime/metrics_xbjh179p.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00070439,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.05378,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":3,"count_skip":0,"count_blocked":2,"ts":1763172584}


=== file: runtime/metrics_xcr4m91b.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0017086,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":22.91885,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":74,"count_skip":0,"count_blocked":2,"ts":1763017786}


=== file: runtime/metrics_xcyd97h5.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180479,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.86567,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":50,"count_skip":0,"count_blocked":2,"ts":1763017663}


=== file: runtime/metrics_xe7x_sje.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00180077,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.10578,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":11,"count_skip":0,"count_blocked":2,"ts":1763017463}


=== file: runtime/metrics_xjeelk50.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.81588,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":137,"count_skip":0,"count_blocked":2,"ts":1763018108}


=== file: runtime/metrics_xqnkdh58.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00103505,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":35.76621,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":83,"count_skip":0,"count_blocked":2,"ts":1763017832}


=== file: runtime/metrics_xuw93ulk.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00110574,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":28.39791,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":20,"count_skip":0,"count_blocked":2,"ts":1763017509}


=== file: runtime/metrics_xxx7__b9.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100764,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":30.35727,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":40,"count_skip":0,"count_blocked":2,"ts":1763017611}


=== file: runtime/metrics_y1mt7hlm.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00139558,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":23.96779,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":132,"count_skip":0,"count_blocked":2,"ts":1763018083}


=== file: runtime/metrics_y5_m62os.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.63395,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":136,"count_skip":0,"count_blocked":2,"ts":1763018103}


=== file: runtime/metrics_yehnyu4c.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0012298,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":36.9624,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":46,"count_skip":0,"count_blocked":2,"ts":1763017642}


=== file: runtime/metrics_yuncl873.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100882,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":32.11364,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":135,"count_skip":0,"count_blocked":2,"ts":1763018098}


=== file: runtime/metrics_yv04iu3_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00100804,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.24244,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":99,"count_skip":0,"count_blocked":2,"ts":1763017914}


=== file: runtime/metrics_z4d75mrh.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.0010083,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":34.42979,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":155,"count_skip":0,"count_blocked":2,"ts":1763018200}


=== file: runtime/metrics_zn9wzer_.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00085662,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":33.21134,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":4,"count_skip":0,"count_blocked":2,"ts":1763017427}


=== file: runtime/metrics_zrz0v00o.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00112066,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":37.19923,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":121,"count_skip":0,"count_blocked":2,"ts":1763018026}


=== file: runtime/metrics_zskuik5i.json ===

{"last_decision":"ENTRY","last_reason":"entry_ok","atr_ref":0.00117464,"atr_gate_state":"open","post_fill_grace":true,"spread":0.5,"adx":27.23949,"min_adx":15.0,"prob_threshold":0.21489285306991637,"min_atr_pct":0.0002,"count_entry":114,"count_skip":0,"count_blocked":2,"ts":1763017990}


=== file: runtime/trade_state.json ===

{"trading_enabled": true}



=== file: scripts/cb_smoke.py ===

from __future__ import annotations

from typing import Any, Optional

import importlib


def main() -> None:
    cb_mod = importlib.import_module("app.services.circuit_breaker")
    scan_and_update = getattr(cb_mod, "scan_and_update", None)
    status = getattr(cb_mod, "status", None)

    if callable(scan_and_update):
        scan_and_update()
    if callable(status):
        s: Optional[dict[str, Any]] = status()
        print(s)
    else:
        print({"circuit": "unknown"})


if __name__ == "__main__":
    main()



=== file: scripts/diagnose_symbol.py ===

# scripts/diagnose_symbol.py
import MetaTrader5 as mt5


def main() -> None:
    if not mt5.initialize():
        print("MT5 init failed:", mt5.last_error())
        return
    try:
        # USDJPYで始まる全候補を列挙
        cands = mt5.symbols_get("USDJPY*")
        print("Candidates:", len(cands))
        for s in cands:
            print(f"- {s.name}  (select={s.select}, bid={s.bid}, ask={s.ask}, point={s.point})")
    finally:
        mt5.shutdown()


if __name__ == "__main__":
    main()



=== file: scripts/dryrun_smoke.py ===

# scripts/dryrun_smoke.py
from __future__ import annotations

import argparse
import math
import random
import sys
import time
from pathlib import Path
from typing import Dict

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from loguru import logger

from app.core import logger as app_logger
from app.core.config_loader import load_config
from app.services import circuit_breaker, trade_state
from app.services.execution_stub import ExecutionStub, reset_atr_gate_state
from core.ai.service import AISvc
from core.utils.hashing import hash_features
from core.utils.timeutil import now_jst_iso


def parse_args(argv: list[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Dry-run smoke simulator")
    parser.add_argument("--sim", action="store_true", help="Use synthetic tick stream")
    parser.add_argument("--atr-open", action="store_true", help="Force ATR gate open")
    parser.add_argument("--n", type=int, default=200, help="Number of ticks to simulate")
    parser.add_argument("--dt", type=int, default=50, help="Tick interval in milliseconds")
    parser.add_argument("--base", type=float, default=150.20, help="Base mid price")
    parser.add_argument("--spread", type=float, default=0.5, help="Spread in pips")
    parser.add_argument("--atrpct", type=float, default=0.0005, help="ATR percentage baseline")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument("--symbol", type=str, default=None, help="Override runtime symbol")
    return parser.parse_args(argv)


def _build_features(base_price: float, spread_pips: float, tick_idx: int, rng: random.Random) -> Dict[str, float]:
    drift = math.sin(tick_idx / 6.0)
    noise = rng.uniform(-0.02, 0.02)
    ema_5 = base_price * (1 + drift * 0.002) + noise
    ema_20 = base_price * (1 - drift * 0.001) - noise
    rsi_14 = 60.0 + drift * 25.0 + rng.uniform(-3, 3)
    atr_14 = abs(drift) * 0.002 + spread_pips * 0.0001
    adx_14 = 22.0 + abs(drift) * 15.0 + rng.uniform(-1.5, 1.5)
    bbp = 0.5 + drift * 0.35 + rng.uniform(-0.05, 0.05)
    vol_chg = drift * 0.08 + rng.uniform(-0.02, 0.02)
    wick_ratio = 0.5 + drift * 0.3 + rng.uniform(-0.05, 0.05)
    return {
        "ema_5": float(ema_5),
        "ema_20": float(ema_20),
        "rsi_14": float(max(0.0, min(100.0, rsi_14))),
        "atr_14": float(abs(atr_14)),
        "adx_14": float(max(5.0, adx_14)),
        "bbp": float(max(0.0, min(1.0, bbp))),
        "vol_chg": float(vol_chg),
        "wick_ratio": float(max(0.0, min(1.0, wick_ratio))),
    }


def _ensure_config_overrides(cfg: dict, args: argparse.Namespace) -> None:
    filters_cfg = cfg.setdefault("filters", {})
    hy = filters_cfg.setdefault("atr_hysteresis", {})
    if args.atr_open:
        filters_cfg["min_atr_pct"] = 0.0
        hy["enable_min_pct"] = 0.0
        hy["disable_min_pct"] = 0.0


def prepare_state(cfg: dict, args: argparse.Namespace) -> ExecutionStub:
    runtime_cfg = cfg.get("runtime", {})
    entry_cfg = cfg.get("entry", {})
    prob_threshold = float(entry_cfg.get("prob_threshold", entry_cfg.get("threshold_buy", 0.60)))
    trade_state.update(
        trading_enabled=True,
        threshold_buy=float(entry_cfg.get("threshold_buy", prob_threshold)),
        threshold_sell=float(entry_cfg.get("threshold_sell", prob_threshold)),
        prob_threshold=prob_threshold,
        side_bias=str(entry_cfg.get("side_bias", "auto") or "auto"),
    )

    cb_cfg = cfg.get("circuit_breaker", {}) if isinstance(cfg, dict) else {}
    cb = circuit_breaker.CircuitBreaker(
        max_consecutive_losses=int(cb_cfg.get("max_consecutive_losses", cfg.get("risk", {}).get("max_consecutive_losses", 5))),
        daily_loss_limit_jpy=float(cb_cfg.get("daily_loss_limit_jpy", 0.0)),
        cooldown_min=int(cb_cfg.get("cooldown_min", 30)),
    )
    ai = AISvc(threshold=prob_threshold)
    try:
        reset_atr_gate_state()
    except Exception:
        pass
    return ExecutionStub(cb=cb, ai=ai)


def main(argv: list[str]) -> None:
    app_logger.setup()
    args = parse_args(argv)
    if not args.sim:
        args.sim = True  # default to simulation for smoke test

    rng = random.Random(args.seed)
    cfg = load_config()
    _ensure_config_overrides(cfg, args)

    import core.config as core_config

    core_config.cfg = cfg
    stub = prepare_state(cfg, args)

    runtime_cfg = cfg.get("runtime", {})
    symbol = args.symbol or runtime_cfg.get("symbol", "USDJPY")

    spread_limit = float(runtime_cfg.get("spread_limit_pips", runtime_cfg.get("spread_limit", 1.5)))
    max_positions = int(runtime_cfg.get("max_positions", 1))

    trail_logged = False

    for idx in range(args.n):
        features = _build_features(args.base, args.spread, idx, rng)
        runtime_payload = {
            "spread_pips": float(args.spread),
            "spread_limit_pips": spread_limit,
            "max_positions": max_positions,
            "open_positions": 0,
            "ai_threshold": stub.ai.threshold,
            "min_atr_pct": cfg.get("filters", {}).get("min_atr_pct", 0.0),
            "filters": cfg.get("filters", {}),
        }
        result = stub.on_tick(symbol, features, runtime_payload)
        if not trail_logged:
            logger.info("[TRAIL][DRYRUN] smoke trail ping features_hash={}", hash_features(features))
            trail_logged = True
        if args.dt > 0:
            time.sleep(min(args.dt / 1000.0, 0.1))

    logger.info(
        "[SMOKE] completed n={} dt_ms={} symbol={} time={}",
        args.n,
        args.dt,
        symbol,
        now_jst_iso(),
    )


if __name__ == "__main__":
    main(sys.argv[1:])



=== file: scripts/export_mt5_history.py ===

# --- project root on sys.path ---
import os, sys
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)
# --------------------------------

import MetaTrader5 as mt5
import pandas as pd
from datetime import datetime, timedelta
from typing import NoReturn

SYMBOL = os.environ.get("FXBOT_SYMBOL", "USDJPY-")
TIMEFRAME = mt5.TIMEFRAME_M5

# 端末パスを明示したい場合は環境変数 FXBOT_MT5_TERMINAL を使う
# 例: setx FXBOT_MT5_TERMINAL "C:\Program Files\MetaTrader 5\terminal64.exe"
TERM_PATH = os.environ.get("FXBOT_MT5_TERMINAL")

def die(msg: str) -> NoReturn:
    print(msg)
    mt5.shutdown()
    raise SystemExit(1)

def ensure_init() -> None:
    ok = mt5.initialize() if not TERM_PATH else mt5.initialize(path=TERM_PATH)
    if not ok:
        die(f"MT5 initialize() failed: last_error={mt5.last_error()} term_path={TERM_PATH!r}")
    ver = mt5.version()
    print(f"MT5 initialized. version={ver} term_path={TERM_PATH!r}")

def ensure_logged_in() -> None:
    ai = mt5.account_info()
    if ai is None:
        die(f"Not logged in or terminal not ready. last_error={mt5.last_error()}")
    print(f"Account: {ai.login} / {ai.server}")

def ensure_symbol(symbol: str) -> None:
    info = mt5.symbol_info(symbol)
    if info is None:
        die(f"symbol_info({symbol}) is None. last_error={mt5.last_error()}")
    if not info.visible:
        if not mt5.symbol_select(symbol, True):
            die(f"symbol_select({symbol}) failed. last_error={mt5.last_error()}")
    # 試しに最新ティックも触っておく
    _ = mt5.symbol_info_tick(symbol)
    print(f"Symbol {symbol} ready (visible={mt5.symbol_info(symbol).visible})")

def try_copy_small(symbol: str, timeframe: int, count: int = 1000) -> int:
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None:
        return 0
    return len(rates)

def export_range(symbol: str, timeframe: int, days: int = 365 * 5) -> str:
    to = datetime.now()
    frm = to - timedelta(days=days)

    # まず小さく取れるか診断
    small = try_copy_small(symbol, timeframe, 1000)
    print(f"diagnostic: copy_rates_from_pos count={small}")

    # --- 安全取得モード ---
    print(f"fetching {symbol} {days}days range in chunks ...")
    chunk_days = 30   # 1か月単位で遡る
    frames = []
    cursor_to = to
    while cursor_to > frm:
        cursor_from = cursor_to - timedelta(days=chunk_days)
        rates = mt5.copy_rates_range(symbol, timeframe, cursor_from, cursor_to)
        if rates is None or len(rates) == 0:
            print(f"chunk {cursor_from.date()}~{cursor_to.date()} => no data (skip)")
        else:
            df = pd.DataFrame(rates)
            frames.append(df)
            print(f"chunk {cursor_from.date()}~{cursor_to.date()} => {len(df)} bars")
        cursor_to = cursor_from

    if not frames:
        die("no data returned even by chunked fetch. Try shorter days or different symbol/timeframe.")

    df = pd.concat(frames).drop_duplicates(subset=["time"]).sort_values("time")
    df["Date"] = pd.to_datetime(df["time"], unit="s")
    df = df.rename(columns={
        "open": "open", "high": "high", "low": "low", "close": "close",
        "tick_volume": "volume"
    })
    df = df[["Date", "open", "high", "low", "close", "volume"]]
    df["label"] = (df["close"].shift(-1) > df["close"]).map({True: "BUY", False: "SELL"})

    outdir = os.path.join("data", "usdjpy")
    os.makedirs(outdir, exist_ok=True)
    out = os.path.join(outdir, "USDJPY_M5_mt5.csv")
    df.to_csv(out, index=False)
    print(f"wrote {out} rows:{len(df)}")
    return out


def main() -> None:
    ensure_init()
    ensure_logged_in()
    ensure_symbol(SYMBOL)
    export_range(SYMBOL, TIMEFRAME, days=365*5)
    mt5.shutdown()

if __name__ == "__main__":
    main()



=== file: scripts/export_val_probs.py ===

# scripts/export_val_probs.py
from __future__ import annotations

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import atexit, math, json
from datetime import datetime, timezone, timedelta
from typing import Any

import numpy as np
import pandas as pd
import MetaTrader5 as mt5
from loguru import logger

from core.ai.loader import load_lgb_clf  # 既存ローダを利用
from pathlib import Path

# ====== 設定 ======
SYMBOL = "USDJPY"                 # ブローカー接尾辞は自動吸収します
TIMEFRAME = mt5.TIMEFRAME_M5
START = "2025-08-01 00:00:00"     # 検証開始（JST）
END   = "2025-10-01 00:00:00"     # 検証終了（JST）
BARS_MIN = 400                    # 最低必要バー数（EMA/BB等のため）
OUT_PBUY = Path("logs/val_p_buy_raw.npy")
OUT_Y    = Path("logs/val_y_true.npy")
OUT_META = Path("logs/val_export_meta.json")

# ====== MT5初期化 ======
def _ensure_symbol(symbol: str) -> str:
    if mt5.symbol_select(symbol, True):
        return symbol
    upper = symbol.upper()
    cands = [s.name for s in mt5.symbols_get() if s.name.upper().startswith(upper)]
    if not cands:
        raise RuntimeError(f"no candidates for '{symbol}'")
    best = sorted(cands, key=len)[0]
    best = str(best)
    if not mt5.symbol_select(best, True):
        raise RuntimeError(f"symbol_select failed for '{best}'")
    return best


def _read_feature_order(meta_path: str) -> list[str]:
    try:
        with open(meta_path, "r", encoding="utf-8") as fh:
            meta: dict[str, Any] = json.load(fh)
        return list(meta.get("feature_order", []))
    except Exception:
        return []

def _to_utc(jst_str: str) -> datetime:
    jst = datetime.strptime(jst_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone(timedelta(hours=9)))
    return jst.astimezone(timezone.utc)

# ====== 特徴量（dryrunと同じ定義） ======
def make_features_df(df: pd.DataFrame) -> pd.DataFrame:
    close, high, low, open_ = df["close"], df["high"], df["low"], df["open"]

    ema_5  = close.ewm(span=5, adjust=False).mean()
    ema_20 = close.ewm(span=20, adjust=False).mean()

    delta = close.diff()
    up = delta.clip(lower=0.0)
    down = -delta.clip(upper=0.0)
    roll_up = up.ewm(alpha=1/14, adjust=False).mean()
    roll_down = down.ewm(alpha=1/14, adjust=False).mean()
    rs = roll_up / roll_down.replace(0, np.nan)
    rsi_14 = (100.0 - (100.0 / (1.0 + rs))).fillna(50.0)

    tr1 = (high - low).abs()
    tr2 = (high - close.shift()).abs()
    tr3 = (low - close.shift()).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr_14_abs = tr.ewm(alpha=1/14, adjust=False).mean()
    atr_14 = (atr_14_abs / close.replace(0, np.nan)).fillna(0.0)

    plus_dm  = (high.diff()).clip(lower=0.0)
    minus_dm = (-low.diff()).clip(lower=0.0)
    plus_dm[plus_dm < minus_dm] = 0.0
    minus_dm[minus_dm <= plus_dm] = 0.0
    tr_smooth = tr.ewm(alpha=1/14, adjust=False).mean()
    plus_di = 100 * (plus_dm.ewm(alpha=1/14, adjust=False).mean() / tr_smooth.replace(0, np.nan))
    minus_di = 100 * (minus_dm.ewm(alpha=1/14, adjust=False).mean() / tr_smooth.replace(0, np.nan))
    dx = 100 * ((plus_di - minus_di).abs() / (plus_di + minus_di).replace(0, np.nan))
    adx_14 = dx.ewm(alpha=1/14, adjust=False).mean().fillna(20.0)

    bb_ma = close.rolling(20).mean()
    bb_std = close.rolling(20).std(ddof=0)
    bb_upper = bb_ma + 2 * bb_std
    bb_lower = bb_ma - 2 * bb_std
    bbp = ((close - bb_lower) / (bb_upper - bb_lower)).replace([np.inf, -np.inf], np.nan).clip(0.0, 1.0).fillna(0.5)

    # volume由来の特徴は省略（学習時に使っていれば追加）
    body_high = np.maximum(open_, close)
    body_low  = np.minimum(open_, close)
    upper_wick = (high - body_high).clip(lower=0.0)
    lower_wick = (body_low - low).clip(lower=0.0)
    rng = (high - low).replace(0, np.nan)
    wick_ratio = ((upper_wick + lower_wick) / rng).clip(0.0, 1.0).fillna(0.0)

    out = pd.DataFrame({
        "ema_5": ema_5 - ema_20,
        "ema_20": (ema_20 - close) / close.replace(0, np.nan),
        "rsi_14": rsi_14,
        "atr_14": atr_14,
        "adx_14": adx_14,
        "bbp": bbp,
        "wick_ratio": wick_ratio,
    })
    return out.replace([np.inf, -np.inf], 0.0).fillna(0.0)

def main() -> None:
    if not mt5.initialize():
        raise SystemExit(f"MT5 init failed: {mt5.last_error()}")
    atexit.register(mt5.shutdown)

    symbol = _ensure_symbol(SYMBOL)

    # 余裕を持って過去から取得
    utc_from = _to_utc(START) - timedelta(days=7)
    utc_to   = _to_utc(END)
    rates = mt5.copy_rates_range(symbol, TIMEFRAME, utc_from, utc_to)
    if rates is None or len(rates) < BARS_MIN:
        raise SystemExit(f"not enough bars: {len(rates) if rates is not None else 0}")

    df = pd.DataFrame(rates)
    df["ts"] = pd.to_datetime(df["time"], unit="s", utc=True).dt.tz_convert("Asia/Tokyo")
    df = df[ (df["ts"] >= pd.Timestamp(START, tz="Asia/Tokyo")) & (df["ts"] < pd.Timestamp(END, tz="Asia/Tokyo")) ].copy()
    df = df.reset_index(drop=True)

    feats = make_features_df(df)
    # ラベル定義（次バーの上げ下げ、同値は0とする）
    y_true = (df["close"].shift(-1) > df["close"]).astype(int)[:-1].values
    feats = feats.iloc[:-1, :].copy()
#
    # ===== モデルと列順（返り値の型差を吸収） =====
    lm = load_lgb_clf()  # LoadedModel / sklearn LGBMClassifier / lightgbm.Booster 等

    # モデル本体候補を広く探索
    candidates = [lm]
    for attr in ("model", "clf", "estimator", "inner", "wrapped", "lgbm", "booster", "booster_"):
        if hasattr(lm, attr):
            candidates.append(getattr(lm, attr))

    model = None
    for cand in candidates:
        if hasattr(cand, "predict_proba"):
            model = cand  # sklearn 互換
            use_proba = "predict_proba"
            break
        if hasattr(cand, "predict"):
            model = cand  # Booster 等（predictで確率が返る想定）
            use_proba = "predict"
            # break しないで続けてもいいが、まずはこれで採用
            break

    if model is None:
        raise TypeError("No usable model found: neither predict_proba nor predict detected.")

    # 特徴量順（存在しなければ現在列で進む）
    try:
        order = _read_feature_order("models/LightGBM_clf.features.json")
    except Exception:
        order = None
    if not order:
        order = list(feats.columns)

    # 欠け列は0で補い、余分は落とす
    for col in order:
        if col not in feats.columns:
            feats[col] = 0.0
    X = feats[order].astype(float)

    # ===== 生BUY確率の取得 =====
    if use_proba == "predict_proba":
        proba = model.predict_proba(X)  # (N,2 or 3)
    else:
        # LightGBM Booster 等: predict で確率が返る（binary は陽性確率、multiclass は (N, K)）
        # raw_score=False を渡せる場合は渡す（無くてもOK）
        try:
            proba = model.predict(X, raw_score=False)
        except TypeError:
            proba = model.predict(X)

    # shape を正規化
    proba = np.asarray(proba)
    if proba.ndim == 1:
        # binary の陽性確率が 1 列で返ったケース
        p_buy_raw = proba
    elif proba.ndim == 2:
        # (N,2) or (N,K) を想定。BUY列（陽性）を列1と仮定（必要なら調整）
        if proba.shape[1] >= 2:
            p_buy_raw = proba[:, 1]
        else:
            # よほどの特殊形状。安全側で最大スコア列をBUYとみなす
            idx = np.argmax(proba, axis=1)
            p_buy_raw = (idx == 1).astype(float)
    else:
        raise ValueError(f"Unsupported prediction output shape: {proba.shape}")

    OUT_PBUY.parent.mkdir(parents=True, exist_ok=True)
    np.save(OUT_PBUY, p_buy_raw.astype(float))
    np.save(OUT_Y,    y_true.astype(int))
    OUT_META.write_text(json.dumps({
        "symbol": symbol,
        "timeframe": "M5",
        "start": START,
        "end": END,
        "N": int(len(p_buy_raw)),
        "feature_order_used": order,
        "inferred_api": use_proba,
    }, ensure_ascii=False, indent=2), encoding="utf-8")

    logger.info(f"wrote: {OUT_PBUY} ({len(p_buy_raw)}), {OUT_Y} ({len(y_true)}), api={use_proba}")

#
if __name__ == "__main__":
    main()



=== file: scripts/make_csv_from_mt5.py ===

# scripts/make_csv_from_mt5.py
# -*- coding: utf-8 -*-
"""
MT5 から USDJPY の M5/M15/H1 を 2020-11-01 以降で CSV 化し、
以降は不足分のみを自動追記するユーティリティ。

- 保存先: <プロジェクトルート>/data （相対指定でも最終的に絶対パスへ解決）
- タイムゾーン: JST（Asia/Tokyo）で time 列を naive datetime64[ns] として保存
- 既存CSVがあれば末尾時刻以降を自動で追記（重複は除去）
- シンボル接尾辞（例: USDJPY-）は自動解決
- GaitameFinest 等で copy_rates_range() が失敗する環境に対して
  copy_rates_from() の「現在→過去へページング」フォールバックを搭載
- 実行環境名は --env で明示でき、未指定時は HOST_MAP とヒューリスティックで推定
- 保存レイアウトは --layout で切替（flat | per-symbol）
"""

from __future__ import annotations

import argparse
import os
import socket
from datetime import datetime as pdt, timezone
from pathlib import Path
from typing import Optional, Tuple

import pandas as pd

try:
    import MetaTrader5 as mt5
except Exception as e:
    raise SystemExit(
        "[fatal] MetaTrader5 パッケージが見つかりません。仮想環境で `pip install MetaTrader5 pandas` を実行してください。"
    ) from e


# =========================
# 設定（必要なら編集）
# =========================

SYMBOL_DEFAULT = "USDJPY"
TIMEFRAMES_DEFAULT = ["M5", "M15", "H1"]
START_DATE_DEFAULT = "2020-11-01"  # ここ以前は取得しない

# プロジェクトルート（このスクリプトの1つ上のディレクトリ）
PROJECT_ROOT = Path(__file__).resolve().parents[1]
# デフォルトのデータ保存ディレクトリ（最終的に PROJECT_ROOT/data に解決）
DATA_DIR_DEFAULT = "data"

# MT5 の timeframe 定数マップ
TF_MAP = {
    "M1": mt5.TIMEFRAME_M1,
    "M5": mt5.TIMEFRAME_M5,
    "M15": mt5.TIMEFRAME_M15,
    "M30": mt5.TIMEFRAME_M30,
    "H1": mt5.TIMEFRAME_H1,
    "H4": mt5.TIMEFRAME_H4,
    "D1": mt5.TIMEFRAME_D1,
}

# CSV カラム順（MT5の戻り値に準拠）
CSV_COLS = ["time", "open", "high", "low", "close", "tick_volume", "spread", "real_volume"]

# フォールバック時の1回あたり取得本数と最大ループ回数（必要に応じて調整可能）
PAGE = 20000
MAX_LOOPS = 300

# CSVファイル名で使う “接尾辞なしタグ” を main() 内で設定
FILE_TAG: Optional[str] = None


# =========================
# ユーティリティ
# =========================

def log(msg: str):
    host = os.environ.get("COMPUTERNAME", socket.gethostname())
    ts = pdt.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}][{host}] {msg}")


def ensure_mt5_initialized(terminal_path: Optional[str] = None):
    """
    MT5 を初期化。terminal_path を指定すればその exe に紐づけ。
    Windows で MT5 が起動していなくても、通常は自動で起動・接続可能。
    """
    ok = mt5.initialize(path=terminal_path) if terminal_path else mt5.initialize()
    if not ok:
        code, details = mt5.last_error()
        raise SystemExit(f"[fatal] MT5 initialize 失敗: {code} {details}")

    info = mt5.account_info()
    if info is None:
        log("[warn] account_info() が None。未ログインの可能性。ターミナル側で口座ログインしてください。")
    else:
        log(f"connected login={info.login} server={info.server} balance={info.balance}")


def resolve_symbol(base: str) -> str:
    """
    ブローカー接尾辞違いに対応してシンボル名を解決。
    成功した実在シンボル名を返す。全て失敗なら元の base を返す。
    """
    candidates = [
        base,
        base + "-",
        base + ".",
        base + ".r",
        base + ".m",
        base + ".mini",
        base + "_",
    ]
    tried = []
    for sym in candidates:
        tried.append(sym)
        info = mt5.symbol_info(sym)
        if info is not None:
            if not info.visible:
                mt5.symbol_select(sym, True)
            return sym
    log(f"[warn] symbol resolve failed. tried={tried}")
    return base


def jst_from_mt5_epoch(series):
    """
    MT5の 'time' (Unix秒, UTC) を JST の naive datetime64[ns] に変換。
    series は pandas Series でも DatetimeIndex でも両対応。
    """
    s = pd.to_datetime(series, unit="s", utc=True)
    if isinstance(s, pd.DatetimeIndex):
        return s.tz_convert("Asia/Tokyo").tz_localize(None)
    else:
        return s.dt.tz_convert("Asia/Tokyo").dt.tz_localize(None)


def merge_and_dedup(old: Optional[pd.DataFrame], new: pd.DataFrame) -> pd.DataFrame:
    """
    old と new を縦結合して time 重複を除去（後勝ち）→ time 昇順に揃える。
    """
    if old is None or old.empty:
        out = new.copy()
    else:
        out = pd.concat([old, new], axis=0, ignore_index=True)
        out = out.drop_duplicates(subset=["time"], keep="last")
    return out.sort_values("time").reset_index(drop=True)


# =========================
# データ取得コア
# =========================

def _to_utc_naive(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> UTC naive（tzinfoなし）"""
    return (
        ts_jst.tz_localize("Asia/Tokyo")
        .tz_convert("UTC")
        .to_pydatetime()
        .replace(tzinfo=None)
    )


def _to_local_naive(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> JST naive（ローカルnaiveを要求する環境向け）"""
    return ts_jst.to_pydatetime().replace(tzinfo=None)


def _to_utc_aware(ts_jst: pd.Timestamp) -> pdt:
    """JST naive -> UTC aware（timezone.utc）"""
    return (
        ts_jst.tz_localize("Asia/Tokyo")
        .tz_convert("UTC")
        .to_pydatetime()
        .replace(tzinfo=timezone.utc)
    )


def _range_attempts(symbol: str, tf: int, start_ts: pd.Timestamp, end_ts: pd.Timestamp) -> Tuple[Optional[pd.DataFrame], str]:
    """
    copy_rates_range() を 3 方式（UTC-naive / local-naive / UTC-aware）で試す。
    成功時は (DataFrame, "ok:<tag>")、全滅なら (None, "fail")
    """
    variants = [
        ("utc_naive", _to_utc_naive(start_ts), _to_utc_naive(end_ts)),
        ("local_naive", _to_local_naive(start_ts), _to_local_naive(end_ts)),
        ("utc_aware", _to_utc_aware(start_ts), _to_utc_aware(end_ts)),
    ]
    for tag, dfrom, dto in variants:
        rates = mt5.copy_rates_range(symbol, tf, dfrom, dto)
        if rates is None:
            code, details = mt5.last_error()
            log(f"[try:{tag}] copy_rates_range returned None: {code} {details}")
            continue
        df = pd.DataFrame(rates)
        if len(df) == 0:
            log(f"[ok:{tag}] fetched rows=0")
            return pd.DataFrame(columns=CSV_COLS), f"ok:{tag}"
        df["time"] = jst_from_mt5_epoch(df["time"])
        df = df.sort_values("time").reset_index(drop=True)
        log(f"[ok:{tag}] fetched rows={len(df)}")
        return df[CSV_COLS], f"ok:{tag}"
    return None, "fail"


def fetch_rates(symbol: str, tf: int, start_ts: pd.Timestamp, end_ts: pd.Timestamp) -> pd.DataFrame:
    """
    指定期間のレートを取得して DataFrame で返す（JSTに変換）。
    まず copy_rates_range() を試し、全滅したら copy_rates_from() のバックページングで補う。
    """
    if not isinstance(start_ts, pd.Timestamp):
        start_ts = pd.Timestamp(start_ts)
    if not isinstance(end_ts, pd.Timestamp):
        end_ts = pd.Timestamp(end_ts)
    if end_ts <= start_ts:
        raise ValueError(f"start >= end: {start_ts} .. {end_ts}")

    # 1) range 試行
    df_range, status = _range_attempts(symbol, tf, start_ts, end_ts)
    if status.startswith("ok"):
        return df_range

    # 2) フォールバック: from で過去にページング
    log("[fallback] using copy_rates_from() paging backward")
    dt_to = _to_utc_naive(end_ts)  # MT5は tzinfo なしの UTC を好む

    frames = []
    safety_loops = 0

    while safety_loops < MAX_LOOPS:
        safety_loops += 1
        rates = mt5.copy_rates_from(symbol, tf, dt_to, PAGE)
        if rates is None:
            code, details = mt5.last_error()
            log(f"[fallback] copy_rates_from returned None: {code} {details}")
            break
        if len(rates) == 0:
            log("[fallback] no more bars returned")
            break

        # 生のDataFrame（UTC epoch秒）
        df_raw = pd.DataFrame(rates)

        # まずはJSTへ
        df = df_raw.copy()
        df["time"] = jst_from_mt5_epoch(df["time"])
        df = df.sort_values("time").reset_index(drop=True)

        # 目標期間に重なる分だけ保持（JST基準でフィルタ）
        df_keep = df[(df["time"] >= start_ts) & (df["time"] <= end_ts)]
        if len(df_keep):
            frames.append(df_keep[CSV_COLS])

        # 次ページの終端（さらに過去へ）
        oldest_utc_epoch = int(df_raw["time"].min())  # epoch秒
        # DeprecationWarning 回避：UTC aware で作ってから tzinfo=None で naive UTC へ
        dt_to = pdt.fromtimestamp(oldest_utc_epoch - 1, tz=timezone.utc).replace(tzinfo=None)

        # もう十分遡れたか？
        if len(df) and df["time"].min() <= start_ts:
            break

    if not frames:
        log("[fallback] collected 0 rows")
        return pd.DataFrame(columns=CSV_COLS)

    out = (
        pd.concat(frames, axis=0, ignore_index=True)
        .drop_duplicates(subset=["time"])
        .sort_values("time")
        .reset_index(drop=True)
    )
    log(f"[fallback] fetched rows={len(out)} (min={out['time'].min()} .. max={out['time'].max()})")
    return out[CSV_COLS]


# =========================
# CSV 作成・更新
# =========================

def ensure_csv_for_timeframe(symbol: str, tf_name: str, start_date: str, data_dir: Path) -> Path:
    """
    単一タイムフレームのCSVを作成/更新する。
    - start_date 以降で作成（既存があれば末尾以降のみ追記）
    - 返り値: 保存した CSV のパス
    """
    log(f"=== begin timeframe={tf_name} ===")
    if tf_name not in TF_MAP:
        raise ValueError(f"未知のタイムフレーム: {tf_name}")

    tf_const = TF_MAP[tf_name]
    assert FILE_TAG is not None, "FILE_TAG が未設定です（main() で設定されます）"
    csv_path = data_dir / f"{FILE_TAG}_{tf_name}.csv"
    data_dir.mkdir(parents=True, exist_ok=True)

    start_ts = pd.Timestamp(start_date)  # JST naive
    now_ts = pd.Timestamp.now(tz="Asia/Tokyo").tz_localize(None)

    # 既存CSVの読み込み
    if csv_path.exists():
        old = pd.read_csv(csv_path, parse_dates=["time"])
        old = old[CSV_COLS]
        last_time = old["time"].max()
        fetch_from = max(start_ts, last_time)
        log(f"{csv_path.name}: existing rows={len(old)} last={last_time} -> fetch_from={fetch_from}")
    else:
        old = None
        fetch_from = start_ts
        log(f"{csv_path.name}: not found -> fresh export from {fetch_from}")

    # データ取得（少し余分に取り直して重複で吸収）
    df_new = fetch_rates(symbol, tf_const, fetch_from, now_ts)
    log(f"{csv_path.name}: fetched rows={len(df_new)} [{fetch_from} .. {now_ts}]")

    # マージ＆重複除去
    merged = merge_and_dedup(old, df_new)

    # 型最適化（省メモリ）
    if not merged.empty:
        for c in ["open", "high", "low", "close"]:
            merged[c] = merged[c].astype("float32")
        for c in ["tick_volume", "spread", "real_volume"]:
            merged[c] = merged[c].astype("int32")

    merged.to_csv(csv_path, index=False)
    log(f"{csv_path.name}: wrote rows={len(merged)}")
    log(f"filepath: {csv_path.resolve()}")
    log(f"=== end timeframe={tf_name} ===")
    return csv_path


# =========================
# エントリポイント
# =========================

def main():
    parser = argparse.ArgumentParser(description="Export/Update MT5 rates to CSV per timeframe.")
    parser.add_argument("--symbol", default=SYMBOL_DEFAULT, help="シンボル（例: USDJPY）")
    parser.add_argument("--timeframes", nargs="+", default=TIMEFRAMES_DEFAULT, help="例: M5 M15 H1")
    parser.add_argument("--start", default=START_DATE_DEFAULT, help="開始日（例: 2020-11-01）")
    parser.add_argument("--data-dir", default=DATA_DIR_DEFAULT, help="保存先ディレクトリ（相対はプロジェクトルート基準）")
    parser.add_argument("--terminal", default=None, help="MT5 terminal.exe のフルパス（必要な場合のみ）")

    # 新規：環境と保存レイアウト
    parser.add_argument("--env", choices=["laptop", "desktop", "vps"], default=None,
                        help="環境名を明示（laptop/desktop/vps）。未指定ならホスト名ヒューリスティック。")
    parser.add_argument("--layout", choices=["flat", "per-symbol"], default="flat",
                        help="CSV保存レイアウト。flat= data/直下, per-symbol= data/<SYMBOL>/ohlcv/ 下に保存")

    args = parser.parse_args()

    symbol = args.symbol.upper()
    tfs = [tf.upper() for tf in args.timeframes]

    # data_dir を絶対パス化（相対なら PROJECT_ROOT 基準）
    raw_data_dir = Path(args.data_dir)
    data_dir = raw_data_dir if raw_data_dir.is_absolute() else (PROJECT_ROOT / raw_data_dir)
    data_dir = data_dir.resolve()

    log(f"start export: symbol={symbol} tfs={tfs} start={args.start} data_dir={data_dir}")
    log(f"cwd={Path.cwd()} project_root={PROJECT_ROOT}")

    # 環境推定（明示指定優先 → HOST_MAP → ヒューリスティック）
    host = os.environ.get("COMPUTERNAME", socket.gethostname()).lower()
    HOST_MAP = {
        # 必要に応じて固定マッピングを追加
        # 例: "desktop-8rrd83d": "laptop",
        # "sakura-vps": "vps",
    }
    if args.env:
        env_resolved = args.env
    else:
        env_resolved = HOST_MAP.get(host)
        if not env_resolved:
            if "vps" in host or "sakura" in host or "administrator" in str(Path.home()).lower():
                env_resolved = "vps"
            elif "desk" in host:
                env_resolved = "desktop"
            else:
                env_resolved = "laptop"
    log(f"環境: {env_resolved} (host={host})")

    # MT5 初期化
    ensure_mt5_initialized(terminal_path=args.terminal)

    # シンボル解決（USDJPY / USDJPY- など）
    resolved_symbol = resolve_symbol(symbol)
    if resolved_symbol != symbol:
        log(f"symbol resolved: {symbol} -> {resolved_symbol}")
    symbol = resolved_symbol
    mt5.symbol_select(symbol, True)

    # CSV 用の “接尾辞なしタグ” を作成（英字のみ抽出）
    global FILE_TAG
    FILE_TAG = "".join([c for c in symbol if c.isalpha()]) or symbol

    # 保存先レイアウト（flat | per-symbol）
    if args.layout == "per-symbol":
        save_dir = (data_dir / FILE_TAG / "ohlcv").resolve()
    else:
        save_dir = data_dir
    save_dir.mkdir(parents=True, exist_ok=True)
    log(f"save_dir={save_dir}")

    # 取得・保存
    created = []
    for tf_name in tfs:
        path = ensure_csv_for_timeframe(symbol, tf_name, args.start, save_dir)
        created.append(path)

    mt5.shutdown()
    log("done.")


if __name__ == "__main__":
    main()



=== file: scripts/make_project_snapshot.py ===

#!/usr/bin/env python
"""
プロジェクト全体のフォルダ構造と主要ファイルの内容を
1つの project_snapshot.txt にまとめて出力するスクリプト。

- プロジェクトルート = このファイルの 1 つ上のフォルダ
- 除外ディレクトリや対象拡張子は CONFIG のところで調整可能
- ディレクトリツリーにはファイルの最終更新日時も表示します
"""

from __future__ import annotations

import os
from pathlib import Path
from datetime import datetime
from typing import Iterable, List

# ========================
# 設定
# ========================

# 除外するディレクトリ名（部分一致）
EXCLUDE_DIR_NAMES = {
    ".git",
    ".venv",
    "__pycache__",
    ".mypy_cache",
    ".pytest_cache",
    ".idea",
    ".vscode",
    "logs",
    "data",
    "models",
    "dist",
    "build",
    ".ruff_cache",
}

# 中身を書き出す対象の拡張子
INCLUDE_EXTENSIONS = {
    ".py",
    ".txt",
    ".md",
    ".rst",
    ".yaml",
    ".yml",
    ".ini",
    ".toml",
    ".json",
    ".ps1",
    ".bat",
    ".sh",
}

# ファイルサイズの上限（バイト）。これを超えると中身はスキップしてヘッダだけ書く
MAX_FILE_SIZE_BYTES = 100_000  # 100KB

# 出力ファイル名
SNAPSHOT_FILENAME = "project_snapshot.txt"


# ========================
# ヘルパー関数
# ========================

def iter_tree(root: Path) -> Iterable[Path]:
    """root 配下のファイル・ディレクトリを walk するジェネレータ。
    除外ディレクトリをスキップする。
    """
    for dirpath, dirnames, filenames in os.walk(root):
        # dirnames を in-place でフィルタすると os.walk がそれを辿らなくなる
        dirnames[:] = [
            d for d in dirnames
            if d not in EXCLUDE_DIR_NAMES
        ]
        current_dir = Path(dirpath)
        # ディレクトリ自身
        yield current_dir
        # ファイル
        for fname in filenames:
            yield current_dir / fname


def make_tree_text(root: Path) -> str:
    """ディレクトリツリー文字列を生成する（ファイルには最終更新日時付き）。"""
    lines: List[str] = []

    root_str = root.name
    lines.append(f"{root_str}/")

    # root からの相対パスでソートして表示
    paths = sorted(
        (p for p in iter_tree(root)),
        key=lambda p: str(p.relative_to(root)).lower(),
    )

    seen_dirs = set()

    for p in paths:
        rel = p.relative_to(root)
        parts = rel.parts

        # ルート自身はもう書いているのでスキップ
        if rel == Path("."):
            continue

        indent = "  " * (len(parts) - 1)
        name = parts[-1]

        if p.is_dir():
            # ディレクトリ
            dir_key = rel
            if dir_key in seen_dirs:
                continue
            seen_dirs.add(dir_key)
            lines.append(f"{indent}{name}/")
        else:
            # ファイル（最終更新日時を付ける）
            try:
                mtime = datetime.fromtimestamp(p.stat().st_mtime)
                mtime_str = mtime.strftime("%Y-%m-%d %H:%M:%S")
            except OSError:
                mtime_str = "unknown"
            lines.append(f"{indent}{name} (updated: {mtime_str})")

    return "\n".join(lines)


def should_dump_content(path: Path) -> bool:
    """このファイルの中身をスナップショットに含めるか判定。"""
    if not path.is_file():
        return False

    if path.suffix.lower() not in INCLUDE_EXTENSIONS:
        return False

    try:
        size = path.stat().st_size
    except OSError:
        return False

    if size > MAX_FILE_SIZE_BYTES:
        return False

    return True


def read_text_safely(path: Path) -> str:
    """UTF-8 で読みつつ、読めない文字は置き換える。"""
    try:
        return path.read_text(encoding="utf-8", errors="replace")
    except Exception as e:  # noqa: BLE001
        return f"<< FAILED TO READ FILE: {e} >>"


def collect_files_for_dump(root: Path) -> List[Path]:
    """中身を抜き出す対象のファイル一覧を返す。"""
    files: List[Path] = []
    for p in iter_tree(root):
        if p.is_file() and should_dump_content(p):
            files.append(p)
    files.sort(key=lambda p: str(p.relative_to(root)).lower())
    return files


# ========================
# メイン処理
# ========================

def main() -> int:
    # このスクリプトの 1 つ上をプロジェクトルートとみなす
    script_path = Path(__file__).resolve()
    project_root = script_path.parent.parent
    snapshot_path = project_root / SNAPSHOT_FILENAME

    print(f"[INFO] script_path   = {script_path}")
    print(f"[INFO] project_root  = {project_root}")
    print(f"[INFO] snapshot_path = {snapshot_path}")

    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # 1) ヘッダ
    header_lines = [
        "#" * 60,
        "# project_snapshot",
        "#" * 60,
        f"generated_at: {now}",
        f"project_root: {project_root}",
        "",
        "NOTE:",
        "  - このファイルは ChatGPT にプロジェクト構造と主要ファイルを伝えるためのスナップショットです。",
        "  - ログ・データ・モデル・.git・.venv などは除外しています。",
        "  - ディレクトリツリーにはファイルごとの最終更新日時 (updated: ...) を含みます。",
        "",
        "========================================",
        "=== DIRECTORY TREE =====================",
        "========================================",
        "",
    ]

    tree_text = make_tree_text(project_root)

    # 2) ファイル内容
    files = collect_files_for_dump(project_root)

    content_lines: List[str] = []
    content_lines.append("")
    content_lines.append("")
    content_lines.append("========================================")
    content_lines.append("=== FILE CONTENTS ======================")
    content_lines.append("========================================")
    content_lines.append("")

    for fpath in files:
        rel = fpath.relative_to(project_root)
        content_lines.append("")
        content_lines.append(f"=== file: {rel.as_posix()} ===")
        content_lines.append("")
        txt = read_text_safely(fpath)
        content_lines.append(txt)
        content_lines.append("")  # 区切り

    # 3) スナップショットを書き出し
    all_text = "\n".join(header_lines) + "\n" + tree_text + "\n" + "\n".join(content_lines)

    try:
        snapshot_path.write_text(all_text, encoding="utf-8")
    except Exception as e:  # noqa: BLE001
        print(f"[ERROR] failed to write snapshot: {e}")
        return 1

    print(f"[OK] snapshot written: {snapshot_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())



=== file: scripts/make_toy_model.py ===

from __future__ import annotations

import json
import os
from pathlib import Path

import joblib
import numpy as np
import pandas as pd

USE_LGB = True
try:
    from lightgbm import LGBMClassifier
except Exception:
    USE_LGB = False
    from sklearn.linear_model import LogisticRegression

MODELS_DIR = Path("models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)

FEATURES = [
    "ema_5",
    "ema_20",
    "rsi_14",
    "atr_14",
    "adx_14",
    "bbp",
    "vol_chg",
    "wick_ratio",
]


def make_synthetic(n: int = 5000, seed: int = 42) -> tuple[pd.DataFrame, np.ndarray]:
    rng = np.random.default_rng(seed)
    X = pd.DataFrame(
        {
            "ema_5": rng.normal(0.0, 0.2, n),
            "ema_20": rng.normal(0.0, 0.2, n),
            "rsi_14": rng.uniform(0, 100, n),
            "atr_14": rng.uniform(0, 1, n),
            "adx_14": rng.uniform(5, 40, n),
            "bbp": rng.uniform(-0.5, 1.5, n),
            "vol_chg": rng.normal(0.0, 0.05, n),
            "wick_ratio": rng.uniform(0, 1, n),
        }
    )
    score = (
        0.8 * X["ema_5"]
        - 0.5 * X["ema_20"]
        + 0.01 * (X["rsi_14"] - 50)
        + 0.4 * (X["bbp"] - 0.5)
        - 0.2 * X["vol_chg"]
        + 0.15 * (X["adx_14"] > 20).astype(float)
    )
    p = 1 / (1 + np.exp(-score))
    y = (rng.uniform(0, 1, n) < p).astype(int)
    return X[FEATURES], y


def main() -> None:
    X, y = make_synthetic()

    if USE_LGB:
        model = LGBMClassifier(
            n_estimators=200,
            max_depth=-1,
            num_leaves=31,
            learning_rate=0.05,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=123,
            n_jobs=1,
        )
    else:
        model = LogisticRegression(max_iter=1000, n_jobs=1)

    model.fit(X, y)

    joblib.dump(model, MODELS_DIR / "LightGBM_clf.pkl")
    with open(MODELS_DIR / "LightGBM_clf.features.json", "w", encoding="utf-8") as f:
        json.dump(list(X.columns), f, ensure_ascii=False, indent=2)

    classes = getattr(model, "classes_", None)
    if classes is not None:
        with open(MODELS_DIR / "LightGBM_clf.classes.json", "w", encoding="utf-8") as f:
            json.dump([str(c) for c in classes], f, ensure_ascii=False, indent=2)

    print("[OK] Exported:")
    print(" - models/LightGBM_clf.pkl")
    print(" - models/LightGBM_clf.features.json")
    print(" - models/LightGBM_clf.classes.json (optional)")


if __name__ == "__main__":
    main()



=== file: scripts/mt5_export_csv.py ===

# scripts/mt5_export_csv.py
# MT5のローカルヒストリから rates を取得→ data/{SYMBOL}_{TF}.csv に保存
from __future__ import annotations
import argparse
from datetime import datetime, timedelta, timezone
from pathlib import Path
import pandas as pd

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--symbol", required=True)
    ap.add_argument("--timeframe", default="M5", help="M1/M5/M15/M30/H1/H4/D1")
    ap.add_argument("--days", type=int, default=365*2, help="過去n日を取得")
    args = ap.parse_args()

    import MetaTrader5 as mt5  # pip install MetaTrader5

    if not mt5.initialize():
        raise SystemExit("MT5 initialize failed")

    tf_map = {
        "M1": mt5.TIMEFRAME_M1, "M5": mt5.TIMEFRAME_M5, "M15": mt5.TIMEFRAME_M15,
        "M30": mt5.TIMEFRAME_M30, "H1": mt5.TIMEFRAME_H1, "H4": mt5.TIMEFRAME_H4,
        "D1": mt5.TIMEFRAME_D1
    }
    tf = tf_map.get(args.timeframe.upper(), mt5.TIMEFRAME_M5)

    utc_to = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(days=args.days)

    rates = mt5.copy_rates_range(args.symbol, tf, utc_from, utc_to)
    mt5.shutdown()
    if rates is None or len(rates) == 0:
        raise SystemExit("no rates from MT5")

    df = pd.DataFrame(rates)
    # MT5の time はunix秒
    df["time"] = pd.to_datetime(df["time"], unit="s")
    df.rename(columns={"real_volume":"tick_volume"}, inplace=True)

    out_dir = Path(__file__).resolve().parents[1] / "data"
    out_dir.mkdir(parents=True, exist_ok=True)
    out = out_dir / f"{args.symbol.upper()}_{args.timeframe.upper()}.csv"
    df[["time","open","high","low","close","tick_volume"]].to_csv(out, index=False)
    print(f"wrote: {out} rows={len(df)}")

if __name__ == "__main__":
    main()



=== file: scripts/mt5_smoke.py ===

# scripts/mt5_smoke.py
#
# 目的:
#   - MetaTrader5 の初期化が成功するか
#   - 現在ログインしている口座情報が取れるか
# を確認するスモークテスト。

import MetaTrader5 as mt5


def main() -> None:
    print("[mt5_smoke] initialize() ...")
    if not mt5.initialize():
        print(f"[mt5_smoke] initialize() FAILED: last_error={mt5.last_error()}")
        print("  -> MT5 が起動しているか、ログイン状態かを確認してください。")
        return

    print("[mt5_smoke] initialize() OK")

    info = mt5.account_info()
    if info is None:
        print("[mt5_smoke] account_info() is None.")
        print("  -> MT5 が起動しているか、デモ口座などにログインしているか確認してください。")
    else:
        print("[mt5_smoke] account_info():")
        print(f"  login   = {info.login}")
        print(f"  name    = {info.name}")
        print(f"  balance = {info.balance}")
        print(f"  equity  = {info.equity}")

    mt5.shutdown()
    print("[mt5_smoke] shutdown() done.")


if __name__ == "__main__":
    main()



=== file: scripts/print_runtime.py ===

from __future__ import annotations

from core.config import cfg
from core.utils.runtime import is_live

print("is_live:", is_live())
print("filters:", cfg.get("filters"))
print("entry:", cfg.get("entry"))
print("session:", cfg.get("session"))



=== file: scripts/promote_model.py ===

# scripts/promote_model.py
import os, shutil, json, sys
from core.config import cfg
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)


def main() -> None:
    tr = cfg.get("training", {})
    staging = tr.get("staging_dir", "models/_staging")
    prod = tr.get("model_out_dir", "models")
    if not os.path.isdir(staging):
        raise SystemExit("staging not found")
    prom = os.path.join(staging, "PROMOTE.json")
    if os.path.exists(prom):
        st = json.load(open(prom, "r", encoding="utf-8"))
        print("PROMOTE.json:", st)
    # 上書き昇格
    os.makedirs(prod, exist_ok=True)
    ts = __import__("datetime").datetime.now().strftime("%Y%m%d_%H%M%S")
    bk = os.path.join(prod, f"_backup_{ts}")
    os.makedirs(bk, exist_ok=True)
    for fn in os.listdir(prod):
        if fn.startswith("_backup_"): continue
        shutil.copy2(os.path.join(prod, fn), os.path.join(bk, fn))
    for fn in os.listdir(staging):
        shutil.copy2(os.path.join(staging, fn), os.path.join(prod, fn))
    print("PROMOTED. backup:", bk)


if __name__ == "__main__":
    main()



=== file: scripts/register_weekly_task.ps1 ===

param(
  [string]$TaskName = "FXBot_WeeklyRetrain",
  [string]$PythonExe = "$env:USERPROFILE\AppData\Local\Programs\Python\Python313\python.exe",
  [string]$ProjectDir = "C:\fxbot",
  [string]$StartTime = "03:05",   # JST
  [ValidateSet("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")]
  [string]$DayOfWeek = "Sunday"
)

$Action = New-ScheduledTaskAction -Execute $PythonExe -Argument "scripts/walkforward_retrain.py" -WorkingDirectory $ProjectDir
$Trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek $DayOfWeek -At $StartTime
$Settings = New-ScheduledTaskSettingsSet -StartWhenAvailable -AllowStartIfOnBatteries -DontStopOnIdleEnd
Register-ScheduledTask -TaskName $TaskName -Action $Action -Trigger $Trigger -Settings $Settings -Description "Weekly walk-forward retrain & promote if metrics pass"
Write-Host "Registered task:" $TaskName



=== file: scripts/rollback_model.py ===

# scripts/rollback_model.py
import os, shutil, sys
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)


def main() -> None:
    prod = "models"
    bks = sorted([d for d in os.listdir(prod) if d.startswith("_backup_")])
    if not bks:
        raise SystemExit("no backups found")
    last = os.path.join(prod, bks[-1])
    for fn in os.listdir(last):
        shutil.copy2(os.path.join(last, fn), os.path.join(prod, fn))
    print("ROLLED BACK to:", last)


if __name__ == "__main__":
    main()



=== file: scripts/selftest_order_flow.py ===

from __future__ import annotations

from core.config import cfg  # noqa: F401  # ensure configuration loads
from app.services.trade_service import TradeService


def main() -> None:
    ts = TradeService()

    order_id = "SELFTEST-ORDER"
    ts.mark_order_inflight(order_id)
    ts.on_order_result(order_id=order_id, ok=True, symbol="USDJPY")
    ts.on_order_success(ticket=123456789, side="SELL", symbol="USDJPY")
    ts.record_trade_result(symbol="USDJPY", side="SELL", profit_jpy=-500.0)

    print("open_count(in guard):", ts.pos_guard.state.open_count)
    print("last_ticket:", ts.state.last_ticket)
    print("CB status:", ts.cb.status())


if __name__ == "__main__":
    main()



=== file: scripts/sim_trailing.py ===

import argparse

from app.services.trailing import AtrTrailer, TrailConfig, TrailState


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--side", choices=["BUY", "SELL"], default="BUY")
    parser.add_argument("--entry", type=float, required=True)
    parser.add_argument(
        "--atr",
        type=float,
        required=True,
        help="ATR in price units, e.g., 0.12 for 12 pips on USDJPY",
    )
    parser.add_argument("--pip", type=float, default=0.01, help="pip size, USDJPY=0.01")
    parser.add_argument("--point", type=float, default=0.001, help="point size, USDJPY=0.001")
    parser.add_argument("--activate", type=float, default=0.5)
    parser.add_argument("--step", type=float, default=0.25)
    parser.add_argument("--lockbe", type=float, default=0.3)
    parser.add_argument("--floor", type=float, default=5.0)
    args = parser.parse_args()

    cfg = TrailConfig(
        pip_size=args.pip,
        point=args.point,
        atr=args.atr,
        activate_mult=args.activate,
        step_mult=args.step,
        lock_be_mult=args.lockbe,
        hard_floor_pips=args.floor,
        only_in_profit=True,
        max_layers=20,
    )
    state = TrailState(side=args.side, entry=args.entry)
    trailer = AtrTrailer(cfg, state)

    steps = []
    for i in range(0, 31):
        delta = cfg.atr * 0.1 * i
        if args.side == "BUY":
            steps.append(args.entry + delta)
        else:
            steps.append(args.entry - delta)

    print(f"# side={args.side} entry={args.entry} atr={args.atr}")
    print("# price, activated, be_locked, layers, current_sl, new_sl")
    for px in steps:
        new_sl = trailer.suggest_sl(px)
        print(f"{px:.5f}, {state.activated}, {state.be_locked}, {state.layers}, {state.current_sl}, {new_sl}")


if __name__ == "__main__":
    main()



=== file: scripts/swap_model.py ===

from __future__ import annotations

import glob
import hashlib
import json
import os
import shutil
import sys
import time
from pathlib import Path

ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

MODELS_DIR = "models"
LIVE_LINK = os.path.join(MODELS_DIR, "live")


def _is_ready(bundle_dir: str) -> bool:
    ready = os.path.exists(os.path.join(bundle_dir, "READY"))
    manifest = os.path.exists(os.path.join(bundle_dir, "manifest.json"))
    return ready and manifest


def _latest_ready() -> str:
    candidates: list[tuple[float, str]] = []
    for bundle in glob.glob(os.path.join(MODELS_DIR, "*")):
        if (
            os.path.isdir(bundle)
            and _is_ready(bundle)
            and os.path.basename(bundle) not in {"live", "prev"}
        ):
            candidates.append((os.path.getmtime(bundle), bundle))
    if not candidates:
        raise SystemExit("no READY bundles found.")
    candidates.sort(reverse=True)
    return candidates[0][1]


def _sha256(path: Path) -> str:
    digest = hashlib.sha256()
    with open(path, "rb") as fh:
        for chunk in iter(lambda: fh.read(1024 * 1024), b""):
            digest.update(chunk)
    return digest.hexdigest()


def _load_latest_best_threshold() -> tuple[float | None, str | None]:
    try:
        report_dir = os.path.join("logs", "retrain")
        pattern = os.path.join(report_dir, "report_*.json")
        reports = sorted(glob.glob(pattern))
        if not reports:
            return None, None
        latest = reports[-1]
        with open(latest, "r", encoding="utf-8") as fh:
            data = json.load(fh)
        metrics = data.get("metrics_test") or {}
        best_t = metrics.get("best_threshold")
        if isinstance(best_t, (int, float)):
            return float(best_t), latest
        return None, latest
    except Exception:
        return None, None


def activate_model_from_pkl(
    pkl_path: Path, models_dir: Path, target_name: str = "LightGBM_clf.pkl"
) -> int:
    models_dir.mkdir(parents=True, exist_ok=True)
    active_pkl = models_dir / target_name
    tmp = active_pkl.with_suffix(".tmp.pkl")
    shutil.copy2(pkl_path, tmp)
    if active_pkl.exists():
        backup = models_dir / f"backup_{int(time.time())}.pkl"
        shutil.copy2(active_pkl, backup)
    os.replace(tmp, active_pkl)

    best_t, source_report = _load_latest_best_threshold()

    meta = {
        "activated_at_jst": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
        "source": "direct-pkl",
        "source_path": str(pkl_path.resolve()),
        "target_path": str(active_pkl.resolve()),
        "sha256": _sha256(active_pkl),
        "version": str(time.time()),
        "features_hash": None,
    }
    meta["best_threshold"] = None if best_t is None else f"{float(best_t):.6f}"
    if source_report:
        meta["best_threshold_source_report"] = source_report
    with open(models_dir / "active_model.json", "w", encoding="utf-8") as fh:
        json.dump(meta, fh, ensure_ascii=False, indent=2)
    print(f"[swap_model] ACTIVATED direct pkl -> {active_pkl}")
    return 0


def _fallback_main(args: list[str]) -> int:
    # READY探索に失敗した場合のフォールバック。単独実行にも対応。
    if args:
        candidate = Path(args[0])
        if candidate.suffix.lower() == ".pkl" and candidate.exists():
            return activate_model_from_pkl(candidate, Path("models"))
    print("no READY bundles found and no valid .pkl path provided.")
    return 1


def main(args: list[str] | None = None) -> int:
    if args is None:
        args = sys.argv[1:]
    try:
        target = _latest_ready()
    except SystemExit as exc:
        if exc.code:
            print(exc.code)
        return _fallback_main(args)

    prev = os.path.join(MODELS_DIR, "prev")
    if os.path.exists(prev):
        shutil.rmtree(prev)
    if os.path.exists(LIVE_LINK):
        shutil.move(LIVE_LINK, prev)
    shutil.copytree(target, LIVE_LINK)
    print(f"[SWAP] live -> {os.path.basename(target)} (prev saved)")
    return 0


if __name__ == "__main__":
    # 既存のREADY切替ロジックがsys.exitする前に、フォールバックを呼べるようにする保険。
    sys.exit(main())



=== file: scripts/train_calibrator.py ===

# scripts/train_calibrator.py
from __future__ import annotations
import numpy as np, joblib
from pathlib import Path
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression

import sys
if not (Path("logs/val_p_buy_raw.npy").exists() and Path("logs/val_y_true.npy").exists()):
    sys.exit("val_p_buy_raw.npy / val_y_true.npy が見つかりません。先に検証推論を実行して保存してください。")
    
# 検証データの「生BUY確率」と「正解ラベル」を用意して保存しておく
# 例: logs/val_p_buy_raw.npy (shape (N,)), logs/val_y_true.npy (0/1)
p_raw = np.load("logs/val_p_buy_raw.npy").astype(float).ravel()
y_true = np.load("logs/val_y_true.npy").astype(int).ravel()

models = Path("models"); models.mkdir(exist_ok=True, parents=True)

# Platt
lr = LogisticRegression(max_iter=1000)
lr.fit(p_raw.reshape(-1,1), y_true)
joblib.dump(lr, models / "calib_platt.pkl")

# Isotonic
iso = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds="clip")
iso.fit(p_raw, y_true)
joblib.dump(iso, models / "calib_isotonic.pkl")

print("wrote: models/calib_platt.pkl, models/calib_isotonic.pkl")



=== file: scripts/verify_smoke.ps1 ===

param(
  [ValidateSet("smoke","strict","live")]
  [string]$Mode = "smoke",
  [int]$Ticks = 600,
  [int]$DtMs = 50,
  [double]$Base = 150.20,
  [double]$Spread = 0.5,
  [double]$AtrPct = 0.00050
)

$ErrorActionPreference = "Stop"

$root = Split-Path -Parent $PSCommandPath
Set-Location (Join-Path $root "..")

$python = "python"

function Invoke-Py {
  param([string]$ArgsLine)
  & $python -c $ArgsLine
}

function PyMod {
  param(
    [string]$Module,
    [string[]]$Arguments
  )
  & $python -m $Module @Arguments
}

Write-Host "== Step 1: Compile =="
PyMod "compileall" @("app","core","scripts") | Out-Null
Write-Host "OK: compile"

Write-Host "== Step 2: Runtime mode check =="
$live = Invoke-Py "from core.utils.runtime import is_live; print('LIVE' if is_live() else 'DRYRUN')"
Write-Host "Runtime:" $live

Write-Host "== Step 3: Show key filters =="
Invoke-Py "from core.config import cfg; f=cfg.get('filters',{}); hy=f.get('atr_hysteresis',{}); print('min_atr_pct=', f.get('min_atr_pct'), 'enable_min=', hy.get('enable_min_pct'), 'disable_min=', hy.get('disable_min_pct')); entry=cfg.get('entry',{}); print('prob_threshold=', entry.get('prob_threshold')); print('session.allow_hours_jst=', cfg.get('session',{}).get('allow_hours_jst'))"

Write-Host "== Step 4: Ensure logs dir =="
$logDir = "logs\decisions"
if (!(Test-Path $logDir)) {
  New-Item -ItemType Directory -Force -Path $logDir | Out-Null
}

Write-Host "== Step 5: Launch GUI =="
Start-Process -WindowStyle Minimized powershell -ArgumentList "-NoLogo -NoExit -Command `"$python -m app.gui.main`""
Start-Sleep -Seconds 2

Write-Host "== Step 6: Dryrun / Replay =="
$extra = @()
switch ($Mode) {
  "smoke"  { $extra = @("--atr-open") }
  "strict" { $extra = @() }
  "live"   { $extra = @() }
}

$argsList = @("--sim") + $extra + @(
  "--n", "$Ticks",
  "--dt", "$DtMs",
  "--base", "$Base",
  "--spread", "$Spread",
  "--atrpct", "$AtrPct"
)
Write-Host "Running: python -m scripts.dryrun_smoke $($argsList -join ' ')"
PyMod "scripts.dryrun_smoke" $argsList

Write-Host "== Step 7: Log scan =="
$decisionLog = Get-ChildItem -Path $logDir -Filter "decisions_*.jsonl" | Sort-Object LastWriteTime | Select-Object -Last 1
if ($null -eq $decisionLog) {
  Write-Warning "No decisions_*.jsonl found."
  $entryCount = 0
} else {
  $entryCount = (Select-String -Path $decisionLog.FullName -Pattern '"decision": "ENTRY"' | Measure-Object).Count
}
$trailFlags = Get-Content "logs\app.log" -ErrorAction SilentlyContinue | Select-String "\[TRAIL\]\[(DRYRUN|OK|NG)\]"
$trailCount = ($trailFlags | Measure-Object).Count

Write-Host "ENTRY count =" $entryCount
Write-Host "TRAIL events =" $trailCount

Write-Host "== Step 8: PASS/FAIL =="
$pass = $false
if ($Mode -eq "smoke") {
  if ($trailCount -ge 1) { $pass = $true }
} else {
  if ($entryCount -ge 1 -or $trailCount -ge 1) { $pass = $true }
}

if ($pass) {
  Write-Host "RESULT: PASS" -ForegroundColor Green
  exit 0
} else {
  Write-Host "RESULT: FAIL" -ForegroundColor Red
  Write-Host "Hints:"
  Write-Host " - smokeモードでは --atr-open が有効か確認してください"
  Write-Host " - [TRAIL] ログが出力されているかを確認してください"
  Write-Host " - strict モードでは atrpct を調整してゲートを跨ぐか検討してください"
  exit 1
}



=== file: scripts/walkforward_retrain.py ===

from __future__ import annotations

import argparse
import json
import os
import sys
import warnings
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path

import lightgbm as lgbm
import numpy as np
import pandas as pd
from joblib import dump
from sklearn.metrics import log_loss, precision_recall_curve, roc_auc_score
from sklearn.model_selection import train_test_split

# ------------------------------------------------------------
# 基本設定（フォルダなど）
# ------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"
MODELS_DIR = PROJECT_ROOT / "models"
LOGS_DIR = PROJECT_ROOT / "logs"
MODELS_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)


def resolve_data_root(cli_data_dir: str | None) -> Path:
    """
    データのルート候補を複数試して、最初に存在したディレクトリを採用する。
    優先順位:
      1) --data-dir 引数
      2) 環境変数 FXBOT_DATA
      3) このスクリプトのプロジェクトルート配下の data/
      4) カレントディレクトリ配下の data/
    """
    candidates: list[Path] = []

    # 1) CLI 引数
    if cli_data_dir:
        candidates.append(Path(cli_data_dir))

    # 2) 環境変数
    env_dir = os.getenv("FXBOT_DATA")
    if env_dir:
        candidates.append(Path(env_dir))

    # 3) プロジェクトルートの data (C:\Users\...\fxbot\data / D:\...\fxbot\data / C:\fxbot\data)
    candidates.append(DATA_DIR)

    # 4) 念のためカレントディレクトリの data
    candidates.append(Path.cwd() / "data")

    existing = [p for p in candidates if p.is_dir()]
    if existing:
        return existing[0].resolve()

    # どれもなければ最後に DATA_DIR を返す（存在しなくてもエラー時のメッセージ用）
    return DATA_DIR.resolve()


RNG = np.random.default_rng(42)
pd.options.display.width = 200
warnings.filterwarnings("ignore", category=UserWarning)


# ------------------------------------------------------------
# ユーティリティ
# ------------------------------------------------------------
def jst_now_str() -> str:
    return datetime.now(UTC).astimezone().isoformat(timespec="seconds")


def safe_log(msg: str):
    ts = jst_now_str()
    print(f"{ts} | {msg}", flush=True)


def find_csv(symbol: str, timeframe: str, data_dir: str | None = None) -> Path:
    """
    CSVレイアウト両対応:
      - flat:       data/USDJPY_M5.csv
      - per-symbol: data/USDJPY/ohlcv/  内の  {symbol}_{tf}.csv もしくは  {tf}.csv
    優先順: 明示一致 → タイムスタンプが新しいもの
    """
    # ルート決定（--data-dir / FXBOT_DATA / PROJECT_ROOT/data / ./data の順で存在を確認）
    root = resolve_data_root(data_dir)

    symU = symbol.upper()
    symL = symbol.lower()
    tf = timeframe.upper()

    # 記号付きシンボル（USDJPY- 等）から英字だけのバージョンも作る
    symU_clean = "".join(ch for ch in symU if ch.isalpha())
    symL_clean = symU_clean.lower()

    candidates: list[Path] = []

    # --- flat layout (data/直下)
    candidates += list(root.glob(f"{symU}_{tf}.csv"))
    candidates += list(root.glob(f"{symL}_{tf}.csv"))
    candidates += list(root.glob(f"{symU_clean}_{tf}.csv"))
    candidates += list(root.glob(f"{symL_clean}_{tf}.csv"))
    candidates += list(root.glob(f"*_{tf}.csv"))  # 例: ANYTHING_M5.csv

    # --- per-symbol layout（推奨: data/USDJPY/ohlcv/）
    base_dirs = [
        root / symU / "ohlcv",
        root / symL / "ohlcv",
        root / symU_clean / "ohlcv",
        root / symL_clean / "ohlcv",
        root / symU,
        root / symL,
        root / symU_clean,
        root / symL_clean,
    ]

    for b in base_dirs:
        candidates += list(b.glob(f"{symU}_{tf}.csv"))
        candidates += list(b.glob(f"{symL}_{tf}.csv"))
        candidates += list(b.glob(f"{symU_clean}_{tf}.csv"))
        candidates += list(b.glob(f"{symL_clean}_{tf}.csv"))
        candidates += list(b.glob(f"*_{tf}.csv"))  # 例: anyprefix_M5.csv
        candidates += list(b.glob(f"{tf}.csv"))    # 例: M5.csv

    # 実在ファイルだけ、重複除去
    uniq: list[Path] = []
    seen = set()
    for p in candidates:
        if p.is_file():
            try:
                key = p.resolve()
            except Exception:
                key = p
            if key not in seen:
                seen.add(key)
                uniq.append(p)

    if not uniq:
        tried = [
            root / f"{symU}_{tf}.csv",
            root / f"{symU_clean}_{tf}.csv",
            root / symU / "ohlcv" / f"{symU}_{tf}.csv",
            root / symU_clean / "ohlcv" / f"{symU_clean}_{tf}.csv",
        ]
        msg = (
            "CSVが見つかりません。\\n"
            f"  symbol={symbol} timeframe={timeframe}\\n"
            f"  data_dir={root}\\n"
            "  試した場所の例:\\n    - " + "\\n    - ".join(str(p) for p in tried)
        )
        raise FileNotFoundError(msg)

    # 明示一致（{symbol}_{tf}.csv / clean版）があれば最優先
    exact = [
        p
        for p in uniq
        if p.name.lower()
        in {
            f"{symL}_{tf.lower()}.csv",
            f"{symL_clean}_{tf.lower()}.csv",
        }
    ]
    if exact:
        return exact[0]

    # それ以外は最終更新が新しいもの
    return max(uniq, key=lambda p: p.stat().st_mtime)


# ------------------------------------------------------------
# 特徴量生成
# ------------------------------------------------------------
def rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = np.clip(delta, 0, None)
    down = -np.clip(delta, None, 0)
    ma_up = up.rolling(period, min_periods=period).mean()
    ma_down = down.rolling(period, min_periods=period).mean()
    rs = ma_up / (ma_down + 1e-12)
    return 100 - (100 / (1 + rs))


def atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    prev_close = close.shift(1)
    tr = pd.concat(
        [
            (high - low),
            (high - prev_close).abs(),
            (low - prev_close).abs(),
        ],
        axis=1,
    ).max(axis=1)
    return tr.rolling(period, min_periods=period).mean()


def build_features(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    入力: time, open, high, low, close, tick_volume などのOHLCVを想定
    出力: 特徴量 DataFrame（欠損除去済み）
    """
    df = df_raw.copy()

    # 必須列チェック（ここで止まる場合はCSV修正が必要）
    need = {"time", "open", "high", "low", "close"}
    miss = need - set(df.columns)
    if miss:
        safe_log(f"[WFO][error] CSV missing columns: {sorted(miss)}")
        return pd.DataFrame()

    # 時刻整備
    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time", kind="stable").drop_duplicates(subset=["time"])

    n = len(df)

    # --- ミニ特徴量モード（行数が少ないときの救済） ---
    # 60行未満なら、ロール系は使わずに最低限の特徴量だけで返す
    if n < 60:
        safe_log(
            f"[WFO][warn] tiny dataset detected ({n} rows). Using mini feature set."
        )
        rng = (df["high"] - df["low"]).replace(0, np.nan)
        mini = pd.DataFrame(
            {
                "time": df["time"],
                "open": df["open"],
                "high": df["high"],
                "low": df["low"],
                "close": df["close"],
                # 最低限：1本リターン、レンジ内位置
                "ret1": df["close"].pct_change().fillna(0.0),
                "pos_in_range": ((df["close"] - df["low"]) / rng).fillna(0.5),
            }
        )
        # 数学的におかしい値を除去
        mini = mini.replace([np.inf, -np.inf], np.nan).dropna()
        return mini

    # --- 通常のフル特徴量モード ---
    # 基本の戻りとボラ
    df["ret1"] = df["close"].pct_change()
    df["ret3"] = df["close"].pct_change(3)
    df["ret5"] = df["close"].pct_change(5)
    df["vol20"] = df["close"].pct_change().rolling(20, min_periods=10).std()

    # 移動平均・バンド（min_periodsで消滅を抑制）
    for w in (5, 10, 20, 50):
        df[f"sma{w}"] = df["close"].rolling(w, min_periods=max(2, w // 2)).mean()
        df[f"ema{w}"] = df["close"].ewm(span=w, adjust=False).mean()
    df["bb_mid"] = df["close"].rolling(20, min_periods=10).mean()
    df["bb_std"] = df["close"].rolling(20, min_periods=10).std()
    df["bb_p"] = (df["close"] - df["bb_mid"]) / (df["bb_std"] + 1e-12)

    # RSI / ATR
    def _rsi(series: pd.Series, period: int = 14) -> pd.Series:
        delta = series.diff()
        up = np.clip(delta, 0, None)
        down = -np.clip(delta, None, 0)
        ma_up = up.rolling(period, min_periods=period // 2).mean()
        ma_down = down.rolling(period, min_periods=period // 2).mean()
        rs = ma_up / (ma_down + 1e-12)
        return 100 - (100 / (1 + rs))

    def _atr(df_: pd.DataFrame, period: int = 14) -> pd.Series:
        high, low, close = df_["high"], df_["low"], df_["close"]
        prev_close = close.shift(1)
        tr = pd.concat(
            [(high - low), (high - prev_close).abs(), (low - prev_close).abs()],
            axis=1,
        ).max(axis=1)
        return tr.rolling(period, min_periods=period // 2).mean()

    df["rsi14"] = _rsi(df["close"], 14)
    df["atr14"] = _atr(df, 14)

    # ヒゲ比率（レンジのどこで引けたか）
    rng = (df["high"] - df["low"]).replace(0, np.nan)
    df["pos_in_range"] = (df["close"] - df["low"]) / rng

    # 出来高代理（あれば）
    if "tick_volume" in df.columns:
        df["vol_sma20"] = df["tick_volume"].rolling(20, min_periods=10).mean()
        df["vol_chg"] = df["tick_volume"].pct_change()

    feature_cols = [
        "ret1",
        "ret3",
        "ret5",
        "vol20",
        "sma5",
        "sma10",
        "sma20",
        "sma50",
        "ema5",
        "ema10",
        "ema20",
        "ema50",
        "bb_p",
        "rsi14",
        "atr14",
        "pos_in_range",
        "vol_sma20",
        "vol_chg",
    ]
    feature_cols = [c for c in feature_cols if c in df.columns]

    keep_cols = ["time", "open", "high", "low", "close"] + feature_cols
    df = df[keep_cols].copy()

    # 先頭のNaNを一括でトリム（最大ウィンドウ50に合わせる）
    trim = 50
    if len(df) > trim:
        df = df.iloc[trim:].copy()

    # それでも残るNaN/infは除去
    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    return df


def make_label(df: pd.DataFrame, horizon: int = 10, pips: float = 0.0) -> pd.Series:
    """
    horizon 後の方向ラベル:
      close_{t+h} - close_t > 0 なら 1, それ以外 0
    pips を与えた場合は閾値として使う（pipsは価格差 0.01=1pips 相当の口座もあるので注意）
    """
    future = df["close"].shift(-horizon)
    diff = future - df["close"]
    if pips and pips > 0:
        y = (diff > pips).astype(int)
    else:
        y = (diff > 0).astype(int)
    return y


# ------------------------------------------------------------
# WFO スキーム
# ------------------------------------------------------------
@dataclass
class WFOMetrics:
    fold: int
    train_start: str
    train_end: str
    test_start: str
    test_end: str
    n_train: int
    n_test: int
    auc: float
    logloss: float
    f1_at_thr: float
    thr: float


def iter_wfo_slices(df: pd.DataFrame, train_bars: int, test_bars: int, step_bars: int):
    """
    walk-forward: 固定長学習→固定長テスト→stepで前進
    """
    n = len(df)
    start = 0
    fold = 0
    while True:
        train_start = start
        train_end = train_start + train_bars
        test_end = train_end + test_bars
        if test_end > n:
            break

        yield fold, slice(train_start, train_end), slice(train_end, test_end)
        fold += 1
        start += step_bars


# ------------------------------------------------------------
# しきい値最適化
# ------------------------------------------------------------
def pick_threshold(y_true: np.ndarray, prob: np.ndarray) -> tuple[float, float]:
    """
    PR 曲線から F1 最大点を採用。閾値を返す。
    """
    precision, recall, thresholds = precision_recall_curve(y_true, prob)
    # thresholds の長さは len(precision)-1
    f1 = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-12)
    idx = int(np.nanargmax(f1))
    best_thr = float(np.clip(thresholds[idx], 0.05, 0.95))
    return best_thr, float(f1[idx])


# ------------------------------------------------------------
# 学習（LightGBM）
# ------------------------------------------------------------
def train_lgbm(X: pd.DataFrame, y: pd.Series) -> lgbm.LGBMClassifier:
    params = dict(
        objective="binary",
        boosting_type="gbdt",
        n_estimators=400,
        learning_rate=0.05,
        num_leaves=63,
        max_depth=-1,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_alpha=0.0,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=1,  # VPS 2GB 想定で控えめ
        verbose=-1,
    )
    model = lgbm.LGBMClassifier(**params)
    # DataFrame のまま渡す（列順・名前を維持）
    model.fit(X, y)
    return model


# ------------------------------------------------------------
# メイン
# ------------------------------------------------------------
def main():
    # 引数なしでも安全に動くように、デフォルト値と説明を追加
    ap = argparse.ArgumentParser(
        description="LightGBM walk-forward retrain",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    ap.add_argument(
        "--symbol",
        default="USDJPY-",
        help="例: USDJPY- （未指定時は安全なデフォルト）",
    )
    ap.add_argument(
        "--timeframe",
        default="M5",
        help="例: M5, M15, H1",
    )
    ap.add_argument("--horizon", type=int, default=10, help="予測先 (bars)")
    ap.add_argument(
        "--train_bars",
        type=int,
        default=90_000,
        help="学習バー数（例: 90k≈数ヶ月~年）",
    )
    ap.add_argument(
        "--test_bars",
        type=int,
        default=7_000,
        help="テストバー数（例: 1週間分くらい）",
    )
    ap.add_argument(
        "--step_bars",
        type=int,
        default=7_000,
        help="前進幅（通常 test_bars と同じ）",
    )
    ap.add_argument(
        "--model_name",
        default="LightGBM_clf",
        help="保存名のベース",
    )
    ap.add_argument(
        "--data-dir",
        type=str,
        default=None,
        help="CSVのルートディレクトリ（未指定なら FXBOT_DATA / PROJECT_ROOT/data / ./data の順で探索）",
    )
    # 危険操作制御フラグ
    ap.add_argument(
        "--apply",
        action="store_true",
        help="新しいモデルとしきい値を active_model.json に反映する",
    )
    ap.add_argument(
        "--dry-run",
        action="store_true",
        help="モデル評価のみ行い、active_model.json などは一切更新しない",
    )
    args = ap.parse_args()

    safe_log(
        f"[WFO] start walkforward retrain | symbol={args.symbol} tf={args.timeframe}"
    )

    # CSV 探索 & 読み込み
    csv_path = find_csv(args.symbol, args.timeframe, data_dir=args.data_dir)
    print(f"[retrain] using CSV: {csv_path}")
    safe_log(f"[WFO] load csv: {csv_path}")
    df_raw = pd.read_csv(csv_path)

    # 最低限の列チェック
    need_cols = {"time", "open", "high", "low", "close"}
    missing = need_cols - set(df_raw.columns)
    if missing:
        raise ValueError(f"CSV に必要な列が不足しています: {missing}")

    # 特徴量
    feats = build_features(df_raw)
    if feats.empty:
        safe_log("[WFO] feature building aborted (not enough rows).")
        sys.exit(1)

    # ラベル
    y = make_label(feats, args.horizon)
    feats = feats.iloc[: -args.horizon, :].reset_index(drop=True)
    y = y.iloc[: -args.horizon].reset_index(drop=True)

    # 特徴量行数チェック
    if feats.shape[0] == 0:
        safe_log(
            "[WFO][error] no rows after feature engineering + horizon alignment. "
            "Likely because rows <= horizon. Provide a longer CSV or reduce --horizon."
        )
        sys.exit(1)

    # 説明変数
    drop_cols = ["time", "open", "high", "low", "close"]
    X = feats.drop(columns=[c for c in drop_cols if c in feats.columns])

    # 念のための欠損除去
    mask = ~X.isna().any(axis=1)
    X, y = X[mask], y[mask]
    X = X.astype(np.float32)

    n_total = len(X)
    if n_total < (args.train_bars + args.test_bars + 1):
        # データが少ない場合は 80/20 の単純スプリットで学習→保存のみ
        safe_log("[WFO] dataset is small; using simple 80/20 split instead of WFO.")
        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, shuffle=False)

        clf = train_lgbm(Xtr, ytr)
        prob = clf.predict_proba(Xte)[:, 1]  # DataFrameのまま渡している
        auc = float(roc_auc_score(yte, prob))
        ll = float(log_loss(yte, np.clip(prob, 1e-6, 1 - 1e-6)))

        thr, f1 = pick_threshold(yte.values, prob)
        safe_log(
            f"[WFO] simple-split auc={auc:.4f} logloss={ll:.4f} thr={thr:.3f} f1={f1:.3f}"
        )

        # 全データ再学習→保存
        final_clf = train_lgbm(X, y)
        model_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = MODELS_DIR / f"{args.model_name}_{model_ts}.pkl"
        dump(final_clf, model_path)
        meta = {
            "model_name": args.model_name,
            "version": model_ts,
            "features": list(X.columns),
            "horizon": args.horizon,
            "metrics": {"auc": auc, "logloss": ll, "thr": thr, "f1": f1},
            "source_csv": str(csv_path.name),
        }
        meta_path = MODELS_DIR / f"{args.model_name}_{model_ts}.meta.json"
        meta_path.write_text(
            json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
        )

        # アクティブモデル更新（--apply のときだけ）
        safe_log(f"[WFO] wrote: {model_path.name}, {meta_path.name}")
        if args.dry_run:
            safe_log(
                "[WFO] DRY-RUN のため active_model.json は更新しません。"
            )
        elif not args.apply:
            safe_log(
                "[WFO] --apply が指定されていないため active_model.json は更新しません。"
            )
        else:
            active = {
                "model_file": str(model_path.name),
                "meta_file": str(meta_path.name),
                "best_threshold": thr,
                "updated_at": jst_now_str(),
            }
            (MODELS_DIR / "active_model.json").write_text(
                json.dumps(active, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            safe_log(
                f"[WFO] active_model.json updated (best_threshold={thr:.3f})"
            )
        return

    # --- WFO ---
    safe_log(
        f"[WFO] bars: total={n_total} train={args.train_bars} "
        f"test={args.test_bars} step={args.step_bars}"
    )

    metrics: list[WFOMetrics] = []
    prob_oof = np.full(n_total, np.nan, dtype=np.float64)
    thr_list: list[float] = []

    for fold, s_tr, s_te in iter_wfo_slices(
        X, args.train_bars, args.test_bars, args.step_bars
    ):
        Xtr, ytr = X.iloc[s_tr], y.iloc[s_tr]
        Xte, yte = X.iloc[s_te], y.iloc[s_te]

        # 学習
        clf = train_lgbm(Xtr, ytr)

        # 予測（DataFrameのまま）
        proba = clf.predict_proba(Xte)[:, 1]

        # メトリクス
        try:
            auc = float(roc_auc_score(yte, proba))
        except ValueError:
            auc = float("nan")

        ll = float(log_loss(yte, np.clip(proba, 1e-6, 1 - 1e-6)))
        thr, f1 = pick_threshold(yte.values, proba)

        # OOF へ
        prob_oof[s_te] = proba
        thr_list.append(thr)

        # 期間情報
        t_idx = feats.iloc[s_tr, :]["time"]
        tr_start = str(t_idx.iloc[0]) if len(t_idx) else ""
        tr_end = str(t_idx.iloc[-1]) if len(t_idx) else ""
        t_idx2 = feats.iloc[s_te, :]["time"]
        te_start = str(t_idx2.iloc[0]) if len(t_idx2) else ""
        te_end = str(t_idx2.iloc[-1]) if len(t_idx2) else ""

        m = WFOMetrics(
            fold=fold,
            train_start=tr_start,
            train_end=tr_end,
            test_start=te_start,
            test_end=te_end,
            n_train=len(Xtr),
            n_test=len(Xte),
            auc=auc,
            logloss=ll,
            f1_at_thr=f1,
            thr=thr,
        )
        metrics.append(m)
        safe_log(
            f"[WFO][fold {fold}] auc={auc:.4f} logloss={ll:.4f} "
            f"thr={thr:.3f} f1={f1:.3f} n={len(Xtr)}/{len(Xte)}"
        )

    # WFO 全体まとめ
    valid_idx = ~np.isnan(prob_oof)
    if valid_idx.sum() == 0:
        safe_log("[WFO] no valid test predictions; abort.")
        sys.exit(1)

    y_oof = y.values[valid_idx]
    p_oof = prob_oof[valid_idx]
    auc_oof = float(roc_auc_score(y_oof, p_oof))
    ll_oof = float(log_loss(y_oof, np.clip(p_oof, 1e-6, 1 - 1e-6)))
    thr_oof, f1_oof = pick_threshold(y_oof, p_oof)

    # 少し引き気味に（過適合/ズレ対策で 0.95 を掛ける）
    best_thr = float(np.clip(thr_oof * 0.95, 0.05, 0.95))

    safe_log(
        f"[WFO][OOF] auc={auc_oof:.4f} logloss={ll_oof:.4f} "
        f"thr*={best_thr:.3f} (raw={thr_oof:.3f}) f1={f1_oof:.3f}"
    )

    # 全データで最終モデル
    final_clf = train_lgbm(X, y)
    model_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = MODELS_DIR / f"{args.model_name}_{model_ts}.pkl"
    dump(final_clf, model_path)

    meta = {
        "model_name": args.model_name,
        "version": model_ts,
        "features": list(X.columns),
        "horizon": args.horizon,
        "oof_metrics": {
            "auc": auc_oof,
            "logloss": ll_oof,
            "thr_oof": thr_oof,
            "f1_oof": f1_oof,
            "thr_final": best_thr,
        },
        "folds": [asdict(m) for m in metrics],
        "source_csv": str(csv_path.name),
        "bars": {
            "total": n_total,
            "train": args.train_bars,
            "test": args.test_bars,
            "step": args.step_bars,
        },
    }
    meta_path = MODELS_DIR / f"{args.model_name}_{model_ts}.meta.json"
    meta_path.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
    )

    safe_log(f"[WFO] wrote: {model_path.name}, {meta_path.name}")

    # active_model.json 更新（GUI/実運用が読むファイル）: --apply のときだけ
    if args.dry_run:
        safe_log(
            "[WFO] DRY-RUN のため active_model.json は更新しません。"
        )
    elif not args.apply:
        safe_log(
            "[WFO] --apply が指定されていないため active_model.json は更新しません。"
        )
    else:
        active = {
            "model_file": str(model_path.name),
            "meta_file": str(meta_path.name),
            "best_threshold": best_thr,
            "updated_at": jst_now_str(),
        }
        (MODELS_DIR / "active_model.json").write_text(
            json.dumps(active, ensure_ascii=False, indent=2), encoding="utf-8"
        )
        safe_log(
            f"[WFO] active_model.json updated (best_threshold={best_thr:.3f})"
        )
    safe_log("[WFO] done.")


if __name__ == "__main__":
    main()



=== file: scripts/walkforward_train.py ===

# scripts/walkforward_train.py
from __future__ import annotations
import argparse, os, json, time, shutil, hashlib, random
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Tuple
import numpy as np
import pandas as pd

# 依存がなければロジ回帰にフォールバック
try:
    import lightgbm as lgb
    HAVE_LGB = True
except Exception:
    from sklearn.linear_model import LogisticRegression
    HAVE_LGB = False
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.calibration import CalibratedClassifierCV
import pickle

MODELS_DIR = "models"
DATA_DIR = "data"

def _now_iso() -> str:
    return datetime.now(timezone.utc).astimezone().isoformat(timespec="seconds")

def _sha256(path: str) -> str:
    import hashlib
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for ch in iter(lambda: f.read(8192), b""):
            h.update(ch)
    return h.hexdigest()

def load_dataset(csv_glob: str) -> pd.DataFrame:
    import glob
    files = sorted(glob.glob(os.path.join(DATA_DIR, csv_glob)))
    if not files:
        raise FileNotFoundError(f"No dataset CSVs under data/ matched: {csv_glob}")
    dfs = []
    for f in files:
        df = pd.read_csv(f)
        dfs.append(df)
    df = pd.concat(dfs, axis=0, ignore_index=True)
    # 期待カラム: time, open, high, low, close, label (0/1 for BUY=1), ...features...
    if "label" not in df.columns:
        raise ValueError("dataset must contain 'label' column (0/1)")
    return df

def split_walkforward(df: pd.DataFrame, weeks_train: int, weeks_valid: int, steps: int) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
    """
    直近から遡るWF。週単位で train/valid を切って steps 回。
    """
    if "time" in df.columns:
        dt = pd.to_datetime(df["time"])
    else:
        # 疑似時系列
        base = datetime(2020,1,1)
        dt = pd.Series([base + timedelta(minutes=i) for i in range(len(df))])
    df = df.copy()
    df["__dt__"] = dt

    spans = []
    end = df["__dt__"].max()
    for k in range(steps):
        valid_end = end - timedelta(weeks=k*weeks_valid)
        valid_start = valid_end - timedelta(weeks=weeks_valid)
        train_end = valid_start
        train_start = train_end - timedelta(weeks=weeks_train)

        tr = df[(df["__dt__"]>=train_start) & (df["__dt__"]<train_end)]
        va = df[(df["__dt__"]>=valid_start) & (df["__dt__"]<valid_end)]
        if len(tr) < 100 or len(va) < 100:
            continue
        spans.append((tr, va))
    spans.reverse()  # 古い→新しい
    return spans[-1:]  # 直近の1ステップだけで十分（高速）

def _train_one(tr: pd.DataFrame, va: pd.DataFrame, features: List[str]) -> Tuple[Any, Any, Dict[str, float]]:
    Xtr, ytr = tr[features].values, tr["label"].values
    Xva, yva = va[features].values, va["label"].values

    if HAVE_LGB:
        clf = lgb.LGBMClassifier(
            n_estimators=300, learning_rate=0.05, max_depth=-1,
            subsample=0.8, colsample_bytree=0.8, random_state=42
        )
    else:
        clf = LogisticRegression(max_iter=200)

    clf.fit(Xtr, ytr)
    pva = clf.predict_proba(Xva)[:,1]
    auc = roc_auc_score(yva, pva)
    acc = accuracy_score(yva, (pva>=0.5).astype(int))

    # キャリブレータ（isotonic優先、フォールバックはsigmoid）
    try:
        cal = CalibratedClassifierCV(base_estimator=clf, method="isotonic", cv=5)
        cal.fit(Xtr, ytr)
        cal_name = "isotonic"
    except Exception:
        cal = CalibratedClassifierCV(base_estimator=clf, method="sigmoid", cv=5)
        cal.fit(Xtr, ytr)
        cal_name = "platt"

    return clf, cal, {"auc": float(auc), "acc": float(acc), "calibrator": cal_name}

def _features_from(df: pd.DataFrame) -> List[str]:
    # 最低限：OHLCや派生が入っている前提。label/time/非数値は除外。
    feats = [c for c in df.columns if c not in ("time","label") and pd.api.types.is_numeric_dtype(df[c])]
    if not feats:
        raise ValueError("no numeric features found.")
    return feats

def save_bundle(tag: str, clf: Any, cal: Any, features: List[str], classes: Dict[str, int], metrics: Dict[str, Any]) -> str:
    out = os.path.join(MODELS_DIR, tag)
    os.makedirs(out, exist_ok=True)

    with open(os.path.join(out, "LightGBM_clf.pkl"), "wb") as f:
        pickle.dump(clf, f)
    with open(os.path.join(out, "features.json"), "w", encoding="utf-8") as f:
        json.dump(features, f, ensure_ascii=False)
    with open(os.path.join(out, "classes.json"), "w", encoding="utf-8") as f:
        json.dump(classes, f, ensure_ascii=False)

    if metrics.get("calibrator") == "isotonic":
        with open(os.path.join(out, "calib_isotonic.pkl"), "wb") as f:
            pickle.dump(cal, f)
    else:
        with open(os.path.join(out, "calib_platt.pkl"), "wb") as f:
            pickle.dump(cal, f)

    # manifest
    sums = {}
    for name in ("LightGBM_clf.pkl","features.json","classes.json","calib_isotonic.pkl","calib_platt.pkl"):
        p = os.path.join(out, name)
        if os.path.exists(p):
            sums[name] = _sha256(p)

    mani = {
        "tag": tag,
        "ts": _now_iso(),
        "metrics": metrics,
        "sha256": sums,
        "features_hash": hashlib.sha256(json.dumps(features).encode()).hexdigest(),
    }
    with open(os.path.join(out, "manifest.json"), "w", encoding="utf-8") as f:
        json.dump(mani, f, ensure_ascii=False, indent=2)

    # READY は最後に
    with open(os.path.join(out, "READY"), "w") as f:
        f.write("ok\n")
    return out

def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", default="*.csv", help="data/ 以下で読むCSVのglob")
    ap.add_argument("--weeks-train", type=int, default=156, help="学習期間（週）=3年")
    ap.add_argument("--weeks-valid", type=int, default=12,  help="検証期間（週）=3ヶ月")
    ap.add_argument("--min-auc", type=float, default=0.55,  help="差し替えの最低AUC")
    ap.add_argument("--tag", default=None, help="出力タグ（デフォルトは日時）")
    args = ap.parse_args()

    df = load_dataset(args.csv)
    feats = _features_from(df)
    spans = split_walkforward(df, args.weeks_train, args.weeks_valid, steps=3)
    if not spans:
        raise SystemExit("not enough data for walk-forward.")

    tr, va = spans[-1]
    clf, cal, m = _train_one(tr, va, feats)

    print(f"[METRICS] AUC={m['auc']:.4f} ACC={m['acc']:.4f} CAL={m['calibrator']}")

    if m["auc"] < args.min_auc:
        print(f"[SKIP] AUC {m['auc']:.4f} < min_auc {args.min_auc}")
        return

    tag = args.tag or datetime.now().strftime("lgb_%Y%m%d_%H%M%S")
    bundle = save_bundle(tag, clf, cal, feats, {"BUY":1, "SELL":0}, m)
    print(f"[READY] {bundle}")

if __name__ == "__main__":
    main()



=== file: scripts/weekly_retrain.py ===

#!/usr/bin/env python
"""
scripts/weekly_retrain.py

週次自動再学習ジョブ用スクリプト。

    データ取得
    -> 特徴量作成
    -> LightGBM 学習
    -> Walk-Forward 検証
    -> しきい値最適化
    -> モデル保存 & 署名 (active_model.json 更新)

前提:
- ルート直下 (fxbot/) から実行すること
- 設定: configs/config.yaml もしくは --config で指定
- 価格CSV: data/USDJPY/ohlcv/USDJPY_M5.csv のような構造
"""

from __future__ import annotations

import argparse
import hashlib
import json
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path

import lightgbm as lgb
import numpy as np
import numpy.typing as npt
import pandas as pd
import yaml
from joblib import dump
from loguru import logger

# ---- 定数 (Ruff の magic number 対策も兼ねる) -----------------------------

MIN_WFO_SPLITS: int = 2
DEFAULT_CLASS_THRESHOLD: float = 0.5

JST = UTC  # 後で必要なら Asia/Tokyo に変更してもOK


# ------------------------
# 設定読み込みまわり
# ------------------------


@dataclass
class PathsConfig:
    data_dir: Path
    models_dir: Path
    logs_dir: Path


@dataclass
class RetrainConfig:
    symbol: str
    timeframe: str
    label_horizon: int = 10  # 何バー先をラベルにするか
    min_pips: float = 1.0  # クラス分けに使う最小pips
    n_splits: int = 4  # Walk-Forward の分割数
    threshold_grid: list[float] | None = None  # None を許容

    def __post_init__(self) -> None:
        if self.threshold_grid is None:
            # DEFAULT_CLASS_THRESHOLD を中心に、少し前後を見る
            self.threshold_grid = [
                DEFAULT_CLASS_THRESHOLD - 0.05,
                DEFAULT_CLASS_THRESHOLD,
                DEFAULT_CLASS_THRESHOLD + 0.05,
                DEFAULT_CLASS_THRESHOLD + 0.10,
                DEFAULT_CLASS_THRESHOLD + 0.15,
            ]


@dataclass
class WeeklyRetrainConfig:
    paths: PathsConfig
    retrain: RetrainConfig


def load_config(config_path: Path) -> WeeklyRetrainConfig:
    """YAML 設定のロード。"""

    if not config_path.exists():
        raise FileNotFoundError(f"config.yaml が見つかりません: {config_path}")

    with config_path.open("r", encoding="utf-8") as f:
        raw = yaml.safe_load(f)

    paths_raw = raw.get("paths", {}) or {}
    runtime_raw = raw.get("runtime", {}) or {}
    ai_raw = raw.get("ai", {}) or {}
    retrain_raw = ai_raw.get("retrain", {}) or {}

    data_dir = Path(paths_raw.get("data_dir", "./data")).expanduser()
    models_dir = Path(paths_raw.get("models_dir", "./models")).expanduser()
    logs_dir = Path(paths_raw.get("logs_dir", "./logs")).expanduser()

    symbol = runtime_raw.get("symbol", "USDJPY")
    timeframe = runtime_raw.get("timeframe_exec", "M5")

    label_horizon = int(retrain_raw.get("label_horizon_bars", 10))
    min_pips = float(retrain_raw.get("min_pips", 1.0))
    n_splits = int(retrain_raw.get("wfo_n_splits", 4))

    thr_raw = retrain_raw.get("threshold_grid")
    if thr_raw is None:
        threshold_grid: list[float] | None = None
    else:
        threshold_grid = [float(x) for x in thr_raw]

    cfg = WeeklyRetrainConfig(
        paths=PathsConfig(
            data_dir=data_dir,
            models_dir=models_dir,
            logs_dir=logs_dir,
        ),
        retrain=RetrainConfig(
            symbol=symbol,
            timeframe=timeframe,
            label_horizon=label_horizon,
            min_pips=min_pips,
            n_splits=n_splits,
            threshold_grid=threshold_grid,
        ),
    )
    return cfg


# ------------------------
# データ & 特徴量
# ------------------------


def load_price_data(csv_path: Path) -> pd.DataFrame:
    """MT5 から書き出した価格CSVを読み込む。"""

    if not csv_path.exists():
        raise FileNotFoundError(f"価格CSVが見つかりません: {csv_path}")

    df = pd.read_csv(csv_path)
    if "time" not in df.columns:
        raise ValueError("CSV に 'time' 列がありません。")

    df["time"] = pd.to_datetime(df["time"])
    df = df.sort_values("time").reset_index(drop=True)

    # 列名のゆらぎに対応
    vol_col: str | None = None
    for cand in ("tick_volume", "volume", "vol"):
        if cand in df.columns:
            vol_col = cand
            break
    if vol_col is None:
        df["volume"] = 0.0
        vol_col = "volume"

    for col in ("open", "high", "low", "close"):
        if col not in df.columns:
            raise ValueError(f"CSV に '{col}' 列がありません。")

    return df[["time", "open", "high", "low", "close", vol_col]].rename(
        columns={vol_col: "volume"}
    )


def compute_rsi(close: pd.Series, period: int = 14) -> pd.Series:
    diff = close.diff()
    gain = diff.clip(lower=0)
    loss = -diff.clip(upper=0)
    avg_gain = gain.ewm(alpha=1 / period, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1 / period, adjust=False).mean()
    rs = avg_gain / (avg_loss + 1e-9)
    rsi = 100 - (100 / (1 + rs))
    return rsi


def compute_atr(
    high: pd.Series,
    low: pd.Series,
    close: pd.Series,
    period: int = 14,
) -> pd.Series:
    prev_close = close.shift(1)
    tr1 = high - low
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = tr.ewm(alpha=1 / period, adjust=False).mean()
    return atr


def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    非常にシンプルな特徴量セット。
    後で core/feature_pipeline.py に差し替えてもOK。
    """

    out = pd.DataFrame(index=df.index)

    out["ret_1"] = df["close"].pct_change()
    out["ret_5"] = df["close"].pct_change(5)
    out["ema_5"] = df["close"].ewm(span=5, adjust=False).mean()
    out["ema_20"] = df["close"].ewm(span=20, adjust=False).mean()
    out["ema_ratio"] = out["ema_5"] / (out["ema_20"] + 1e-9)

    out["rsi_14"] = compute_rsi(df["close"], period=14)
    out["atr_14"] = compute_atr(df["high"], df["low"], df["close"], period=14)

    out["range"] = (df["high"] - df["low"]) / (df["close"].shift(1) + 1e-9)
    out["vol_chg"] = df["volume"].pct_change().fillna(0.0)

    out = out.replace([np.inf, -np.inf], np.nan)
    out = out.dropna()
    return out


def build_labels(
    df: pd.DataFrame,
    horizon: int = 10,
    min_pips: float = 1.0,
) -> pd.Series:
    """
    horizon 足後の方向ラベルを作る。
    - USDJPY 前提で 1pips = 0.01 として計算。
    - 上昇(min_pips超) = 1, 下降(min_pips超) = 0
      それ以外（変化が小さい）は NaN にして除外。
    """

    future = df["close"].shift(-horizon)
    delta = future - df["close"]
    pips = delta * 100.0  # USDJPY 前提
    y = pd.Series(index=df.index, dtype="float32")
    y[pips >= min_pips] = 1.0
    y[pips <= -min_pips] = 0.0
    return y


def align_features_and_labels(
    feats: pd.DataFrame,
    labels: pd.Series,
) -> tuple[pd.DataFrame, pd.Series]:
    df = feats.join(labels.rename("y"), how="left")
    df = df.dropna()
    y = df.pop("y").astype(int)
    X = df
    return X, y


# ------------------------
# Walk-Forward 検証 & 学習
# ------------------------


@dataclass
class FoldResult:
    fold: int
    train_start: str
    train_end: str
    val_start: str
    val_end: str
    logloss: float
    accuracy: float
    n_train: int
    n_val: int


@dataclass
class WFOResult:
    folds: list[FoldResult]
    mean_logloss: float
    mean_accuracy: float


def iter_walkforward_indices(
    n_samples: int,
    n_splits: int,
) -> list[tuple[np.ndarray, np.ndarray]]:
    """
    非常にシンプルな walk-forward。
    - データは既に time でソートされている前提
    - n_splits+1 個のブロックに分割し、前方累積を train、次ブロックを val にする
    """

    if n_splits < MIN_WFO_SPLITS:
        raise ValueError("n_splits は最低 2 以上を推奨します。")

    block = n_samples // (n_splits + 1)
    indices = np.arange(n_samples, dtype=int)

    splits: list[tuple[np.ndarray, np.ndarray]] = []
    for k in range(n_splits):
        train_end = block * (k + 1)
        val_end = block * (k + 2)
        if val_end <= train_end:
            break
        train_idx = indices[:train_end]
        val_idx = indices[train_end:val_end]
        splits.append((train_idx, val_idx))
    return splits


def train_lightgbm_wfo(
    X: pd.DataFrame,
    y: pd.Series,
    cfg: RetrainConfig,
) -> Tuple[WFOResult, List[lgb.Booster], npt.NDArray[np.float64]]:
    params: dict[str, object] = {
        "objective": "binary",
        "metric": ["binary_logloss"],
        "learning_rate": 0.05,
        "num_leaves": 31,
        "max_depth": -1,
        "min_data_in_leaf": 50,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "verbosity": -1,
        "force_col_wise": True,
    }

    n = len(X)
    splits = iter_walkforward_indices(n, cfg.n_splits)

    oof_pred: npt.NDArray[np.float_] = np.full(
        shape=n,
        fill_value=np.nan,
        dtype="float32",
    )
    boosters: list[lgb.Booster] = []
    fold_results: list[FoldResult] = []

    for fold_idx, (tr_idx, va_idx) in enumerate(splits):
        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]
        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]

        train_data = lgb.Dataset(X_tr, label=y_tr)
        valid_data = lgb.Dataset(X_va, label=y_va)

        logger.info(
            f"[WFO] fold={fold_idx} train={len(X_tr)} val={len(X_va)} "
            f"from={tr_idx[0]} to={va_idx[-1]}"
        )

        booster = lgb.train(
            params,
            train_data,
            num_boost_round=500,
            valid_sets=[valid_data],
            valid_names=["valid"],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
            ],
        )

        boosters.append(booster)

        y_proba: npt.NDArray[np.float_] = booster.predict(
            X_va,
            num_iteration=booster.best_iteration,
        )
        oof_pred[va_idx] = y_proba.astype("float32")

        # メトリクス
        eps = 1e-15
        y_clipped: npt.NDArray[np.float_] = np.clip(
            y_proba,
            eps,
            1 - eps,
        )
        logloss = float(
            -np.mean(y_va * np.log(y_clipped) + (1 - y_va) * np.log(1 - y_clipped))
        )

        preds_label = (y_proba >= DEFAULT_CLASS_THRESHOLD).astype(int)
        acc = float(((y_va == preds_label).sum()) / len(y_va))

        fold_results.append(
            FoldResult(
                fold=fold_idx,
                train_start=str(tr_idx[0]),
                train_end=str(tr_idx[-1]),
                val_start=str(va_idx[0]),
                val_end=str(va_idx[-1]),
                logloss=logloss,
                accuracy=acc,
                n_train=int(len(X_tr)),
                n_val=int(len(X_va)),
            )
        )

        logger.info(f"[WFO] fold={fold_idx} logloss={logloss:.5f} acc={acc:.4f}")

    valid_mask = ~np.isnan(oof_pred)
    mean_logloss = float("nan")
    mean_accuracy = float("nan")
    if valid_mask.sum() > 0:
        y_valid_arr: npt.NDArray[np.int_] = y[valid_mask].to_numpy()
        p_valid: npt.NDArray[np.float_] = oof_pred[valid_mask]

        eps = 1e-15
        p_clip = np.clip(p_valid, eps, 1 - eps)
        mean_logloss = float(
            -np.mean(
                y_valid_arr * np.log(p_clip) + (1 - y_valid_arr) * np.log(1 - p_clip)
            )
        )
        preds_valid = (p_valid >= DEFAULT_CLASS_THRESHOLD).astype(int)
        mean_accuracy = float((y_valid_arr == preds_valid).sum() / len(y_valid_arr))

    wfo_result = WFOResult(
        folds=fold_results,
        mean_logloss=mean_logloss,
        mean_accuracy=mean_accuracy,
    )

    return wfo_result, boosters, oof_pred


# ------------------------
# しきい値最適化
# ------------------------


def optimize_threshold(
    y: pd.Series,
    oof_pred: npt.NDArray[np.float_],
    threshold_grid: list[float],
) -> dict[str, float]:
    """
    非常にシンプルな「1トレード +1 / -1」の疑似損益で最適なしきい値を決める。
    """

    valid_mask = ~np.isnan(oof_pred)
    y_valid_arr: npt.NDArray[np.int_] = y[valid_mask].to_numpy()
    p_valid: npt.NDArray[np.float_] = oof_pred[valid_mask]

    best_thr = DEFAULT_CLASS_THRESHOLD
    best_score = -1e9
    results: list[tuple[float, float, float]] = []

    for thr in threshold_grid:
        trade_mask = p_valid >= thr
        if trade_mask.sum() == 0:
            continue

        y_tr = y_valid_arr[trade_mask]
        pnl = np.where(y_tr == 1, 1.0, -1.0)
        equity = pnl.cumsum()
        total = float(equity[-1])
        winrate = float((pnl > 0).sum() / len(pnl))
        results.append((thr, total, winrate))

        if total > best_score:
            best_score = total
            best_thr = thr

    logger.info(
        "[THR] grid_results="
        + ", ".join(
            f"thr={thr:.3f} total={total:.1f} win={win:.3f}"
            for thr, total, win in results
        )
    )
    logger.info(f"[THR] best_thr={best_thr:.3f} equity={best_score:.1f}")

    return {
        "best_threshold": float(best_thr),
        "best_equity": float(best_score),
    }


# ------------------------
# モデル保存 & 署名
# ------------------------


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def save_model_and_meta(  # noqa: PLR0913  (引数多めでもここはOKとする)
    booster: lgb.Booster,
    cfg: WeeklyRetrainConfig,
    wfo_result: WFOResult,
    threshold_info: dict[str, float],
    feature_cols: list[str],
    data_info: dict[str, str],
) -> Path:
    cfg.paths.models_dir.mkdir(parents=True, exist_ok=True)

    ts = datetime.now(tz=UTC)
    ts_str = ts.strftime("%Y%m%d_%H%M%S")
    version = ts.timestamp()

    model_name = f"LightGBM_clf_{ts_str}.pkl"
    model_path = cfg.paths.models_dir / model_name

    dump(booster, model_path)

    sha = sha256_file(model_path)

    meta = {
        "model_name": "LightGBM_clf",
        "file": model_name,
        "created_at_utc": ts.isoformat(),
        "version": version,
        "symbol": cfg.retrain.symbol,
        "timeframe": cfg.retrain.timeframe,
        "label_horizon_bars": cfg.retrain.label_horizon,
        "min_pips": cfg.retrain.min_pips,
        "features": list(feature_cols),
        "wfo": {
            "mean_logloss": wfo_result.mean_logloss,
            "mean_accuracy": wfo_result.mean_accuracy,
            "folds": [asdict(f) for f in wfo_result.folds],
        },
        "threshold": threshold_info,
        "data": data_info,
        "sha256": sha,
    }

    meta_path = cfg.paths.models_dir / f"{model_name}.meta.json"
    with meta_path.open("w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    logger.info(f"[SAVE] model={model_path} sha256={sha}")
    logger.info(f"[SAVE] meta={meta_path}")

    active = {
        "model_name": "LightGBM_clf",
        "file": model_name,
        "meta_file": meta_path.name,
        "version": version,
        "best_threshold": threshold_info.get("best_threshold"),
    }
    active_path = cfg.paths.models_dir / "active_model.json"
    with active_path.open("w", encoding="utf-8") as f:
        json.dump(active, f, ensure_ascii=False, indent=2)

    logger.info(f"[SAVE] active_model={active_path}")
    return model_path


# ------------------------
# メイン処理
# ------------------------


def run_weekly_retrain(cfg: WeeklyRetrainConfig, dry_run: bool = False) -> None:
    paths = cfg.paths
    rt = cfg.retrain

    paths.logs_dir.mkdir(parents=True, exist_ok=True)
    log_file = (
        paths.logs_dir / f"weekly_retrain_{datetime.now().strftime('%Y%m%d')}.log"
    )
    logger.add(log_file, encoding="utf-8")

    logger.info(
        f"[CFG] symbol={rt.symbol} tf={rt.timeframe} label_horizon={rt.label_horizon}"
    )

    # config の symbol が "USDJPY-" でも、
    # 実データは data/USDJPY/ohlcv/USDJPY_M5.csv を読む
    symbol_dir = rt.symbol.replace("-", "")
    symbol_file = rt.symbol.replace("-", "")

    csv_path = (
        paths.data_dir / symbol_dir / "ohlcv" / f"{symbol_file}_{rt.timeframe}.csv"
    )

    logger.info(f"[STEP] load_price_data csv={csv_path}")
    df_prices = load_price_data(csv_path)
    logger.info(
        f"[STEP] loaded rows={len(df_prices)} "
        f"from={df_prices['time'].min()} to={df_prices['time'].max()}"
    )

    logger.info("[STEP] build_features")
    feats = build_features(df_prices)
    logger.info(f"[STEP] features shape={feats.shape}")

    logger.info("[STEP] build_labels")
    labels = build_labels(
        df_prices,
        horizon=rt.label_horizon,
        min_pips=rt.min_pips,
    )

    logger.info("[STEP] align_features_and_labels")
    X, y = align_features_and_labels(feats, labels)
    logger.info(
        f"[DATA] X={X.shape} y_pos={int((y == 1).sum())} y_neg={int((y == 0).sum())}"
    )

    if len(X) < 1000:
        logger.warning(
            "[WARN] 学習データが少なすぎます(1000行未満)。処理を中止します。"
        )
        return

    logger.info("[STEP] train_lightgbm_wfo")
    wfo_result, boosters, oof_pred = train_lightgbm_wfo(X, y, rt)
    logger.info(
        f"[WFO] mean_logloss={wfo_result.mean_logloss:.5f} "
        f"mean_acc={wfo_result.mean_accuracy:.4f}"
    )

    logger.info("[STEP] optimize_threshold")
    thr_info = optimize_threshold(y, oof_pred, rt.threshold_grid or [])

    if dry_run:
        logger.info(
            "[DRYRUN] dry-run 指定のためモデル保存/署名は行いません。ここで終了します。"
        )
        return

    logger.info("[STEP] train final model on all data")
    params: dict[str, object] = {
        "objective": "binary",
        "metric": ["binary_logloss"],
        "learning_rate": 0.05,
        "num_leaves": 31,
        "max_depth": -1,
        "min_data_in_leaf": 50,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "verbosity": -1,
        "force_col_wise": True,
    }
    train_all = lgb.Dataset(X, label=y)
    best_iters = [b.best_iteration or 200 for b in boosters]
    num_boost_round = int(np.median(best_iters))
    booster_all = lgb.train(
        params,
        train_all,
        num_boost_round=num_boost_round,
    )

    logger.info("[STEP] save_model_and_meta")
    data_info = {
        "csv_path": str(csv_path),
        "from": df_prices["time"].min().isoformat(),
        "to": df_prices["time"].max().isoformat(),
        "n_rows_raw": int(len(df_prices)),
        "n_rows_train": int(len(X)),
    }
    model_path = save_model_and_meta(
        booster=booster_all,
        cfg=cfg,
        wfo_result=wfo_result,
        threshold_info=thr_info,
        feature_cols=list(X.columns),
        data_info=data_info,
    )

    logger.info(f"[DONE] weekly retrain completed. model={model_path}")


def main() -> None:
    parser = argparse.ArgumentParser(description="週次自動再学習 (weekly_retrain)")
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="設定ファイルへのパス (default: configs/config.yaml)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="学習だけ行い、モデル保存や active_model 更新は行わない",
    )
    args = parser.parse_args()

    # デフォルト候補: configs/config.yaml
    default_config = Path("configs/config.yaml")
    config_path = Path(args.config) if args.config else default_config

    cfg = load_config(config_path)
    run_weekly_retrain(cfg, dry_run=args.dry_run)


if __name__ == "__main__":
    main()



=== file: scripts/weekly_wf.ps1 ===

# scripts/weekly_wf.ps1
param(
  [double]$MinAuc = 0.55
)

$ErrorActionPreference = "Stop"
Set-Location (Split-Path -Parent $PSCommandPath)

# 1) Compile
python -m compileall app core scripts | Out-Null

# 2) Train (WF)
python -m scripts.walkforward_train --csv *.csv --weeks-train 156 --weeks-valid 12 --min-auc $MinAuc

# 3) Swap to live (if new READY exists, it will be the latest)
python -m scripts.swap_model

# 4) Smoke (strict) to ensure it runs
powershell -ExecutionPolicy Bypass -File scripts\verify_smoke.ps1 -Mode strict -Ticks 300 -DtMs 20 -AtrPct 0.00080

Write-Host "[WEEKLY] done"



=== file: setup_weekly_job.ps1 ===

param(
  [string]$TaskName   = "FXBot_WeeklyRetrain",
  [string]$PythonExe  = "$env:USERPROFILE\AppData\Local\Programs\Python\Python313\python.exe",
  [string]$ProjectDir = "$env:USERPROFILE\OneDrive\fxbot",
  [ValidateSet("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")]
  [string]$DayOfWeek  = "Saturday",
  [string]$StartTime  = "03:05",
  [switch]$RunHighest = $true
)

$ErrorActionPreference = "Stop"

$Py = Resolve-Path -LiteralPath $PythonExe
$ScriptPath = Join-Path $ProjectDir "scripts\weekly_retrain.py"
if (!(Test-Path -LiteralPath $ScriptPath)) { throw "not found: $ScriptPath" }

$h,$m = $StartTime.Split(":")
$at = [DateTime]::Today.AddHours([int]$h).AddMinutes([int]$m)
$trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek $DayOfWeek -At $at

$action  = New-ScheduledTaskAction -Execute $Py -Argument "`"$ScriptPath`""

$principal = New-ScheduledTaskPrincipal -UserId $env:UserName -RunLevel ($(if ($RunHighest) {"Highest"} else {"Limited"}))

try { Unregister-ScheduledTask -TaskName $TaskName -Confirm:$false -ErrorAction SilentlyContinue } catch {}

Register-ScheduledTask -TaskName $TaskName -Action $action -Trigger $trigger -Principal $principal `
  -Description "Weekly retrain job for fxbot (LightGBM/XGB WFO + promote)"

Write-Host "`n[OK] Task '$TaskName' registered for $DayOfWeek $StartTime (JST)."
Write-Host "Python: $Py"
Write-Host "Script: $ScriptPath"



=== file: temp_equity_check.py ===

import pandas as pd, pathlib, numpy as np
p = pathlib.Path(r"D:\macht\OneDrive\fxbot\logs\backtest\USDJPY\M5\equity_curve.csv")
df = pd.read_csv(p)
print("[equity] cols:", df.columns.tolist())
sig = df.get("signal", pd.Series(0, index=df.index)).astype('int', errors='ignore')
chg = sig.ne(sig.shift(1)).fillna(sig.iloc[0] != 0)
print("signal unique:", sorted(pd.unique(sig)))
print("nonzero count:", int((sig != 0).sum()), "/", len(sig))
print("change points:", int(chg.sum()))
print("first rows:\n", df.head(3))



=== file: tests/test_sanity.py ===

# tests/test_sanity.py
def test_sanity() -> None:
    assert 1 + 1 == 2



=== file: To Doリスト.txt ===

最高。ここまで具体なら、あとは“実装順序”と“落とし穴回避”が鍵です。まずは仕様に少しだけ追記してから、ゼロ→運用までの作業リスト（DoD=完了条件つき）を一気に並べます。

# 追記しておくと安全になるルール（最小限の抜け漏れ）

* **プロセス設計**：GUI(メイン)／市場データ購読（非同期）／売買実行（逐次）／学習（別プロセス）の**分離**。学習・最適化は必ず別プロセスにしてGUIを固めない。
* **フェイルセーフ**：

  1. “本口座”ボタンは**二段階確認**＋`max_lot_cap`制限必須。
  2. **日次DDストップ**・**連敗ストップ**・**スリッページ急増**でサーキットブレーカー。
  3. **max_positions=1**をブローカー側の約定応答待ちロックで厳守（非同期競合対策）。
* **ロット算出の基準**：`lot = floor((equity/100000)*1)/100` ではなく**明示**：「有効証拠金 10,000 円で 0.01 lot、20,000 円で 0.02 lot…」＝ `lot = max(min_lot, round_down(equity/1_000_000, 2))`（USDJPY/標準口座想定。ブローカーの最小ロット・刻みに合わせて正規化）。
* **時刻軸**：全ログ・集計は**JST**基準、ブローカーサーバ時刻との差は統一ラッパで吸収。
* **データ品質**：MT5履歴の**ギャップ検知**・**重複バー除去**・**TZ補正**を前処理で強制。欠損 >0.5% なら学習中止。
* **モデル管理**：`models/`下に**署名（SHA256）**と**メタ情報（特徴量定義のハッシュ）**を保存。特徴量ズレ検知で**強制ブロック**。
* **ログ基準**：全トレードに**Decision Trace**（入力→各モデル予測→メタ判定→最終注文）をJSONで保存。再現性を確保。
* **Sakura VPS(2GB)**：LSTMは小型化（隠れ次元≤64、層≤2）＋バッチ学習は週末のみ。平時はLightGBM/XGBoostの推論中心。

---

# 全体アーキテクチャ（ざっくり）

* **UI層（PyQt6 + PyQtGraph + matplotlib）**
  Qtスレッド＝GUI専用。ZMQ/Queueでバックエンドの状態を購読。
* **バックエンド（Python 3.13）**

  * 市場データ購読：MT5 API → 内部Pub/Subバスへ
  * シグナル推論：前処理→特徴量→各モデル→メタモデル
  * 執行：注文ラッパ（リトライ、価格改善、約定イベント）
  * ロガー：構造化JSON + ローテーション
* **学習/バッチ**（週末）：データ抽出→特徴量→学習→WFO→最適化→モデル署名→配布
* **設定**：`config.yaml`（デモ／本番切替、リスク、戦略、学習パラメータ）＋`.env`（口座認証）

---

# 作業リスト（フェーズ別・完了条件つき）

## フェーズ0：環境/骨格（MVPの土台）

1. **Python 3.13 + MT5 + PyQt6 セットアップ**

   * 依存：`MetaTrader5`, `PyQt6`, `pyqtgraph`, `matplotlib`, `lightgbm`, `xgboost`, `numpy`, `pandas`, `scikit-learn`, `joblib`, `shap`, `pydantic`, `pyyaml`, `loguru`
   * **DoD**：`python -c "import MetaTrader5; print('ok')"`が通る。MT5接続テストOK（デモ口座）。

2. **リポジトリ雛形 & フォルダ構成**

   ```
   fxai/
     app/                 # GUI
     core/                # 取引/推論/執行
     data/                # キャッシュ・生成特徴量
     models/              # pkl/onnx + meta.json
     logs/                # 構造化ログ
     batch/               # 学習・WFO・最適化
     tests/               # pytest
     config.yaml
     .env.example
     README.md
   ```

   * **DoD**：`pytest -q`が最低限パス、`pre-commit`でblack/isort/ruff通過。

3. **設定ファイル雛形（デモ/本番切替）**

   ```yaml
   account:
     mode: demo           # demo | live
     server: "OANDA-XYZ"
   risk:
     max_positions: 1
     daily_loss_stop: 0.02
     dd_stop: 0.10
     slip_stop_pips: 2.0
     lot:
       min: 0.01
       step: 0.01
       formula: "scale_10000yen_per_0_01lot"
       max_cap: 0.50
   strategy:
     symbol: "USDJPY"
     timeframes: ["M15","H1"]   # Phase1
     entry_threshold: 0.60
     tp_sl:
       mode: "fixed"            # fixed | atr | ai
       tp_pips: 15
       sl_pips: 10
   features:
     ema: [10,20,50]
     rsi: [14]
     atr: [14]
     adx: [14]
     bbands: [20,2.0]
     lookback: 1000
   training:
     val_ratio: 0.2
     cv_folds: 5
     enable_lstm: false
   logging:
     level: "INFO"
     jst: true
   ```

   * **DoD**：`pydantic`等でバリデーションが通る。

## フェーズ1：GUIの骨格 + 口座/ポジション可視化

4. **Dashboardタブ（口座＋ポジション一覧）**

   * MT5口座情報を5秒更新、ポジションをテーブル表示（Buy=青/Sell=赤）
   * **DoD**：残高/Equity/含み損益/ポジ数/ロット合計が正確、色分けOK、NPEなし。
5. **Controlタブ（Start/Stop/学習/バックテストボタン、スライダー）**

   * 取引ON/OFFトグル、ロット／閾値スライダー、SL/TP入力
   * **DoD**：UI操作→バックエンド設定に即反映。誤操作ガード表示。

## フェーズ2：データ取得→特徴量→推論（M15+H1、固定TP/SL）

6. **履歴取得・前処理**

   * MT5からM15/H1のOHLCVをDL、ギャップ・重複処理、JST整列
   * **DoD**：欠損率<0.5%、ユニットテストで時系列一貫性OK。
7. **特徴量生成（共通）**

   * EMA/RSI/ATR/ADX/BB、変化率、出来高、ヒゲ比率など
   * **DoD**：`features.parquet`生成、列メタ（定義ハッシュ）保存。
8. **ラベリング**

   * 例：`y_dir`（10バー後の上昇/下降/横ばい）＋`y_pips`（回帰）
   * **DoD**：クラス分布・ターゲットリーク検査OK。
9. **個別モデル（LGB/XGB）学習 & 推論API**

   * まず**分類(LGB, XGB)**と**回帰(LGB)**。LSTMは無効で進む。
   * **DoD**：`predict_proba/predict_pips`がjsonableで返る、推論<5ms/サンプル（VPS）。
10. **メタモデル（ロジスティック or LGB）**

* 各モデルの出力→最終Buy/Sell/Skip
* **DoD**：検証AUC/Loglossが単体モデルを上回ること（最低わずかでも）。

## フェーズ3：執行レイヤ & サーキットブレーカー

11. **発注ラッパ（成行/SL/TP/約定イベント）**

* 価格ズレ再試行、ネットワーク/タイムアウト耐性
* **DoD**：サンドボックスでミニ発注→約定→クローズまで自動完了。

12. **max_positions=1の強制**

* **約定待ちロック**（in-flight注文がある間は新規禁止）
* **DoD**：ストレステストで同時信号でも2件目はブロックされる。

13. **ロット計算（有効証拠金スケール）**

* `lot = max(min_lot, round_to_step(equity/1_000_000, step))`、`<= max_cap`
* **DoD**：10,000円→0.01 lot、20,000円→0.02 lot…の確認テスト。

14. **サーキットブレーカー**

* 日次DD/総DD/スリッページ急増/連敗 N 回
* **DoD**：閾値到達で自動停止、UIに赤バナー表示、手動解除のみ再開。

## フェーズ4：AIタブ/Chartタブの“見える化”

15. **AIタブ**

* 勝率/PF/DD/連勝連敗/総取引、モデル名、最新学習日時、直近確率バー、FIランキング（LGB/XGB）、SHAP上位3
* **DoD**：最新の推論イベントからリアルタイム更新、SHAPはキャッシュ利用で即時描画。

16. **Chartタブ**

* エクイティ曲線、DD推移、時間帯×日付ヒートマップ、AI確率の時系列
* **DoD**：1日10,000点超でも滑らか（pyqtgraph使用）、CPU占有<40%。

## フェーズ5：出口の進化（ATR→AI最適化）

17. **固定TP/SL → ATR連動**

* 強トレンド時TP拡張、レンジ時短縮
* **DoD**：週次ABテストで基準よりPF↑かDD↓のいずれかを達成。

18. **AI出口（回帰pred_pips＋勢いスコア）**

* 目標TP/SLとトレーリング開始点をメタ最適化
* **DoD**：WFOで基準比 シャープ↑、または最大DD↓を確認。

## フェーズ6：運用評価・ドリフト・週末ジョブ

19. **KPI/SLOウォッチャー**

* 移動窓のPF/勝率/期待値/スリッページ等
* **DoD**：SLO割れで自動「縮小運転」へ（ロット1/2、または停止）。

20. **ドリフト検知**

* PSI/KL/KS＋性能（Brier/Logloss）
* **DoD**：警告→緊急再学習ジョブ起動（別プロセス）。

21. **週末バッチ（学習・WFO・最適化）**

* 土曜朝JST：学習→WFO→ベイズ最適→モデル署名→展開
* **DoD**：自動レポート（HTML/PDF）生成→Logタブ通知。

## フェーズ7：M5の追加→スタッキング完成

22. **M5特徴の追加（タイミング補完）**

* 短期EMA傾き/ヒゲ比/直近ATR 等
* **DoD**：エントリーの平均不利約定距離（slippage方向）改善、または平均保持時間短縮。

23. **M15+M5+H1のスタッキング**

* 3系の出力をメタで統合
* **DoD**：WFOでPhase2比の総合改善（PF or DD or Sharpe）。

---

# 主要モジュールと責務（実装時の地図）

* `core/mt5_client.py`：接続、相場購読、発注、約定イベント
* `core/position_guard.py`：max_positions、約定待ちロック、重複防止
* `core/risk_manager.py`：日次DD・総DD・連敗・スリッページ監視
* `core/feature_pipeline.py`：履歴DL→前処理→特徴量保存/ロード
* `core/models/`：`lgb_clf.py` `xgb_clf.py` `lgb_reg.py` `lstm.py` `meta_model.py`
* `core/inference.py`：各モデル推論→メタ判定→シグナル出力
* `core/execution.py`：注文生成、SL/TP設定、リトライ
* `core/logger.py`：構造化ログ、Decision Trace
* `app/gui.py`：PyQt6各タブ、スレッド安全な購読
* `batch/train.py`：学習、モデル保存、メタJSON出力（特徴量ハッシュ含む）
* `batch/walkforward.py`：期間スキーム、集計、DoD判定
* `batch/optimize.py`：ベイズ最適化（閾値/ATR係数/トレール係数）
* `tests/`：ユニット＋統合テスト（疑似MT5スタブ）

---

# Logタブのイベント規約（例）

```json
{
  "ts_jst":"2025-10-30T13:05:12",
  "type":"trade_open",
  "symbol":"USDJPY",
  "signal":{"p_buy":0.67,"p_sell":0.31,"meta":"BUY"},
  "features_hash":"a1b2c3...",
  "models":{"lgb_clf":0.64,"xgb_clf":0.62,"lgb_reg_pred_pips":8.3},
  "order":{"req_lot":0.03,"lot":0.03,"sl_pips":10,"tp_pips":15},
  "guard":{"inflight":false,"max_positions":1},
  "slippage":0.2
}
```

---

# ムード表示（AIステータス）

* ルール：直近N分の**ATR変化率**・**成行拒否率**・**トレンド強度(ADX)**で3状態を判定
  例：「市場は静穏」「ボラ急上昇注意」「トレンド反転警戒中」
* DoD：ムード状態が閾値でヒステリシス制御（ピコピコ切替防止）

---

# テスト計画（要点）

* **機能**：max_positions競合、ロット計算、DDストップ、発注リトライ、データギャップ処理
* **性能**：推論QPS、UI描画FPS、I/O待ち
* **回帰**：特徴量ハッシュが変わったら旧モデルを拒否
* **バックテスト**：WFOで`PF, Sharpe, MaxDD, Win%`の四天王を最低出力

---

# 省メモリ＆高速化メモ（2GB想定）

* pandasは`float32`/`int32`に縮小、特徴量は**学習用と推論用で列最小化**
* LightGBMは`num_leaves`と`feature_fraction`で軽量化
* SHAPは**説明用サンプルをサブセット**（上位最近2000件など）
* LSTMは**週末のみ**トレーニング、平時は凍結推論 or 無効

---

# 初回スプリント（1週間目の着地イメージ）

1. フェーズ0〜1を完了（GUI骨格＋口座/ポジ表示＋制御系）
2. フェーズ2の前処理〜単体LGB分類の推論まで通し
3. **デモ口座**で1日ペーパートレード運用→ログ検証

---

# 次に広げるなら

* 本番切替の**二重確認**
* 「失敗パターンのマイニング」から**自動フィルタ候補**生成
* 週末ジョブの**HTMLレポート**（モデル比較・KPIダッシュボード）

このリストで実装すれば、まずは**M15+H1×固定TP/SL**の安定稼働→徐々に**ATR/AI出口**→**M5統合のスタッキング**という、壊れにくい進化コースになります。次は「フェーズ0〜1のテンプレ（ひな形コード／`config.yaml`／GUIワイヤー）」を私から用意して、そのままVS Codeで走るところまで一気にいきましょう。



=== file: To Doリスト2.txt ===

最高。ここまで具体なら、あとは“実装順序”と“落とし穴回避”が鍵です。まずは仕様に少しだけ追記してから、ゼロ→運用までの作業リスト（DoD=完了条件つき）を一気に並べます。

# 追記しておくと安全になるルール（最小限の抜け漏れ）

* **プロセス設計**：GUI(メイン)／市場データ購読（非同期）／売買実行（逐次）／学習（別プロセス）の**分離**。学習・最適化は必ず別プロセスにしてGUIを固めない。
* **フェイルセーフ**：

  1. “本口座”ボタンは**二段階確認**＋`max_lot_cap`制限必須。
  2. **日次DDストップ**・**連敗ストップ**・**スリッページ急増**でサーキットブレーカー。
  3. **max_positions=1**をブローカー側の約定応答待ちロックで厳守（非同期競合対策）。
* **ロット算出の基準**：`lot = floor((equity/100000)*1)/100` ではなく**明示**：「有効証拠金 10,000 円で 0.01 lot、20,000 円で 0.02 lot…」＝ `lot = max(min_lot, round_down(equity/1_000_000, 2))`（USDJPY/標準口座想定。ブローカーの最小ロット・刻みに合わせて正規化）。
* **時刻軸**：全ログ・集計は**JST**基準、ブローカーサーバ時刻との差は統一ラッパで吸収。
* **データ品質**：MT5履歴の**ギャップ検知**・**重複バー除去**・**TZ補正**を前処理で強制。欠損 >0.5% なら学習中止。
* **モデル管理**：`models/`下に**署名（SHA256）**と**メタ情報（特徴量定義のハッシュ）**を保存。特徴量ズレ検知で**強制ブロック**。
* **ログ基準**：全トレードに**Decision Trace**（入力→各モデル予測→メタ判定→最終注文）をJSONで保存。再現性を確保。
* **Sakura VPS(2GB)**：LSTMは小型化（隠れ次元≤64、層≤2）＋バッチ学習は週末のみ。平時はLightGBM/XGBoostの推論中心。

---

# 全体アーキテクチャ（ざっくり）

* **UI層（PyQt6 + PyQtGraph + matplotlib）**
  Qtスレッド＝GUI専用。ZMQ/Queueでバックエンドの状態を購読。
* **バックエンド（Python 3.13）**

  * 市場データ購読：MT5 API → 内部Pub/Subバスへ
  * シグナル推論：前処理→特徴量→各モデル→メタモデル
  * 執行：注文ラッパ（リトライ、価格改善、約定イベント）
  * ロガー：構造化JSON + ローテーション
* **学習/バッチ**（週末）：データ抽出→特徴量→学習→WFO→最適化→モデル署名→配布
* **設定**：`config.yaml`（デモ／本番切替、リスク、戦略、学習パラメータ）＋`.env`（口座認証）

---

# 作業リスト（フェーズ別・完了条件つき）

## フェーズ0：環境/骨格（MVPの土台）

1. **Python 3.13 + MT5 + PyQt6 セットアップ**

   * 依存：`MetaTrader5`, `PyQt6`, `pyqtgraph`, `matplotlib`, `lightgbm`, `xgboost`, `numpy`, `pandas`, `scikit-learn`, `joblib`, `shap`, `pydantic`, `pyyaml`, `loguru`
   * **DoD**：`python -c "import MetaTrader5; print('ok')"`が通る。MT5接続テストOK（デモ口座）。

2. **リポジトリ雛形 & フォルダ構成**

   ```
   fxai/
     app/                 # GUI
     core/                # 取引/推論/執行
     data/                # キャッシュ・生成特徴量
     models/              # pkl/onnx + meta.json
     logs/                # 構造化ログ
     batch/               # 学習・WFO・最適化
     tests/               # pytest
     config.yaml
     .env.example
     README.md
   ```

   * **DoD**：`pytest -q`が最低限パス、`pre-commit`でblack/isort/ruff通過。

3. **設定ファイル雛形（デモ/本番切替）**

   ```yaml
   account:
     mode: demo           # demo | live
     server: "OANDA-XYZ"
   risk:
     max_positions: 1
     daily_loss_stop: 0.02
     dd_stop: 0.10
     slip_stop_pips: 2.0
     lot:
       min: 0.01
       step: 0.01
       formula: "scale_10000yen_per_0_01lot"
       max_cap: 0.50
   strategy:
     symbol: "USDJPY"
     timeframes: ["M15","H1"]   # Phase1
     entry_threshold: 0.60
     tp_sl:
       mode: "fixed"            # fixed | atr | ai
       tp_pips: 15
       sl_pips: 10
   features:
     ema: [10,20,50]
     rsi: [14]
     atr: [14]
     adx: [14]
     bbands: [20,2.0]
     lookback: 1000
   training:
     val_ratio: 0.2
     cv_folds: 5
     enable_lstm: false
   logging:
     level: "INFO"
     jst: true
   ```

   * **DoD**：`pydantic`等でバリデーションが通る。

## フェーズ1：GUIの骨格 + 口座/ポジション可視化

4. **Dashboardタブ（口座＋ポジション一覧）**

   * MT5口座情報を5秒更新、ポジションをテーブル表示（Buy=青/Sell=赤）
   * **DoD**：残高/Equity/含み損益/ポジ数/ロット合計が正確、色分けOK、NPEなし。
5. **Controlタブ（Start/Stop/学習/バックテストボタン、スライダー）**

   * 取引ON/OFFトグル、ロット／閾値スライダー、SL/TP入力
   * **DoD**：UI操作→バックエンド設定に即反映。誤操作ガード表示。

## フェーズ2：データ取得→特徴量→推論（M15+H1、固定TP/SL）

6. **履歴取得・前処理**

   * MT5からM15/H1のOHLCVをDL、ギャップ・重複処理、JST整列
   * **DoD**：欠損率<0.5%、ユニットテストで時系列一貫性OK。
7. **特徴量生成（共通）**

   * EMA/RSI/ATR/ADX/BB、変化率、出来高、ヒゲ比率など
   * **DoD**：`features.parquet`生成、列メタ（定義ハッシュ）保存。
8. **ラベリング**

   * 例：`y_dir`（10バー後の上昇/下降/横ばい）＋`y_pips`（回帰）
   * **DoD**：クラス分布・ターゲットリーク検査OK。
9. **個別モデル（LGB/XGB）学習 & 推論API**

   * まず**分類(LGB, XGB)**と**回帰(LGB)**。LSTMは無効で進む。
   * **DoD**：`predict_proba/predict_pips`がjsonableで返る、推論<5ms/サンプル（VPS）。
10. **メタモデル（ロジスティック or LGB）**

* 各モデルの出力→最終Buy/Sell/Skip
* **DoD**：検証AUC/Loglossが単体モデルを上回ること（最低わずかでも）。

## フェーズ3：執行レイヤ & サーキットブレーカー

11. **発注ラッパ（成行/SL/TP/約定イベント）**

* 価格ズレ再試行、ネットワーク/タイムアウト耐性
* **DoD**：サンドボックスでミニ発注→約定→クローズまで自動完了。

12. **max_positions=1の強制**

* **約定待ちロック**（in-flight注文がある間は新規禁止）
* **DoD**：ストレステストで同時信号でも2件目はブロックされる。

13. **ロット計算（有効証拠金スケール）**

* `lot = max(min_lot, round_to_step(equity/1_000_000, step))`、`<= max_cap`
* **DoD**：10,000円→0.01 lot、20,000円→0.02 lot…の確認テスト。

14. **サーキットブレーカー**

* 日次DD/総DD/スリッページ急増/連敗 N 回
* **DoD**：閾値到達で自動停止、UIに赤バナー表示、手動解除のみ再開。

## フェーズ4：AIタブ/Chartタブの“見える化”

15. **AIタブ**

* 勝率/PF/DD/連勝連敗/総取引、モデル名、最新学習日時、直近確率バー、FIランキング（LGB/XGB）、SHAP上位3
* **DoD**：最新の推論イベントからリアルタイム更新、SHAPはキャッシュ利用で即時描画。

16. **Chartタブ**

* エクイティ曲線、DD推移、時間帯×日付ヒートマップ、AI確率の時系列
* **DoD**：1日10,000点超でも滑らか（pyqtgraph使用）、CPU占有<40%。

## フェーズ5：出口の進化（ATR→AI最適化）

17. **固定TP/SL → ATR連動**

* 強トレンド時TP拡張、レンジ時短縮
* **DoD**：週次ABテストで基準よりPF↑かDD↓のいずれかを達成。

18. **AI出口（回帰pred_pips＋勢いスコア）**

* 目標TP/SLとトレーリング開始点をメタ最適化
* **DoD**：WFOで基準比 シャープ↑、または最大DD↓を確認。

## フェーズ6：運用評価・ドリフト・週末ジョブ

19. **KPI/SLOウォッチャー**

* 移動窓のPF/勝率/期待値/スリッページ等
* **DoD**：SLO割れで自動「縮小運転」へ（ロット1/2、または停止）。

20. **ドリフト検知**

* PSI/KL/KS＋性能（Brier/Logloss）
* **DoD**：警告→緊急再学習ジョブ起動（別プロセス）。

21. **週末バッチ（学習・WFO・最適化）**

* 土曜朝JST：学習→WFO→ベイズ最適→モデル署名→展開
* **DoD**：自動レポート（HTML/PDF）生成→Logタブ通知。

## フェーズ7：M5の追加→スタッキング完成

22. **M5特徴の追加（タイミング補完）**

* 短期EMA傾き/ヒゲ比/直近ATR 等
* **DoD**：エントリーの平均不利約定距離（slippage方向）改善、または平均保持時間短縮。

23. **M15+M5+H1のスタッキング**

* 3系の出力をメタで統合
* **DoD**：WFOでPhase2比の総合改善（PF or DD or Sharpe）。

---

# 主要モジュールと責務（実装時の地図）

* `core/mt5_client.py`：接続、相場購読、発注、約定イベント
* `core/position_guard.py`：max_positions、約定待ちロック、重複防止
* `core/risk_manager.py`：日次DD・総DD・連敗・スリッページ監視
* `core/feature_pipeline.py`：履歴DL→前処理→特徴量保存/ロード
* `core/models/`：`lgb_clf.py` `xgb_clf.py` `lgb_reg.py` `lstm.py` `meta_model.py`
* `core/inference.py`：各モデル推論→メタ判定→シグナル出力
* `core/execution.py`：注文生成、SL/TP設定、リトライ
* `core/logger.py`：構造化ログ、Decision Trace
* `app/gui.py`：PyQt6各タブ、スレッド安全な購読
* `batch/train.py`：学習、モデル保存、メタJSON出力（特徴量ハッシュ含む）
* `batch/walkforward.py`：期間スキーム、集計、DoD判定
* `batch/optimize.py`：ベイズ最適化（閾値/ATR係数/トレール係数）
* `tests/`：ユニット＋統合テスト（疑似MT5スタブ）

---

# Logタブのイベント規約（例）

```json
{
  "ts_jst":"2025-10-30T13:05:12",
  "type":"trade_open",
  "symbol":"USDJPY",
  "signal":{"p_buy":0.67,"p_sell":0.31,"meta":"BUY"},
  "features_hash":"a1b2c3...",
  "models":{"lgb_clf":0.64,"xgb_clf":0.62,"lgb_reg_pred_pips":8.3},
  "order":{"req_lot":0.03,"lot":0.03,"sl_pips":10,"tp_pips":15},
  "guard":{"inflight":false,"max_positions":1},
  "slippage":0.2
}
```

---

# ムード表示（AIステータス）

* ルール：直近N分の**ATR変化率**・**成行拒否率**・**トレンド強度(ADX)**で3状態を判定
  例：「市場は静穏」「ボラ急上昇注意」「トレンド反転警戒中」
* DoD：ムード状態が閾値でヒステリシス制御（ピコピコ切替防止）

---

# テスト計画（要点）

* **機能**：max_positions競合、ロット計算、DDストップ、発注リトライ、データギャップ処理
* **性能**：推論QPS、UI描画FPS、I/O待ち
* **回帰**：特徴量ハッシュが変わったら旧モデルを拒否
* **バックテスト**：WFOで`PF, Sharpe, MaxDD, Win%`の四天王を最低出力

---

# 省メモリ＆高速化メモ（2GB想定）

* pandasは`float32`/`int32`に縮小、特徴量は**学習用と推論用で列最小化**
* LightGBMは`num_leaves`と`feature_fraction`で軽量化
* SHAPは**説明用サンプルをサブセット**（上位最近2000件など）
* LSTMは**週末のみ**トレーニング、平時は凍結推論 or 無効

---

# 初回スプリント（1週間目の着地イメージ）

1. フェーズ0〜1を完了（GUI骨格＋口座/ポジ表示＋制御系）
2. フェーズ2の前処理〜単体LGB分類の推論まで通し
3. **デモ口座**で1日ペーパートレード運用→ログ検証

---

# 次に広げるなら

* 本番切替の**二重確認**
* 「失敗パターンのマイニング」から**自動フィルタ候補**生成
* 週末ジョブの**HTMLレポート**（モデル比較・KPIダッシュボード）

このリストで実装すれば、まずは**M15+H1×固定TP/SL**の安定稼働→徐々に**ATR/AI出口**→**M5統合のスタッキング**という、壊れにくい進化コースになります。次は「フェーズ0〜1のテンプレ（ひな形コード／`config.yaml`／GUIワイヤー）」を私から用意して、そのままVS Codeで走るところまで一気にいきましょう。



=== file: tools/__init__.py ===




=== file: tools/backtest_equity_curve.py ===

# tools/backtest_equity_curve.py
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))  # ←これを追加

import argparse, json, glob, os
from typing import Any
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from core.ai.loader import load_lgb_clf
from core.ai.features import build_features

def _load_active_meta() -> dict[str, Any]:
    p = "models/active_model.json"
    j = json.load(open(p, encoding="utf-8"))
    best_t = float(j.get("best_threshold", 0.2))
    lookahead = int(j.get("selected_lookahead", 15))
    return {"best_threshold": best_t, "lookahead": lookahead}

def _load_dataset(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    # 必要列: time/open/high/low/close/volume を想定
    if not set(["open","high","low","close","volume"]).issubset(df.columns):
        raise ValueError("CSVにOHLCV列が不足しています")
    df = build_features(df)  # 学習時と同じ拡張20列
    df = df.dropna().reset_index(drop=True)
    return df

def _ensure_feature_order(df: pd.DataFrame, model: Any) -> tuple[pd.DataFrame, list[str]]:
    # ラッパは model.expected_features を持たせてある想定（無ければ推定）
    feat = getattr(model, "expected_features", None)
    if feat is None:
        # モデル側に無い場合は、学習で使っていそうな20列を拾う（応急）
        candidates = ["open","high","low","close","volume",
                      "ret_1","ret_3","ret_5","ret_10",
                      "ret_std_10","ret_std_20",
                      "tr","atr_14","rsi_14","adx_14","bbp_20",
                      "upper_wick_ratio","lower_wick_ratio","body_ratio","vol_zscore_20"]
        feat = [c for c in candidates if c in df.columns]
    X = df[feat].copy()
    return X, feat

def run_backtest(csv_path: str, out_csv: str, init_equity: float = 100_000.0, show: bool = True) -> None:
    meta = _load_active_meta()
    best_t = float(meta.get("best_threshold", 0.2))
    L = int(meta.get("lookahead", 15))
    model = load_lgb_clf("models/LightGBM_clf.pkl")  # calib付きでロードされる
    df_raw = pd.read_csv(csv_path)
    df = _load_dataset(csv_path)
    X, feat = _ensure_feature_order(df, model)

    # (B) build_features → dropna 後に行数チェック
    # 窓を使う指標（RSI14/ATR14/ret_std_20 等）で冒頭がNaNになるため、
    # 短すぎるCSVだと0行になってしまう。足りない場合は早期終了して案内。
    if X.shape[0] == 0 or X.shape[0] < 40:
        print("[bt][error] too few rows after feature engineering (need ≈40+ rows). "
              "Your CSV is too short; provide more bars (100+ recommended).")
        return

    # 予測
    # (A) 列名付き DataFrame のまま渡す（学習時の列名を維持）
    proba = model.predict_proba(X)  # shape (n,2) を想定（[neg, pos]）
    # ここでは [p_sell, p_buy] ではなく [p0,p1]=[neg,pos] を buy=pos とみなす
    if proba.shape[1] == 2:
        p_buy = proba[:,1]
        p_sell = proba[:,0]
    else:
        # 2クラスでない場合の保険
        p_buy = proba.ravel()
        p_sell = 1.0 - p_buy

    closes = df["close"].to_numpy()
    n = len(df)

    equity = init_equity
    equity_curve = []
    pos = 0        # 0:ノーポジ, +1:買い, -1:売り
    entry_idx = -1
    entry_price = np.nan

    for i in range(n):
        # エグジット判定
        if pos != 0 and (i - entry_idx) >= L:
            exit_price = closes[i]
            ret = (exit_price/entry_price - 1.0) * (1 if pos>0 else -1)
            equity *= (1.0 + ret)
            pos = 0
            entry_idx = -1
            entry_price = np.nan

        # エントリー判定（ノーポジ時のみ）
        if pos == 0:
            if (p_buy[i] >= best_t) and (p_buy[i] > p_sell[i]):
                pos = +1
                entry_idx = i
                entry_price = closes[i]
            elif (p_sell[i] >= best_t) and (p_sell[i] > p_buy[i]):
                pos = -1
                entry_idx = i
                entry_price = closes[i]

        equity_curve.append(equity)

    out = pd.DataFrame({
        "time": df_raw.loc[df.index, "time"] if "time" in df_raw.columns else np.arange(n),
        "close": closes,
        "p_buy": p_buy,
        "p_sell": p_sell,
        "equity": equity_curve
    })
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    out.to_csv(out_csv, index=False, encoding="utf-8")
    print(f"[bt] wrote equity: {out_csv}  (final={equity_curve[-1]:.2f}, ret={(equity_curve[-1]/init_equity-1)*100:.2f}%)")

    if show:
        plt.figure(figsize=(9,4))
        plt.plot(out["equity"])
        plt.title(f"Equity Curve (start={init_equity:.0f} JPY, L={L}, thr={best_t})")
        plt.xlabel("bars")
        plt.ylabel("equity (JPY)")
        plt.tight_layout()
        plt.show()

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", default=None, help="バックテスト対象CSV。未指定なら data/*.csv の最新を使用")
    ap.add_argument("--out", default="logs/backtest/equity_curve.csv")
    ap.add_argument("--capital", type=float, default=100_000.0)
    ap.add_argument("--no-show", action="store_true")
    args = ap.parse_args()

    csv = args.csv or sorted(glob.glob("data/*.csv"))[-1]
    run_backtest(csv, args.out, init_equity=args.capital, show=not args.no_show)



=== file: tools/backtest_run.py ===

# tools/backtest_run.py
from __future__ import annotations
import argparse
from pathlib import Path
import json
import pandas as pd
import numpy as np
from datetime import timedelta
from app.strategies.ai_strategy import (
    load_active_model, build_features, predict_signals, trades_from_signals
)

PROJECT_ROOT = Path(__file__).resolve().parents[1]
LOG_DIR = PROJECT_ROOT / "logs" / "backtest"

# === equity utils ===
from dataclasses import dataclass

@dataclass
class Trade:
    entry_time: pd.Timestamp
    entry_price: float
    exit_time: pd.Timestamp
    exit_price: float
    direction: int     # +1 long, -1 short
    profit_jpy: float

def equity_from_bnh(df: pd.DataFrame, capital: float) -> pd.Series:
    """
    Buy&Hold（現物1倍）相当の指数エクイティ。close/close0でスケール。
    """
    close = df["close"].astype(float)
    idx = close / close.iloc[0]
    return capital * idx

def trades_from_signal_series(df: pd.DataFrame, sig: pd.Series,
                              lot: float = 0.1, contract_size: int = 100_000) -> list[Trade]:
    """
    signal（1/-1/0）からフリップ方式でトレード列を作る。
    - signal が 1→ロング保有、-1→ショート保有、0→ノーポジ
    - signal が変わった時点で前ポジをクローズ→新ポジを建てる
    - JPYペアを想定：損益[JPY] = (exit - entry) * direction * lot * contract_size
    """
    sig = sig.astype(int).reindex(df.index).fillna(0)
    close = df["close"].astype(float)
    times = pd.to_datetime(df["time"])

    cur_dir = 0
    cur_price = None
    cur_time = None
    out: list[Trade] = []

    for t, px, s in zip(times, close, sig):
        s = int(s)
        if cur_dir == 0:
            if s in (1, -1):
                cur_dir = s
                cur_price = px
                cur_time = t
        else:
            if s == cur_dir:
                continue
            # 方向が変わった/0になった → クローズ
            profit = (px - cur_price) * cur_dir * lot * contract_size
            out.append(Trade(cur_time, cur_price, t, px, cur_dir, profit))
            cur_dir = 0
            cur_price = None
            cur_time = None
            # 新規に建て直す（0でなければ）
            if s in (1, -1):
                cur_dir = s
                cur_price = px
                cur_time = t

    # 終端でオープン中なら、最終値でクローズしてしまう
    if cur_dir != 0:
        px = close.iloc[-1]
        t  = times.iloc[-1]
        profit = (px - cur_price) * cur_dir * lot * contract_size
        out.append(Trade(cur_time, cur_price, t, px, cur_dir, profit))

    return out

def equity_from_trades(df: pd.DataFrame, trades: list[Trade], capital: float) -> pd.Series:
    """
    トレード配列からエクイティ曲線を作る（逐次加算）。
    """
    eq = pd.Series(capital, index=pd.to_datetime(df["time"]))
    cum = capital
    i = 0
    for tr in trades:
        # 成立時刻で損益を反映
        while i < len(eq.index) and eq.index[i] <= tr.exit_time:
            if eq.index[i] == tr.exit_time:
                cum += tr.profit_jpy
            eq.iloc[i] = cum
            i += 1
    # 以降もラスト値で埋める
    while i < len(eq.index):
        eq.iloc[i] = cum
        i += 1
    return eq

def equity_from_trade_df(df_ohlcv: pd.DataFrame, trades_df: pd.DataFrame, capital: float) -> pd.Series:
    """
    trades_df 形式（DataFrame）から全バーに展開したエクイティ曲線を作る。
    必須カラム: exit_time, pnl
    任意: entry_time, entry_price, exit_price, direction（無くてもOK）
    """
    if trades_df is None or trades_df.empty:
        # 取引なしならフラット
        idx = pd.to_datetime(df_ohlcv["time"])
        return pd.Series(capital, index=idx)

    td = trades_df.copy()

    # 時刻カラムを時系列化（存在するものだけ）
    for col in ("entry_time", "exit_time"):
        if col in td.columns:
            td[col] = pd.to_datetime(td[col], errors="coerce")

    if "exit_time" not in td.columns:
        raise ValueError("trades_df に exit_time 列が必要です。")

    # pnl は数値化
    td["pnl"] = pd.to_numeric(td.get("pnl", 0.0), errors="coerce").fillna(0.0)

    # exit_time でグルーピングして、同時決済があれば合算
    pnl_by_exit = td.groupby("exit_time")["pnl"].sum().sort_index()

    # 全バーへ展開：exit_time のバーでのみ損益を加算、以降は前値でFFill
    idx = pd.to_datetime(df_ohlcv["time"])
    eq = pd.Series(capital, index=idx)
    cum = capital
    i = 0
    exit_times = pnl_by_exit.index.to_list()
    k = 0

    while i < len(idx):
        t = idx[i]
        # このバーの exit_time に決済があればすべて加算
        while k < len(exit_times) and exit_times[k] <= t:
            cum += float(pnl_by_exit.iloc[k])
            k += 1
        eq.iloc[i] = cum
        i += 1

    return eq


def to_equity(close: pd.Series, capital: float=100000.0) -> pd.DataFrame:
    close = close.astype(float)
    ret = close.pct_change().fillna(0.0)
    eq = (1.0 + ret).cumprod() * capital
    return pd.DataFrame({"time": close.index, "equity": eq.values})

def _max_consecutive(x: pd.Series, val: int) -> int:
    # 最大連続カウント（val=1を数える）
    c = 0; m = 0
    for v in x:
        if v == val:
            c += 1; m = max(m, c)
        else:
            c = 0
    return m

def _dd_duration_max(eq: pd.Series) -> int:
    """ドローダウン期間の最大日数を算出。時系列がintならスキップする。"""
    peak = -np.inf
    last_peak_time = None
    max_days = 0
    for t, v in eq.items():
        # t が datetime でない場合は飛ばす
        if not hasattr(t, "to_pydatetime") and not hasattr(t, "year"):
            continue
        if v > peak:
            peak = v
            last_peak_time = pd.Timestamp(t)
        else:
            if last_peak_time is not None:
                d = (pd.Timestamp(t) - last_peak_time).days
                if d > max_days:
                    max_days = d
    return int(max_days)


def metrics_from_equity(eq: pd.Series) -> dict:
    ret = eq.pct_change().fillna(0.0)
    total = eq.iloc[-1] / eq.iloc[0] - 1.0
    dd = (eq / eq.cummax() - 1.0).min()
    sharpe = (ret.mean() / (ret.std() + 1e-12)) * np.sqrt(252*24*12)  # M5相当の便宜スケール
    return {
      "start_equity": float(eq.iloc[0]),
      "end_equity": float(eq.iloc[-1]),
      "total_return": float(total),
      "max_drawdown": float(dd),
      "sharpe_like": float(sharpe),
      "bars": int(len(eq)),
      "max_dd_days": _dd_duration_max(eq),
    }

def monthly_returns_from_equity(eq_df: pd.DataFrame) -> pd.DataFrame:
    df = eq_df.copy()
    df = df.set_index(pd.to_datetime(df["time"]))
    m = df["equity"].resample("ME").last().pct_change().dropna()
    out = m.to_frame(name="m_return")
    out["year"] = out.index.year
    out["month"] = out.index.month
    return out.reset_index(drop=True)

def trades_from_buyhold(df: pd.DataFrame, capital: float) -> pd.DataFrame:
    # テンプレ：開始→終了の単一トレード（将来は戦略で複数トレードに差し替え）
    if df.empty:
        return pd.DataFrame(columns=["entry_time","exit_time","pnl","holding_bars","holding_days","win"])
    entry = df["time"].iloc[0]
    exit_  = df["time"].iloc[-1]
    close = df["close"].astype(float)
    ret = (close.iloc[-1] / close.iloc[0]) - 1.0
    pnl = capital * ret
    holding_bars = len(df)
    holding_days = (pd.Timestamp(exit_) - pd.Timestamp(entry)).days
    win = int(pnl > 0)
    return pd.DataFrame([{
        "entry_time": entry, "exit_time": exit_,
        "pnl": float(pnl),
        "holding_bars": int(holding_bars),
        "holding_days": int(holding_days),
        "win": win
    }])

def trade_metrics(trades: pd.DataFrame) -> dict:
    if trades.empty:
        return {
            "trades": 0, "win_rate": 0.0, "avg_pnl": 0.0, "profit_factor": 0.0,
            "avg_holding_bars": 0.0, "avg_holding_days": 0.0,
            "max_consec_win": 0, "max_consec_loss": 0
        }
    wins = trades["pnl"] > 0
    sum_win = trades.loc[wins, "pnl"].sum()
    sum_loss_abs = (-trades.loc[~wins, "pnl"]).clip(lower=0).sum()
    pf = float(sum_win / sum_loss_abs) if sum_loss_abs > 0 else float("inf") if sum_win > 0 else 0.0

    seq = wins.astype(int)
    consec_win = _max_consecutive(seq, 1)
    consec_loss = _max_consecutive(1 - seq, 1)

    return {
        "trades": int(len(trades)),
        "win_rate": float(wins.mean()) if len(trades) else 0.0,
        "avg_pnl": float(trades["pnl"].mean()) if len(trades) else 0.0,
        "profit_factor": pf,
        "avg_holding_bars": float(trades["holding_bars"].mean()) if len(trades) else 0.0,
        "avg_holding_days": float(trades["holding_days"].mean()) if len(trades) else 0.0,
        "max_consec_win": int(consec_win),
        "max_consec_loss": int(consec_loss),
    }

def slice_period(df: pd.DataFrame, start: str, end: str) -> pd.DataFrame:
    m = (df["time"] >= pd.Timestamp(start)) & (df["time"] <= pd.Timestamp(end))
    return df.loc[m].reset_index(drop=True)

def run_backtest(data_csv: Path, start: str, end: str, capital: float, out_dir: Path):
    print("[bt] start", flush=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    print(f"[bt] read_csv {data_csv}", flush=True)
    df = pd.read_csv(data_csv, parse_dates=["time"])
    print(f"[bt] slice {start} .. {end}", flush=True)
    df = slice_period(df, start, end)
    if df.empty:
        raise RuntimeError("No data in the requested period.")
    close = df["close"]; close.index = df["time"]

    print("[bt] equity compute", flush=True)
    eq_df = to_equity(close, capital)
    eq_df["signal"] = 0  # Buy&Holdなのでシグナル無し
    eq_csv = out_dir / "equity_curve.csv"
    print(f"[bt] write equity {eq_csv}", flush=True)
    eq_df.to_csv(eq_csv, index=False)

    # 月次損益
    mr = monthly_returns_from_equity(eq_df)
    mr.to_csv(out_dir / "monthly_returns.csv", index=False)

    # 仮トレード（Buy&Hold）
    trades = trades_from_buyhold(df, capital)
    trades.to_csv(out_dir / "trades.csv", index=False)

    # メトリクス
    base = metrics_from_equity(eq_df["equity"])
    tmet = trade_metrics(trades)
    base.update(tmet)
    (out_dir / "metrics.json").write_text(json.dumps(base, ensure_ascii=False, indent=2))

    print("[bt] done", flush=True)
    return eq_csv

def run_wfo(data_csv: Path, start: str, end: str, capital: float, out_dir: Path, train_ratio: float=0.7):
    print("[wfo] start", flush=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    print(f"[wfo] read_csv {data_csv}", flush=True)
    df = pd.read_csv(data_csv, parse_dates=["time"])
    print(f"[wfo] slice {start} .. {end}", flush=True)
    df = slice_period(df, start, end)
    if df.empty:
        raise RuntimeError("No data in the requested period.")
    n = len(df)
    n_tr = max(10, int(n*train_ratio))
    df_tr = df.iloc[:n_tr].reset_index(drop=True)
    df_ts = df.iloc[n_tr:].reset_index(drop=True)

    ##
    def _one(d: pd.DataFrame, name: str):
        print(f"[wfo] equity compute {name}", flush=True)
        feat = build_features(d, params={})

        # --- 必須列の補完 ---
        if "time" not in feat.columns:
            feat["time"] = pd.to_datetime(d["time"]).reset_index(drop=True)
        if "close" not in feat.columns:
            feat["close"] = pd.Series(d["close"].astype(float)).reset_index(drop=True)

        try:
            kind, payload, threshold, params = load_active_model()
            print(f"[wfo] using model: {payload} threshold={threshold}", flush=True)

            # 予測 → シグナル
            feat["signal"] = predict_signals(kind, payload, feat, threshold, params)
            signal_series = feat["signal"].astype(int).reset_index(drop=True)

            # ログで“出てるか”チェック
            nz = int((signal_series != 0).sum())
            print(f"[wfo] signals nonzero={nz} / {len(signal_series)}", flush=True)
            (out_dir / f"signals_{name}.csv").write_text(
                pd.DataFrame({"time": feat["time"], "signal": signal_series}).to_csv(index=False)
            )

            # トレード生成
            trades = trades_from_signals(feat, capital, params)

            # エクイティ展開（DataFrame でも list[Trade] でもOKにする）
            if isinstance(trades, pd.DataFrame):
                eq_series = equity_from_trade_df(feat, trades, capital)
            else:
                eq_series = equity_from_trades(feat, trades, capital)

            eq_df = pd.DataFrame({
                "time": eq_series.index,
                "equity": eq_series.values,
                "signal": signal_series.values
            })
        except Exception as e:
            print(f"[wfo] AI model not used ({e}) -> fallback to buy&hold", flush=True)
            close = d["close"]; close.index = d["time"]
            eq_df = to_equity(close, capital)
            eq_df["signal"] = 0
            trades = trades_from_buyhold(d, capital)

        p = out_dir / f"equity_{name}.csv"
        print(f"[wfo] write {p}", flush=True)
        eq_df.to_csv(p, index=False)
        trades.to_csv(out_dir / f"trades_{name}.csv", index=False)

        mr = monthly_returns_from_equity(eq_df)
        mr.to_csv(out_dir / f"monthly_returns_{name}.csv", index=False)
        m = metrics_from_equity(eq_df["equity"])
        m.update(trade_metrics(trades))
        return m
    m_tr = _one(df_tr, "train")
    m_ts = _one(df_ts, "test")

    # 可視化用に test をメインへコピー
    (out_dir / "equity_curve.csv").write_text((out_dir / "equity_test.csv").read_text())
    (out_dir / "metrics_wfo.json").write_text(json.dumps({"train": m_tr, "test": m_ts}, ensure_ascii=False, indent=2))

    print("[wfo] done", flush=True)
    return out_dir / "equity_curve.csv"

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True)
    ap.add_argument("--start", required=True)
    ap.add_argument("--end", required=True)
    ap.add_argument("--capital", type=float, default=100000.0)
    ap.add_argument("--mode", choices=["bt","wfo"], default="bt")
    ap.add_argument("--symbol", default="USDJPY")
    ap.add_argument("--timeframe", default="M5")
    ap.add_argument("--layout", choices=["flat","per-symbol"], default="per-symbol")
    ap.add_argument("--train-ratio", type=float, default=0.7)
    args = ap.parse_args()

    csv = Path(args.csv).resolve()
    out_dir = LOG_DIR / args.symbol / args.timeframe
    print(f"[main] mode={args.mode} csv={csv}", flush=True)
    if args.mode == "bt":
        p = run_backtest(csv, args.start, args.end, args.capital, out_dir)
    else:
        p = run_wfo(csv, args.start, args.end, args.capital, out_dir, train_ratio=args.train_ratio)
    print(str(p), flush=True)

if __name__ == "__main__":
    main()



=== file: tools/dump_feature_importance.py ===

# tools/dump_feature_importance.py
import csv
import glob
import json
import os
from datetime import datetime
from typing import Any, Iterable, Tuple

import joblib


def _load_latest_report() -> Tuple[str | None, dict[str, Any] | None]:
    rps = sorted(glob.glob(os.path.join("logs", "retrain", "report_*.json")))
    if not rps:
        return None, None
    rp = rps[-1]
    with open(rp, encoding="utf-8") as f:
        j = json.load(f)
    return rp, j


def _load_features_from_report(j: dict[str, Any]) -> list[str]:
    feats = j.get("features") or []
    return list(feats)


def _load_model(pkl_path: str) -> Any:
    return joblib.load(pkl_path)


def _write_feat_csv(model: Any, feat_cols: list[str], out_csv: str) -> str:
    try:
        booster = getattr(model, "booster_", None)
        if booster is None:
            split_importance = getattr(model, "feature_importances_", None)
            gain_importance = None
            names = feat_cols
        else:
            names = booster.feature_name()
            split_importance = booster.feature_importance(importance_type="split")
            gain_importance = booster.feature_importance(importance_type="gain")

        def _all_column_style(xs: Iterable[str]) -> bool:
            if not xs:
                return False
            return all(str(x).startswith("Column_") for x in xs)

        if not names or len(names) != len(feat_cols) or _all_column_style(names):
            names = feat_cols[:]

        rows: list[dict[str, float | str]] = []
        for i, name in enumerate(names):
            s = (
                float(split_importance[i])
                if (split_importance is not None and i < len(split_importance))
                else 0.0
            )
            g = (
                float(gain_importance[i])
                if (gain_importance is not None and i < len(gain_importance))
                else 0.0
            )
            rows.append({"feature": name, "gain": g, "split": s})
        rows.sort(key=lambda r: (r["gain"], r["split"]), reverse=True)

        os.makedirs(os.path.dirname(out_csv), exist_ok=True)
        with open(out_csv, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["feature", "gain", "split"])
            w.writeheader()
            for r in rows:
                w.writerow(r)
        return out_csv
    except Exception as e:
        print(f"[dump] failed: {e}")
        return ""


def main() -> None:
    # モデルは active_model.json or 既定の models/LightGBM_clf.pkl を使う
    active_meta = os.path.join("models", "active_model.json")
    if os.path.exists(active_meta):
        try:
            j = json.load(open(active_meta, encoding="utf-8"))
            model_path = (
                j.get("target_path")
                or j.get("source_path")
                or os.path.join("models", "LightGBM_clf.pkl")
            )
        except Exception:
            model_path = os.path.join("models", "LightGBM_clf.pkl")
    else:
        model_path = os.path.join("models", "LightGBM_clf.pkl")

    rp, rep = _load_latest_report()
    if rep is None:
        print("[dump] no reports found. specify features manually.")
        return
    feats = _load_features_from_report(rep)
    if not feats:
        print("[dump] features not found in report.")
        return

    model = _load_model(model_path)
    tag = "manual"
    if rep is not None:
        lh = (rep.get("lookahead") or {}).get("selected")
        tag = f"lk{lh}" if lh is not None else "manual"
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_csv = os.path.join("logs", "retrain", f"feat_importance_{tag}_{ts}.csv")
    out = _write_feat_csv(model, feats, out_csv)
    if out:
        print("[dump] wrote:", out)
        print("[dump] top5 preview:")
        with open(out, encoding="utf-8") as f:
            for i, line in enumerate(f):
                print(line.rstrip())
                if i >= 5:
                    break


if __name__ == "__main__":
    main()



=== file: tools/export_tree_clean.ps1 ===

# tools/export_tree_clean.ps1
param(
  [string]$Root = ".",
  [string]$OutFile = "project_tree.txt",
  [switch]$IncludeDirs = $true
)

$ErrorActionPreference = "Stop"
$Utf8NoBom = New-Object System.Text.UTF8Encoding($false)

# 完全に除外するディレクトリ名（必要なら追加）
$ExcludeNames = @(".git", ".venv", ".vscode", "__pycache__", ".mypy_cache", ".pytest_cache", ".ruff_cache", "site-packages", "node_modules", "dist", "build")

function PathHasExcludedSegment {
  param([string]$FullPath)
  $norm = ($FullPath -replace '\\','/').TrimEnd('/')
  # パスをセグメントに分割して **完全一致** で判定
  $segs = $norm -split '/'
  foreach ($seg in $segs) {
    foreach ($ex in $ExcludeNames) {
      if ($seg -eq $ex) { return $true }
    }
  }
  return $false
}

Write-Host "🌳 Exporting clean tree from: $Root"
Write-Host "🧹 Excluding: $($ExcludeNames -join ', ')"

$lines = New-Object System.Collections.Generic.List[string]

# まずディレクトリ→次にファイル、の順で列挙（順序が安定）
Get-ChildItem -LiteralPath $Root -Recurse -Force -Directory -ErrorAction SilentlyContinue |
  Where-Object { -not (PathHasExcludedSegment $_.FullName) } |
  ForEach-Object {
    $rel = Resolve-Path -LiteralPath $_.FullName -Relative
    $rel = ($rel -replace '^[.][\\/]', '') -replace '\\','/'
    if ($IncludeDirs -and $rel) { $lines.Add($rel) }
  }

Get-ChildItem -LiteralPath $Root -Recurse -Force -File -ErrorAction SilentlyContinue |
  Where-Object { -not (PathHasExcludedSegment $_.FullName) } |
  ForEach-Object {
    $rel = Resolve-Path -LiteralPath $_.FullName -Relative
    $rel = ($rel -replace '^[.][\\/]', '') -replace '\\','/'
    if ($rel) { $lines.Add($rel) }
  }

$lines = $lines | Sort-Object
[System.IO.File]::WriteAllLines($OutFile, $lines, $Utf8NoBom)
Write-Host "✅ Wrote $OutFile (UTF-8, no BOM). Count=$($lines.Count)"



=== file: tools/inspect_report.py ===

﻿import json, glob

rp = sorted(glob.glob("logs/retrain/report_*.json"))[-1]
with open(rp, encoding="utf-8") as f:
    j = json.load(f)

lk = j.get("lookahead", {})
print("report:", rp)
print("status:", j.get("status"))
print("selected_lookahead:", lk.get("selected"))
print("promote_thresholds:", j.get("promote_thresholds"))
print("metrics_test:", j.get("metrics_test"))
print("calibration:", j.get("calibration"))
print("candidates (lookahead -> f1@best, auc@cal):")
for c in lk.get("candidates", []):
    m = c["metrics_test"]
    print(f"  L={c['lookahead']}: f1@best={m.get('f1@best')}, auc@cal={m.get('auc@cal', m.get('auc'))}")



=== file: tools/train_lightgbm.py ===

# tools/train_lightgbm.py
from __future__ import annotations
from pathlib import Path
import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
from app.strategies.ai_strategy import build_features_recipe

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_ROOT / "data" / "USDJPY" / "ohlcv" / "USDJPY_M15.csv"
MODEL_DIR = PROJECT_ROOT / "models"
MODEL_DIR.mkdir(exist_ok=True, parents=True)

# =============================
# パラメータ設定
# =============================
LOOKAHEAD = 5  # 5本先の終値と比較して上昇しているかを分類
THRESH_PCT = 0.001  # 0.1%以上上昇を1とみなす

# =============================
# データ読み込み & 特徴量生成
# =============================
print(f"[train] load {DATA_PATH}")
df = pd.read_csv(DATA_PATH)
df["time"] = pd.to_datetime(df["time"])

# build_features_recipe() は ai_strategy.py にある既存関数
feat = build_features_recipe(df, "ohlcv_tech_v1")

# 目的変数：5バー後に0.1%以上上昇しているか
feat["target"] = (feat["close"].shift(-LOOKAHEAD) / feat["close"] - 1.0 > THRESH_PCT).astype(int)
feat = feat.dropna().reset_index(drop=True)

X = feat.drop(columns=["time", "target"])
y = feat["target"]

print(f"[train] samples={len(X)} features={X.shape[1]} pos_rate={y.mean():.3f}")

# =============================
# モデル学習
# =============================
params = dict(
    objective="binary",
    metric="binary_logloss",
    learning_rate=0.05,
    num_leaves=31,
    n_estimators=200,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
)

model = lgb.LGBMClassifier(**params)
model.fit(X, y)

# =============================
# モデル保存（pkl と Booster の二刀流）
# =============================
MODEL_PATH = MODEL_DIR / "LightGBM_clf.pkl"
BOOSTER_PATH = MODEL_DIR / "LightGBM_clf.txt"

# pkl（互換性重視）
joblib.dump(model, MODEL_PATH, compress=0, protocol=4)
print(f"[train] saved model (pkl) -> {MODEL_PATH}")

# booster テキスト（フォールバック用）
try:
    booster = model.booster_
    booster.save_model(str(BOOSTER_PATH))
    print(f"[train] saved model (booster txt) -> {BOOSTER_PATH}")
except Exception as e:
    print(f"[train] WARN: booster save failed: {e}")



=== file: tools/train_scaler.py ===

# tools/train_scaler.py
from pathlib import Path
import json
import pandas as pd
from sklearn.preprocessing import StandardScaler
from app.strategies.ai_strategy import build_features_recipe

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_ROOT / "data" / "USDJPY" / "ohlcv" / "USDJPY_M15.csv"
INFO_PATH = PROJECT_ROOT / "models" / "LightGBM_info.json"
OUT_DIR = PROJECT_ROOT / "models" / "scalers"
OUT_DIR.mkdir(parents=True, exist_ok=True)

print(f"[train_scaler] load {DATA_PATH}")
df = pd.read_csv(DATA_PATH)
feat = build_features_recipe(df, "ohlcv_tech_v1")

# 学習時の列順を取り出す
info = json.loads(INFO_PATH.read_text(encoding="utf-8"))
cols = info["features"]
missing = [c for c in cols if c not in feat.columns]
if missing:
    raise RuntimeError(f"[train_scaler] missing columns from build_features: {missing}")

X = feat.loc[:, cols].dropna()

scaler = StandardScaler()
scaler.fit(X.values)

import joblib
joblib.dump(scaler, OUT_DIR / "std_v1.pkl")
print(f"[train_scaler] saved: {OUT_DIR / 'std_v1.pkl'} (shape={X.shape})")



=== file: やるべきこと.txt ===

scripts/register_weekly_task.ps1
の実行
powershell -ExecutionPolicy Bypass -File scripts\register_weekly_task.ps1 -ProjectDir "C:\fxbot" -StartTime "03:05" -DayOfWeek Sunday


=== file: フェーズA~Hまでの結果.txt ===

フェーズB結果
変更内容

app/services/trade_service.py (line 49) で旧ロジックを廃し、decide_entry_from_probs を唯一の判定窓口としてラップするよう decide_entry を簡素化。生の設定値から prob_threshold / entry_min_edge / side_bias を取り出して ENTRY/SKIP 判定と side を返す新ロジックに差し替えました (app/services/trade_service.py (line 54))。
app/services/execution_stub.py (line 213) で AI 出力処理を統合ロジックへ移行。フィルタ通過後に decide_entry_from_probs を呼び出し、SKIP 時は理由を保持したまま即時 return。ENTRY 時はポジションガードを再確認し、許可されれば signal を組み立て、必要なら _build_exit_plan／_build_decision_exit_plan を利用して exit_plan を付与し、ログ・戻り値ともに dec 情報付きで返すよう更新。

PS D:\macht\OneDrive\fxbot> python -m compileall app core scripts
Listing 'app'...
Listing 'app\\core'...
Listing 'app\\data'...
Listing 'app\\gui'...
Listing 'app\\models'...
Listing 'app\\services'...
Listing 'core'...
Listing 'core\\ai'...
Listing 'core\\utils'...
Listing 'scripts'...


フェーズC結果
configs/config.yaml (line 25) で side_bias を "auto" に明示して統一し、min_edge: 0.0 # または entry_min_edge: 0.0 を追加してエッジ条件を開放しました。

フェーズD結果
scripts/dryrun_smoke.py 実行で decision: 'ENTRY'（SELL側）を確認済み。該当ログは logs/decisions/decisions_USDJPY-.jsonl に追記されています。
ログ検証
Get-Content ... | Select-String '"decision": "ENTRY"' にて ENTRY 行を検出。直近では SELL エントリーが記録。
"meta": "BUY" 出現数: 20、"meta": "SELL" 出現数: 20。
補足対応
app/services/trade_service.py (line 47) で設定読み込みを都度行うようにし、core.config への依存を解消。
scripts/dryrun_smoke.py (line 23) と  (line 67) 付近で trade_state.update(...) を追加し、スモーク実行時にも取引許可・閾値が反映されるよう調整。

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 17:31:06.113 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 17:31:06.243 | INFO     | app.services.execution_stub:_emit:183 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'ENTRY', 'reason': None, 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': '1a32d184aa294d6463e0c7ac60fb4d195a9485084626b92dcd20d0b97c4fec63', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SELL'}, 'filters': {'spread': 0.6, 'spread_limit': 1.5, 'adx': 31.46339, 'min_adx': 15.0, 'adx_disabled': True, 'atr_pct': 0.00048845, 'min_atr_pct': 0.0004, 'prob_threshold': 0.5, 'side_bias': 'auto', 'blocked': None}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"decision": "ENTRY"'

{"ts_jst": "2025-10-31T17:28:23+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.1, "spread_limit": 1. 
5, "adx": 32.31987, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00049188, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": "pos_guard"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SELL", "threshold": 0.5, "decision": {"act 
ion": "BLOCKED", "reason": "pos_guard", "ai_meta": "SELL", "dec 
": {"decision": "ENTRY", "meta": "SELL", "side": "SELL", "thres 
hold": 0.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + 
 calib[isotonic]", "version": "1761815421.0", "features_hash":  
"6acc2276334fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cb 
e0", "p_buy": 0.49677207571944404, "p_sell": 0.503227924280556, 
 "p_skip": 0.49677207571944404, "meta": "SELL"}, "cb": {"trippe 
d": false, "reason": null, "consecutive_losses": 0, "threshold_ 
losses": 5, "reset_marker": null}, "features_hash": "6acc227633 
4fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cbe0", "model 
": "LightGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 
 0.10000000000047748, "spread_limit_pips": 1.5, "min_adx": 15.0 
, "disable_adx_gate": true, "min_atr_pct": 0.0004, "prob_thresh 
old": 0.5, "side_bias": "auto", "max_positions": 1, "ai_thresho 
ld": 0.5}}
{"ts_jst": "2025-10-31T17:30:06+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 31.42594, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00046944, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": null}, "probs": {"buy": 0.496 
772, "sell": 0.503228, "skip": 0.496772}, "calibrator": "isoton 
ic", "meta": "SELL", "threshold": 0.5, "decision": {"action": " 
ENTRY", "reason": null, "ai_meta": "SELL", "signal": {"side": " 
SELL", "prob": 0.503227924280556, "meta": "SELL"}, "dec": {"dec 
ision": "ENTRY", "meta": "SELL", "side": "SELL", "threshold": 0 
.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + calib[i 
sotonic]", "version": "1761815421.0", "features_hash": "02928ba 
944155c7912993a4f55f6eac9025b1d4941b7321330edacf7de11f167", "p_ 
buy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_skip 
": 0.49677207571944404, "meta": "SELL"}, "cb": {"tripped": fals 
e, "reason": null, "consecutive_losses": 0, "threshold_losses": 
 5, "reset_marker": null}, "features_hash": "02928ba944155c7912 
993a4f55f6eac9025b1d4941b7321330edacf7de11f167", "model": "Ligh 
tGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 0.60000 
00000000227, "spread_limit_pips": 1.5, "min_adx": 15.0, "disabl 
e_adx_gate": true, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "max_positions": 1, "ai_threshold": 0.5} 
}
{"ts_jst": "2025-10-31T17:31:06+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 31.46339, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00048845, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": null}, "probs": {"buy": 0.496 
772, "sell": 0.503228, "skip": 0.496772}, "calibrator": "isoton 
ic", "meta": "SELL", "threshold": 0.5, "decision": {"action": " 
ENTRY", "reason": null, "ai_meta": "SELL", "signal": {"side": " 
SELL", "prob": 0.503227924280556, "meta": "SELL"}, "dec": {"dec 
ision": "ENTRY", "meta": "SELL", "side": "SELL", "threshold": 0 
.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + calib[i 
sotonic]", "version": "1761815421.0", "features_hash": "1a32d18 
4aa294d6463e0c7ac60fb4d195a9485084626b92dcd20d0b97c4fec63", "p_ 
buy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_skip 
": 0.49677207571944404, "meta": "SELL"}, "cb": {"tripped": fals 
e, "reason": null, "consecutive_losses": 0, "threshold_losses": 
 5, "reset_marker": null}, "features_hash": "1a32d184aa294d6463 
e0c7ac60fb4d195a9485084626b92dcd20d0b97c4fec63", "model": "Ligh 
tGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 0.60000 
00000000227, "spread_limit_pips": 1.5, "min_adx": 15.0, "disabl 
e_adx_gate": true, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "max_positions": 1, "ai_threshold": 0.5} 
}

PS D:\macht\OneDrive\fxbot> (Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"meta": "BUY"').Count       
20
PS D:\macht\OneDrive\fxbot> (Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"meta": "SELL"').Count      
21


フェーズE結果
configs/config.yaml (line 26) で prob_threshold を 0.62（コメントは英語に置換）へ、configs/config.yaml (line 30) で min_edge を 0.02 に設定し、configs/config.yaml (lines 53-54) で ADXゲートを有効 (adx_disable: false)・ATRフィルタを 0.0006 に引き上げました。
.venv\Scripts\python.exe -m scripts.dryrun_smoke 実行は WARNING ログとなり、フィルタ判定で ATR 下限 (atr_low) により ENTRY はブロックされました（AIメタは SKIP）。
ログ検証：Select-String '"blocked": "atr_low"' は過去分と今回分を検出、"blocked": "adx_low" はヒットなしでした。

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 17:35:26.183 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 17:35:26.289 | WARNING  | app.services.execution_stub:_emit:179 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': '285310bf40fa5081802422eded472a7c034186013956c3e95a54a401b5d84c52', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SKIP'}, 'filters': {'spread': 0.6, 'spread_limit': 1.5, 'adx': 30.66809, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00047144, 'min_atr_pct': 0.0006, 'prob_threshold': 0.62, 'side_bias': 'auto', 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}  

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"blocked": "atr_low"'        

{"ts_jst": "2025-10-31T16:05:51+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 33.68549, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00048653, "min_atr_pct": 0.02, "prob_threshold": 0.55, 
 "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy": 0 
.502041, "sell": 0.497959, "skip": 0.497959}, "calibrator": "is 
otonic", "meta": "SKIP", "threshold": 0.55, "decision": {"actio 
n": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "Ligh 
tGBM_clf + calib[isotonic]", "version": "1761815421.0", "featur 
es_hash": "e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e 
6a96912863e1", "p_buy": 0.5020408163265306, "p_sell": 0.4979591 
8367346936, "p_skip": 0.49795918367346936, "meta": "SKIP"}, "cb 
": {"tripped": false, "reason": null, "consecutive_losses": 0,  
"threshold_losses": 5, "reset_marker": null}, "features_hash":  
"e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e6a96912863 
e1", "model": "LightGBM_clf + calib[isotonic]", "runtime": {"sp 
read_pips": 0.49999999999954525, "spread_limit_pips": 1.5, "min 
_adx": 15.0, "disable_adx_gate": true, "min_atr_pct": 0.02, "pr 
ob_threshold": 0.55, "side_bias": "auto", "max_positions": 1, " 
ai_threshold": 0.55}}
{"ts_jst": "2025-10-31T17:34:14+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.2, "spread_limit": 1. 
5, "adx": 31.46339, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00049362, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "303e2ece8a2138840f6be93a86da70447f176216708018e2d 
4c5ce3db35a5e0e", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "303e2ece8a2138840f6be93a86da70447f176216708018e2d4c5ce3db35a 
5e0e", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.20000000000095497, "spread_limit_pips": 1.5, "m 
in_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006 
, "prob_threshold": 0.62, "side_bias": "auto", "max_positions": 
 1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:35:26+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 30.66809, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00047144, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "285310bf40fa5081802422eded472a7c034186013956c3e95 
a54a401b5d84c52", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "285310bf40fa5081802422eded472a7c034186013956c3e95a54a401b5d8 
4c52", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.6000000000000227, "spread_limit_pips": 1.5, "mi 
n_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006, 
 "prob_threshold": 0.62, "side_bias": "auto", "max_positions":  
1, "ai_threshold": 0.62}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"blocked": "adx_low"' 

フェーズG結果
対応内容

configs/config.yaml (line 26) でエントリーしきい値を本番寄りに戻しつつ、exits ブロック（固定/ATR/トレーリングの各パラメータ）を追加しました。
core/indicators.py を新規作成し、true_range とシンプルな ATR 計算ヘルパを提供。
app/services/trade_service.py (line 8) に _atr を取り込み、build_exit_plan（同:86）で exits 設定から固定・ATR・none の計画を生成するよう実装。
app/services/execution_stub.py (line 64) でトレースに exit_plan を含め、:254 以降で trade_service.build_exit_plan を用いたシグナル組み立てとフォールバック処理を追加。
確認

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Select-String -Path logs\decisions\decisions_*.jsonl -Pattern '"exit_plan"' で JSONL に exit_plan が出力されることを確認。

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String -Pattern '"exit_plan": {'
>> 

{"ts_jst": "2025-10-31T17:41:35+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.4, "spread_limit": 1.
5, "adx": 29.75924, "min_adx": 15.0, "adx_disabled": false, "at
r_pct": 0.00046644, "min_atr_pct": 0.0006, "prob_threshold": 0.
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy"
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator": 
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea
tures_hash": "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b46601
3696a29fe67c604", "p_buy": 0.49677207571944404, "p_sell": 0.503
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, "
cb": {"tripped": false, "reason": null, "consecutive_losses": 0
, "threshold_losses": 5, "reset_marker": null}, "features_hash"
: "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b466013696a29fe67
c604", "model": "LightGBM_clf + calib[isotonic]", "exit_plan":  
{"mode": "none"}, "runtime": {"spread_pips": 0.4000000000019099 
4, "spread_limit_pips": 1.5, "min_adx": 15.0, "disable_adx_gate 
": false, "min_atr_pct": 0.0006, "prob_threshold": 0.62, "side_ 
bias": "auto", "max_positions": 1, "ai_threshold": 0.62}}  


フェーズH
集計結果

DECISIONS: Counter({'BLOCKED': 18, 'BUY': 17, 'SKIP': 15, 'SELL': 12, 'ENTRY': 2})
REASONS: Counter({'ai_threshold': 29, 'ai_skip': 15, 'consecutive_losses(5)>=threshold(5)': 9, 'atr_low': 5, 'test_trip': 3, 'pos_guard': 1})
ENTRY は decisions_USDJPY.jsonl 内で 2 件。decisions_USDJPY-.jsonl 側は 0 件でした。

次のステップ

ENTRY を増やすためには一時的に entry.prob_threshold を 0.49 へ下げるか min_edge: 0.0 に戻して挙動を確認してください。





=== file: 仮想環境.txt ===

.\.venv\Scripts\Activate.ps1



=== file: 作業6～8.txt ===

作業6 結果
configs/config.yaml (line 54) に atr_hysteresis を追加し、ATRゲート用の入場・解除閾値とロールバック長を設定できるようにしました。
app/services/execution_stub.py 冒頭で deque/statistics を導入し、_atr_gate_ok（中央値ヒステリシス付き）を実装、ATR判定部をそのゲートで置き換えました。ログには atr_ref・atr_gate_state・atr_enable_min・atr_disable_min を出力するよう調整済みです。runtime_payload へ filters 情報を渡し、ゲートが設定値を参照できるようにもしています。
検証

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"'
最新スモークでは atr_ref ≈ 0.00042 で atr_gate_state が closed のままなため atr_low ブロックが発生しましたが、ロールング中央値が enable_min_pct (0.00055) を超えればゲートが開放され、ENTRY チャンスが戻る想定です。必要に応じて atr_hysteresis の値調整や追加データで挙動を確認してください。

PS D:\macht\OneDrive\fxbot> python -m compileall app core scripts
Listing 'app'...
Listing 'app\\core'...
Listing 'app\\data'...
Listing 'app\\gui'...
Listing 'app\\models'...
Listing 'app\\services'...
Listing 'core'...
Listing 'core\\ai'...
Listing 'core\\utils'...
Listing 'scripts'...

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 17:57:01.763 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 17:57:01.876 | WARNING  | app.services.execution_stub:_emit:246 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': 'f43d5483282d4351a91740f83541dbff9ebca276686ed0016fda4fafec290039', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SKIP'}, 'filters': {'spread': 0.5, 'spread_limit': 1.5, 'adx': 25.93729, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00043559, 'min_atr_pct': 0.0006, 'prob_threshold': 0.62, 'side_bias': 'auto', 'atr_ref': 0.00043559, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"'

{"ts_jst": "2025-10-31T16:05:51+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 33.68549, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00048653, "min_atr_pct": 0.02, "prob_threshold": 0.55, 
 "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy": 0 
.502041, "sell": 0.497959, "skip": 0.497959}, "calibrator": "is 
otonic", "meta": "SKIP", "threshold": 0.55, "decision": {"actio 
n": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "Ligh 
tGBM_clf + calib[isotonic]", "version": "1761815421.0", "featur 
es_hash": "e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e 
6a96912863e1", "p_buy": 0.5020408163265306, "p_sell": 0.4979591 
8367346936, "p_skip": 0.49795918367346936, "meta": "SKIP"}, "cb 
": {"tripped": false, "reason": null, "consecutive_losses": 0,  
"threshold_losses": 5, "reset_marker": null}, "features_hash":  
"e96f3ba5fe964f26dcbb972e863aede247ee60084cef42a9b39e6a96912863 
e1", "model": "LightGBM_clf + calib[isotonic]", "runtime": {"sp 
read_pips": 0.49999999999954525, "spread_limit_pips": 1.5, "min 
_adx": 15.0, "disable_adx_gate": true, "min_atr_pct": 0.02, "pr 
ob_threshold": 0.55, "side_bias": "auto", "max_positions": 1, " 
ai_threshold": 0.55}}
{"ts_jst": "2025-10-31T17:34:14+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.2, "spread_limit": 1. 
5, "adx": 31.46339, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00049362, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "303e2ece8a2138840f6be93a86da70447f176216708018e2d 
4c5ce3db35a5e0e", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "303e2ece8a2138840f6be93a86da70447f176216708018e2d4c5ce3db35a 
5e0e", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.20000000000095497, "spread_limit_pips": 1.5, "m 
in_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006 
, "prob_threshold": 0.62, "side_bias": "auto", "max_positions": 
 1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:35:26+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 30.66809, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00047144, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "285310bf40fa5081802422eded472a7c034186013956c3e95 
a54a401b5d84c52", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "285310bf40fa5081802422eded472a7c034186013956c3e95a54a401b5d8 
4c52", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.6000000000000227, "spread_limit_pips": 1.5, "mi 
n_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006, 
 "prob_threshold": 0.62, "side_bias": "auto", "max_positions":  
1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:40:31+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.6, "spread_limit": 1. 
5, "adx": 29.84356, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00046456, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "ef03c983519f3fa491ff38083c2450a80a4e24f1fc8774135 
8d935e056dab66d", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "ef03c983519f3fa491ff38083c2450a80a4e24f1fc87741358d935e056da 
b66d", "model": "LightGBM_clf + calib[isotonic]", "runtime": {" 
spread_pips": 0.6000000000000227, "spread_limit_pips": 1.5, "mi 
n_adx": 15.0, "disable_adx_gate": false, "min_atr_pct": 0.0006, 
 "prob_threshold": 0.62, "side_bias": "auto", "max_positions":  
1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:41:35+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.4, "spread_limit": 1. 
5, "adx": 29.75924, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00046644, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "blocked": "atr_low"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SKIP", "threshold": 0.62, "decision": {"ac 
tion": "BLOCKED", "reason": "atr_low"}, "ai": {"model_name": "L 
ightGBM_clf + calib[isotonic]", "version": "1761815421.0", "fea 
tures_hash": "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b46601 
3696a29fe67c604", "p_buy": 0.49677207571944404, "p_sell": 0.503 
227924280556, "p_skip": 0.49677207571944404, "meta": "SKIP"}, " 
cb": {"tripped": false, "reason": null, "consecutive_losses": 0 
, "threshold_losses": 5, "reset_marker": null}, "features_hash" 
: "711291d6ce7efb7e283a233b3cf1cbfc9b791e07e91b466013696a29fe67 
c604", "model": "LightGBM_clf + calib[isotonic]", "exit_plan":  
{"mode": "none"}, "runtime": {"spread_pips": 0.4000000000019099 
4, "spread_limit_pips": 1.5, "min_adx": 15.0, "disable_adx_gate 
": false, "min_atr_pct": 0.0006, "prob_threshold": 0.62, "side_ 
bias": "auto", "max_positions": 1, "ai_threshold": 0.62}}       
{"ts_jst": "2025-10-31T17:55:35+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 25.93729, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.0004221, "min_atr_pct": 0.0006, "prob_threshold": 0.6 
2, "side_bias": "auto", "atr_ref": 0.0004221, "atr_gate_state": 
 "closed", "atr_enable_min": 0.00055, "atr_disable_min": 0.0004 
5, "blocked": "atr_low"}, "probs": {"buy": 0.496772, "sell": 0. 
503228, "skip": 0.496772}, "calibrator": "isotonic", "meta": "S 
KIP", "threshold": 0.62, "decision": {"action": "BLOCKED", "rea 
son": "atr_low"}, "ai": {"model_name": "LightGBM_clf + calib[is 
otonic]", "version": "1761815421.0", "features_hash": "6eab60cd 
e26cbccacdf8c2042aa19b8894099c914f9b3586116ee35e63abb842", "p_b 
uy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_skip" 
: 0.49677207571944404, "meta": "SKIP"}, "cb": {"tripped": false 
, "reason": null, "consecutive_losses": 0, "threshold_losses":  
5, "reset_marker": null}, "features_hash": "6eab60cde26cbccacdf 
8c2042aa19b8894099c914f9b3586116ee35e63abb842", "model": "Light 
GBM_clf + calib[isotonic]", "exit_plan": {"mode": "none"}, "run 
time": {"spread_pips": 0.49999999999954525, "spread_limit_pips" 
: 1.5, "min_adx": 15.0, "disable_adx_gate": false, "min_atr_pct 
": 0.0006, "prob_threshold": 0.62, "side_bias": "auto", "max_po 
sitions": 1, "ai_threshold": 0.62}}
{"ts_jst": "2025-10-31T17:57:01+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.5, "spread_limit": 1. 
5, "adx": 25.93729, "min_adx": 15.0, "adx_disabled": false, "at 
r_pct": 0.00043559, "min_atr_pct": 0.0006, "prob_threshold": 0. 
62, "side_bias": "auto", "atr_ref": 0.00043559, "atr_gate_state 
": "closed", "atr_enable_min": 0.00055, "atr_disable_min": 0.00 
045, "blocked": "atr_low"}, "probs": {"buy": 0.496772, "sell":  
0.503228, "skip": 0.496772}, "calibrator": "isotonic", "meta":  
"SKIP", "threshold": 0.62, "decision": {"action": "BLOCKED", "r 
eason": "atr_low"}, "ai": {"model_name": "LightGBM_clf + calib[ 
isotonic]", "version": "1761815421.0", "features_hash": "f43d54 
83282d4351a91740f83541dbff9ebca276686ed0016fda4fafec290039", "p 
_buy": 0.49677207571944404, "p_sell": 0.503227924280556, "p_ski 
p": 0.49677207571944404, "meta": "SKIP"}, "cb": {"tripped": fal 
se, "reason": null, "consecutive_losses": 0, "threshold_losses" 
: 5, "reset_marker": null}, "features_hash": "f43d5483282d4351a 
91740f83541dbff9ebca276686ed0016fda4fafec290039", "model": "Lig 
htGBM_clf + calib[isotonic]", "exit_plan": {"mode": "none"}, "r 
untime": {"spread_pips": 0.49999999999954525, "spread_limit_pip 
s": 1.5, "min_adx": 15.0, "disable_adx_gate": false, "min_atr_p 
ct": 0.0006, "prob_threshold": 0.62, "side_bias": "auto", "max_ 
positions": 1, "ai_threshold": 0.62}}


作業7　結果
変更点

configs/config.yaml (line 8) に post_fill_grace_sec: 60 を追加し、filters (line 55) へ ATR ヒステリシス設定を追記。
core/utils/clock.py を新設し now_jst() を提供。
app/services/trade_service.py (line 96) で mark_filled_now / post_fill_grace_active を追加し、エントリー後の免責ウィンドウを管理。
app/services/execution_stub.py (line 17) で deque ベースの ATR ヒステリシス _atr_gate_ok を導入、フィルタ基準に免責やログ用メタ情報（post_fill_grace, atr_ref 等）を追加、ENTRY 計上時に mark_filled_now() を呼び出し。pos_guard ブロックでは即座に position_guard.on_order_rejected_or_canceled() で解除。
core/position_guard.py (line 61) に注文拒否/取消時のリリース API を追加。
検証

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "pos_guard"'（過去ログが1件のみ、新規再試行で連続ブロックは発生せず）
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"' で ATR 判定を確認（中央値が閾値未満のため引き続きブロック中）
補足

現状 ATR は 0.00044 前後で推移しており、ヒステリシスの有効閾値 (0.00055) を超えるまではゲートが閉じたままです。実トレード以降に mark_filled_now() が呼ばれると免責期間中はスプレッド/ADX/ATRの軽微なNGをスキップします。ENTRY 発生後の挙動を観察しつつ、必要なら enable_min_pct を再調整してください。

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 18:04:55.275 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 18:04:55.389 | WARNING  | app.services.execution_stub:_emit:250 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': 'ea09b090b9a024b0608b14034af07615bdb08f5183c4c9485f6d62c1cd4face7', 'p_buy': 0.49677207571944404, 'p_sell': 0.503227924280556, 'p_skip': 0.49677207571944404, 'meta': 'SKIP'}, 'filters': {'spread': 0.5, 'spread_limit': 1.5, 'adx': 23.78869, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00045428, 'min_atr_pct': 0.0006, 'prob_threshold': 0.62, 'side_bias': 'auto', 'atr_ref': 0.00045428, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'post_fill_grace': False, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "pos_guard"'

{"ts_jst": "2025-10-31T17:28:23+09:00", "type": "decision", "sy
mbol": "USDJPY-", "filters": {"spread": 0.1, "spread_limit": 1. 
5, "adx": 32.31987, "min_adx": 15.0, "adx_disabled": true, "atr 
_pct": 0.00049188, "min_atr_pct": 0.0004, "prob_threshold": 0.5 
, "side_bias": "auto", "blocked": "pos_guard"}, "probs": {"buy" 
: 0.496772, "sell": 0.503228, "skip": 0.496772}, "calibrator":  
"isotonic", "meta": "SELL", "threshold": 0.5, "decision": {"act 
ion": "BLOCKED", "reason": "pos_guard", "ai_meta": "SELL", "dec 
": {"decision": "ENTRY", "meta": "SELL", "side": "SELL", "thres 
hold": 0.5, "edge": 0.0}}, "ai": {"model_name": "LightGBM_clf + 
 calib[isotonic]", "version": "1761815421.0", "features_hash":  
"6acc2276334fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cb 
e0", "p_buy": 0.49677207571944404, "p_sell": 0.503227924280556, 
 "p_skip": 0.49677207571944404, "meta": "SELL"}, "cb": {"trippe 
d": false, "reason": null, "consecutive_losses": 0, "threshold_ 
losses": 5, "reset_marker": null}, "features_hash": "6acc227633 
4fcbb88085b0d42f7f043d5040c38034d99c3ea44a11f5d308cbe0", "model 
": "LightGBM_clf + calib[isotonic]", "runtime": {"spread_pips": 
 0.10000000000047748, "spread_limit_pips": 1.5, "min_adx": 15.0 
, "disable_adx_gate": true, "min_atr_pct": 0.0004, "prob_thresh 
old": 0.5, "side_bias": "auto", "max_positions": 1, "ai_thresho 
ld": 0.5}}


作業8　結果
configs/config.yaml (line 8) で prob_threshold を 0.56、min_edge を 0.01 に調整し、filters の min_atr_pct を 0.0005 へ緩和。session.allow_hours_jst を追加し、JST の稼働時間帯を明示できるようにしました。
core/utils/clock.py を新規作成して now_jst() を提供、app/services/trade_service.py (line 8) ではこれを利用して約定記録 (mark_filled_now) と免責判定 (post_fill_grace_active) を実装。
app/services/execution_stub.py に JST 時間帯フィルタ _session_hour_allowed とゾーン情報 import を追加。ヒステリシス情報や免責状態を含む base_filters を拡充し、セッション外では session_closed 理由で SKIP を記録。ENTRY 時には trade_service.mark_filled_now() を呼び、pos_guard ブロック発生時に position_guard.on_order_rejected_or_canceled(symbol) で即解除。
core/position_guard.py (line 72) に拒否/取消時のアンロック API を追加、runtime.post_fill_grace_sec を 60 秒とする設定を反映。
検証

.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.dryrun_smoke
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "session_closed"'（該当なし）
Get-Content logs\decisions\decisions_*.jsonl | Select-String '"blocked": "atr_low"'
サマリ: logs/decisions\decisions_USDJPY-.jsonl Counter({'BUY': 10, 'SELL': 9, 'SKIP': 8, 'BLOCKED': 3}), logs/decisions\decisions_USDJPY.jsonl Counter({'BLOCKED': 21, 'SKIP': 7, 'BUY': 7, 'SELL': 3, 'ENTRY': 2})
ENTRYは現状 ATR 閾値で抑制されていますが、段階的に prob_threshold を戻しつつ稼働時間帯フィルタで“死んだ時間”を避ける準備が整いました。

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 18:08:53.349 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 18:08:53.476 | WARNING  | app.services.execution_stub:_emit:278 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': 'b97fff3648654fffb2eb94398653d0ad8ae16cee673bc34b06c302ac054783a3', 'p_buy': 0.4785202863961814, 'p_sell': 0.5214797136038186, 'p_skip': 0.4785202863961814, 'meta': 'SKIP'}, 'filters': {'spread': 0.0, 'spread_limit': 1.5, 'adx': 22.83583, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.000447, 'min_atr_pct': 0.0005, 'prob_threshold': 0.56, 'side_bias': 'auto', 'atr_ref': 0.000447, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'post_fill_grace': False, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}} 

PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"reason": "session_closed"'




=== file: 作業9.txt ===

作業9　結果
configs/config.yaml (line 26) → しきい値を Step1 設定 (prob_threshold: 0.56, min_edge: 0.01) に変更し、exits.atr.trailing に発火/刻み/BEロック/ハードフロア/price_source 含む詳細ノブを追加。
app/services/trade_service.py (line 8) → datetime インポートを拡張し、build_exit_plan の ATR 戻り値に新 trailing ノブをそのまま反映。mark_filled_now/post_fill_grace_active を利用しつつ Optional[datetime] の型注釈を整備。
app/services/trailing.py（新規）→ ATR トレーリング用の TrailConfig/TrailState/AtrTrailer を実装。発火・刻み・BEロック・hard_floor・only_in_profit を一括管理。
app/services/execution_stub.py:18,86,212,352 付近 → session フィルタ導入、ATR ゲートのヒステリシス拡張、ENTRY 時の trail_state 登録/runtime_trail_states 保持、ティック毎に _update_trailing_state で TRAIL_UPDATE をログする仕組みを追加。価格算出ヘルパ (_tick_to_dict, _current_price_for_side) や pip/point 推定、トレーリング初期化/更新処理と trail_state スナップショット出力を組み込み。
scripts/sim_trailing.py（新規）→ CLI で AtrTrailer の挙動を可視化するシミュレータを用意。
app/services/execution_stub.py (line 422) など → runtime_payload に tick 情報を渡すよう変更し、scripts.dryrun_smoke からも同情報が on_tick に届くように調整。
確認済み
.venv\Scripts\python.exe -m compileall app core scripts
.venv\Scripts\python.exe -m scripts.sim_trailing --side BUY --entry 150.000 --atr 0.120 --activate 0.5 --step 0.25 --lockbe 0.3 --floor 5
（dryrun は低ATRのため atr_low で BLOCKED。ENTRY発生後に TRAIL_UPDATE を確認）

メモ

configs/config.yaml を python -c "import yaml, json; cfg=yaml.safe_load(open('configs/config.yaml')); print(json.dumps(cfg['exits']['atr']['trailing'], indent=2))" などで確認するとノブが反映されているのを確認できます。
ATR ゲートが開く相場（enable_min_pct ≈ 0.00055 以上）で ENTRY が出ると、logs/decisions/ に TRAIL_UPDATE と trail_state が記録されます。必要に応じ atr_hysteresis.enable_min_pct を一時的に緩和してください。
実売買へ持ち込む際は StopLevel/FreezeLevel と価格丸めを必ず考慮し、BUY=bid / SELL=ask などブローカー仕様に合わせて _current_price_for_side の選択を調整してください。

PS D:\macht\OneDrive\fxbot> python -m scripts.sim_trailing --side BUY --entry 150.000 --atr 0.120 --activate 0.5 --step 0.25 --lockbe 0.3 --floor 5
# side=BUY entry=150.0 atr=0.12
# price, activated, be_locked, layers, current_sl, new_sl       
150.00000, False, False, 0, None, None
150.01200, False, False, 0, None, None
150.02400, False, False, 0, None, None
150.03600, False, False, 0, None, None
150.04800, False, False, 0, None, None
150.06000, True, False, 0, 150.05, 150.05
150.07200, True, True, 0, 150.05, None
150.08400, True, True, 2, 150.05, None
150.09600, True, True, 3, 150.066, 150.066
150.10800, True, True, 3, 150.066, None
150.12000, True, True, 4, 150.09, 150.09
150.13200, True, True, 4, 150.09, None
150.14400, True, True, 4, 150.09, None
150.15600, True, True, 5, 150.126, 150.126
150.16800, True, True, 5, 150.126, None
150.18000, True, True, 6, 150.15, 150.15
150.19200, True, True, 6, 150.15, None
150.20400, True, True, 6, 150.15, None
150.21600, True, True, 7, 150.186, 150.186
150.22800, True, True, 7, 150.186, None
150.24000, True, True, 8, 150.21, 150.21
150.25200, True, True, 8, 150.21, None
150.26400, True, True, 8, 150.21, None
150.27600, True, True, 9, 150.246, 150.246
150.28800, True, True, 9, 150.246, None
150.30000, True, True, 10, 150.27, 150.27
150.31200, True, True, 10, 150.27, None
150.32400, True, True, 10, 150.27, None
150.33600, True, True, 11, 150.306, 150.306
150.34800, True, True, 11, 150.306, None
150.36000, True, True, 12, 150.33, 150.33

PS D:\macht\OneDrive\fxbot> python -m scripts.dryrun_smoke
2025-10-31 19:01:13.499 | INFO     | core.ai.loader:load_lgb_clf:45 - Loaded model: models\LightGBM_clf.pkl
2025-10-31 19:01:14.273 | WARNING  | app.services.execution_stub:_emit:458 - {'mode': 'dryrun', 'symbol': 'USDJPY-', 'decision': 'BLOCKED', 'reason': 'atr_low', 'ai': {'model_name': 'LightGBM_clf + calib[isotonic]', 'version': '1761815421.0', 'features_hash': '06df81b926ebe26648452e8e57552b98b1161017190bbdc97c388801a32338cc', 'p_buy': 0.4785202863961814, 'p_sell': 0.5214797136038186, 'p_skip': 0.4785202863961814, 'meta': 'SKIP'}, 'filters': {'spread': 0.3, 'spread_limit': 1.5, 'adx': 22.33428, 'min_adx': 15.0, 'adx_disabled': False, 'atr_pct': 0.00039826, 'min_atr_pct': 0.0005, 'prob_threshold': 0.56, 'side_bias': 'auto', 'atr_ref': 0.00039826, 'atr_gate_state': 'closed', 'atr_enable_min': 0.00055, 'atr_disable_min': 0.00045, 'post_fill_grace': False, 'blocked': 'atr_low'}, 'cb': {'tripped': False, 'reason': None, 'consecutive_losses': 0, 'threshold_losses': 5, 'reset_marker': None}}
PS D:\macht\OneDrive\fxbot> Get-Content logs\decisions\decisions_*.jsonl | Select-String '"TRAIL_UPDATE"|trail_new_sl|trail_state'




=== file: 質問.txt ===

PS C:\Users\macht\OneDrive\fxbot> python -m tools.backtest_run `
>>   --csv data\USDJPY\ohlcv\USDJPY_M5.csv `
>>   --start 2024-01-01 --end 2025-11-09 `
>>   --capital 100000 `
>>   --mode wfo `
>>   --symbol USDJPY `
>>   --timeframe M5 `
>>   --layout per-symbol `
>>   --train-ratio 0.7
[main] mode=wfo csv=C:\Users\macht\OneDrive\fxbot\data\USDJPY\ohlcv\USDJPY_M5.csv
[wfo] start
[wfo] read_csv C:\Users\macht\OneDrive\fxbot\data\USDJPY\ohlcv\USDJPY_M5.csv
[wfo] slice 2024-01-01 .. 2025-11-09
[wfo] equity compute train
[wfo] AI model not used (C:\Users\macht\OneDrive\fxbot\active_model.json not found.) -> fallback to buy&hold
[wfo] equity compute test
[wfo] AI model not used (C:\Users\macht\OneDrive\fxbot\active_model.json not found.) -> fallback to buy&hold
[wfo] done
C:\Users\macht\OneDrive\fxbot\logs\backtest\USDJPY\M5\equity_curve.csv
